id,title,text
6fac86cbd1,You're Using Neural Networks Every Day Online—Here's How They Work,"If you use Google’s new Photos app, Microsoft’s Cortana, or Skype’s new translation function, you’re using a form of AI on a daily basis. AI was first dreamed up in the 1950s, but has only recently become a practical reality — all thanks to software systems called neural networks. This is how they work. Plenty of things that humans find difficult can be done in a snap by a computer. Want to solve a partial differential equation? No problem. How about creating accurate weather forecasts or scouring the internet for a single web page? Piece of cake. But ask a computer to tell you the differences between porn and renaissance art? Or whether you just said “night” or “knight”? Good luck with that. Computers just can’t reason in the same way humans do. They struggle to interpret the context of real-world situations or make the nuanced decisions that are vital to truly understanding the human world. That’s why neural networks were first developed way back in the 1950s as potential solution to that problem.
Taking inspiration from the human brain, neural networks are software systems that can train themselves to make sense of the human world. They use different layers of mathematical processing to make ever more sense of the information they’re fed, from human speech to a digital image. Essentially, they learn and change over time. That’s why they provide computers with a more intelligent and nuanced understanding of what confronts them. But it’s taken a long time to make that happen. Back in the 1950s, researchers didn’t know how the human brain was intelligent—we still don’t, not exactly—but they did know that it was smart. So, they asked themselves how the human brain works, in the physical sense, and whether it could be mimicked to create an artificial version of that intelligence. Advertisement
The brain is made up of billions of neurons, long thin cells that link to each other in a network, transmitting information using low-powered electrical charges. Somehow, out of that seemingly straightforward biological system, emerges something much more profound: the kind of mind that can recognize faces, develop philosophical treatises, puzzle through particle physics, and so much more. If engineers could recreate this biological system electronically, engineers figured, an artificial intelligence might emerge too. There were some successful early examples of artificial neural networks, such as Frank Rosenblatt’s Perceptron which used analog electrical components to create a binary classifier. That’s fancy talk for a system that can take an input—say, a picture of a shape—and classify it into one of two categories like “square” or “not-square.” But researchers soon ran into barriers. First, computers at the time didn’t have enough processing power to effectively handle lots of these kinds of decisions. Second, the limited number of synthetic neurons also limited the complexity of the operations that a network could achieve. In the case of the Rosenblatt’s Perceptron, for instance, a single set of artificial neurons was able to discern a square from non-squares. But if you wanted to introduce the ability to perceive something else about the squares—whether they were red or not red for example—you’d need a whole extra set. Advertisement
While the biology of the brain may be straightforward at the microscopic level, taken as a whole it is incredibly complex. And that macro-level complexity was too much for 1950s computers to handle. As a result, over the following decades neural networks fell from favor. It became the “winter of neural networks,” as Google’s Jason Freidenfelds put it to me. But one person’s winter is another’s summer. From the 1960s onwards, our understanding of the human brain progressed by leaps and bounds. Advertisement
In those early days of neuroscience, much of the focus was on our visual systems. Professor Charles Cadieu from MIT a Research Affiliate at MIT and co-founder and CEO of a startup currently in stealth mode, explains: It’s probably the best understood sensory modality, and probably the best understood part of the brain. We’ve known for decades now that neurons fire differently as you pass up the visual stream. In the retina, neurons are receptive to points of light and darkness; in the primary visual cortex there’s excitement of neurons by edge-like shapes; and in the higher areas of the visual cortex neurons respond to faces, hands... all sort of complex objects, both natural and man-made. In fact, up there, the neurons don’t respond to light and dark patches or edge-like features at all. Image by CLIPAREA l Custom media/Shutterstock It turns out that different parts of the brain’s biological network are responsible for different aspects of what we know as visual recognition. And these parts are arranged hierarchically. Advertisement
This is true for other aspects of cognition too. Parts of the brain that process speech and perhaps even language itself appear to work in the same way. A hierarchy of different neuronal levels each provides their own insight, then passes it on to another, more senior level to make a higher level judgement. At each stage, the reasoning becomes more abstract, allowing a string of sounds to be recognized as a word that means something to us, or a cluster of bright and dark patterns on our retina to be rendered as “cat” in our brain. These kinds of hierarchies were a crucial clue for researchers who still dared to think about artificial neural networks. “That’s really been a guiding light for neural networks,” explains Cadieu. “We just didn’t know how to make them behave that way.” In truth, the artificial networks in use today aren’t really modeled on the brain in the way that pioneers in the field may have expected. They are “only loosely inspired by the brain, “ muses Cadieu, in the sense that they’re really software systems that employ a layered approach to developing understanding, rather than being a network of nodes passing information back and forth between each other. Advertisement
These software systems use one algorithm to process insight about an input, then pass it on to the next layer to process using a different algorithm to gain some higher-level understanding, and so on. In other words, it makes more sense to think of artificial neural networks as cascaded mathematical equations that can spot distinctive features and patterns, according to Freidenfelds. In the case of image recognition, for instance, the first layer of a neural network may analyze pixel brightness, before passing it to a second to identify edges and lines formed by strips of similar pixels. The next layers may be able to identify shapes and textures, then further up the chain they may identify clustering of some of these abstract image features into actual physical features, such as eyes or wheels. Then towards the very end, clustering of the these higher-levels features may be interpreted as actual objects: two eyes, a nose and a mouth may form a human face, say, while wheels, a seat and some handlebars resemble a bike. At the I/O developer conference in May, Google announced that the neural networks that power its products like Google Photos now use 30 different layers in total to make sense of images. Image by fdecomite under Creative Commons license. Neural networks aren’t just restricted to image recognition, though that is our most advanced use of it currently. In the case of something like speech recognition, the neural network chops up the speech it’s hearing into short segments, then identifies vowel sounds. The subsequent layers can work out how the different vowel sounds fit together to form words, words fit together to form phrases, and finally infer meaning from what you just mumbled into your telephone. Advertisement
As you can probably tell, this is a major step up from the single-layer Perceptron system—a real big step. In fact, all this time there were two things holdings back the successful implementation of neural networks. The first was computational power, which we now have. The second? Enough information to teach the things how to work properly. Neural networks can’t learn until you throw throw enough data at them. They need large quantities of information to consider, pass through their layers, and attempt to classify. Then, they can compare their classifications to the real answers, and either pat themselves on the back or try a little harder.
Advertisement
In the case of image analysis, that means supplying your neural network with a slew of tagged photos known as a training set. Google has used YouTube to supply this training set in the past. In the case of speech recognition, the training set might be a series of audio clips along with a description of what’s being said. Provided with this huge chunk of inputs, the neural network will make an attempt at classifying each item, piecing together what information it can from its various layers to make guesses about what it’s seeing or hearing. During training, the machine’s answer is compared to the human-created description of what should have been observed. If it’s right, props to the networks.
But what if it’s wrong?
“If what it just analyzed was a face and it said house, then it uses a techniques called back-propagation to adjust itself,” explains Cadieu. “It steps back through each of the previous layers, and each time it tweaks the mathematical expression for that layer, just enough so that it would get the answer right next time.”
Advertisement
At first the network will make mistakes all the time, but gradually its performance improves, the layers gradually becoming more and more able to discern exactly what they see. This iterative process of passing a sample through the network and back prorogation to correct itself is known as deep learning, and it’s what imbues the networks with human-like intelligence. In the past, it’s been tough to amass enough information to feed something as hungry for data as a neural network. But these days there’s so much data floating around online that it’s relatively straightforward. There are images posted all over the internet with descriptions that describe exactly—or at least, exactly enough—what the computer should see, or dialogue from movies and corresponding scripts to help speech recognition systems learn how humans sound. And armed with all that data, neural networks can grow ever more intelligent.
And it’s working. When Google released its first neural network-powered voice recognition system in 2011 —the one baked into the likes of Now and Chrome—it had an error rate of around 25%. That means that it messed up one in four times that you used it.
Advertisement
Now, after three years of tweaks and learning, that’s fallen to just 8 percent. Google’s latest attempt at showing off the power of neural networks is with Photos, which is almost uncanny in its intelligence. When Gizmodo’s Mario Aguilar first tried it, he explained: It’s crazy how well this works. Creepy even. We were amazed first, then suspicious. We were only working with a small smattering of the photos I currently have on this phone... [but] despite the limited pool, Google Photos was actually able to make quite a bit of sense out of what I shot. It correctly identified one of my best friends who I have many photos of as a unique person... But where Photos really wrecks your brain is when you start searching for random things in your collection. Beers? It finds photos of beers. Bars? It finds photos of bars. It’s not just Google, though. Last year Facebook unveiled its DeepFace algorithm, which can recognize specific human faces with 97% accuracy. That’s about as good as human. Wolfram Alpha has created a software system that can identify objects and even allows you to roll it in to your own software. And Microsoft’s Cortana digital personal assistant is so sharp that it can recognize the difference between a picture of a Pembroke Welsh Corgi and a Cardigan Welsh Corgi. Can you recognize the differences between a Pembroke Welsh Corgi and a Cardigan Welsh Corgi? I sure as hell can’t. Image by Virginia Hill and Tundra Ice. The Cardigan Welsh Corgi is on the left. And it’s not all image recognition. Skype now uses neural networks to translate from one language to another on the fly; Chinese search firm Baidu uses them to target ads on its search engine; and just recently Google unveiled a chatbot system that had been trained using them.
Advertisement
Neural networks are finally giving computers the ability to understand the human world, and make smart inferences about it. Hell, they drip with so much knowledge and experience these days that they can dream and create psychedelic art in the process. But like anything “smart,” neural networks can and do go wrong. Just this month, Google’s Photos app mistakenly tagged two black people in a photograph as “gorillas.” Flickr’s smart new image recognition tool, powered by Yahoo’s neural network, also tagged a black man as an “ape.” Clearly, neither are perfect, and may reflect the (racist) ways that humans have tagged images in public data. Google’s quick fix, it seems, was to simply remove the “gorilla” tag from Photos altogether; in the longer term, it will no doubt train its networks harder. Advertisement
It’s easy enough to point a finger at the technology for just not being up to scratch. Certainly, it’s possible to fool a neural network. The images below, for instance, are enough to trick neural networks into thinking that they see penguins, baseballs and remote controls, but to humans they just look like abstract patterns, albeit ones inspired by those real objects. “The algorithm’s confusion is due to differences in how it sees the world compared with humans,” said Jeff Clune from the University of Wyoming in Laramie, who discovered this quirk, to New Scientist. “While [humans] identify a cheetah by looking for the whole package—the right body shape, patterning and so on—a [neural network] is only interested in the parts of an object that most distinguish it from others.” Advertisement
It doesn’t help, of course, that neural networks don’t have the in-built sense of decorum and caution that humans have. Like a child blurting out whatever springs into its mind, neural networks eagerly provide their best guess in the hope that it’s correct—even if they’re not 100% certain. In those cases, a human might demur; perhaps a neural network should learn to do the same. Gorillagate may be just as much to do with human bias as dubious technology. Speaking to the Wall Street Journal, artificial intelligence expert Vivienne Ming pointed out that the training sets used by Google are perhaps to blame. The reams of photographs on the internet display an overwhelmingly white world, so it’s plausible that the neural networks simply don’t have the experience they need to identify black people accurately. Other than feeding neural networks accurate data, researchers are also improving the software by combining networks together. A recent research collaboration between Google and Stanford University has begun producing software that can actually describe whole scenes instead of just identifying one object in it, like a cat. Advertisement
This frankly remarkable feat is achieved by combining an image-recognition neural network with a natural-language network. Give a natural language network a full sentence such as “We sat down to a delightful lunch of hoagies and BLTs,” and it abstracts it into high-level concepts like “eating” and “sandwiches.” From there, the output can be fed into another natural language processing network that can make sense of these concepts and translate it into a sentence that says much the same thing, using different wording.
This is how the neural networks used by Skype work for their on-the-fly translation. What the researchers from Stanford and Google have done, though, is replace the first neural network with an image-recognition network and fed that into an English natural language network. The first produces high-level concepts of what’s depicted in the image—say, a man, a sandwich and eating. The second the attempts to convert those concepts into a sentence describing what’s shown, like “A man is eating a sandwich.” As you can see in the chart below, the results aren’t perfect—but they’re certainty pretty impressive. So if neural networks are developing so rapidly, is the sky the limit?
“You can certainly expect to see major improvements in image and speech recognition in the coming years,” says Cadieu, pointing out that these modern neural networks have only really been around for a couple of years anyway.
Advertisement
As for language processing, it’s less clear that neural networks will be able to deal with the problems so well. While image and speech recognition definitely work in the layered way that modern neural networks do, there’s less neuroscientifc evidence to suggest that language is processed in the same way, according to Cadieu. That may mean that artificial language processing will soon run into conceptual barriers. One thing is clear, though: These kinds of artificial intelligences are already lending a huge helping hand to humans. In the past, you had to sift through your photographs to compile an album from your latest vacation or find that pic of your buddy Bob drinking a beer. But today, neural network software can do that for you. Google Photos prepares albums automatically, and its smart search function will find images with alarming accuracy.
Advertisement
And this kind of consumer-focused software is a mere gimmick compared to the feats that neural networks could one day perform for us. It’s not hard to imagine image-processing algorithms gaining enough intelligence to vet medical images for tumors, with doctors merely checking their result. Voice recognition systems could become so advanced that telemarketing campaigns will be run by software alone. Language processing networks will allow news stories to be written by machine.
In fact, all these things are already happening, to some extent. The changes are profound enough that researchers at the University of Oxford estimate that up to a half of jobs, including the one possessed by yours truly, will be lost to AI systems powered by neural networks in coming years. Advertisement
But shifts in economies and employment have been driven by technology many times before, from the printing press and motor car, to computers and the internet. Though there will be social upheaval, there will also be benefits. Ultimately, neural networks will give everyone access to intelligence that currently lies in the hands of a few. And that will lead to smarter systems, better services, and more time to solve the human problems that computers will never be able to fix. Correction: This article orginally stated that Charles Cadieu was based at MIT. He ins in fact a Research Affiliate at MIT and co-founder and CEO of a startup currently in stealth mode. Top image by pogonici/Shutterstock"
5a4e6c2642,[1610.02273] Near-Data Processing for Differentiable Machine Learning Models,"Link back to: arXiv, form interface, contact."
caa8c601b3,"Leaf Classification Competition: 1st Place Winner’s Interview, Ivan Sosnovik | No Free Hunch","Kaggle Team|03.24.2017 Can you see the random forest for its leaves? The Leaf Classification playground competition ran on Kaggle from August 2016 to February 2017. Kagglers were challenged to correctly identify 99 classes of leaves based on images and pre-extracted features. In this winner's interview, Kaggler Ivan Sosnovik shares his first place approach. He explains how he had better luck using logistic regression and random forest algorithms over XGBoost or convolutional neural networks in this feature engineering competition.
I am an MSc student in Data Analysis at Skoltech, Moscow. I joined Kaggle about a year ago when I attended my first ML course at university. The first competition was What’s Cooking. Since that, I’ve participated in several Kaggle competitions, but didn’t pay so much attention to it. It was more like a bit of practice to understand how ML approaches work. Ivan Sosnovik on Kaggle. The idea of Leaf Classification was very simple and challenging. Seemed like I wouldn’t have to stack so many models and the solution could be elegant. Moreover, the total volume of data was just 100+ Mb and the process of learning could be performed even with a laptop. It was very promising because the majority of the computations was supposed to be done on my MacBook Air with 1,3 GHz Intel Core i5 and 4 Gb RAM. I have worked with black-and-white images before. And there is a forest near my house. However, it didn’t give me so much profit in this competition. When I joined the competition, several kernels with top 20% scores were published. The solutions used the initially extracted features and Logistic Regression. It gave . And no significant improvement could be achieved by tuning of the parameters. In order to enhance the quality, feature engineering had to be performed. Seemed like no one had done it because the top solution had slightly better score than mine. I did first things first and plotted the images for each of the classes. 10 images from the train set for each of 7 randomly chosen classes. The raw images had different resolution, rotation, aspect ratio, width, and height. However, the variation of each of the parameters within the class is less than between the classes. Therefore, some informative features could be constructed just on the fly. They are: Another very useful feature that seemed to help is the average value of the pixels of the image.
I added these features to the already extracted ones. Logistic regression enhanced the result. However, most of the work was yet to be done. All of the above-described features represent nothing about the content of the image.
Despite the success of neural networks as feature extractors, I still like PCA. It is simple and allows one to get the useful representation of the image in . First of all, the images were rescaled to the size of . Then PCA was applied. The components were added to the set of previously extracted features.
Eigenvalues of the covariance matrix. The number of components was varied. Finally, I used
principle components. This approach showed . In order to generate even more features, I used OpenCV. There is a great tutorial on how to get the moments and hull of the image. I also added some pairwise multiplication of several features. The final set of features is the following: The Logistic Regression demonstrated .
All of the above-described demonstrated good result. Such result would be appropriate for real life application. However, it could be enhanced.
The majority of objects had certain decision: there was the only class with
and the rest had . However,
I found several objects with uncertainty in a prediction like this: Prediction of logistic regression. The set of confusion classes was small (15 classes divided into several subgroups), so I decided to look at the pictures of the leaves and check if I can classify them. Here is the result: Quercus confusion group. Eucalyptus and Cornus confusion group. I must admit that Quercus’ (Oak) leaves look almost the same for different subspecies. I assume, that I could distinguish Eucalyptus from Cornus, but the classification of subspecies seems complicated to me.
The key idea of my solution was to create another classifier, which will make predictions only for confusion classes. The first one I tried was RandomForestClassifier from sklearn and it gave excellent result after the tuning of hyperparameters. The random forest was trained on the same data as logistic regression, but only the objects from confusion classes were used. If logistic regression gave uncertain predictions for an object then the prediction of the random forest classifier was used. Random forest gave the probabilities for 15 classes, the rest assumed to be absolute . The final pipeline is the following: Final pipeline. The leaderboard score was calculated on the whole dataset. That is why some risky approaches could be used in this competition.
Submissions are evaluated using the multi-class logloss.
where
- number of objects and classes respectively,
is the prediction and
is the indicator:
if object
is in class , otherwise it equals to . If the model correctly chose the class, then the following approach will decrease the overall logloss, otherwise, it will increase dramatically.
After thresholding, I got the score of . That’s it. All the labels are correct. I’ve tried several methods that showed appropriate result but was not used in the final pipeline. Moreover, I had some ideas on how to make the solution more elegant. In this section, I’ll try to discuss them. XGBoost by dmlc is a great tool. I’ve used it in several competitions before and decided to train it on the initially extracted features. It demonstrated the same score as logistic regression or even worse, but the time consumption was a way bigger. Before I came up with an idea of Random Forest as the second classifier, I tried different one-model methods. Therefore I collected lots of submissions. The trivial idea is to blend the submissions: to use the mean of the predictions or weighted mean. The result did not impress me either.
Neural networks were one of the first ideas I tried to implement. Convolutional Neural Networks are good feature extractors, therefore, they could be used as a first-level model or even as a main classifier. The original images came with different resolution. I rescaled them to . The training of CNN on my laptop was too time-consuming to choose the right architecture in reasonable time, so I declined this idea after several hours of training. I believe, that CNNs could give accurate predictions for this dataset.
I am Ivan Sosnovik. I am a second-year master student at Skoltech and MIPT. Deep learning and applied mathematics are of great interest to me. You can visit my GitHub to check some stunning projects. Very interesting idea for using Random forest to give the probabilities for 15 classes which are hard for logistic regression. Congratulations Ivan..!!
Thanks for sharing the approach. got some good ideas on feature engineering for images and multiclass classification. Hey,
Thanks for sharing your solution ! I have some questions (I'll do my best I'm french) :
- "" If logistic regression gave uncertain predictions"" means that you use a threshold, like if the best prediction is under 0.95 we use random forest ?
-
"" The random forest was trained on the same data as logistic regression, but only the objects from confusion classes were used "" means that you train the random forest only with the images having confusion classes ?
best,
Julien Hi Julien
- For each prediction we check the number of classes with probability > 0.05. If it is greater than 1, random forest is used. It is close to your suggestion.
- Yes. Thank you!;) Yes I was close to your answer, I'm working on a kaggle for classifying fishes, https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/submissions Good luck with this competition! I have a question, why not to use random forest for the entire data set? Hello Stefanie
Good question. Actually, it was one of the first approaches I've tried. However, it didn't demonstrate good result after some tuning.
On each iteration, tree-methods divide the space into 2 half-spaces by the hyperplane in order to separate the objects of different classes. And seems like it is quite difficult to perform it for 99 classes. However, it could be divided in a very accurate way for the less number of classes. Thank you very much for your nice explanation. This makes sense! I have to keep that in mind, I am new to ML and up to know worked only with binary classification. Well done, nice solution without using NNs! ""simplicity is the ultimate form of sophistication"". thats a great approach to the problem at hand. great article."
e9f4c4bb68,How AI Can Predict Heart Failure Before it's Diagnosed | NVIDIA Blog,"The last place you want to learn you have heart failure is where it often winds up being diagnosed: in the emergency room.
Researchers analyzing electronic health records are using  artificial intelligence and GPUs to get ahead of this curve. They’ve shown they can predict heart failure as much as nine months before doctors can now deliver the diagnosis. A research team from Sutter Health, a Northern California not-for-profit health system, and the Georgia Institute of Technology, believe their method has the potential to reduce heart failure rates and possibly save lives. “The earlier we can detect the disease, the more likely we can change health outcomes for people and improve their quality of life,” said Andy Schuetz, a senior data scientist at Sutter Health and an author of a paper describing one aspect of the research. “That’s what’s exciting to me – the potential to change the future.” A paper on the complete research process and findings is being considered for publication by a major medical journal.
The stakes could hardly be higher. Each year, about 23 million people worldwide, including nearly 6 million Americans, have heart failure, according to the American Heart Association. It occurs when the heart muscle is weakened and can’t pump enough blood and oxygen to meet the body’s needs. Half of those diagnosed die within five years. If doctors know which patients are likely to develop heart failure, they can prescribe medication or recommend lifestyle and diet changes that could delay its onset or even prevent it entirely, said Schuetz. AI and NVIDIA are playing a growing role in advancing healthcare around the country. NVIDIA announced last week that it’s working with Massachusetts General Hospital to apply the latest AI techniques to improve disease detection, diagnosis, treatment and management. The team analyzed electronic health records from more than 265,000 Sutter Health patients. From these, it studied 3,884 patients with heart failure and about 28,900 patients as a control group. Researchers analyzed the records using deep learning, a type of artificial intelligence that can solve complex technical problems like face or speech recognition — sometimes even topping human performance. This problem required computers to understand many types of health records — prescriptions or summaries of visits to doctors, for example — in many formats. Another challenge was tracking changes in the records over time to provide a full picture of patient health. The problem of predicting heart failure onset isn’t new, said Jimeng Sun, an associate professor at Georgia Tech and an author of the paper. What’s different is deep learning and GPUs. Unlike traditional machine learning, deep learning does not require a human expert to define every factor the computer should evaluate in the data — a time-intensive process. In earlier research, Sun said he and others spent a couple of years working with experts to build machine learning models. Then Georgia Tech doctoral candidate Edward Choi, another author of the paper, spent a summer at Sutter Health and applied deep learning to the problem. “In three months, he was able to outperform what we’d done,” Sun said. GPUs provided the speed required to train the neural networks — where the learning takes place in deep learning — on the hundreds of thousands of Sutter Health records, said Choi. “Without widely available GPUs, this work would not have been feasible,” Schuetz added. The team used Theano, CUDA 7 and a Tesla K80 GPU in their research. Although this work focused on heart failure risk, the researchers say their approach could be used to predict any kind of disease. Their next target may be detecting sepsis in emergency room patients, Schuetz said. Sepsis is a life-threatening response to an infection and the ninth-leading cause of death in the U.S. But it’s maddeningly difficult to detect. By the time doctors realize a patient has sepsis and start treatment, it may be too late. Researchers are also working on a platform that will let them put the prediction tools into the hands of doctors, Schuetz said.
Friend's Email Address
Your Name
Your Email Address
Comments
Send Email"
e71b320d50,Even a mask won't hide you from the latest face recognition tech | New Scientist,"Daily news
7 September 2017
John Powell/REX/Shutterstock By Matt Reynolds Ditch the hat and scarf – it’s not fooling anyone. Face recognition software can now see through your cunning disguise – even you are wearing a mask. Amarjot Singh at the University of Cambridge and his colleagues trained a machine learning algorithm to locate 14 key facial points. These are the points the human brain pays most attention to when we look at someone’s face. The researchers then hand-labelled 2000 photos of people wearing hats, glasses, scarves and fake beards to indicate the location of those same key points, even if they couldn’t be seen. The algorithm looked at a subset of these images to learn how the disguised faces corresponded with the undisguised faces.
The system accurately identified people a wearing scarf 77 per cent of the time – a cap and scarf 69 per cent of the time and a cap, scarf and glasses 55 per cent of the time. This isn’t as good as systems that recognise undisguised human faces, but it is the best at seeing through disguises, says Singh. The system only needs to be able to see a fraction of facial key points – most of which are around the eyes and mouth – to be able to guess where the other points are likely to be. Based on that guess, it can identify the person if it has already been shown a map of their key points. “In effect, it is able to see through your mask,” says Singh. You can also probably say goodbye to CV Dazzle, the vaunted face recognition camouflage makeup that has been mooted as the way to stay anonymous in a world of face recognition. “This will work very well for this type of camouflage because it works on key points of the face,” he says. He will present his findings at the International Conference on Computer Vision in Italy in late October. Singh has plans to take this research even further and see if it’s possible to design an algorithm that can identify someone wearing a rigid plastic mask, like the V for Vendetta masks that are popular at some protests. The system could be used to identify criminals who are trying to hide their identities, says Singh. But he admits it could also be used by authoritarian governments to identify protesters. “It kind of impinges on the privacy of people,” he says. Automatic face recognition software is catching on with law enforcement across the world. In August, the UK government said it planned to spend £4.6 million on upgrading face recognition software so algorithms could automatically spot suspects from live video footage. “There’s always a trade-off between security and privacy,” says Anil Jain at Michigan State University. But he says that people in public spaces are already under constant surveillance by security cameras, so they shouldn’t be too worried about every improvement in the technology. For now, the system is far from perfect. The fewer facial key points it can see, the worse the software is at recognising a person in a photo. It’s also thrown off by busy backgrounds, so can only identify a person wearing a cap, glasses and scarf 43 per cent of the time if they’re standing in front of a complicated background. It’s also not clear how well this system would work in the real world, Jain says. The algorithm was only trained on photos of 25 people, which he says isn’t enough to really determine its efficacy. And anti-surveillance tricks are keeping pace with improvements. Last year, a team of researchers from Carnegie Mellon University found they could trick face recognition software by wearing specially designed glasses. Now might be the time to trade in your fake beard for a pair of jazzy specs. Reference: https://arxiv.org/pdf/1708.09317.pdf     More on these topics:
A shorter version of this article was published in New Scientist magazine on 16 September 2017"
a412df64a4,Improving clinical trials with machine learning,"UCL News
15 November 2017
Machine learning could improve our ability to determine whether a new drug works in the brain, potentially enabling researchers to detect drug effects that would be missed entirely by conventional statistical tests, finds a new UCL study published today in Brain. “Current statistical models are too simple. They fail to capture complex biological variations across people, discarding them as mere noise. We suspected this could partly explain why so many drug trials work in simple animals but fail in the complex brains of humans. If so, machine learning capable of modelling the human brain in its full complexity may uncover treatment effects that would otherwise be missed,” said the study’s lead author, Dr Parashkev Nachev (UCL Institute of Neurology). To test the concept, the research team looked at large-scale data from patients with stroke, extracting the complex anatomical pattern of brain damage caused by the stroke in each patient, creating in the process the largest collection of anatomically registered images of stroke ever assembled. As an index of the impact of stroke, they used gaze direction, objectively measured from the eyes as seen on head CT scans upon hospital admission, and from MRI scans typically done 1-3 days later. They then simulated a large-scale meta-analysis of a set of hypothetical drugs, to see if treatment effects of different magnitudes that would have been missed by conventional statistical analysis could be identified with machine learning. For example, given a drug treatment that shrinks a brain lesion by 70%, they tested for a significant effect using conventional (low-dimensional) statistical tests as well as by using high-dimensional machine learning methods. The machine learning technique took into account the presence or absence of damage across the entire brain, treating the stroke as a complex “fingerprint”, described by a multitude of variables.  “Stroke trials tend to use relatively few, crude variables, such as the size of the lesion, ignoring whether the lesion is centred on a critical area or at the edge of it. Our algorithm learned the entire pattern of damage across the brain instead, employing thousands of variables at high anatomical resolution. By illuminating the complex relationship between anatomy and clinical outcome, it enabled us to detect therapeutic effects with far greater sensitivity than conventional techniques,” explained the study’s first author, Tianbo Xu (UCL Institute of Neurology). The advantage of the machine learning approach was particularly strong when looking at interventions that reduce the volume of the lesion itself. With conventional low-dimensional models, the intervention would need to shrink the lesion by 78.4% of its volume for the effect to be detected in a trial more often than not, while the high-dimensional model would more than likely detect an effect when the lesion was shrunk by only 55%. “Conventional statistical models will miss an effect even if the drug typically reduces the size of the lesion by half, or more, simply because the complexity of the brain’s functional anatomy—when left unaccounted for—introduces so much individual variability in measured clinical outcomes. Yet saving 50% of the affected brain area is meaningful even if it doesn’t have a clear impact on behaviour. There’s no such thing as redundant brain,” said Dr Nachev. The researchers say their findings demonstrate that machine learning could be invaluable to medical science, especially when the system under study—such as the brain—is highly complex. “The real value of machine learning lies not so much in automating things we find easy to do naturally, but formalising very complex decisions. Machine learning can combine the intuitive flexibility of a clinician with the formality of the statistics that drive evidence-based medicine. Models that pull together 1000s of variables can still be rigorous and mathematically sound. We can now capture the complex relationship between anatomy and outcome with high precision,” said Dr Nachev. “We hope that researchers and clinicians begin using our methods the next time they need to run a clinical trial,” said co-author Professor Geraint Rees (Dean, UCL Faculty of Life Sciences). The study was funded by Wellcome and the National Institute for Health Research University College London Hospitals Biomedical Research Centre. Tel: +44 (0)20 7679 9222 Email: chris.lane [at] ucl.ac.uk"
95d2d50a11,Mckinsey used machine learning to discover the best way to teach science — Quartz,"There is a long-standing, red-hot debate in educational circles about the most effective way to teach kids. Some favor more traditional teacher-directed methods, with the teacher presenting materials and responding to questions about it. Others advocate for inquiry-based learning—where students drive their own learning (pdf) through discovery and exploration, working with peers and developing their own ideas—arguing it results in deeper, and more meaningful learning. The two are sometimes pitted against each other as “sage on a stage” (teacher directed) vs. “guide on the side” (student-led, or inquiry based). Both cite ample evidence to prove the superiority of their method (see here for teacher-directed, and here for inquiry-based). McKinsey applied machine learning to the world’s largest student database to try and come up with a more scientific answer. The bottom line: A mixture of the two methods is best, but between the two, teacher-directed came out stronger. In all five regions of the world, scores were generally higher when teachers took the lead. “The more frequently teacher-directed happens, the better students do,” said Marc Krawitz,
an associate partner at McKinsey. Conversely, “Student outcomes tend to decline with inquiry-based, as it is increased in isolation.” The data come from the Organisation for Economic Co-operation and Development, which tests 15-year-olds around the world on mathematics, reading, and science every three years (the Programme for International Student Assessment, or PISA). McKinsey used the 2015 test, which focused on science and covered more than half a million students across 72 countries. The greatest learning gains—+26 points on the PISA test—happened when “many-to-all” of the lessons were teacher-directed and “some to many” were inquiry-based. In other words, a healthy combination was best. But the bottom left quadrant shows that inquiry-based without a lot of teacher direction is definitively not helpful, at least as far as the PISA test goes, whereas students in all-teacher-led systems still scored moderately higher than the baseline. The authors offers two potential explanations for why teacher-directed produces better scores. First, “students cannot progress to inquiry-based methods without a strong foundation of knowledge, gained through teacher-directed learning,” the report says. Second, inquiry-based teaching is harder to do, and teachers who try it without adequate training and support will struggle. McKinsey’s report won’t settle the debate. A PISA score is not a perfect measure of a good education. The test is only administered in 72 countries, and plenty of people argue PISA itself is flawed. And there’s an important caveat: The report notes that inquiry-based teaching increases students’ joy in science significantly more than teacher-directed learning does. (Teacher-directed is also positively correlated with joy, though the impact is less.) Inquiry-based also helps convince students that science is worthwhile for their future careers. Since passion often results in perseverance, which can lead to better student (and life) outcomes, joy matters. Also, joy just matters. “Inquiry-based practices have a stronger positive affect on students’ joy in science and their beliefs that doing well in school will help them have a brighter future,” said Mona Mourshed, lead author of the report and global head of the education practice at McKinsey. “That’s why, across all regions, blending teacher-directed instruction with inquiry-based learning produces the greatest overall benefit.” Good teachers of course draw on multiple methods, depending on the subject and the learner. And good teachers also know that most things in life—from the value of standardized tests, to ideological debates about teaching—are not black and white."
459c877dc2,Ad-Tech Company AppNexus Just Launched a Machine Learning-Enabled Ad-Buying Tool – Adweek,"Share AppNexus is adding machine learning to its ad-buying software, making the company the newest in a series of ad-tech firms to add artificial intelligence to its platform. Today, the company is debuting what it says is the first programmable demand side platform, which could help media buyers manage and customize their campaigns. The AppNexus Programmable Platform (APP)—which launches after three years of research involving more than 200 buy-side engineers, data scientists and developers—will be ready for 2018 campaigns. So far, those that have tested APP in beta say they’ve seen promising results. For example, WPP-owned Greenhouse Group used the platform and were able to decrease trade times by 73 percent while receiving 13 percent better performance than when buyers did them manually. (According to a Business Insider report, OMD and Xaxis have also been testing the technology.) AppNexus is also changing how it targets, using Facebook-type phrases like “people-based targeting” to describe cross-device targeting beyond cookies. It will also charge based on views rather than impressions with a “viewable only” mode and allow automatic bidding through a price optimizer. “The next generation of DSPs should leverage machine learning to more efficiently manage an ever-larger number of campaign variables, minimize manual intervention, and deliver superior optimization tied to marketer KPIs,” AppNexus CEO Brian O’Kelley said in a statement. “With APP, the world’s first and only programmable DSP, traders can spend less time on setup and delivery and more time strategizing on the intuitive, creative aspects of advertising that change a person’s perception of a brand.” AppNexus isn’t the only company adding machine learning to its media-buying offerings. Several others in recent months have also debuted their own services with artificial intelligence. This summer Sizmek cited Rocket Fuel’s AI-powered platform as one reason for acquiring the company. In September, IBM officially unveiled its Watson Advertising platform, which uses AI for everything from content creation to localized buying optimization. Xaxis is also building its own internal platform that uses AI for retargeting, audience segmentation and supply optimization."
42f81c25fe,Amazon Go Store Could Be Shut Down by Too Many Customers | Fortune,"Amazon needs a little more time to address some odd troubles it’s facing with its new cashier-free stores, according to a new report. The online retail giant will not open its Amazon Go cashier-less stores later this month as planned, The Wall Street Journal is reporting, citing people who claim to have knowledge of its plans. Instead, Amazon needs more time to address some quirks it’s found in tracking customers at the store when they pick out their desired items. According to the report, Amazon’s sensors are only able to track approximately 20 people in Amazon Go marketplaces at a time. Up to 20 people, the automated store can track customers and check them out without trouble. Once more customers enter the store or when they slow their movements, Amazon’s technology cannot so easily track them and the purchasing process breaks down, the report says. Get Data Sheet, Fortune’s technology newsletter Amazon (AMZN) in December unveiled several brick-and-mortar store concepts as part of a broader push by the online retail giant to expand its presence in cities around the world. Amazon Go is among the smaller store concepts and acts more as a convenience store for customers rather than a larger grocery store. Amazon had previously said it hopes to open the marketplace in its hometown Seattle in “early 2017,” where it’s currently testing the concept. The company is also reportedly considering drive-through marketplaces, as well as grocery store concepts. Amazon has already opened pop-stores in malls and a handful of bookstores across the U.S. Amazon Go is designed to be a cashier-free market where patrons are tracked with cameras, motion sensors, and artificial intelligence to determine what customers are picking up and whether their purchase matches the items the technology believes they chose. If it all checks out, customers will automatically make a purchase through their smartphones without ever needing to head to a register and be able to leave the store with their items in hand.
The futuristic concept, which tries to take human employees out of the retail equation, is breaking down in a mock store Amazon has erected in a warehouse in Seattle, according to the Journal‘s sources. Employees act as the mock customers. It’s unclear what Amazon might need to do to improve the system’s accuracy and track more than 20 people in a store at the time. But the Journal‘s sources say Amazon is anticipating significant interest for its Amazon Go stores and will not open the marketplaces until the technology is more sophisticated. Amazon did not immediately respond to a Fortune request for comment."
2b5c68fa2f,Why Netflix Never Implemented The Algorithm That Won The Netflix $1 Million Challenge | Techdirt,"Innovation by
Mike Masnick Fri, Apr 13th 2012 12:07am
Filed Under:
contest, data, recommendation algorithm, streaming
Companies:
netflix
Permalink.Short link. Subscribe: RSS View by: Time | Thread [ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
[ reply to this | link to this | view in chronology ]
Explore some core concepts: read all » Join the Insider Chat Email This This feature is only available to registered users. Register or sign in to use it."
ba2c9688b6,Scientists Use Machine-learning to Analyze Language in Movies  | Technology Networks,"News   Nov 14, 2017
| Original story from University of Washington
In the movie Frozen, only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of agency and power as Cinderella, a movie character that debuted in 1950. Credit: University of Washington
At first glance, the movie ""Frozen"" might seem to have two strong female protagonists -- Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom.But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists.The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed.""'Frozen' is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,"" said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies.""Anna is actually portrayed with the same low levels of power and agency as Cinderella, which is a movie that came out more than 60 years ago. That's a pretty sad finding,"" Sap said.The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like ""Heathers"" to romantic comedies like ""500 Days of Summer"" to war films like ""Apocalypse Now.""In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men. For example, male characters spoke more in imperative sentences (""Bring me my horse"") while female characters tended to hedge their statements (""Maybe I am wrong""). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives.To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on ""connotation frames"" that give insights into how different verbs can empower or weaken different characters through their connotative meanings. The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments.The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3.""For example, if a female character 'implores' her husband, that implies the husband has a stance where he can say no. If she 'instructs' her husband, that implies she has more power,"" said co-author Ari Holtzman, an Allen School doctoral student. ""What we found was that men systematically have more power and agency in the film script universe.""Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose.Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb's subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies.The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters -- exposing subtle differences and biases.In 2010's ""Black Swan,"" a movie centered around a female lead -- a perfectionist ballerina who slowly loses grip on reality -- the movie's dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film.In the 2007 movie ""Juno,"" about an offbeat young woman who unexpectedly gets pregnant, male characters' scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue.The UW team's tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man.The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers.""We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,"" said co-author and Allen School doctoral student Hannah Rashkin.Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn't limited to movies, but could be applied to books, plays or any other texts.""We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,"" said senior author Yejin Choi, an associate professor in the Allen School. ""We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.""This article has been republished from materials provided by University of Washington. Note: material may have been edited for length and content. For further information, please contact the cited source.
SLIMS Users Can Now Connect to the iSpecimen Marketplace
Genohm and iSpecimen have jointly announced that Genohm has established a partnership with iSpecimen to enable users of the SLIMS laboratory information management system to seamlessly connect to the iSpecimen Marketplace.
Ovation and Coriell Life Sciences Partner to Streamline Genetic Reporting Workflows
Ovation.io, Inc. (Ovation), makers of the fastest-growing clinical laboratory information and commercialization platform, and Coriell Life Sciences, Inc., an innovative provider of clinical genetic reporting solutions, announced a partnership and platform integration designed to create a seamless ecosystem for sample and workflow management, genetic data interpretation, and external client communication.
Blue Brain Nexus: An Open-Source Tool for Data-Driven Science
The Blue Brain Nexus will help enable data-driven neuroscience through searching, integrating and tracking large-scale data and models.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 11, 2018 Video News   Jan 11, 2018 News   Jan 11, 2018 Article Article"
6b9507b981,About Face ID advanced technology - Apple Support,"Learn how Face ID helps protect your information on your iPhone X. Much of our digital lives are stored on iPhone and it's important to protect that information. In the same way that Touch ID revolutionized authentication using a fingerprint, Face ID revolutionizes authentication using facial recognition. Face ID provides intuitive and secure authentication enabled by the state-of-the-art TrueDepth camera system with advanced technologies to accurately map the geometry of your face. With a simple glance, Face ID securely unlocks your iPhone X. You can use it to authorize purchases from the iTunes Store, App Store, iBooks Store, and payments with Apple Pay. Developers can also allow you to use Face ID to sign into their apps. Apps that already support Touch ID will automatically support Face ID. The technology that enables Face ID is some of the most advanced hardware and software that we’ve ever created. The TrueDepth camera captures accurate face data by projecting and analyzing over 30,000 invisible dots to create a depth map of your face and also captures an infrared image of your face. A portion of the A11 Bionic chip's neural engine — protected within the Secure Enclave — transforms the depth map and infrared image into a mathematical representation and compares that representation to the enrolled facial data. Face ID automatically adapts to changes in your appearance, such as wearing cosmetic makeup or growing facial hair. If there is a more significant change in your appearance, like shaving a full beard, Face ID confirms your identity by using your passcode before it updates your face data. Face ID is designed to work with hats, scarves, glasses, contact lenses, and many sunglasses. Furthermore, it's designed to work indoors, outdoors, and even in total darkness. To start using Face ID, you need to first enroll your face. You may do this during the initial set up process, or at a later time by going to Settings > Face ID & Passcode. To unlock your iPhone X using Face ID, simply glance at it. Face ID requires that the TrueDepth camera sees your face, whether your iPhone X is lying on a surface or you're holding it in a natural position. The TrueDepth camera has a similar range of view as when you take a photo or make a FaceTime call with the front camera. Face ID works best when the device is arm’s length or less from your face (25-50 cm away from your face). The TrueDepth camera is intelligently activated; for example, by raising to wake your iPhone X, tapping to wake your screen, or from an incoming notification that wakes the screen. Each time you unlock your iPhone X, the TrueDepth camera recognizes you by capturing accurate depth data and an infrared image. This information is matched against the stored mathematical representation to authenticate. Security is important to all of us to protect information on our devices. We have done some important things to safeguard your information, the same way we did with Touch ID. Face ID uses the TrueDepth camera and machine learning for a secure authentication solution. Face ID data - including mathematical representations of your face - is encrypted and protected with a key available only to the Secure Enclave. The probability that a random person in the population could look at your iPhone X and unlock it using Face ID is approximately 1 in 1,000,000 (versus 1 in 50,000 for Touch ID). As an additional protection, Face ID allows only five unsuccessful match attempts before a passcode is required. The statistical probability is different for twins and siblings that look like you and among children under the age of 13, because their distinct facial features may not have fully developed. If you're concerned about this, we recommend using a passcode to authenticate. Face ID matches against depth information, which isn’t found in print or 2D digital photographs. It's designed to protect against spoofing by masks or other techniques through the use of sophisticated anti-spoofing neural networks. Face ID is even attention-aware. It recognizes if your eyes are open and looking towards the device. This makes it more difficult for someone to unlock your iPhone without your knowledge (such as when you are sleeping). To use Face ID, you must set up a passcode on your iPhone. You must enter your passcode for additional security validation when: If your device is lost or stolen, you can prevent Face ID from being used to unlock your device with Find My iPhone Lost Mode. Privacy is incredibly important to Apple. Face ID data - including mathematical representations of your face - is encrypted and protected by the Secure Enclave. This data will be refined and updated as you use Face ID to improve your experience, including when you successfully authenticate. Face ID will also update this data when it detects a close match but a passcode is subsequently entered to unlock the device. Face ID data doesn’t leave your device and is never backed up to iCloud or anywhere else. Only in the case that you wish to provide Face ID diagnostic data to AppleCare for support will this information be transferred from your device. And even in this case, data isn’t automatically sent to Apple; you can first review and approve the diagnostic data before it’s sent. If you choose to enroll in Face ID, you can control how it's used or disable it at any time. For example, if you don’t want to use Face ID to unlock your iPhone, open Settings > Face ID & Passcode > Use Face ID, and disable iPhone Unlock. To disable Face ID, open Settings > Face ID & Passcode, and tap Reset Face ID. Doing so will delete Face ID data, including mathematical representations of your face, from your device. If you choose to erase or reset your device using Find My iPhone or erasing all content and settings, all Face ID data will be deleted. Even if you don’t enroll in Face ID, the TrueDepth camera intelligently activates to support attention aware features, like dimming the display if you aren't looking at your iPhone or lowering the volume of alerts if you're looking at your device. For example, when using Safari, your device will check to determine if you're looking at your device and turns the screen off if you aren’t. If you don’t want to use these features, you can open Settings > Face ID & Passcode and disable Attention Aware Features. Within supported apps, you can enable Face ID for authentication. Apps are only notified as to whether the authentication is successful. Apps can’t access Face ID data associated with the enrolled face. iPhone and the TrueDepth camera system have been thoroughly tested and meet international safety standards. The TrueDepth camera system is safe to use under normal usage conditions. The system will not cause any harm to eyes or skin, due to its low output. It's important to know that the infrared emitters could be damaged during repair or disassembly, so your iPhone should always be serviced by Apple or an authorized service provider. The TrueDepth camera system incorporates tamper-detection features. If tampering is detected, the system may be disabled for safety reasons. When viewed through certain types of cameras, you may notice light output from the TrueDepth camera. This is expected as some cameras may detect infrared light. Some may also notice a faint light output from the TrueDepth camera when viewed in a very dark room. This is expected in extremely dark settings. Accessibility is an integral part of Apple products. Users with physical limitations can select “Accessibility Options” during enrollment that doesn't require the full range of head motion to capture different angles and is still secure to use but requires more consistency in how you look at iPhone X. Face ID also has an accessibility feature to support individuals who are blind or have low vision. If you don't want Face ID to require that you look with your eyes open at iPhone X, you can open Settings > General > Accessibility, and disable Require Attention for Face ID. This is automatically disabled if you enable VoiceOver during initial set up."
db67c6e116,The Top 10 AI And Machine Learning Use Cases Everyone Should Know About,"Machine learning is a buzzword in the technology world right now, and for good reason: It represents a major step forward in how computers can learn. Very basically, a machine learning algorithm is given a “teaching set” of data, then asked to use that data to answer a question. For example, you might provide a computer a teaching set of photographs, some of which say, “this is a cat” and some of which say, “this is not a cat.” Then you could show the computer a series of new photos and it would begin to identify which photos were of cats. Machine learning then continues to add to its teaching set. Every photo that it identifies — correctly or incorrectly — gets added to the teaching set, and the program effectively gets “smarter” and better at completing its task over time. It is, in effect, learning.
Malware is a huge — and growing — problem. In 2014, Kaspersky Lab said it had detected 325,000 new malware files every day. But, institutional intelligence company Deep Instinct says that each piece of new malware tends to have almost the same code as previous versions — only between 2 and 10% of the files change from iteration to iteration. Their learning model has no problem with the 2–10% variations, and can predict which files are malware with great accuracy. In other situations, machine learning algorithms can look for patterns in how data in the cloud is accessed, and report anomalies that could predict security breaches. If you’ve flown on an airplane or attended a big public event lately, you almost certainly had to wait in long security screening lines. But machine learning is proving that it can be an asset to help eliminate false alarms and spot things human screeners might miss in security screenings at airports, stadiums, concerts, and other venues. That can speed up the process significantly and ensure safer events. Many people are eager to be able to predict what the stock markets will do on any given day — for obvious reasons. But machine learning algorithms are getting closer all the time. Many prestigious trading firms use proprietary systems to predict and execute trades at high speeds and high volume. Many of these rely on probabilities, but even a trade with a relatively low probability, at a high enough volume or speed, can turn huge profits for the firms. And humans can’t possibly compete with machines when it comes to consuming vast quantities of data or the speed with which they can execute a trade.
Source: Shutterstock Machine learning algorithms can process more information and spot more patterns than their human counterparts. One study used computer assisted diagnosis (CAD) when to review the early mammography scans of women who later developed breast cancer, and the computer spotted 52% of the cancers as much as a year before the women were officially diagnosed. Additionally, machine learning can be used to understand risk factors for disease in large populations. The company Medecision developed an algorithm that was able to identify eight variables to predict avoidable hospitalizations in diabetes patients. The more you can understand about your customers, the better you can serve them, and the more you will sell.  That’s the foundation behind marketing personalisation. Perhaps you’ve had the experience in which you visit an online store and look at a product but don’t buy it — and then see digital ads across the web for that exact product for days afterward. That kind of marketing personalization is just the tip of the iceberg. Companies can personalize which emails a customer receives, which direct mailings or coupons, which offers they see, which products show up as “recommended” and so on, all designed to lead the consumer more reliably towards a sale. Machine learning is getting better and better at spotting potential cases of fraud across many different fields. PayPal, for example, is using machine learning to fight money laundering. The company has tools that compare millions of transactions and can precisely distinguish between legitimate and fraudulent transactions between buyers and sellers. You’re probably familiar with this use if you use services like Amazon or Netflix. Intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch next. These recommendations are getting smarter all the time, recognizing, for example, that you might purchase certain things as gifts (and not want the item yourself) or that there might be different family members who have different TV preferences. Perhaps the most famous use of machine learning, Google and its competitors are constantly improving what the search engine understands. Every time you execute a search on Google, the program watches how you respond to the results. If you click the top result and stay on that web page, we can assume you got the information you were looking for and the search was a success.  If, on the other hand, you click to the second page of results, or type in a new search string without clicking any of the results, we can surmise that the search engine didn’t serve up the results you wanted — and the program can learn from that mistake to deliver a better result in the future. NLP is being used in all sorts of exciting applications across disciplines. Machine learning algorithms with natural language can stand in for customer service agents and more quickly route customers to the information they need. It’s being used to translate obscure legalese in contracts into plain language and help attorneys sort through large volumes of information to prepare for a case. IBM recently surveyed top auto executives, and 74% expected that we would see smart cars on the road by 2025. A smart car would not only integrate into the Internet of Things, but also learn about its owner and its environment. It might adjust the internal settings — temperature, audio, seat position, etc. — automatically based on the driver, report and even fix problems itself, drive itself, and offer real time advice about traffic and road conditions. Bernard Marr is a best-selling author & keynote speaker on business, technology and big data. His new book is Data Strategy. To read his future posts simply join his network here."
4be9ef0413,Voice recognition and machine learning make service bots better | VentureBeat,"We are on the cusp of a technological revolution whereby increasingly sophisticated tasks can be handed over from humans to machines. Organizations are embracing advancements in artificial intelligence, robotics, and natural language technology to adopt platforms that can “learn” from experience and actually interact with users. The next wave of these chatbots will have enhanced real-time data analytics and automation capabilities and the ability to integrate intelligence across multiple digital channels to engage customers in natural conversations using voice or text. So what will the integration of this technology mean for you as a customer? When you have a question about a product or service, you will be presented with the best agent, who possesses the entire company’s collective experience and a huge wealth of knowledge to address your issue. Think about what happens today when you call your bank or the help desk of an ecommerce site. In most cases, you reach a recorded message asking you to select the reason for your call and maybe prompting you to solve basic issues by going to the website instead. This approach may help companies cut customer service costs, but most people I know find navigating these menus quite frustrating, leading them to hit 0 to just get through to a human operator. That doesn’t necessarily help the situation, because it often leads to long hold times waiting for an operator to become available. And it’s not uncommon for the long wait to be followed by a painful conversation with an agent whom you can’t understand because they have a headset problem or who puts you on hold to transfer you to another operator. You get the point. Now imagine your call is answered immediately by a chatbot with a name and a voice you recognize. After a few security checks to confirm your identity — maybe performed invisibly using voice recognition — the bot knows everything about all your interactions with the company, including orders, failed orders, searches on the website, past transactions, previous calls, and anything you’ve shared on this call without you having to wrack your brain, trawl through your inbox, or repeat yourself. The bot addresses your concerns using the collected wisdom of the company, including the absolute latest data on issues other customers face and how to solve them. It is able to take action to rectify most problems without calling in specialists or requesting approvals, and it can do so without errors. Later the bot follows up with you to make sure everything went OK using the medium of your choice: email, messaging apps, a call back to your preferred number, a notification to your account on the website, or any combination of these — again, without errors. The information generated throughout the entire process can then be leveraged in myriad ways, from improving the experience for the next caller, to refining products and services, to personalizing future interactions with you. If properly implemented, the process becomes a virtuous circle. More often than not, your issue can get resolved in the first interaction. This type of machine-led interaction can increase customer satisfaction and inspire customer loyalty if well executed. If customers get to choose the medium they prefer, if the service they get is consistently faster and more effective, and if the bot has the best characteristics of a human agent (and none of the worst ones), customers could actually find the experience superior to dealing with a human. This is especially true for the generations of users who have grown up seeing smartphones, social media, and chat as an extension of themselves. Facebook Messenger chatbots are emerging as a popular customer engagement platform for retailers. For example, eBay has a ShopBot that makes searching, comparing, and buying items quicker and more seamless. The machine learning capabilities of this bot mean that it can make more targeted recommendations to users based on the items they have shopped or searched before. At the higher end of the scale is Burberry — a brand that is well-known as a digital trailblazer in the fashion business. The brand’s chatbot has a wealth of data, imagery, and links to the company’s latest collections to help customers put together their desired look. As chatbots become more commonplace in customer service, certain jobs will be replaced by machines and humans will be free to deal with more complicated or sensitive tasks faster. This shift will enable humans to offer augmented, enhanced service for the most valuable customers and open the door to hybrid service models, wherein machine learning-enabled technologies help humans even more. These hybrid models are already implemented, and widespread use of chatbots will happen faster than you might think. Given that it can take a long time to retrain and reskill employees, it is imperative that companies begin planning for these transitions now. The implications for the human workforce that are created as a result of advancements in chatbot technology are likely to be profound. As the battle between humans and machines intensifies, there are bound to be both winners and losers. The winners will be the individuals and organizations that look most aggressively at how they can leverage these latest digital technologies for a competitive advantage. Peter Quinlan is vice president of unified communications and collaboration product management at Tata Communications, a global provider of telecommunications solutions and services."
d54d22013a,4 Mind-Blowing Ways Facebook Uses Artificial Intelligence,"Continued from page 1 1. Textual analysis  A large proportion of the data shared on Facebook is still text. Video may involve larger data volumes in terms of megabytes, but in terms of insights, text can still be just as rich. A picture may paint 1,000 words, but if you just want to answer a simple question, you often don’t need 1,000 words. Every bit of data which isn’t essential to answering your question is just noise, and more importantly, a waste of resources to store and analyze. Facebook uses a tool it developed itself called DeepText to extract meaning from words we post by learning to analyze them contextually. Neural networks analyze the relationship between words to understand how their meaning changes depending on other words around them. Because this is semi-unsupervised learning, the algorithms do not necessarily have reference data – for example a dictionary – explaining the meaning of every word. Instead, it learns for itself based on how words are used. This means that it won’t be tripped up by variations in spelling, slang or idiosyncrasies of language use. In fact, Facebook say the technology is “language agnostic” – due to the way it assigns labels to words, it can easily switch between working across different human languages and apply what it has learned from one to another.
At present the tool is used to direct people towards products they may want to purchase based on conversations they are having – this video gives an example of how it decides whether providing a user with a shopping link is appropriate or not, depending on the context. 2. Facial recognition  Facebook uses a DL application called DeepFace to teach it to recognize people in photos. It says that its most advanced image recognition tool is more successful than humans in recognizing whether two different images are of the same person or not – with DeepFace scoring a 97% success rate compared to humans with 96%. It’s fair to say that use of this technology has proven controversial. Privacy campaigners said it went too far as it would allow Facebook – based on a high resolution photograph of a crowd -   to put names to many of the faces which is clearly an obstacle to our freedom to move in public anonymously. EU legislators agreed and persuaded Facebook to remove the functionality from European citizens’ accounts in 2013. Back then the social media giant was using an earlier version of the facial recognition tool which did not use Deep Learning. Facebook has been somewhat quiet about the development of this technology since it first hit headlines, and can be assumed to be waiting on the outcome of pending privacy cases before saying more about their plans to roll it out."
7bd4003b9b,Sophia Genetics is machine learning is speeding up cancer diagnosis | WIRED UK,"Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
By
Matt Burgess
Jurgi Camblong plans to work with ""liquid biopsies"" making the process less invasive and faster This article was first published in the June 2016 issue of WIRED magazine. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. Jurgi Camblong is diagnosing cancer using thousands of people's DNA. The 38-year-old Sophia Genetics co-founder detects cancer in the lungs, skin, ovaries and breast, as well as congenital diseases, by sequencing the genomes of patient's tissue samples – then uses machine learning to compare the results and suggest the most effective treatments.
""The problem is not producing the content or the data but really analysing to find the important information so you can act on a disease,"" says Camblong. Sophia Genetics's machine learning system, created in 2011, is used in more than 100 hospitals across 20 European countries, including Oxford University's John Radcliffe Hospital and Liverpool Women's NHS Foundation Trust. Hospitals pay each time they use the tool. In its first 18 months, the company was involved in the diagnosis of 25,000 patients. In 2016, it aims to make 80,000 diagnoses. Once the startup, which has raised more than £20 million from investors including Mike Lynch, receives patient data from a hospital, it can find genes related to diseases, such as BRCA-1 in breast cancer, within two hours. Genetic sequencing and treatment is being tackled by some of the world's biggest organisations: Google and Amazon both help scientists analyse genetic data, and the NHS is sequencing 100,000 genomes from 70,000 people. Camblong says 60-person Sophia Genetics' advantage is that it can compare patient data across hospitals. ""Algorithms recognise the context in which this raw data has been produced, eliminate biases, and make the results comparable,"" he says. The company is monitoring the success of each treatment. ""In two years time we could tell you that your cancer looks like the cancer of 1,000 other patients, 500 received that drug and 80 per cent survived,"" says Camblong. It's not the cure - but it's a better diagnosis.
By
Matt Burgess
By
Matt Burgess
By
Emma Bryce"
2d6ceeb95f,"Google AI experiments add details to low-res, blurred images | WIRED UK","Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
Two Google Brain neural networks were combined to create the enhanced images
By
Matt Burgess
Two artificial intelligence systems built by Google are able to transform a heavily pixellated, low quality, image into a clear photo or a person or object.
Computer scientists from Google Brain, the central Google AI team, have shown it's not only possible to enhance a picture's resolution, they can fill in missing details in the process. In a paper – Pixel Recursive Super Resolution – three researchers from the Silicon Valley firm trained their system on small 8x8 pixel images of celebrity faces and photos of bedrooms.
Artificial Intelligence The combination of a conditioning neural network and a prior neural network analysed the images to produce higher resolution 32x32 pixel versions. The process sees a blurry, almost unrecognisable, picture turned into something that clearly represents a human or a room.
By
Eleanor Peake
In particular, the AI system works by taking a two-pronged approach. The conditioning network takes the low-resolution image and compares it to high-resolution images to determine whether a face or a room is in the image. It's possible to compare the low res image, the researchers explain, to a high image by scaling down the large image to the same 8x8 pixel size.
""When some details do not exist in the source image, the challenge lies not only in ‘deblurring’ an image, but also in generating new image details that appear plausible to a human observer,"" the Google Brain researchers wrote in their paper.
When both images are the same size it is relatively simple for the AI to identify similar pixels and shapes between the different versions. For example, the system can recognise an ear of a particular shape and compare it with the pixels in another image – telling the AI it is looking at a face.
Once the first AI network has completed its role, the Google researchers use the PixelCNN to add extra pixels to the 8x8 image. As Ars Technica explained, the PixelCNN adds detail by using what it knows about certain types of images. Lips are likely to be a shade of pink, so pink pixels are added to areas identified as such.
Algorithms At the end of each neural network's process, the Google researchers combined the results to create a final image. They describe the process of adding details to how an artist works. ""By incorporating the prior knowledge of the faces and their typical variations, an artist is able to paint believable details,"" they wrote.
To prove the AI-generated images are believable, the researchers tested their system on human volunteers. A group of participants was shown a true, low res image, alongside the one created by the AI. They were then asked to guess which was from a camera.
When looking at the images of celebrities, 10 per cent of the time the humans believed the artificially-created shot was taken by a camera. ""Note that 50 per cent would say that an algorithm perfectly confused the subjects,"" the researchers say. In the future, with further development, similar systems could be developed to add detail to pictures and video that are low resolution. One current category that lends itself to this is blurry CCTVimages. Although, the method is yet to be tested with any such databases of images and the AI creations are currently the machine's ""best guesses"" rather than entirely accurate portrayals.
By
Matt Burgess
By
Matt Burgess
By
Lee Bell
By
WIRED"
3dd063c093,A New Alexa-Like System Is Helping Robots Understand Context Clues,"Unlike human communication, which involves a variety of nuances and subtleties, today’s robots understand only the literal. While they can learn by repetition, for machines, language is about direct commands, and they are pretty inept when it comes to complex requests. Even the seemingly minor differences between “pick up the red apple” and “pick it up” can be too much for a robot to decipher. However, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) want to change that. They think they can help robots process these complex requests by teaching machine systems to understand context. In a paper they presented at the International Joint Conference on Artificial Intelligence (IJCAI) in Australia last week, the MIT team showcased ComText — short for “commands in context” — an Alexa-like system that helps a robot understand commands that involve contextual knowledge about objects in its environment.
Essentially, ComText allows a robot to visualize and understand its immediate environment and infer meaning from that environment by developing what’s called an “episodic memory.” These memories are more “personal” than semantic memories, which are generally just facts, and they could include information about an encountered object’s size, shape, position, and even if it belongs to someone. When they tested ComText on a two-armed humanoid robot called Baxter, the researchers observed that the bot was able to perform 90 percent of the complex commands correctly. “The main contribution is this idea that robots should have different kinds of memory, just like people,” explained lead researcher Andrei Barbu in a press release. “We have the first mathematical formulation to address this issue, and we’re exploring how these two types of memory play and work off of each other.” Of course, ComText still has a great deal of room for improvement, but eventually, it could be used to narrow the communication gap between humans and machines. “Where humans understand the world as a collection of objects and people and abstract concepts, machines view it as pixels, point-clouds, and 3-D maps generated from sensors,” noted Rohan Paul, one of the study’s lead authors. “This semantic gap means that, for robots to understand what we want them to do, they need a much richer representation of what we do and say.” Ultimately, a system like ComText could allow us to teach robots to quickly infer an action’s intent or to follow multi-step directions. With so many different industries poised to take advantage of autonomous technologies and artificially intelligent (AI) systems, the implications of that could be widespread. Everything from self-driving cars to the AIs being used for healthcare could benefit from an improved ability to interact with the world and people around them.
References:
Phys.org, IJCAI, MIT News
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
34e28e3abf,The Trick That Makes Google's Self-Driving Cars Work - The Atlantic,"Google is engaging in unprecedented, massive, ongoing data collection to transform intractable problems into solvable chores. Google's self-driving cars can tour you around the streets of Mountain View, California.  I know this. I rode in one this week. I saw the car's human operator take his hands from the wheel and the computer assume control. ""Autodriving,"" said a woman's voice, and just like that, the car was operating autonomously, changing lanes, obeying traffic lights, monitoring cyclists and pedestrians, making lefts. Even the way the car accelerated out of turns felt right.  It works so well that it is, as The New York Times' John Markoff put it, ""boring."" The implications, however, are breathtaking.  Perfect, or near-perfect, robotic drivers could cut traffic accidents, expand the carrying capacity of the nation's road infrastructure, and free up commuters to stare at their phones, presumably using Google's many services.  But there's a catch.  Today, you could not take a Google car, set it down in Akron or Orlando or Oakland and expect it to perform as well as it does in Silicon Valley. Here's why: Google has created a virtual track out of Mountain View.  The key to Google's success has been that these cars aren't forced to process an entire scene from scratch. Instead, their teams travel and map each road that the car will travel. And these are not any old maps. They are not even the rich, road-logic-filled maps of consumer-grade Google Maps. They're probably best thought of as ultra-precise digitizations of the physical world, all the way down to tiny details like the position and height of every single curb. A normal digital map would show a road intersection; these maps would have a precision measured in inches.  But the ""map"" goes beyond what any of us know as a map. ""Really, [our maps] are any geographic information that we can tell the car in advance to make its job easier,"" explained Andrew Chatham, the Google self-driving car team's mapping lead. ""We tell it how high the traffic signals are off the ground, the exact position of the curbs, so the car knows where not to drive,"" he said. ""We'd also include information that you can't even see like implied speed limits."" Google has created a virtual world out of the streets their engineers have driven. They pre-load the data for the route into the car's memory before it sets off, so that as it drives, the software knows what to expect.  ""Rather than having to figure out what the world looks like and what it means from scratch every time we turn on the software, we tell it what the world is expected to look like when it is empty,"" Chatham continued. ""And then the job of the software is to figure out how the world is different from that expectation. This makes the problem a lot simpler."" While it might make the in-car problem simpler, but it vastly increases the amount of work required for the task. A whole virtual infrastructure needs to be built on top of the road network! Very few companies, maybe only Google, could imagine digitizing all the surface streets of the United States as a key part of the solution of self-driving cars. Could any car company imagine that they have that kind of data collection and synthesis as part of their core competency? Whereas, Chris Urmson, a former Carnegie Mellon professor who runs Google's self-driving car program, oozed confidence when asked about the question of mapping every single street where a Google car might want to operate. ""It's one of those things that Google, as a company, has some experience with our Google Maps product and Street View,"" Urmson said. ""We've gone around and we've collected this data so you can have this wonderful experience of visiting places remotely. And it's a very similar kind of capability to the one we use here."" So far, Google has mapped 2,000 miles of road. The US road network has something like 4 million miles of road. ""It is work,"" Urmson added, shrugging, ""but it is not intimidating work."" That's the scale at which Google is thinking about this project. All this makes sense within the broader context of Google's strategy. Google wants to make the physical world legible to robots, just as it had to make the web legible to robots (or spiders, as they were once known) so that they could find what people wanted in the pre-Google Internet of yore.  In fact, it might be better to stop calling what Google is doing mapping, and come up with a different verb to suggest the radical break they've made with previous ideas of maps. I'd say they're crawling the world, meaning they're making it legible and useful to computers.  Self-driving cars sit perfectly in-between Project Tango—a new effort to ""give mobile devices a human-scale understanding of space and motion""—and Google's recent acquisition spree of robotics companies. Tango is about making the ""human-scale"" world understandable to robots and the robotics companies are about creating the means for taking action in that world.  The more you think about it, the more the goddamn Googleyness of the thing stands out: the ambition, the scale, and the type of solution they've come up with to this very hard problem. What was a nearly intractable ""machine vision"" problem, one that would require close to human-level comprehension of streets, has become a much, much easier machine vision problem thanks to a massive, unprecedented, unthinkable amount of data collection.  Last fall, Anthony Levandowski, another Googler who works on self-driving cars, went to Nissan for a presentation that immediately devolved into a Q&A with the car company's Silicon Valley team. The Nissan people kept hectoring Levandowski about vehicle-to-vehicle communication, which the company's engineers (and many in the automotive industry) seemed to see as a significant part of the self-driving car solution.  He parried all of their queries with a speed and confidence just short of condescension. ""Can we see more if we can use another vehicle's sensors to see ahead?"" Levandowski rephrased one person's question. ""We want to make sure that what we need to drive is present in everyone's vehicle and sharing information between them could happen, but it's not a priority."" What the car company's people couldn't or didn't want to understand was that Google does believe in vehicle-to-vehicle communication, but serially over time, not simultaneously in real-time. After all, every vehicle's data is being incorporated into the maps. That information ""helps them cheat, effectively,"" Levandowski said. With the map data—or as we might call it, experience—all the cars need is their precise position on a super accurate map, and they can save all that parsing and computation (and vehicle to vehicle communication).  There's a fascinating parallel between what Google's self-driving cars are doing and what the Andreesen Horowitz-backed startup Anki is doing with its toy car racing game. When you buy Anki Drive, they sell you a track on which the cars race, which has positioning data embedded. The track is the physical manifestation of a virtual racing map. Last year, Anki CEO (and like Urmson, a Carnegie Mellon robotics guy) Boris Sofman told me knowing the racing environment in advance allows them to more easily sync the state of the virtual world in which their software is running with the physical world in which the cars are driving.  ""We are able to turn the physical world into a virtual world,"" Sofman said. ""We can take all these physical characters and abstract away everything physical about them and treat them as if they were virtual characters in a video game on the phone."" Of course, when there are bicyclists and bad drivers involved, navigating the hybrid virtual-physical world of Mountain View is not easy: the cars still have to ""race"" around the track, plotting trajectories and avoiding accidents. The Google cars are not dumb machines. They have their own set of sensors: radar, a laser spinning atop the Lexus SUV, and a suite of cameras. And they have some processing on board to figure out what routes to take and avoid collisions. This is a hard problem, but Google is doing the computation with what Levandowski described at Nissan as a ""desktop"" level system. (The big computation and data processing are done by the teams back at Google's server farms.) What that on-board computer does first is integrate the sensor data. It takes the data from the laser and the cameras and integrates them into a view of the world, which it then uses to orient itself (with the rough guidance of GPS) in virtual Mountain View. ""We can align what we're seeing to what's stored on the map. That allows us to very accurately—within a few centimeters—position ourselves on the map,"" said Dmitri Dolgov, ​the self-driving car team's software lead. ""Once we know where we are, all that wonderful information encoded in our maps about the geometry and semantics of the roads becomes available to the car."" Once they know where they are in space, the cars can do the work of watching for and modeling the behavior of dynamic objects like other cars, bicycles, and pedestrians.  Here, we see another Google approach. Dolgov's team uses machine learning algorithms to create models of other people on the road. Every single mile of driving is logged, and that data fed into computers that classify how different types of objects act in all these different situations. While some driver behavior could be hardcoded in (""When the lights turn green, cars go""), they don't exclusively program that logic, but learn it from actual driver behavior.  In the way that we know that a car pulling up behind a stopped garbage truck is probably going to change lanes to get around it, having been built with 700,000 miles of driving data has helped the Google algorithm to understand that the car is likely to do such a thing.  Most driving situations are not hard to comprehend, but what about the tough ones or the unexpected ones? In Google's current process, a human driver would take control, and (so far) safely guide the car. But fascinatingly, in the circumstances when a human driver has to take over, what the Google car would have done is also recorded, so that engineers can test what would have happened in extreme circumstances without endangering the public.  So, each Google car is carrying around both the literal products of previous drives—the imagery and data captured from crawling the physical world—as well as the computed outputs of those drives, which are the models for how other drivers might behave.  There is, at least in an analogical sense, a connection between how the Google cars work and how our own brains do. We think about the way we see as accepting sensory input and acting accordingly. Really, our brains are making predictions all the time, which guide our perception. The actual sensory input—the light falling on retinal cells—is secondary to the prior experience that we've built into our brains through years of experience being in the world.  That Google's self-driving cars are using these principles is not surprising. That they are having so much success doing so is.  Peter Norvig, the head of AI at Google, and two of his colleagues coined the phrase ""the unreasonable effectiveness of data"" in an essay to describe the effect of huge amounts of data on very difficult artificial intelligence problems. And that is exactly what we're seeing here. A kind of Googley mantra concludes the Norvig essay: ""Now go out and gather some data, and see what it can do."" Even if it means continuously and neverendingly driving 4 million miles of roads with the most sophisticated cars on Earth and then hand-massaging that data—they'll do it. That's the unreasonable effectiveness of Google.
Ghost bikes serve as memorials of lives tragically lost—and reminders of our own fragility.
The president cannot seem to imagine Haitians, Salvadorans, or Africans as coequal citizens. Speaking to lawmakers Thursday, President Trump angrily pushed back on their desire to restore protections for immigrants from Haiti, El Salvador, and African countries and said the U.S. should instead seek immigrants from Norway, the country whose prime minister he welcomed Wednesday. “Why are we having all these people from shithole countries come here?” Trump said, according to The Washington Post. NBC News confirmed the account. CNN reported that Trump said, “Why do we need more Haitians? Take them out.” NBC added that someone in the room replied, “Because if you do, it will be obvious why.” The White House issued a statement in which a spokesman said the president “will always fight for the American people,” but did not deny Trump had made the remarks. Digital messages mimic the speed of real conversation, but often what people like best is the ability to put them off. The defining feature of conversation is the expectation of a response. It would just be a monologue without one. In person, or on the phone, those responses come astoundingly quickly: After one person has spoken, the other replies in an average of just 200 milliseconds. In recent decades, written communication has caught up—or at least come as close as it’s likely to get to mimicking the speed of regular conversation (until they implant thought-to-text microchips in our brains). It takes more than 200 milliseconds to compose a text, but it’s not called “instant” messaging for nothing: There is an understanding that any message you send can be replied to more or less immediately. But there is also an understanding that you don’t have to reply to any message you receive immediately. As much as these communication tools are designed to be instant, they are also easily ignored. And ignore them we do. Texts go unanswered for hours or days, emails sit in inboxes for so long that “Sorry for the delayed response” has gone from earnest apology to punchline. Mark Zuckerberg’s radical decision to reinvent the News Feed is a plea for mercy. Mark Zuckerberg moved fast and broke shit, lots of shit. He broke journalism, by radically deflating the value of the digital advertising on which the livelihood of media now depends; he broke the reading habits of his users, the lab rats in his grand experiment, by constantly manipulating them and feeding them an endless stream of dreck to jack up their “engagement” with his site; and in a way, he broke American democracy, by sitting on his hands as a foreign adversary exploited his platform and by creating the world’s most efficient vehicle for spreading political lies and agitprop. Now, with the announcement that he’s largely stripping the News Feed of news, he’s breaking his own site, too. According to previously unpublished findings, the blue-collar whites at the core of his coalition have lost faith over his first year in office. A massive new source of public-opinion research offers fresh insights into the fault lines emerging in Donald Trump’s foundation of support. Previously unpublished results from the nonpartisan online-polling firm SurveyMonkey show Trump losing ground over his tumultuous first year not only with the younger voters and white-collar whites who have always been skeptical of him, but also with the blue-collar whites central to his coalition. Trump retains important pillars of support. Given that he started in such a strong position with those blue-collar whites, even after that decline he still holds a formidable level of loyalty among them—particularly men and those over 50 years old. What’s more, he has established a modest but durable beachhead among African American and Hispanic men, even while confronting overwhelming opposition from women in those demographic groups. Black Panther, A Wrinkle in Time, Annihilation, Solo: A Star Wars Story, plus a bunch of sequels (and some exciting original projects) Another year is upon us, as is another calendar of Hollywood projects, for better or for worse. From Black Panther to A Wrinkle in Time to the return of Mary Poppins, even 2018’s biggest film franchises feature exciting actors and creators, though there are also plenty of smaller projects hoping to be remembered by the end of the year. Here’s a look at some of 2018’s most notable movies. What It Is: Clint Eastwood’s latest tale of true-life heroism after his back-to-back hits American Sniper and Sully. But this time, rather than casting a movie star like Tom Hanks or Bradley Cooper, Eastwood is filming the real-life heroes Spencer Stone, Anthony Sadler, and Alek Skarlatos, three Americans who helped stop a gunman on a French train in 2015. The trio will play themselves, with Judy Greer, Jenna Fischer, and Jaleel White making up this very unusual project’s ensemble. A Princeton glaciologist says a set of mega-engineering projects may be able to stabilize the world’s most dangerous glaciers. PRINCETON, N.J.—Geo-engineering, its most enthusiastic advocates will tell you, isn’t only possible. It’s already happening. We know, they say, because we’re doing it—we just call it global warming. As humanity dumps billions of tons of greenhouse gases into the atmosphere every year, we’ve engineered a different climate system: one that is hotter, wetter, and more unwieldy than what people have lived in since the dawn of agriculture. So far, the most promising—and least expensive—forms of reverse-engineering this change have taken a similarly whole-world approach. Researchers speculate that planes could periodically spray a clear gas into the high atmosphere that would prevent some sunlight from reaching Earth’s surface, cooling the globe in turn. There’s a lot of buzz about this idea, which is dubbed solar geo-engineering: More than 100 scientists discussed it at an off-the-record gathering in August; Harvard University has opened a $7.5-million center to study it. Its faith-based 12-step program dominates treatment in the United States. But researchers have debunked central tenets of AA doctrine and found dozens of other treatments more effective. J.G. is a lawyer in his early 30s. He’s a fast talker and has the lean, sinewy build of a distance runner. His choice of profession seems preordained, as he speaks in fully formed paragraphs, his thoughts organized by topic sentences. He’s also a worrier—a big one—who for years used alcohol to soothe his anxiety. J.G. started drinking at 15, when he and a friend experimented in his parents’ liquor cabinet. He favored gin and whiskey but drank whatever he thought his parents would miss the least. He discovered beer, too, and loved the earthy, bitter taste on his tongue when he took his first cold sip. His drinking increased through college and into law school. He could, and occasionally did, pull back, going cold turkey for weeks at a time. But nothing quieted his anxious mind like booze, and when he didn’t drink, he didn’t sleep. After four or six weeks dry, he’d be back at the liquor store. A letter from prominent French women embodied a backlash against #MeToo—and then sparked its own backlash. PARIS—After the backlash, comes the backlash to the backlash. It’s been fascinating to follow the torrent of responses in France this week to an open letter in Le Monde signed by French actress Catherine Deneuve and 99 other women, effectively saying that the #MeToo movement had gone too far and that women should own up to their own sexual agency. (I wrote about the letter here and you can find a full translation here.) “Sexual Freedom Threatened, REALLY?” ran the headline of Thursday’s Libération, a left-wing daily, beneath photos of three signatories: Deneuve, Catherine Millet, the author of The Sexual Life of Catherine M., and Brigitte Lahaie, a talk-show host and former porn actress. The implication was, why should we listen to these women? “To entertain the idea that a groper on the metro ‘is the expression of a great sexual misery, or a non-event’”—as the Deneuve letter had—“presumes that you live on a planet without rush hour and have enough power and/or hours on the couch to relativize it at your leisure,” the paper wrote in an editorial. A half-century ago, much of the world appeared to be in a state of crisis, with protests around the world, the Vietnam War, and the assassinations of Dr. Martin Luther King Jr. and Senator Robert Kennedy. But there was some progress to be found as well. A half-century ago, much of the world appeared to be in a state of crisis. Protests erupted in France, Czechoslovakia. Germany, Mexico, Brazil, the United States, and many other places. Some of these protests ended peacefully; many were put down harshly. Two of the biggest catalysts for protest were the U.S. involvement in the Vietnam War and the ongoing lack of civil rights in the U.S. and elsewhere. Two of America’s most prominent leaders, Dr. Martin Luther King Jr. and Senator Robert F. Kennedy, were assassinated within months of each other. But some lessons were being learned and some progress was being made—this was also the year that NASA first sent astronauts around the moon and back, and the year President Lyndon Johnson signed the Civil Rights Act into law. It’s fitting that I post this retrospective today, since it is the day I was born—January 10, 1968. So, a 50th birthday present from me to you today: a look back at 1968. A new breed of online retailer doesn’t make or even touch products, but they’ve got a few other tricks for turning nothing into money. It all started with an Instagram ad for a coat, the West Louis (TM) Business-Man Windproof Long Coat to be specific. It looked like a decent camel coat, not fancy but fine. And I’d been looking for one just that color, so when the ad touting the coat popped up and the price was in the double-digits, I figured: hey, a deal! The brand, West Louis, seemed like another one of the small clothing companies that has me tagged in the vast Facebook-advertising ecosystem as someone who likes buying clothes: Faherty, Birdwell Beach Britches, Life After Denim, some wool underwear brand that claims I only need two pairs per week, sundry bootmakers. Perhaps the copy on the West Louis site was a little much, claiming “West Louis is the perfection of modern gentlemen clothing,” but in a world where an oil company can claim to “fuel connections,” who was I to fault a small entrepreneur for some purple prose? The overbearing “queen bee” boss stereotype is a toxic feedback loop. Tristan Harris speaks to PBS Newshour about the importance of digital detox. At the world's largest grooming show, dogs are transformed into fantastical works of art. Get 10 issues a year and save 65% off the cover price.
TheAtlantic.com Copyright (c) 2018 by The Atlantic Monthly Group. All Rights Reserved."
afd8a194e0,The AI that can turn any selfie into a 3D image | Daily Mail Online,"By
Cheyenne Macdonald For Dailymail.com
Published:
23:32, 20 September 2017
|
Updated:
00:13, 21 September 2017
14 View
comments Researchers have developed an AI that can create a 3D model of your face just by looking at a single photo. Typically, 3D face reconstruction poses ‘extraordinary difficulty,’ as it requires multiple images and must work around the varying poses and expressions, along with differences in lightning, according to the team. By training a neural network on a dataset of both 2D images and 3D facial models or scans, however, their AI can reconstruct the entire face – even adding in parts that might not have been visible in the photo. Scroll down for video and click here to try it on your own snaps Researchers have developed an AI that can create a 3D model of your face just by looking at a single photo. In one example on their website, the researchers tested it out on a photo of former president Barack Obama The team trained the convolutional neural network (CNN) on a dataset of more than 60,000 2D facial images and 3D meshes. Through its training, it learned to map the face from pixels to 3D coordinates, showing end-to-end learning. According to the researchers, their system achieves a ‘direct regression of volumetric representation of the 3D facial geometry,’ from the image. This means it can predict the coordinates of the 3D vertices based on the given 2D image. And, these reconstructions can be further improved by incorporating 3D facial landmark localizations. The system developed by researchers at the University of Nottingham and Kingston University relies on a convolutional neural network (CNN) to overcome some of the challenges of 3D face reconstruction. The network learned how to map a face from pixels to 3D coordinates, and essentially works with any picture of a face, the team explains in the paper published to arXiv. It can even reconstruct faces in arbitrary poses or expressions. To prove its capabilities, the researchers have also released a version you can try yourself, by uploading a photo of your choice. For the best results, they recommend using a frontal photo, or as close to this as possible. The network learned how to map a face from pixels to 3D coordinates, and essentially works with any picture of a face, the team explains in the paper published to arXiv. Apple CEO Tim Cook is pictured To prove its capabilities, the researchers have also released a version you can try yourself , by uploading a photo of your choice. A 3D image of Rihanna is shown above While the AI might make the process seem easy, researchers continue to struggle with 3D face reconstruction, the team explains in the paper. ‘Current systems often assume the availability of multiple facial images (sometimes from the same subject) as input, and must address a number of methodological challenges such as establishing dense correspondences across large facial poses, expressions, and non-uniform illumination,’ the authors write. ‘In general these methods require complex and inefficient pipelines for model building and fitting.’ The new AI, on the other hand, does not adhere to these limitations. By training a neural network on a dataset of both 2D images and 3D facial models or scans, however, their AI can reconstruct the entire face – even adding in parts that might not have been visible in the photo. A 3D render of Rihanna's face is pictured  The team trained the convolutional neural network on a dataset of more than 60,000 2D facial images and 3D meshes. It doesn’t require accurate alignment, and can overcome arbitrary differences in pose and expression for its reconstruction. According to the researchers, their system achieves a ‘direct regression of volumetric representation of the 3D facial geometry,’ from the image. The system developed by researchers at the University of Nottingham and Kingston University relies on a convolutional neural network (CNN) to overcome some of the challenges of 3D face reconstruction. Its scan of actor Elijah Wood is pictured  For the best results, they recommend using a frontal photo, or as close to this as possible. This image of Selena Gomez, for example, did not turn out quite as well as the others  This means it can predict the coordinates of the 3D vertices based on the given 2D image. And, these reconstructions can be further improved by incorporating 3D facial landmark localizations. As of now, the team says this is the first time this particular method has been used, and offers a ‘very simple approach’ to the challenge.  Typically, 3D face reconstruction poses ‘extraordinary difficulty,’ as it requires multiple images and must work around the varying poses and expressions, along with differences in lightning, according to the team. But, the new system can overcome these challenges
Trump CANCELS his visit to London: President pulls out of £750m US embassy opening and blames Obama for selling old site 'for peanuts' as British politicians hit back it's because 'no-one wants you here'
Published by Associated Newspapers Ltd Part of the Daily Mail, The Mail on Sunday & Metro Media Group"
1f12bf96e5,Google’s psychedelic ‘paint brush’ raises the oldest question in art - The Washington Post,"A recent San Francisco art show hosted by Google included the typical trappings — a big crowd, striking pieces of art and expensive price tags. But there was one noteworthy distinction of this particular art show — all of the pieces were created with the help of a Google computer program. The artwork relies on software that remakes an image to include whatever it is being told to see in the image. The end result is a hallucination of the image that has a psychedelic look. The new software provide artists an opportunity to be creative like never before. With this fresh approach to art, there are questions about who should get credit for the artwork. Should it be the artist, the computer software, or the software’s creator, and is the final product really art? For the London man behind one of the pieces that sold for $8,000, the price was an endorsement of artists’ continued value in a world that is increasingly reliant on technology and machines. (One other piece of art at the show also sold for $8,000.) While the creator Memo Akten said that Google has made a better “paintbrush,” the human artist is essential. Not just any image fed into the computer program can sell for so much, he said. Akten began creating his artwork called “GCHQ” with a Google Maps screenshot of GCHQ, the British intelligence and security agency. He input it into a Google program called DeepDream, a website that anyone can use. But Akten went a step further, tweaking the code to produce multiple morphed versions of the Google Maps screenshot that were to his liking. One was covered in eyes, another had web-like detail and a third had a textured look. Then Akten merged those three images using Adobe AfterEffects into a single image, his final product. While the initial version of his art took only a couple hours to make, Akten later spent a couple weeks creating a larger scale version of the art. (The print that was sold was about six feet by three feet.) The inspiration behind the piece was Akten’s belief that tech companies such as Google and government agencies are like modern deities that play the role of religions in previous years. “We have a problem, we ask Google instead of praying,” Akten said. “We’re provided with a false sense of safety. You’re being watched, don’t do that, we’ll find you.” He thinks it’s “quite scary” given the data and artificial intelligence expertise Google has acquired. For him, it was fitting to take a Google product and use it to help make his statement about how omnipresent technology companies are in our lives. And this perspective has resonated with some in the art world. “It’s beautiful and yet does something more than demonstrate computing prowess,” said Peter Patchen, chair of digital arts at the Pratt Institute. “It questions everything from corporate control of culture to changing socio-religious beliefs and taps into our deep distrust of seemingly omnipotent power structures.” Patchen was happy to see digital art being given a value in the thousands, but said there’s still room for growth. (The most coveted classic pieces sell for millions.) As a young, affluent generation of art collectors start to buy works, he expects they’ll appreciate digital techniques, and be willing to pay higher price points. Patchen thinks credit for Atken’s work should be shared between the artist and the algorithm’s creators. Until the algorithm itself becomes a thinking being, he thinks it’s merely a tool, deserving of no credit. For Christiane Paul, the adjunct curator of new media arts at the Whitney Museum of American Art, credit solely lies with the artist. She noted that an artist using Adobe Photoshop to make a piece of art wouldn’t credit the team that had made Photoshop. Paul said artists have long sought new tools to elevate their work, such as creating a new color or finding a new material to work with. “Whether it’s a new paintbrush or pigment or neural network, they are all potentially great tools for creating sophisticated art,” said Paul, who felt Akten’s work was rich in creativity. Others were not as impressed. Emily L. Spratt, an art history PhD candidate at Princeton with a research focus on artificial intelligence, described the artist’s explanation behind his work as an anarchist’s rant, and reflective of our paranoia around machines. Spratt wasn’t sure who ultimately deserved credit for the work, the artist, the software or the team that made the software. She cautioned that the painting’s price tag shouldn’t be seen as an acceptance into the art world, because it was sold at a charity auction with money going to Gray Area, a reputable nonprofit organization that attendees had an interest in.
Still, new techniques such as using artificial intelligence to create art are promising in her view. She mentioned how the emergence of oil paints was a breakthrough for artists centuries ago. Now artificial intelligence creates new possibilities for the creative class. “It’s definitely a venue for more possible creativity. Will it be as simple as uploading a photo into Google DeepDream and then seeing what type of distorted image it will produce?” Spratt said. “I imagine artists will be using this type of technology in a more complicated, nuanced and sophisticated type of way.”"
f390e2e794,CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning,"Chest X-rays are currently the best available method for diagnosing pneumonia, playing a crucial role in clinical care and epidemiological studies. Pneumonia is responsible for more than 1 million hospitalizations and 50,000 deaths per year in the US alone. We train CheXNet on the recently released ChestX-ray14 dataset, which contains 112,120 frontal-view chest X-ray images individually labeled with up to 14 different thoracic diseases, including pneumonia. We use dense connections and batch normalization to make the optimization of such a deep network tractable. The dataset, released by the NIH, contains 112,120 frontal-view X-ray images of 30,805 unique patients, annotated with up to 14 different thoracic pathology labels using NLP methods on radiology reports. We label images that have pneumonia as one of the annotated pathologies as positive examples and label all other images as negative examples for the pneumonia detection task. We collected a test set of 420 frontal chest X-rays. Annotations were obtained independently from four practicing radiologists at Stanford University, who were asked to label all 14 pathologies, even though . We then evaluate the performance of an individual radiologist by using the majority vote of the other 3 radiologists as ground truth. Similarly, we evaluate CheXNet using the majority vote of 3 of 4 radiologists, repeated four times to cover all groups of 3. ChexNet is tested against radiologists on sensitivity (which measures the proportion of positives that are correctly identified as such) and specificity (which measures the proportion of negatives that are correctly identified as such). A single radiologist’s performance is represented by an orange marker, while the average is represented by green. CheXNet outputs the probability of detecting pneumonia in a Chest X-ray, and the blue curve is generated by varying the thresholds used for the classification boundary. The sensitivity-specificity point for each radiologist and for the average lie below the blue curve, signifying that CheXNet is able to detect pneumonia at a level matching or exceeding radiologists."
35642a329b,Google to use AI and machine learning to tackle extremist content on YouTube - SiliconANGLE,"Home » Emerging Tech
by
Duncan Riley
UPDATED 21:14 EST . 18 JUNE 2017
Google Inc. is upping its fight against the spread of extremist content on its YouTube service by rolling out new artificial-intelligence services in conjunction with additional human oversight to stop it faster. The advanced AI platform will use ongoing machine learning research that in theory can automatically flag and remove terrorist videos but at the same time not take down legitimate news reports. The challenge is for the platform to determine which is which. For example, a terrorist organization may post a clip that is then used in part by a legitimate news service such as the BBC, the latter an acceptable presentation of the content. “We have used video analysis models to find and assess more than 50 percent of the terrorism-related content we have removed over the past six months,” Google General Counsel Kent Walker said on the official Google blog. “We will now devote more engineering resources to apply our most advanced machine learning research to train new ‘content classifiers’ to help us more quickly identify and remove extremist and terrorism-related content.” Algorithms that use machine learning and AI will be complemented by a human touch, with YouTube greatly increasing the number of independent experts in its Trusted Flagger program. “Machines can help identify problematic videos, but human experts still play a role in nuanced decisions about the line between violent propaganda and religious or newsworthy speech,” Walker added. In addition to taking extremist videos down, videos described as not clearly violating YouTube’s policies, such as those that contain inflammatory religious or supremacist content, will also be treated more harshly in the future. Those videos will no longer be able to run ads, comments on them will be turned off and they will be made harder to find. The announcement follows criticism by European governments, most notably U.K. Prime Minister Theresa May, that Google, along with Facebook Inc., provide a “safe space” for terrorists. May and her conservative party went into the recent U.K. General Election with a “manifesto” that proposed the creation of a new, censored Internet that would aim to make Britain the “global leader in the regulation of the use of personal data and the internet.” Like Free Content?
Subscribe to follow. LG and JBL unveil smart display speakers with Google Assistant integration Responding to criticism, Facebook makes news feed changes to spur more interaction Red Hat says microservices benefits can be realized in just six months Report claims hackers could start a nuclear war through vulnerable weapons systems Microsoft brings end-to-end encryption to Skype through Signal partnership PC market shows small signs of life as Q4 worldwide shipments grow Microsoft brings end-to-end encryption to Skype through Signal partnership APPS - BY DUNCAN RILEY . 9 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 14 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 18 HOURS AGO Google buys Redux, a startup that uses vibrations to turn phone screens into speakers EMERGING TECH - BY ERIC DAVID . 19 HOURS AGO YouTube decides to punish Logan Paul for his controversial suicide video APPS - BY JAMES FARRELL . 1 DAY AGO Best of CES: Kika Tech's Tami Zhu wants to put emotion into AI BIG DATA - BY MARK ALBERTSON . 2 DAYS AGO Microsoft recruits Qualcomm to help it catch up in the voice assistant market EMERGING TECH - BY MARIA DEUTSCHER . 3 DAYS AGO ObEN raises $10M to build AI-powered copies of real people for hospitality and retail EMERGING TECH - BY ERIC DAVID . 3 DAYS AGO Cisco consultants provide aspirin and vitamins for network health BIG DATA - BY MARK ALBERTSON . 3 DAYS AGO Microsoft brings end-to-end encryption to Skype through Signal partnership APPS - BY DUNCAN RILEY . 9 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 14 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 18 HOURS AGO Google buys Redux, a startup that uses vibrations to turn phone screens into speakers EMERGING TECH - BY ERIC DAVID . 19 HOURS AGO YouTube decides to punish Logan Paul for his controversial suicide video APPS - BY JAMES FARRELL . 1 DAY AGO Best of CES: Kika Tech's Tami Zhu wants to put emotion into AI BIG DATA - BY MARK ALBERTSON . 2 DAYS AGO Microsoft recruits Qualcomm to help it catch up in the voice assistant market EMERGING TECH - BY MARIA DEUTSCHER . 3 DAYS AGO ObEN raises $10M to build AI-powered copies of real people for hospitality and retail EMERGING TECH - BY ERIC DAVID . 3 DAYS AGO Cisco consultants provide aspirin and vitamins for network health BIG DATA - BY MARK ALBERTSON . 3 DAYS AGO Check out more cube videos and stories Analytics feeds machine maker’s global ambitions Modern infrastructure management: accelerating productivity through machine learning Fleet analytics provider spins vehicle sensor data into productivity gold Big data leads a transformation in PC gaming For personalised content stay with us. Forgot Password? Like Free Content? Subscribe to follow."
9c43559360,European Commission : CORDIS : News and Events : Making road traffic greener by boosting car-sharing and improving road quality,"© Ni_racha, Shutterstock
Last updated on: 2017-11-15
Record Number: 128663
Share this page"
4c78de8063,The new algorithms enabling Facebook’s data fixation  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Two billion photos find their way onto Facebook’s family of apps every single day and the company is racing to understand them and their moving counterparts with the hope of increasing engagement. And while machine learning is undoubtedly the map to the treasure, Facebook and its competitors are still trying to work out how to deal with the spoils once they find them. Facebook AI Similarity Search (FAISS), released as an open-source library last month, began as an internal research project to address bottlenecks slowing the process of identifying similar content once a user’s preferences are understood. Under the leadership of Yann LeCun, Facebook’s AI Research (FAIR) lab is making it possible for everyone to more quickly relate needles within a haystack. On its own, training a machine learning model is already an incredibly intensive computational process. But a funny thing happens when machine learning models comb over videos, pictures and text — new information gets created! FAISS is able to efficiently search across billions of dimensions of data to identify similar content. In an interview with TechCrunch, Jeff Johnson, one of the three FAIR researchers working on the project, emphasized that FAISS isn’t so much a fundamental AI advancement as it is a fundamental AI-enabling technique. Imagine you wanted to perform object recognition on a public video that a user shared to understand its contents so you could serve up a relevant ad. First you’d have to train and run that algorithm on the video, coming up with a bunch of new data. From that, let’s say you discover that your target user is a big fan of trucks, the outdoors and adventure. This is helpful, but it’s still hard to say what advertisement you should display — a rugged tent? An ATV? A Ford F-150? To figure this out, you would want to create a vector representation of the video you analyzed and compare it to your corpus of advertisements with the intent of finding the most similar video. This process would require a similarity search, whereby vectors are compared in multi-dimensional space. In this representation of a similarity search, the blue vector is the query. The distance between the “arrows” reflects their relative similarity. In real life, the property of being an adventurous outdoorsy fan of trucks could constitute hundreds or even thousands of dimensions of information. Multiply this by the number of different videos you’re searching across and you can see why the library you implement for similarity search is important. “At Facebook we have massive amounts of computing power and data and the question is how we can best take advantage of that by combining old and new techniques,” posited Johnson. Facebook reports that implementing k-nearest neighbor across GPUs resulted in an 8.5x improvement in processing time. Within the previously explained vector space, nearest neighbor algorithms let us identify the most closely related vectors. More efficient similarity search opens up possibilities for recommendation engines and personal assistants alike. Facebook M, its own intelligent assistant, relies on having humans in the loop to assist users. Facebook considers “M” to be a test bed to experiment with the relationship between humans and AI. LeCun noted that there are a number of domains within M where FAISS could be useful. “An intelligent virtual assistant looking for an answer would need to look through a very long list,” LeCun explained to me. “Finding nearest neighbors is a very important functionality.” Improved similarity search could support memory networks to help keep track of context and basic factual knowledge, LeCun continued. Short-term memory contrasts with learned skills like finding the optimal solution to a puzzle. In the future, a machine might be able to watch a video or read a story and then answer critical follow-up questions about it. More broadly, FAISS could support more dynamic content on the platform. LeCun noted that news and memes change every day and better methods of searching content could drive better user experiences. Two billion new photos a day presents Facebook with a billion and a half opportunities to better understand its users. Each and every fleeting chance at boosting engagement is dependent on being able to quickly and accurately sift through content and that means more than just tethering GPUs. Latest headlines delivered to you daily"
670ef225d7,Machine Learning – the new catalyst in higher education,"Who would have thought that the stories around self-driven cars could actually come true, so much so that machine learning algorithms can enable computers to communicate with humans, drive cars, play games and do things human cannot do. Machine Learning with its mathematical algorithms and scientific innovations have become a huge part of our lives. For example, when Google auto-corrects a misspelled word, it applies probability algorithm, an action performed using Machine Learning, which compares the database of the previous searches done by millions of other users and predicts the word we intend to use.
With the ever-increasing knowledge in science and technology, machine learning is not far behind to be the new switchboard for Higher Education, personalising education at all levels. It reads and identifies the data patterns to inform algorithms that can make data-driven predictions and decisions. The more data we feed in the computers, the smarter the algorithms become, tapping into the diverse areas of statistical pattern recognition. Today, Machine Learning plays a vital role in the education space, improving formal and informal curriculum in many ways. Educators have witnessed that the integration of Machine Learning has made education and teaching efficient on both ends, synchronising teachers and students allowing customisable learning experience. Hence, Higher Education institutions should take advantage of the possibilities that Machine Learning brings on the table and succeed at all levels.
Rising concerns for privacy argued that data was key to personalised education and that access should be role-based, requiring policymakers to embrace personalisation and privacy. Machine Learning helps institutions use analytics to identify a user profile and provide them with the required access they need to login to multiple systems depending upon their roles. In case of any suspicious activity or unauthorised access, it triggers additional factors in the authentication process to make sure that no cyber-breach takes place.
It is important for Higher Education institutions to analyse past data in order to improve student success and institution efficiency. Machine Learning studies student patterns to understand the level of risk which helps build predictive models of attrition drivers, to help institutions reduce churn and increase student retention rate. It also helps institutions with learning algorithms which studies the historical data and similar kind of predictions to resolve queries and improve the classification of requests and route issues with greater ease.
Analytics help institutions unlock historical data, to assist them answer real tactical questions for making informed decisions. Based on this data, the institutions can determine the risk, outcomes and costs associated with building new processes and making decisions that can drive engagement. Machine Learning offers the institutions with an intuitive dashboard to track trend analysis, key performance indicators and improve student retention.
It is the perfect time for institutions to analyse and examine the student performance in order to help improve and provide them with a better studying environment. Machine learning can predict a student’s future performance based on the calculations on their ongoing academic records. Historical data helps institutions to analyse and monitor student progress. This also gives them a heads up to address any issues and challenges that a student may face or make sure that they receive all the help they need.
The whole enrollment process can be heavy for students. Machine learning studies the student’s behavior, starting from the day they apply to a university to the day they sign for courses, analysing their rationale behind the choices made. Universities mostly use two predictive algorithms of Machine Learning, linear and logistic regression, picking up on certain behaviors which emerge during the admission cycle, with the student’s academic record and the recommendations, making the whole enrollment process seamless for the universities. Education is growing faster than ever. In order for the students to grow at the same rate, it is important for the education industry to adopt a fully engaged environment. Machine Learning can help the institutions build statistical models of student knowledge through learning analytics and algorithms. Now is the time for Higher Education to step into Machine Learning as an important business function and transform their operational output.  (Disclaimer: The views and opinions expressed in this article are those of the author and do not necessarily reflect the views of YourStory.) Social Built with love in India Report an issue Copyright 2018 YourStory Media Pvt. Ltd."
f58d901118,AI beats the world's best gamers after just two weeks of learning,"Rich Haridy 2 pictures A bot has beaten the world's top Dota2 players during The International, a giant eSports tournament with a prize pool of over US$20 million dollars(Credit: Jakob Wells / Flickr CC BY 2.0) The past year has proven to be a landmark in artificial intelligence research. We have seen several big breakthroughs in AI development that were years in the making, from finally decimating human competitors in the incredibly difficult game of Go, to cracking the complexity of poker and snatching a heap of money from the professionals.
Games have turned out to be an important training ground for artificial intelligence. The complex, and dynamic, problems that surface while playing a game often require solutions that can't easily be ""programmed"". Elon Musk's OpenAI research company has long had an emphasis on reinforcement learning, a type of machine learning where a system improves the quality of its actions independently through trial and error. The company has recently unleashed its latest bot on The International, a giant eSports tournament focusing on the game Dota 2. OpenAI used Dota 2 as a test project for its machine learning systems due to the complexity and interactivity of the game. Dota 2 requires players to plan, trick, and deceive opponents with a great deal of sophistication. ""The rules of Dota are so complicated that if you just think really hard about how the game works and try to write those rules down, you're not even gonna be able to reach the performance of a reasonable player,"" says Greg Brockman from OpenAI. So the team set the bot to teach itself how to play the game through self-play. The system learned to conquer the game from scratch by playing a mirror of itself. After just two weeks of learning, the bot beat several of the world's top Dota 2 players including ""Dendi"", a professional regarded as one of the most creative and unorthodox players on the scene. At this stage the bot only plays in the more simplistic one-to-one version of Dota 2. The full, and exponentially more complex, version is played by two teams of five. The OpenAI team are now working on teaching teams of bots to play this complete version, aiming to unleash the AI on The International again next year.
""Beyond that we want to start mixing together AIs and human players on a single team and try to reach a level of performance that neither of them could reach on their own,"" says Brockman. These kinds of AI experiments are more than just simple novelty. They allow researchers the ability to further refine the machine learning algorithms that will be key to functional AI in the future. The better the machines are at learning on the fly, the better they will be able to function in the real world when confronted with unique or anomalous circumstances. Take a look at the OpenAI Dota 2 experiment in the video below. Source: OpenAI 1 / 2
A bot has beaten the world's top <em>Dota2</em> players during The International, a giant eSports tournament with a prize pool of over US$20 million dollars
(Credit: Jakob Wells / Flickr <a href=""http://creativecommons.org/licenses/by/2.0"" rel=""nofollow"">CC BY 2.0</a>)
Elon Musk / Twitter
(Credit: Twitter)
Tags Share this article See the stories that matter in your inbox every morning
See the stories that matter in your inbox every morning Copyright © Gizmag Pty Ltd 2018"
3f46ceea8c,A New Algorithm Can Spot Pneumonia Better Than a Radiologist - MIT Technology Review,"Click search or press enter Add diagnosing dangerous lung diseases to the growing list of things artificial intelligence can do better than humans. A new arXiv paper by researchers from Stanford explains how CheXNet, the convolutional neural network they developed, achieved the feat. CheXNet was trained on a publicly available data set of more than 100,000 chest x-rays that were annotated with information on 14 different diseases that turn up in the images. The researchers had four radiologists go through a test set of x-rays and make diagnoses, which were compared with diagnoses performed by CheXNet. Not only did CheXNet beat radiologists at spotting pneumonia, but once the algorithm was expanded, it proved better at identifying the other 13 diseases as well. Early detection of pneumonia could help prevent some of the 50,000 deaths the disease causes in the U.S. each year. Pneumonia is also the single largest infectious cause of death for children worldwide, killing almost a million children under the age of five in 2015. Andrew Ng, a coauthor of the paper and the former head of AI research at Baidu, thinks AI is going to be relied upon in medicine more and more. He previously worked on an algorithm that can, after being trained on electrocardiogram (ECG) data, identify heart arrhythmias better than a human expert. Another deep-learning algorithm recently published in Nature was able to spot cancerous skin lesions just as well as a board-certified dermatologist. Radiologists in particular have been on notice for a while. Previous research has shown that AI is as good as or better than doctors at spotting problems in CT scans. Geoffrey Hinton, one of the pioneers of deep learning, told the New Yorker that because of the advances in AI, medical schools “should stop training radiologists now.” Analyzing image-based data sets like x-rays, CT scans, and medical photos is what deep-learning algorithms excel at. And they could very well save lives. Posted by Jackie Snow The famed startup incubator Y Combinator put out a call for companies that want to increase human longevity and “health span.” Who they want: Founders with new ideas for treating old-age diseases like Alzheimer’s, “but we will also consider more radical… Read more The famed startup incubator Y Combinator put out a call for companies that want to increase human longevity and “health span.” Who they want: Founders with new ideas for treating old-age diseases like Alzheimer’s, “but we will also consider more radical anti-aging schemes,” YC president Sam Altman told MIT Technology Review.  Why longevity? Efforts to stop old age don’t actually get funded much. “My sense is that economic incentives of drugs companies are screwed up” says Altman. “I don’t think we have enough people saying, How can we make a lot of people a lot heathier?”  The deal: Longevity startups get $500,000 to $1 million in exchange for a 20 percent stake, lab space, and advice (quite a bit more than the $120,000 software companies get if they’re picked for the incubator. But hey, biology is more expensive). They will travel to San Francisco to join the next three-month “class"" of startups, which starts in June. Big bio returns? Y Combinator started accepting biology companies in 2014. One of them, Ginkgo, has gone on to raise hundreds of millions more to fund its organism-hacking business. As for others, Altman says he’s not worried about getting his money back: “You could make a pill that added two years to a person’s life that would be a $100 billion company.” Posted by Antonio Regalado The government of South Korea is considering a ban on cryptocurrency trading, but it’s by no means guaranteed to come into effect. The news: The nation’s justice minister revealed that the government is preparing legislation that would ban cryptocurrency… Read more The government of South Korea is considering a ban on cryptocurrency trading, but it’s by no means guaranteed to come into effect. The news: The nation’s justice minister revealed that the government is preparing legislation that would ban cryptocurrency trading on domestic exchanges, citing “great concerns” about the technology. There are also reports that government officials raided the offices of at least two popular exchanges this week. Why it matters: Digital-currency trading has become immensely popular in South Korea, and it’s now the world’s third-largest cryptocurrency trading market. The crackdown appears to have spooked investors and may have played a role in the latest drop in the price of Bitcoin and other cryptocurrencies. But: A ban is far from a done deal. Once the bill is written, it will require a majority vote of the National Assembly, which means it could take “months or even years” to pass, if it ever does, according to Reuters. Posted by Mike Orcutt Many companies let workers monitor and manage machines—and sometimes entire industrial processes—via mobile apps. The apps promise efficiency gains, but they also create targets for cyberattacks. At worst, hackers could exploit the flaws to destroy machines—and... Many companies let workers monitor and manage machines—and sometimes entire industrial processes—via mobile apps. The apps promise efficiency gains, but they also create targets for cyberattacks. At worst, hackers could exploit the flaws to destroy machines—and potentially entire factories. Posted by Martin Giles The U.S. House of Representatives has voted to maintain the National Security Agency’s warrantless Internet surveillance program. What happened: Section 702 of the Foreign Intelligence Surveillance Act, which allows the NSA to collect electronic communications… Read more The U.S. House of Representatives has voted to maintain the National Security Agency’s warrantless Internet surveillance program. What happened: Section 702 of the Foreign Intelligence Surveillance Act, which allows the NSA to collect electronic communications of “non-U.S. persons” who are “reasonably believed” to be outside the country, is due to expire next week. The House voted to approve its renewal for six years. But: Senate approval and a Trump signature are both required before the deadline for it to remain law. The case for: Reuters says that the White House and intelligence agencies consider the tool “indispensable” for surveillance. The case against: Privacy advocates argue that when the NSA spies on foreigners it believes to be overseas, it also gathers information about Americans (see “Scrutiny Intensifies on the Warrantless Collection of Americans’ Communications”). An amendment to address that problem failed to pass in the House. Posted by Jamie Condliffe
The Download Copious VC funding and generous government support are tempting Chinese nationals and foreign students to work or learn in China. Homeward bound: Chinese nationals who studied overseas, many of whom went through college in the U.S., are heading back… Read more Copious VC funding and generous government support are tempting Chinese nationals and foreign students to work or learn in China. Homeward bound: Chinese nationals who studied overseas, many of whom went through college in the U.S., are heading back to China. Limits to career advancement in the West often seem to motivate the move. Attracting new talent: China is now the third most popular destination for people choosing to study overseas. The majority of students head to China from South Korea, with the U.S. being the next biggest source. According to Axios, China will overtake the U.K. and become the second most popular country later this year. Incentives on offer: A report by CB Insights says Asia is close to surpassing North America in VC funding—raising $70.8 billion in 2017. versus North America’s $74 billion. That, combined with the Chinese government’s support of emerging technologies, makes it an attractive base for young tech stars. Want to stay up to date on all things future of work? Sign up for our new newsletter, Clocking In, launching later this month. Posted by Erin Winick In 2015, Google drew criticism when its Photos image recognition system mislabeled a black woman as a gorilla—but two years on, the problem still isn’t properly fixed. Instead, Google has censored image tags relating to many primates. What’s new: Wired… Read more In 2015, Google drew criticism when its Photos image recognition system mislabeled a black woman as a gorilla—but two years on, the problem still isn’t properly fixed. Instead, Google has censored image tags relating to many primates. What’s new: Wired tested Google Photos again with a bunch of animal photos. The software could identify creatures from pandas to poodles with ease. But images of gorillas, chimps, and chimpanzees? They were never labeled. Wired confirmed with Google that those tags are censored. But: Some of Google’s other computer vision systems, such as Cloud Vision, were able to correctly tag photos of gorillas and provide answers to users. That suggests the tag removal is a platform-specific shame-faced PR move. Bigger than censorship: Human bias exists in data sets everywhere, reflecting the facets of humanity we’d rather not have machines learn. But reducing and removing that bias will take a lot more work than simply blacklisting labels. Posted by Jackie Snow Boeing has unveiled a new drone that can carry 500 pounds of cargo. That’s enough to comfortably carry things like domestic appliances. Vital stats: Boeing explains that the cargo air vehicle (or CAV) uses batteries, electric motors, and eight propeller… Read more Boeing has unveiled a new drone that can carry 500 pounds of cargo. That’s enough to comfortably carry things like domestic appliances. Vital stats: Boeing explains that the cargo air vehicle (or CAV) uses batteries, electric motors, and eight propeller blades to take flight. It measures 15 feet long and 18 feet wide, and weighs 747 pounds. It’s successfully completed test flights at Boeing’s research labs. What we don’t know: So far, Boeing hasn’t said anything about flight performance, so it’s unclear how high or for how long the aircraft can fly. A 500-pound load is sure to put a major dent in flight time. What we want to know: Does it mean Amazon will fly in your next refrigerator? Posted by Jamie Condliffe Not all cryptocurrencies are created equal. Don’t tell that to investors in XRP, though. In the last month the currency owned by Ripple, a company that bills itself as using blockchain technology to build the payment system of the future, soared in price... Not all cryptocurrencies are created equal. Don’t tell that to investors in XRP, though. In the last month the currency owned by Ripple, a company that bills itself as using blockchain technology to build the payment system of the future, soared in price by a whopping 700 percent. XRP’s overall value pushed up to nearly $150 billion and briefly made Chris Larsen, Ripple’s cofounder, one of the richest people on the planet. Posted by Mike Orcutt New York City is suing five of the world's biggest oil firms in a bid to cover expensive preparations for rising sea levels and extreme weather. BP, Exxon Mobil, Chevron, ConocoPhillips, and Shell are all being taken to court by the city, reports the… Read more New York City is suing five of the world's biggest oil firms in a bid to cover expensive preparations for rising sea levels and extreme weather. BP, Exxon Mobil, Chevron, ConocoPhillips, and Shell are all being taken to court by the city, reports the Guardian. The aim: Court documents explain that NYC wants to ""shift the costs of protecting the city ... back on to the companies that have done nearly all they could to create this existential threat."" NYC stands to be hit hard by climate change, being home to masses of expensive but low-lying real estate. Its case: The city will argue that fossil-fuel producers are responsible for the majority of greenhouse-gas emissions since the Industrial Revolution, and that they knew about—but downplayed—the impact they would have on the climate. The kicker: NYC will also pull $5 billion of pension fund investment from fossil-fuel firms. Posted by Jamie Condliffe New research suggests that the labor-saving practice of robotic surgery is making it difficult for junior doctors to learn how to perform operations. Rise of the robo-surgeon: Surgical robots are used by over one-third of U.S. hospitals. Over the past… Read more New research suggests that the labor-saving practice of robotic surgery is making it difficult for junior doctors to learn how to perform operations. Rise of the robo-surgeon: Surgical robots are used by over one-third of U.S. hospitals. Over the past decade, the robots have been used for more and more types of surgery. The impact on training: The robots mean less human work is required in the operating room. According to a paper published Tuesday in Administrative Science Quarterly, that takes away training opportunities from new doctors going through their residencies. In the past, younger doctors had “hands in patient” roles that were useful to senior surgeons. Now they’re superfluous: the robot performs many of their previous tasks. Surgeons without skills: The study found that senior surgeons often took over from younger medics during training, so they didn’t get enough experience to learn. After completing their residencies, young doctors were not adequately prepared to complete robotic surgeries alone. Want to stay up to date on all things future of work? Sign up for our new newsletter, Clocking In, launching later this month! Posted by Erin Winick Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
4e72ab0471,A Machine Learning Approach to Identifying the Thought Markers of Suicidal Subjects: A Prospective Multicenter Trial - Pestian - 2016 - Suicide and Life-Threatening Behavior - Wiley Online Library,"By continuing to browse this site you agree to us using cookies as described in About Cookies
Navigate this article
Previous article in issue:
The Role of Sleep Disturbance in Suicidal and Nonsuicidal Self-Injurious Behavior among Adolescents
View
issue TOC
Volume 47, Issue 1
February 2017
Pages 112–121
Address correspondence to John P. Pestian, Cincinnati Children's Hospital Medical Center, University of Cincinnati, 3333 Burnet Ave., Cincinnati, OH 45229; E-mail: john.pestian@cchmc.org Death by suicide demonstrates profound personal suffering and societal failure. While basic sciences provide the opportunity to understand biological markers related to suicide, computer science provides opportunities to understand suicide thought markers. In this novel prospective, multimodal, multicenter, mixed demographic study, we used machine learning to measure and fuse two classes of suicidal thought markers: verbal and nonverbal. Machine learning algorithms were used with the subjects’ words and vocal characteristics to classify 379 subjects recruited from two academic medical centers and a rural community hospital into one of three groups: suicidal, mentally ill but not suicidal, or controls. By combining linguistic and acoustic characteristics, subjects could be classified into one of the three groups with up to 85% accuracy. The results provide insight into how advanced technology can be used for suicide assessment and prevention. Predicting when someone will commit suicide has been nearly impossible (American Psychiatric Association, 2003; Goldstein, Black, Nasrallah, & Winokur, 1991; Hughes, 1995; Large & Ryan, 2014; Olav Nielssen, 2012; Paris, 2006), but classifying the factors that contribute to suicide risk is possible with standardized, clinical tools when used by well-trained clinicians (Beck, Beck, & Kovacs, 1975; Beck, Kovacs, & Weissman, 1979; Bürk, Kurz, & Möller, 1985; Columbia-Suicide Severity Rating Scale, 2015; Kovacs & Garrison, 1985; Müller & Dragicevic, 2003; Mundt, Greist, & Jefferson, 2013; Pokorny, 1983; Posner et al., 2008; Preston & Hansen, 2005; U.S. Food & Drug Administration, 2012; Winters, Myers, & Proud, 2002). Such tools can, however, be cumbersome and may not reliably translate into routine interactions between clinicians, caregivers, or educators. Here, we describe a novel approach using the subjects’ linguistic and acoustic patterns to classify subjects automatically as either suicidal, mentally ill but not suicidal, or a control. Efforts to understand suicide risks can be roughly clustered into traits or states. Trait analyses focus on stable characteristics rooted in and measured using biological processes (Costanza et al., 2014; Le-Niculescu et al., 2013), whereas state analyses measure dynamic characteristics like verbal and nonverbal communication, termed “thought markers” (Pestian et al., 2015). Machine learning and natural language processing have successfully identified differences in retrospective suicide notes, newsgroups, and social media (Gomez, 2014; Huang, Goh, & Liew, 2007; Matykiewicz, Duch, & Pestian, 2009). Jashinsky et al. (2015) used multiple annotators to identify the risk of suicide from the keywords and phrases (interrater reliability = .79) in geographically based tweets. Thompson, Poulin, and Bryan (2014) and Desmet (2014) used text-based signals to identify suicide risk that ranged from 60% to 90%. Li, Ng, Chau, Wong, and Yip (2013) presented a framework using machine learning to identify individuals expressing suicidal thoughts in web forums; Zhang et al. (2015) used microblog data to build machine learning models that identified suicidal bloggers with approximately 90% accuracy. Pestian, Matykiewicz, and Grupp-Phelan (2008) demonstrated that machine learning algorithms could distinguish between notes written by people who died by suicide and simulated suicide notes written by age- and gender-matched controls better than mental health professionals could (71% vs. 79%; Pestian et al., 2008). In an international, shared task-setting that included multiple groups sharing the same task definition, data set, and scoring metric (Voorhees et al., 2005), 24 teams developed and tested computational algorithms to identify emotions in over 1,319 suicide notes written shortly before death. The results showed that the fusion of multiple methods outperform single methods (Pestian, Matykiewicz, & Linn-Gust, 2012). Suicidal thought markers have also been studied prospectively. The Suicidal Adolescent Clinical Trial (Pestian et al., 2015), the single-site precursor to this study, which used machine learning to analyze interviews with 60 suicidal and control patients, classified patients into suicidal or control groups with greater than 90% accuracy (Pestian et al., 2015). Analysis of acoustic features such as pauses and vowel spacing yielded similar results (Scherer, Morency, Gratch, Pestian, & Playa Vista, 2015; Venek, Scherer, Morency, Rizzo, & Pestian, 2014). The study described herein is novel because it uses a multisite, multicultural setting to show that machine learning algorithms can be trained to automatically identify the suicidal subjects in a group of suicidal, mentally ill, and control subjects. Moreover, the inclusion of acoustic characteristics is most helpful when classifying between suicidal and mentally ill subjects. Between October 2013 and March 2015, 379 subjects were enrolled from emergency departments (EDs) and inpatient and outpatient centers into a three-site, internal review board-approved prospective clinical trial. One hundred twenty-six subjects were enrolled at Cincinnati Children's Hospital Medical Center (CCHMC), a 600-bed urban level 1 academic medical center. One hundred twenty-eight subjects were enrolled at the University of Cincinnati Medical Center (UC), a 498-bed urban academic medical center. One hundred twenty-five subjects were enrolled at Princeton Community Hospital (PCH), a 267-bed Appalachian community hospital in southern West Virginia. The inclusion and exclusion criteria classified subjects into one of three groups: suicidal, mentally ill, or control. Multi-gated inclusion criteria were used. For the first gate, all patients were reviewed using the electronic status board for a complaint of suicide, suicidal ideation, or psychiatric evaluation. For patients with mental illness and for control subjects, any complaint was accepted except those related to suicide. For those who passed the first gate, a second review of their electronic medical record (EMR) was conducted before they were approached for enrollment. Suicidal subjects were approached if they had come to the EDs or psychiatric units because of suicidal ideation or attempts within the previous 24 hours. Patients with mental illness were enrolled from the ED and outpatient mental health clinics if they had a definitive mental illness diagnosis but had not had prior suicidal attempts, active thoughts of suicide, or plans to die by suicide within the previous year as reported by the patient and EMR. Control subjects were patients who came to the ED with no history of mental health diagnoses or suicidal ideation, as reported by the patient and EMR (Figure 1). Receiver operator curve (ROC): suicide versus control (upper), suicide versus mentally ill (middle), and suicide versus mentally ill with control. The ROC curves for adolescents (blue), adults (red), and all subjects (black) generated where the nonsuicidal population is controls (top), mentally ill (middle), and mentally ill and controls, using linguistic and acoustic features. The gray line is the AROC curve for a baseline (random) classifier. Potential subjects were excluded if their native language was not English, if they had any serious medical injury or mental retardation that could prohibit consent, if they would be unavailable for a follow-up interview, or if they could not comply with study procedures. Participation incentives were site-specific. CCHMC and UC subjects were paid $50 for the initial interview and $25 for the follow-up. PCH subjects were paid $25 for the initial interview and $25 for the follow-up interview. Data were collected and validated by trained mental health professionals. During enrollment, each subject completed standardized tools: Columbia-Suicide Severity Rating Scale, Young Mania Rating Scale, and Hamilton Rating Scale for Depression. Each subject also completed the ubiquitous questionnaire (UQ), a semistructured interview with five open-ended questions to stimulate conversation for language sampling: “Do you have hope?” “Do you have any fear?” “Do you have any secrets?” “Are you angry?” and “Does it hurt emotionally?” (Pestian, 2010; Pestian et al., 2015). Both subject and interviewer were video and audio recorded during the UQs. The results were transcribed with 98% accuracy based on completeness, accuracy of transcription, and adherence to the transcription guidelines. Our analysis is based on two types of features: linguistic and acoustic. Feature extraction was performed automatically using the patient audio signals recorded during the interviews and their transcriptions. Linguistically, we followed related work on automatic identification and extraction of word instances (unigrams) and word-pair instances (bi-grams) from the transcriptions. A dictionary that includes all spoken words, word-pairs, and acoustic characteristics was created. The selected vocal and prosodic characteristics include: vocal dynamics—fundamental frequency (f0; Drugman & Alwan, 2011) and square of the amplitude (A2); voice quality–harmonic richness factor (Childers & Lee, 1991), maximum dispersion quotient (Kane, 2012), peak slope (D'Alessandro & Sturmel, 2011), difference between the first and second harmonics (Hillenbrand, Cleveland, & Erickson, 1994), normalized amplitude quotient (Alku, Bäckström, & Vilkman, 2002), quasi-open quotient (Hacki, 1989), and parabolic spectral parameters (Alku, Strik, & Vilkman, 1997); the vocal tract resonance frequencies, as well as pause lengths characterized by formants F1 through F5 (Kwon, Chan, Hao, & Lee, 2003); and pause lengths that have been correlated with depression (Cummins et al., 2015). Once a subset of all features were extracted using the COVAREP software (Degottex, Kane, Drugman, Raitio, & Scherer, 2014), they were normalized by adjusting the measured values from the various features to a common zero-to-one scale (Dodge, 2006). A goal of machine learning is to train a computational model from selected data that can then generalize to unseen (test) data. Machine learning can be roughly divided into three types: supervised learning, when the training data are already labelled; semi-supervised learning, when only part of the training set is labelled; and unsupervised learning, when the challenge is to learn structure in unlabelled data. Here, a supervised learning support vector machine (SVM) approach was used (Schölkopf & Smola, 1998). Support vector machines are based on a computational learning theory called structural risk minimization, whose goal is to find a hypothesis with the lowest true error (Vapnik, 1999). The SVM constructs a hyperplane in a high-dimensional space, which can be used for classification, regression, or other tasks (Press, Teukolsky, Vetterling, & Flannery, 2007). The SVM is appropriate for this study's data because it can be used on multiple class problems, and because its connection to computational learning enables it to be a universal learner (Joachims, 1998) while in support of conceptual frameworks, such as spreading activation. Consequently, it tends to be fairly robust to overfitting (Sebastiani, 2002). The performance of the classifier was based on the area under the receiver operating curve, which was estimated using leave-one-out by subject cross-validation (Efron & Tibshirani, 1997; Molinaro, Simon, & Pfeiffer, 2005). Data were analyzed by members of the study team. A total of 955 patients were approached for enrollment. Of that group, 576 subjects did not meet the inclusion criteria (n = 70), refused to participate (n = 436), or were excluded for other reasons (n = 70). This resulted in 379 subjects enrolled, comprising 130 suicidal patients, 126 nonsuicidal patients with mental illness, and 123 controls. Eight subject interviews were incomplete and were excluded from the final analysis. A total of 371 subjects completed the study. The patient demographic characteristics within each study site are shown in Table 1. Figure 1 and Table 2 show the performance of the machine learning algorithm in classifying subjects into suicidal and nonsuicidal subject groups. Classification performances are shown for adolescents, adults, and the combined adolescent and adult cohort. The table shows that the ROC threshold of 0.80 is met in all cases, except adults in the adult suicide versus mentally ill comparison. When these data are combined with adolescent data, however, the threshold is met. The table also shows that the signal from acoustic characteristics boosts the suicidal versus mentally ill classification. Overall, the results show that machine learning algorithms can be trained to automatically identify the suicidal subjects in a group of suicidal, mentally ill, and control subjects. Moreover, the inclusion of acoustic characteristics is most helpful when classifying between suicidal and mentally ill subjects. We anticipated that when computation methods are applied in multiple sites, their overall accuracy decreases. This decrease could be reduced by including a second measurement mode, such as acoustic data. Although there was a decrease from our initial study (Pestian et al., 2015), the decrease was not substantial. Moreover, the acoustic features did not play a substantial role in the initial study interview. Subsequent research, however, has shown that in some cases the acoustic features are statistically important during follow-up visits (Venek, Scherer, Morency, Rizzo, & Pestian, 2016; Venek et al., 2014). From the results of other studies, this was unexpected. Suggesting that additional research devoted to fusing nonverbal sentiment data with verbal sentiment data is still needed. Clinicians may ask: Were there any differences in what the subjects said? Table 3 shows that the mentally ill and the control patients tended to laugh more during interviews, sigh less, and express less anger, less emotional pain, and more hope. The study's sample was based on the subjects’ self-reports, which means that some patients could have been disingenuous. But, using measures of authenticity, no differences were found between each of the groups (Newman, Pennebaker, Berry, & Richards, 2003). This study's methodology presents strong evidence for a useful objective tool that clinicians and others can use to determine suicidal intention. These computational approaches may provide novel opportunities for large-scale innovations in suicidal care. The methodology described here can be readily translated to such settings as school, shelters, youth clubs, juvenile justice centers, and community centers, where earlier identification may help to reduce suicide attempts and deaths. © 2016 The American Association of Suicidology
Request Permissions
Please note: Wiley-Blackwell is not responsible for the content or functionality of any supporting information supplied by the authors. Any queries (other than missing content) should be directed to the corresponding author for the article. Please enable Javascript to view the related content of this article. Powered by Wiley Online Library Copyright © 1999 - 2018 John Wiley & Sons, Inc. All Rights Reserved"
a2f11141bb,Machine learning identifies suicidal youth | National Institutes of Health (NIH),"More » Search Health Topics Quick Links More » Search the NIH Guide Quick Links More » Quick Links More » Quick Links More » Quick Links More » Quick Links November 7, 2017 Suicide is a major public health concern. Over 40,000 people nationwide die by suicide each year. Suicide was the second leading cause of death in 2015 for young adults aged 18 to 26. Knowing who is at risk for suicide may help reduce the suicide rate. Scientists have been testing several approaches to try to predict suicide risk. A research team led by Dr. Marcel Just at Carnegie Mellon University and Dr. David Brent at the University of Pittsburgh used fMRI to look for signatures of brain activity among young adults with suicidal thoughts. They focused on the networks of activity that represent concepts and the emotions they evoke. Their study was funded by NIH’s National Institute of Mental Health (NIMH). It appeared online in Nature Human Behaviour on October 30, 2017. The team recruited 38 young adults with current suicidal thoughts and 41 healthy controls with no history of a psychiatric disorder or suicide attempt. While in an fMRI scanner, the participants were shown 30 words for 3 seconds each related to suicide (e.g., “death,” funeral,” hopeless”), positive ideas (“bliss,” “carefree,” “comfort”), and negative ideas (“boredom, “gloom,” “worried”). The scientists used data from 33 participants to train a machine-learning system to spot differences in networks of brain activity. They then tested it on brain images from 34 participants. The system correctly identified 15 of 17 suicidal people and 16 of 17 controls—an accuracy of 91%. The strongest differences between the groups, in order, were for the words “death”, “carefree,” “good,” “cruelty,” “praise,” and “trouble.” Within the group of 17 people with suicidal thoughts, the system was also able to distinguish the 9 who had previously made a suicide attempt from the 8 who hadn’t with an accuracy of 94% (16 out of 17). Discriminating regions were spread across several areas of the brain. The researchers also tested the approach on brain images from 21 people with suicidal thoughts who had lower quality scan data. The system could still distinguish these suicidal people from control participants with 87% accuracy. Previous studies have identified signatures of brain activity with different emotions. The researchers found that the neural signatures of the emotions sadness, shame, anger, and pride differed between people with suicidal thoughts and healthy controls for the six discriminating words. Among the 34 participants, a machine-learning system could distinguish between the groups based on emotional signatures with an accuracy of 85%. “People with suicidal thoughts experience different emotions when they think about some of the test concepts,” Just explains. “For example, the concept of ‘death’ evoked more shame and more sadness in the group that thought about suicide. This extra bit of understanding may suggest an avenue to treatment that attempts to change the emotional response to certain concepts.” “Further testing of this approach in a larger sample will determine its generalizability and its ability to predict future suicidal behavior, and could give clinicians in the future a way to identify, monitor, and perhaps intervene with the altered and often distorted thinking that so often characterizes seriously suicidal individuals,” Brent says. The researchers are also investigating other methods that correlate with the fMRI signatures but may be easier to implement in clinical settings. —by Harrison Wein, Ph.D. References: Machine learning of neural representations of suicide and emotion concepts identifies suicidal youth. Marcel Adam Just, Lisa Pan, Vladimir L. Cherkassky, Dana L. McMakin, Christine Cha, Matthew K. Nock & David Brent. Nature Human Behaviour. October 30, 2017. doi:10.1038/s41562-017-0234-y. Funding: NIH’s National Institute of Mental Health (NIMH). Machine learning identifies suicidal youth Brain activity ripples linked to creation of long-term memories CRISPR technology adapted to edit RNA
Subscribe to get NIH Research Matters by email Mailing Address:
NIH Research Matters
Bldg. 31, Rm. 5B52, MSC 2094
Bethesda, MD 20892-2094 Changing gut bacteria in Crohn’s disease Unexplained cases of anaphylaxis linked to red meat allergy Dietary fats influence endoplasmic reticulum membrane
Physical activity key to keeping weight off 2017 Research Highlights — Promising Medical Advances Editor: Harrison Wein, Ph.D. Assistant Editors: Tianna Hicklin, Ph.D., Geriann Piazza NIH Research Matters is a weekly update of NIH research highlights reviewed by NIH’s experts. It's published by the Office of Communications and Public Liaison in the NIH Office of the Director. ISSN 2375-9593 NIH…Turning Discovery Into Health® National Institutes of Health, 9000 Rockville Pike, Bethesda, Maryland 20892 U.S. Department of Health and Human Services"
156facc3f9,Can Facebook's Machine-Learning Algorithms Accurately Predict Suicide? - Scientific American,"We use cookies to provide you with a better onsite experience. By continuing to browse the site you are agreeing to our use of cookies in accordance with our Cookie Policy. The social media giant aims to save lives by quickly flagging and responding to worrying posts When Naika Venant killed herself in January, the Miami-area teen broadcast the event for two hours on Facebook’s popular video live-streaming feature, Facebook Live. A friend of hers saw the video and alerted police but aid did not arrive in time to save the 14-year-old’s life. Other young people have also recently posted suicidal messages on social media platforms including Twitter, Tumblr and Live.me. In a bid to save lives Facebook and other social media giants are now wading into suicide prevention work—creating new alert systems designed to better identify and help at-risk individuals. Last Wednesday Facebook unveiled a new suite of tools including the company’s first pattern recognition algorithms to spot users who may be suicidal or at risk of lesser self-harm. Facebook says the new effort will help it flag concerning posts and connect users with mental health services. It also represents a new front in its machine learning. Suicide is now the 10th-leading cause of death in this country and the second-leading cause of deaths among youth, so social media could be an important intervention point, says psychologist Daniel Reidenberg, executive director of Save.org, one of Facebook’s partner mental health organizations. Facebook currently reports more than a billion daily users around the world. In the U.S. 71 percent of teens between 13 and 17 and 62 percent of adults over 18 have a presence on it, according to two 2015 reports by the Pew Research Center. To reach its at-risk users, Facebook says it is expanding its services that allow friends to report posts containing signs of any suicidal or self-mutilation plans and it provides a menu of options for both those individuals and the friend who reported them. Choices include hotlines to call, prompts to reach out to friends and tips on what to do in moments of crisis. This tool will now be available for Facebook live streams as well. Similar reporting systems exist on a number of social media platforms including Twitter, Pinterest and YouTube. Facebook is now also piloting a program that will let people use Messenger, its instant messaging app, to directly connect with counselors from crisis support organizations including Crisis Text Line and the National Suicide Prevention Lifeline (NSPL).
Facebook also plans to use pattern recognition algorithms to identify people who may be at risk of self-harm, and to provide them with resources to help. The company says its new artificial intelligence program, which will be rolled out a on a limited basis at first, will employ machine learning to identify posts that suggest suicidal thoughts—even if no one on Facebook has yet reported it. William Nevius, a spokesperson at Facebook, says the machine-learning algorithms will use two signals—one from words or phrases that relate to suicide or self-harm in users’ posts and the other from comments added by concerned friends—to determine whether someone is at risk. If the pattern recognition program identifies concerning posts, the “report post” button will appear more prominently to visually encourage users to click it. “The hope is that the artificial intelligence learning will pick up on multiple signals from various points, [put] that together and activate a response, both to the person who might be at risk and to others who [could help],” Reidenberg wrote in an e-mail. If those cues signal a higher level of urgency, the system will automatically alert Facebook’s Community Operations team—a group of staff members who provide technical support and monitor the site for issues such as bullying or hacked accounts. The team will quickly review the post and determine whether the person needs additional support. If so, they make sure the user will see a page of resources appear on his or her news feed. (That page would normally only pop up if a post were reported by a concerned friend.) To help its artificial intelligence learn to flag concerning posts, Facebook mined “tens of thousands of posts that have been reported by friends who are concerned about another friend,” explains John Draper, project director of the NSPL, also a Facebook partner organization. Although the current algorithms are limited to text, Facebook may eventually use AI to identify worrying photos and videos as well. CEO Mark Zuckerberg announced last month that the company has been “researching systems that can look at photos and videos to flag content our team should review” as a part of efforts to assess reported content, including suicides, bullying and harassment. “This is still very early in development but we have started to have it look at some content, and it already generates about one third of all reports to the team that reviews content for our community,” Zuckerberg wrote. Nevius did not provide information about when these additional tools might be applied. Some mental health experts say AI is still limited at identifying suicide risk via language alone. “I think [machine learning] is a step in the right direction,” says Joseph Franklin, a psychologist at Florida State University who studies suicide risk. Franklin and his colleagues recently conducted a meta-analysis of 365 studies from 1965 to 2014. They found that despite decades of research, experts’ ability to detect future suicide attempts have remained no better than chance. “There’s just a tiny predictive signal,” Franklin says. These limitations have spurred him and others to work on developing machine-learning algorithms that help assess risk by analyzing data from electronic health records. “The limitation of health records is that…we can accurately predict [risk] over time, but we don’t know what day they’re going to attempt suicide,” Franklin says. Social media could be very helpful at providing a clearer sense of timing, he adds. But that, too, would still have key limitations: “There's just not a lot you can tell from text, even using more complicated natural language processing, because people can use the words ‘suicide’ or ‘kill myself’ for many different reasons—and you don't know if someone is using it in a particular way.” Some researchers such as Glen Coppersmith, founder and CEO of the mental health analytics company Qntfy, have discovered useful signals in language alone. In a recent examination of publicly available Twitter data, Coppersmith and his colleagues found the emotional content of posts—including text and emojis—could be indicative of risk. He notes, however, these are still “little pieces of the puzzle,” adding, “The other side of it, the sort of nonlanguage signal, is timing.” “Facebook has information about when you’re logging in, when you’re chatting…and what hours are you logging in, [which are] really interesting signals that might be relevant to whether or not you’re at proximal risk for suicide.” Craig Bryan, a researcher at the University of Utah who investigates suicide risk in veteran populations, has started to examine the importance of timing in the path to suicide. “In our newer research, we’ve been looking at the temporal patterns of sequences as they emerge—where [we find] it’s not just having lots of posts about depression or alcohol use, for instance, [but] it’s the order in which you write them,” he says. Another important factor to consider, especially with teens, is how often their language changes, says Megan Moreno, a pediatrician specializing in adolescent medicine at Seattle Children’s Hospital. In a 2016 study Moreno and colleagues discovered that on Instagram (a social media platform for sharing photos and video), once a self-injury–related hashtag was banned or flagged as harmful, numerous spin-off versions would emerge. For example, when Instagram blocked #selfharm, replacements with alternate spelling (#selfharmmm and #selfinjuryy) or slang (#blithe and #cat) emerged. “I continue to think that machine learning is always going to be a few steps behind the way adolescents communicate,” Moreno says. “As much as I admire these efforts, I think we can’t rely on them to be the only way to know whether a kid is struggling.” “The bottom line is that it’s definitely an effort that makes a lot of sense, given the fact that a lot of people connect with social media,” says Jessica Ribeiro, a psychologist applying machine learning to suicide prevention research at Florida State. “At the same time, they’re limited by what the science in this area doesn’t know—and unfortunately, we don't know a lot despite decades of research.” Diana Kwon Diana Kwon is a freelance science writer with a master's degree in neuroscience from McGill University. More of her pieces can be found at www.dianakwon.com. June 1, 2016
—
Nancy Shute November 30, 2016
—
Nathaniel P. Morris October 15, 2016
—
Michelle Carr February 1, 2009
—
Melinda Wenner Neuroscience. Evolution. Health. Chemistry. Physics. Technology. Follow us © 2018 Scientific American, a Division of Nature America, Inc. All Rights Reserved."
c4c3c7e759,TrueAccord Nabs $22M Series B To Bring Machine Learning To Debt Collections,"San Francisco based TrueAccord, announced today that is has closed $22M in additional funding led by Arbor Ventures, with participation from existing and new investors. The Series B funding follows a period of sustained and rapid growth for the company. TrueAccord’s data-driven debt collection platform is disrupting the collections industry by helping businesses collect more debt online than traditional methods. True Accord’s platform is powered by machine learning with a decision engine that analyzes consumer behavior and delivers personalized experiences by communicating with consumers at the right time in the right channel with payment options that meet their needs. The company was established in 2013 by serial entrepreneurs, brothers Ohad (Chief Executive Officer) and Nadav (Chief Innovation Officer) Samet after being treated badly while subjected to collections on a small balance; They realized there was something inherently wrong with the way debt collection was being done and recognized an opportunity to build a system to fundamentally change the debt collection industry, with the help of machine learning.
Ohad Samet, Cofounder and CEO of TrueAccord “It was the personal experience of dealing with a debt collector that made me realize the traditional collections industry was ripe for disruption with technology innovation and a more human approach,” said Ohad Samet, in a phone call last week. “With changing consumer preferences, strong regulatory support for innovation, and clients who understand a customer-focused collection process is good for their business, we're experiencing tremendous demand from the market. We are seizing this opportunity to use machine learning to humanize debt collection for good.” he continued. This additional investment will fund TrueAccord’s strategic growth initiatives, including ongoing product development and innovation of its customer-focused platform, providing audit and compliance functionality, continued expansion into vertical markets, client acquisition and retention, and hiring. TrueAccord's customer dashboard Between 2016 and 2017, the company grew the accounts it collects by 2.5x with more than 2 million customers on its platform since inception. The company claims to beat traditional agencies', for example, ARS, TSI and Northland,  debt collection rates by a minimum of 50% to upwards of 500%. Clients include top 10 issuers, leading creditors, and technology companies such as Yelp and LendUp with more than $1.5 billion of debt flowing through the platform since 2014."
cd1add30e4,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
d63ce49c06,An algorithm for your blind spot  | MIT News,"Login
or
Subscribe Newsletter
A new system from MIT's Computer Science and Artificial Intelligence Laboratory works by analyzing light at the edge of walls, which is impacted by the reflections of objects around the corner from the camera. Image courtesy of the researchers. Using smartphone cameras, system for seeing around corners could help with self-driving cars and search-and-rescue.
Watch Video
Adam Conner-Simons | CSAIL
October 9, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Light lets us see the things that surround us, but what if we could also use it to see things hidden around corners? It sounds like science fiction, but that’s the idea behind a new algorithm out of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) — and its discovery has implications for everything from emergency response to self-driving cars. The CSAIL team’s imaging system, which can work with smartphone cameras, uses information about light reflections to detect objects or people in a hidden scene and measure their speed and trajectory — all in real-time. To explain, imagine that you’re walking down an L-shaped hallway and have a wall between you and some objects around the corner. Those objects reflect a small amount of light on the ground in your line of sight, creating a fuzzy shadow that is referred to as the “penumbra.” Using video of the penumbra, the system — which the team has dubbed “CornerCameras” — can stitch together a series of one-dimensional images that reveal information about the objects around the corner. “Even though those objects aren’t actually visible to the camera, we can look at how their movements affect the penumbra to determine where they are and where they’re going,” says Katherine Bouman SM '13, PhD '17, who is lead author on a new paper about the system. “In this way, we show that walls and other obstructions with edges can be exploited as naturally-occurring ‘cameras’ that reveal the hidden scenes beyond them.” Bouman says that the ability to see around obstructions would be useful for many tasks, from firefighters finding people in burning buildings to drivers detecting pedestrians in their blind spots. She co-wrote the paper with MIT professors Bill Freeman, Antonio Torralba, Greg Wornell, and Fredo Durand; master’s student Vickie Ye; and PhD student Adam Yedidia. She will present the work later this month at the International Conference on Computer Vision in Venice, Italy. How it works Most approaches for seeing around obstacles involve special lasers. Specifically, researchers shine cameras on specific points that are visible to both the observable and hidden scene, and then measure how long it takes for the light to return. However, these so-called “time-of-flight cameras” are expensive and can easily get thrown off by ambient light, especially outdoors. In contrast, the CSAIL team’s technique doesn’t require actively projecting light into the space, and works in a wider range of indoor and outdoor environments and with off-the-shelf consumer cameras. From viewing video of the penumbra, CornerCameras generates one-dimensional images of the hidden scene. A single image isn’t particularly useful, since it contains a fair amount of “noisy” data. But by observing the scene over several seconds and stitching together dozens of distinct images, the system can distinguish distinct objects in motion and determine their speed and trajectory. “The notion to even try to achieve this is innovative in and of itself, but getting it to work in practice shows both creativity and adeptness,” says Professor Marc Christensen, who serves as dean of the Lyle School of Engineering at Southern Methodist University and was not involved in the research. “This work is a significant step in the broader attempt to develop revolutionary imaging capabilities that are not limited to line-of-sight observation.” The team was surprised to find that CornerCameras worked in a range of challenging situations, including weather conditions like rain. “Given that the rain was literally changing the color of the ground, I figured that there was no way we’d be able to see subtle differences in light on the order of a tenth of a percent,” says Bouman. “But because the system integrates so much information across dozens of images, the effect of the raindrops averages out, and so you can see the movement of the objects even in the middle of all that activity.” The system still has some limitations. For obvious reasons, it doesn’t work if there’s no light in the scene, and can have issues if there’s low light in the hidden scene itself. It also can get tripped up if light conditions change, like if the scene is outdoors and clouds are constantly moving across the sun. With smartphone-quality cameras the signal also gets weaker as you get farther away from the corner. The researchers plan to address some of these challenges in future papers, and will also try to get it to work while in motion. The team will soon be testing it on a wheelchair, with the goal of eventually adapting it for cars and other vehicles. “If a little kid darts into the street, a driver might not be able to react in time,” says Bouman. “While we’re not there yet, a technology like this could one day be used to give drivers a few seconds of warning time and help in a lot of life-or-death situations."" This work was supported, in part, by the DARPA REVEAL Program, the National Science Foundation, Shell Research, and a National Defense Science and Engineering Graduate Fellowship. Topics: Research, Algorithms, Machine learning, Computer vision, Computer Science and Artificial Intelligence Laboratory (CSAIL), Networks, Data, School of Engineering, Research Laboratory of Electronics, Imaging, Artificial intelligence, National Science Foundation (NSF), Defense Advanced Research Projects Agency (DARPA) Using video to processes shadows, MIT researchers have developed an algorithm that can see around corners, writes Alyssa Meyers for The Boston Globe. “When you first think about this, you might think it’s crazy or impossible, but we’ve shown that it’s not if you can understand the physics of how light propagates,” says lead author and MIT graduate Katie Bouman. CSAIL researchers have developed a system that detects objects and people hidden around blind corners, writes Anthony Cuthbertson for Newsweek. “We show that walls and other obstructions with edges can be exploited as naturally occurring ‘cameras’ that reveal the hidden scenes beyond them,” says lead author and MIT graduate Katherine Bouman. MIT researchers have developed a new system that can spot moving objects hidden from view by corners, reports Douglas Heaven for New Scientist. “A lot of our work involves finding hidden signals you wouldn’t think would be there,” explains lead author and MIT graduate Katie Bouman.  Wired reporter Matt Simon writes that MIT researchers have developed a new system that analyzes the light at the edges of walls to see around corners. Simon notes that the technology could be used to improve self-driving cars, autonomous wheelchairs, health care robots and more.   This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
88064dae3d,Machine learning startup Graphcore raises $50M in round led by Sequoia Capital - SiliconANGLE,"Home » Emerging Tech
by
Duncan Riley
UPDATED 23:29 EST . 12 NOVEMBER 2017
Machine learning startup Graphcore Ltd. has raised $50 million in new funding in a round led by Sequoia Capital, according to news published Sunday but not yet confirmed by the company itself. It’s also not clear what the valuation was on the deal. A report from Bloomberg Nov. 3 said that the company had been chasing a valuation of $1 billion but “wasn’t able to reach that height.” Founded in 2016, United Kingdom-based Graphcore is building “intelligence processing units,” chips that are specifically designed to assist programmers in creating machine learning computer systems that can be used in fields such as with autonomous cars or medical detection devices. The company claims its IPU accelerators and Poplar software framework deliver “the fastest and most flexible platform for current and future machine intelligence applications, lowering the cost of AI in the cloud and datacenter, improving performance and efficiency by between 10x to 100x.” When the company last raised venture capital in July, it was clear that although building chips dedicated to artificial intelligence and machine learning isn’t a new concept, the market itself is best described as emerging. Others looking to capitalize on growing demand from machine learning processors include Google Inc. through its “Tensor Processing Units” and Intel Corp. with its recently revealed NNP family of chips. According to a report from Allied Market Research, the machine learning chip market is set to rise to $8.2 billion by 2022, offering ample opportunities to gain market share. Machine learning chips are widely used across the applications such as robotics, healthcare, automotive and consumer electronics, the researchers noted. “At present, rising demand for automated electronic devices and trending artificial intelligence are some factors that majorly drive the market,” they said. “Moreover, popularity of internet of things is expected to provide lucrative opportunities to market players.” Including the new round, Graphcore has raised $110 million to date. Like Free Content?
Subscribe to follow. LG and JBL unveil smart display speakers with Google Assistant integration Responding to criticism, Facebook makes news feed changes to spur more interaction Red Hat says microservices benefits can be realized in just six months Report claims hackers could start a nuclear war through vulnerable weapons systems Microsoft brings end-to-end encryption to Skype through Signal partnership PC market shows small signs of life as Q4 worldwide shipments grow Microsoft brings end-to-end encryption to Skype through Signal partnership APPS - BY DUNCAN RILEY . 9 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 13 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 18 HOURS AGO Google buys Redux, a startup that uses vibrations to turn phone screens into speakers EMERGING TECH - BY ERIC DAVID . 19 HOURS AGO Hyundai invests in Southeast Asia Uber rival Grab EMERGING TECH - BY DUNCAN RILEY . 1 DAY AGO Best of CES: Kika Tech's Tami Zhu wants to put emotion into AI BIG DATA - BY MARK ALBERTSON . 2 DAYS AGO Bugsnag picks up $9M to help companies develop more reliable applications APPS - BY MARIA DEUTSCHER . 2 DAYS AGO The 'SoftBank Effect': Megarounds drive U.S. venture funding up 17% in 2017 CLOUD - BY DUNCAN RILEY . 2 DAYS AGO Microsoft forced to stop issuing faulty Meltdown and Spectre patches INFRASTRUCTURE - BY DUNCAN RILEY . 2 DAYS AGO Microsoft brings end-to-end encryption to Skype through Signal partnership APPS - BY DUNCAN RILEY . 9 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 13 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 18 HOURS AGO Google buys Redux, a startup that uses vibrations to turn phone screens into speakers EMERGING TECH - BY ERIC DAVID . 19 HOURS AGO Hyundai invests in Southeast Asia Uber rival Grab EMERGING TECH - BY DUNCAN RILEY . 1 DAY AGO Best of CES: Kika Tech's Tami Zhu wants to put emotion into AI BIG DATA - BY MARK ALBERTSON . 2 DAYS AGO Bugsnag picks up $9M to help companies develop more reliable applications APPS - BY MARIA DEUTSCHER . 2 DAYS AGO The 'SoftBank Effect': Megarounds drive U.S. venture funding up 17% in 2017 CLOUD - BY DUNCAN RILEY . 2 DAYS AGO Microsoft forced to stop issuing faulty Meltdown and Spectre patches INFRASTRUCTURE - BY DUNCAN RILEY . 2 DAYS AGO Check out more cube videos and stories Analytics feeds machine maker’s global ambitions Modern infrastructure management: accelerating productivity through machine learning Fleet analytics provider spins vehicle sensor data into productivity gold Big data leads a transformation in PC gaming For personalised content stay with us. Forgot Password? Like Free Content? Subscribe to follow."
48546b5716,Microsoft uses machine learning to combat security vulnerabilities - SD Times,"Microsoft is applying machine learning and deep neural networks to its software security approach. The company announced a new research project, neural fuzzing, designed to augment traditional fuzzing techniques, discover vulnerabilities, and learn from past software experiences.
The research is based on Microsoft’s Security Risk Detection tool that incorporates artificial intelligence to find and detect software bugs.
Fuzzing is a software security testing technique used to find vulnerabilities in complex software solutions. “Fuzzing involves presenting a target program with crafted malicious input designed to cause crashes, buffer overflows, memory errors, and exceptions,” Microsoft researchers wrote.
Some fuzz testing categories include: blackbox fuzzers that rely on sample input files generate new inputs, whitebox fuzzers that analyze the target program statically or dynamically to help search for new inputs, and greybox fuzzers that uses a feedback loop to guide their search. Microsoft’s new category, neural fuzzing, uses machine learning models to learn from the feedback loop of a greybox fuzzer, according to the researchers. The team says they have been able to improve the code coverage, code paths, and crashes of four input formats: ELF, PDF, PNG and XML.
“We present a learning technique that uses neural networks to learn patterns in the input files from past fuzzing explorations to guide future fuzzing explorations. In particular, the neural models learn a function to predict good (and bad) locations in input files to perform fuzzing mutations based on the past mutations and corresponding code coverage information,” the researchers wrote in their research.
According to William Blum, from the Microsoft Security Risk Detection engineering team, this is only just the beginning of what can be used when applying deep neural networks to fuzz testing. “We could also use it to learn other fuzzing parameters such as the type of mutation or strategy to apply. We are also considering online versions of our machine learning model, in which the fuzzer constantly learns from ongoing fuzzing iterations,” Blum wrote in a post.
deep neural networks, DNN, fuzz testing, fuzzers, machine learning, Microsoft, Microsoft Security Risk Detection, security vulnerabilities, software security
Christina Cardoza is the News Editor of SD Times. She is responsible for the oversight of the daily news published to the website as well as the company's weekly newsletter, News on Monday. She covers agile, DevOps, AI, machine learning, mixed reality and software security. She is an undeniable nerd who loves Marvel comics and Star Wars. On Follow her on Twitter at @chriscatdoza!
Ready for your SD Times magazine? It's just a click away! Subscribe"
d96b05fa4d,Zillow: Machine learning and data disrupt real estate | ZDNet,"This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please
view our cookie policy.
Learn how big data and the Zillow Zestimate changed and disrupted real estate. It's an important case study on the power of machine learning models and digital innovation.
By Michael Krigsman
for Beyond IT Failure
|
July 30, 2017 -- 18:26 GMT (19:26 BST)
| Topic: Digital Transformation: A CXO's Guide
Anyone buying or selling a house knows about Zillow. In 2006, the company introduced the Zillow Estimate, or 	Zestimate for short, which uses a variety of data sources and models to create an approximate value for residential properties. 		 		The impact of Zillow's Zestimate on the real estate industry has been considerable, to say the least. 		 		From the home buyer perspective, Zillow's Zestimate enables significant transparency around prices and information that historically was available only to brokers. The company has genuinely 	democratized real estate information and adds tremendous value to consumers. 		 		For real estate brokers, on the other hand, Zillow is fraught with more difficulty. I asked a top real estate 	broker working in Seattle, Zillow's home turf, for his view of the company. Edward Krigsman sells multimillion-dollar homes in the city and explains some of the challenges:
Automated valuation methods have been around for decades, but Zillow packaged those techniques for retail on a large scale. That was their core innovation. However, Zillow's data often is not accurate and getting them to fix problems is difficult.
Zillow creates pricing expectations among consumers and has become a third party involved in the pre-sales aspects of residential real estate. Accurate or not, Zillow affects the public perception of home value. 	 		 		Zillow's market impact on the real estate industry is large, and the company's data is an important influence on many home transactions.
Zillow offers a textbook example of how data can change established industries, relationships, and economics. The parent company, Zillow Group, runs several real estate marketplaces that together generate about $1 billion in revenue with, reportedly, 75 percent online real estate audience market share. 		 		As part of the 	CXOTALK series of conversations with disruptive innovators, I invited Zillow's Chief Analytics Officer (who is also their Chief Economist), Stan Humphries, to take part in episode 234. 		 		The conversation offers a fascinating look at how Zillow thinks about data, models, and its role in the real estate ecosystem. 		 		Check out the video embedded above and read a complete transcript on the 	CXOTALK site. In the meantime, here is an edited and abridged segment from our detailed and lengthy conversation. 		 		There's always been a lot of data floating around real estate. Though, a lot of that data was largely [hidden] and so it had unrealized potential. As a data person, you love to find that space. 		 		Travel, which a lot of us were in before, was a similar space, dripping with data, but people had not done much with it. It meant that a day wouldn't go by where you wouldn't come up with ""Holy crap! Let's do this with the data!"" 		 		In real estate, multiple listing services had arisen, which were among different agents and brokers on the real estate side; the homes that were for sale. 		 		However, the public record system was completely independent of that, and there were two public records systems: one for deeds and liens on real property, and then another for the tax rolls. 		 		All of that was disparate information. We tried to solve for the fact that all of this was offline. 		 		We had the sense that it was, from a consumer's perspective, like the Wizard of Oz, where it was all behind this curtain. You weren't allowed behind the curtain and really [thought], ""Well, I'd really like to see all the sales myself and figure out what's going on."" You'd like the website to show you both the core sale listings and the core rent listings. 		 		But of course, the people selling you the homes didn't want you to see the rentals alongside them because maybe you might rent a home rather than buy. And we're like, ""We should put everything together, everything in line."" 		 		We had faith that type of transparency was going to benefit the consumer. 		 		You still find that agency representation is very important because it's a very expensive transaction. For most Americans, the most expensive transaction, and the most expensive financial asset they will ever own. So, there continues to be a reasonable reliance on an agent to help hold the consumer's hands as they either buy or sell real estate. 		 		But what has changed is that now consumers have access to the same information that the representation has, either on the buy or sell side. That has enriched the dialogue and facilitated the agents and brokers who are helping the people. Now a consumer comes to the agent with a lot more awareness and knowledge, as a smarter consumer. They work with the agent as a partner where they've got a lot of data and the agent has a lot of insight and experience. Together, we think they make better decisions than they did before. 		 		When we first rolled out in 2006, the Zestimate was a valuation that we placed on every single home that we had in our database at that time, which was 43 million homes. To create that valuation in 43 million homes, it ran about once a month, and we pushed a couple of terabytes of data through about 34 thousand statistical models, which was, compared to what had been done previously an enormously more computationally sophisticated process. 		 		I should just give you a context of what our accuracy was back then. Back in 2006 when we launched, we were at about 14% median absolute percent error on 43 million homes. 		 		Since then, we've gone from 43 million homes to 110 million homes; we put valuations on all 110 million homes. And, we've driven our accuracy down to about 5 percent today which, from a machine learning perspective, is quite impressive. 		 		Those 43 million homes that we started with in 2006 tended to be in the largest metropolitan areas where there was much transactional velocity. There were a lot of sales and price signals with which to train the models. As we went from 43 million to 110, you're now getting out into places like Idaho and Arkansas where there are just fewer sales to look at. 		 		It would have been impressive if we had kept our error rate at 14% while getting out to places that are harder to estimate. But, not only did we more than double our coverage from 43 to 110 million homes, but we almost tripled our accuracy rate from 14 percent down to 5 percent. 		 		The hidden story of achieving that is by collecting enormously more data and getting a lot more sophisticated algorithmically, which requires us to use more computers. 		 		Just to give a context, when we launched, we built 34 thousand statistical models every month. Today, we update the Zestimate every single night and generate somewhere between 7 and 11 million statistical models every single night. Then, when we're done with that process, we throw them away and repeat the next night again. So, it's a big data problem. 		 		We never go above a county level for the modeling system, and large counties, with many transactions, we break that down into smaller regions within the county where the algorithms try to find homogeneous sets of homes in the sub-county level to train a modeling framework. That modeling framework itself contains an enormous number of models. 		 		The framework incorporates a bunch of different ways to think about values of homes combined with statistical classifiers. So maybe it's a decision tree, thinking about it from what you may call a ""hedonic"" or housing characteristics approach, or maybe it's a support vector machine looking at prior sale prices. 		 		The combination of the valuation approach and the classifier together create a model, and there are a bunch of these models generated at that sub-county geography. There are also a bunch of models that become meta-models, which their job is to put together these sub-models into a final consensus opinion, which is the Zestimate. 		 		We believe advertising dollars follow consumers. We want to help consumers the best we can. 		 		We have constructed, in economic language, a two-sided marketplace where we've got consumers coming in who want to access inventory and get in touch with professionals. On the other side of that marketplace, we've got professionals -- be it real estate brokers or agents, mortgage lenders, or home improvers -- who want to help those consumers do things. We're trying to provide a marketplace where consumers can find inventory and professionals to help them get things done. 		 		So, from the perspective of a market-maker versus a market-participant, you want to be completely neutral and unbiased. All you're trying to do is get a consumer the right professional and vice-versa, and that's very important to us. 		 		That means, when it comes to machine learning applications, for example, the valuations that we do, our intent is to come up with the best estimate of what a home is going to sell for. Again, from an economic perspective, it's different from the asking price of the offer price. In a commodities context, you call that a bid-ask spread between what someone is going to ask for in a bid. 		 		In the real-estate context, we call that the offer price and the asking price. And so, what someone's going to offer to sell you his or her house for is different from a buyer saying, ""Hey, would you take this for it?"" There's always a gap between that. 		 		What we're trying to do with Zestimate is to inform some pricing decisions so the bid-ask spread is smaller, [to prevent] buyers from getting taken advantage of when the home was worth a lot less. And, [to prevent} sellers from selling a house for a lot less than they could have got because they just don't know. 		 		We think that having great, competent representation of both sides is one way to mitigate that, which we think is fantastic. Having more information about pricing decision to help you understand that offer-ask ratio, what the offer ask-spread looks like, is very important as well. 		 		Our models are trained such that half of the Earth will be positive and half will be negative; meaning that on any given day, half of [all] homes are going to transact above the Zestimate value and half are going to transact below. Since launching the Zestimate, we have wanted this to be a starting point for a conversation about home values. It's not an ending point. 		 		It's meant to be a starting point for a conversation about value. That conversation, ultimately, needs to involve other means of value, include real estate professionals like an agent or broker, or an appraiser; people who have expert insight into local areas and have seen the inside of a home and can compare it to other comparable homes. 		 		I think that's an influential data point and hopefully, it's useful to people. Another way to think about that stat I just gave you is that on any given day, half of the sellers sell their homes for less than the Zestimate, and half of the buyers buy a home for more than the Zestimate. So, clearly, they're looking at something other than the Zestimate, although hopefully, it's been helpful to them at some point in that process. 		 		I've been involved in machine learning for a while. I started in academia as a researcher at a university setting. Then at Expedia, I was very heavily involved in machine learning, and then here. 		 		I was going to say the biggest change has really been in the tech stack over that period, but, I shouldn't minimize the change in the actual algorithms themselves over those years. Algorithmically, you see the evolution from at Expedia, personalization, we worked more on relatively sophisticated, but more statistical and parametric models for making recommendations; things like unconditional probability,and item-to-item correlations. Now, most of your recommender systems use things like collaborative filtering for algorithms that are optimized for high-volume data and streaming data. 		 		In a predictive context, we've moved from things like decision trees and support vector machines to now a forest of trees; all those simpler trees with much larger numbers of them... And then, more exotic decision trees that have in their leaf nodes more direction components which are very helpful in some contexts. 		 		As a data scientist now, you can start working on a problem on AWS, in the cloud. Then have an assortment of models to quickly deploy much easier than you could back twenty years ago when you had to code a bunch of stuff; start out in MATLAB and import it to C, and you were doing it all by hand.
CXOTALK brings you the world's most innovative business leaders, authors, and analysts for in-depth discussion unavailable anywhere else.
CXO IBM names new CFO as Schroeter moves to global markets role Robotics Robotics in business: Everything humans need to know Mobility Motorola LatAm president becomes European head Security ​Meltdown and Spectre: The looming death of security (and what to do about it) © 2018 CBS Interactive. All rights reserved. Privacy Policy | Cookies | Ad Choice |
Advertise | Terms of Use | Mobile User Agreement"
b49ea06de6,Computer learns to recognize sounds by watching video | MIT News,"Login
or
Subscribe Newsletter
The researchers’ neural network was fed video from 26 terabytes of video data downloaded from the photo-sharing site Flickr. Researchers found the network can interpret natural sounds in terms of image categories. For instance, the network might determine that the sound of birdsong tends to be associated with forest scenes and pictures of trees, birds, birdhouses, and bird feeders. Image: Jose-Luis Olivares/MIT Machine-learning system doesn’t require costly hand-annotated data.
Watch Video
Larry Hardesty | MIT News Office
December 2, 2016
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
In recent years, computers have gotten remarkably good at recognizing speech and images: Think of the dictation software on most cellphones, or the algorithms that automatically identify people in photos posted to Facebook. But recognition of natural sounds — such as crowds cheering or waves crashing — has lagged behind. That’s because most automated recognition systems, whether they process audio or visual information, are the result of machine learning, in which computers search for patterns in huge compendia of training data. Usually, the training data has to be first annotated by hand, which is prohibitively expensive for all but the highest-demand applications. Sound recognition may be catching up, however, thanks to researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). At the Neural Information Processing Systems conference next week, they will present a sound-recognition system that outperforms its predecessors but didn’t require hand-annotated data during training. Instead, the researchers trained the system on video. First, existing computer vision systems that recognize scenes and objects categorized the images in the video. The new system then found correlations between those visual categories and natural sounds. “Computer vision has gotten so good that we can transfer it to other domains,” says Carl Vondrick, an MIT graduate student in electrical engineering and computer science and one of the paper’s two first authors. “We’re capitalizing on the natural synchronization between vision and sound. We scale up with tons of unlabeled video to learn to understand sound.” The researchers tested their system on two standard databases of annotated sound recordings, and it was between 13 and 15 percent more accurate than the best-performing previous system. On a data set with 10 different sound categories, it could categorize sounds with 92 percent accuracy, and on a data set with 50 categories it performed with 74 percent accuracy. On those same data sets, humans are 96 percent and 81 percent accurate, respectively. “Even humans are ambiguous,” says Yusuf Aytar, the paper’s other first author and a postdoc in the lab of MIT professor of electrical engineering and computer science Antonio Torralba. Torralba is the final co-author on the paper. “We did an experiment with Carl,” Aytar says. “Carl was looking at the computer monitor, and I couldn’t see it. He would play a recording and I would try to guess what it was. It turns out this is really, really hard. I could tell indoor from outdoor, basic guesses, but when it comes to the details — ‘Is it a restaurant?’ — those details are missing. Even for annotation purposes, the task is really hard.” Complementary modalities Because it takes far less power to collect and process audio data than it does to collect and process visual data, the researchers envision that a sound-recognition system could be used to improve the context sensitivity of mobile devices. When coupled with GPS data, for instance, a sound-recognition system could determine that a cellphone user is in a movie theater and that the movie has started, and the phone could automatically route calls to a prerecorded outgoing message. Similarly, sound recognition could improve the situational awareness of autonomous robots. “For instance, think of a self-driving car,” Aytar says. “There’s an ambulance coming, and the car doesn’t see it. If it hears it, it can make future predictions for the ambulance — which path it’s going to take — just purely based on sound.” Visual language The researchers’ machine-learning system is a neural network, so called because its architecture loosely resembles that of the human brain. A neural net consists of processing nodes that, like individual neurons, can perform only rudimentary computations but are densely interconnected. Information — say, the pixel values of a digital image — is fed to the bottom layer of nodes, which processes it and feeds it to the next layer, which processes it and feeds it to the next layer, and so on. The training process continually modifies the settings of the individual nodes, until the output of the final layer reliably performs some classification of the data — say, identifying the objects in the image. Vondrick, Aytar, and Torralba first trained a neural net on two large, annotated sets of images: one, the ImageNet data set, contains labeled examples of images of 1,000 different objects; the other, the Places data set created by Oliva's group and Torralba's group, contains labeled images of 401 different scene types, such as a playground, bedroom, or conference room. Once the network was trained, the researchers fed it the video from 26 terabytes of video data downloaded from the photo-sharing site Flickr. “It’s about 2 million unique videos,” Vondrick says. “If you were to watch all of them back to back, it would take you about two years.” Then they trained a second neural network on the audio from the same videos. The second network’s goal was to correctly predict the object and scene tags produced by the first network. The result was a network that could interpret natural sounds in terms of image categories. For instance, it might determine that the sound of birdsong tends to be associated with forest scenes and pictures of trees, birds, birdhouses, and bird feeders. Benchmarking To compare the sound-recognition network’s performance to that of its predecessors, however, the researchers needed a way to translate its language of images into the familiar language of sound names. So they trained a simple machine-learning system to associate the outputs of the sound-recognition network with a set of standard sound labels. For that, the researchers did use a database of annotated audio — one with 50 categories of sound and about 2,000 examples. Those annotations had been supplied by humans. But it’s much easier to label 2,000 examples than to label 2 million. And the MIT researchers’ network, trained first on unlabeled video, significantly outperformed all previous networks trained solely on the 2,000 labeled examples. “With the modern machine-learning approaches, like deep learning, you have many, many trainable parameters in many layers in your neural-network system,” says Mark Plumbley, a professor of signal processing at the University of Surrey. “That normally means that you have to have many, many examples to train that on. And we have seen that sometimes there’s not enough data to be able to use a deep-learning system without some other help. Here the advantage is that they are using large amounts of other video information to train the network and then doing an additional step where they specialize the network for this particular task. That approach is very promising because it leverages this existing information from another field.” Plumbley says that both he and colleagues at other institutions have been involved in efforts to commercialize sound recognition software for applications such as home security, where it might, for instance, respond to the sound of breaking glass. Other uses might include eldercare, to identify potentially alarming deviations from ordinary sound patterns, or to control sound pollution in urban areas. “I really think that there’s a lot of potential in the sound-recognition area,” he says. Topics: Research, School of Engineering, Artificial intelligence, Big data, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Computer vision, Data This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
f418dbcb53,The Top 10 AI And Machine Learning Use Cases Everyone Should Know About,"Machine learning is a buzzword in the technology world right now, and for good reason: It represents a major step forward in how computers can learn. Very basically, a machine learning algorithm is given a “teaching set” of data, then asked to use that data to answer a question. For example, you might provide a computer a teaching set of photographs, some of which say, “this is a cat” and some of which say, “this is not a cat.” Then you could show the computer a series of new photos and it would begin to identify which photos were of cats. Machine learning then continues to add to its teaching set. Every photo that it identifies — correctly or incorrectly — gets added to the teaching set, and the program effectively gets “smarter” and better at completing its task over time. It is, in effect, learning.
Malware is a huge — and growing — problem. In 2014, Kaspersky Lab said it had detected 325,000 new malware files every day. But, institutional intelligence company Deep Instinct says that each piece of new malware tends to have almost the same code as previous versions — only between 2 and 10% of the files change from iteration to iteration. Their learning model has no problem with the 2–10% variations, and can predict which files are malware with great accuracy. In other situations, machine learning algorithms can look for patterns in how data in the cloud is accessed, and report anomalies that could predict security breaches. If you’ve flown on an airplane or attended a big public event lately, you almost certainly had to wait in long security screening lines. But machine learning is proving that it can be an asset to help eliminate false alarms and spot things human screeners might miss in security screenings at airports, stadiums, concerts, and other venues. That can speed up the process significantly and ensure safer events. Many people are eager to be able to predict what the stock markets will do on any given day — for obvious reasons. But machine learning algorithms are getting closer all the time. Many prestigious trading firms use proprietary systems to predict and execute trades at high speeds and high volume. Many of these rely on probabilities, but even a trade with a relatively low probability, at a high enough volume or speed, can turn huge profits for the firms. And humans can’t possibly compete with machines when it comes to consuming vast quantities of data or the speed with which they can execute a trade.
Source: Shutterstock Machine learning algorithms can process more information and spot more patterns than their human counterparts. One study used computer assisted diagnosis (CAD) when to review the early mammography scans of women who later developed breast cancer, and the computer spotted 52% of the cancers as much as a year before the women were officially diagnosed. Additionally, machine learning can be used to understand risk factors for disease in large populations. The company Medecision developed an algorithm that was able to identify eight variables to predict avoidable hospitalizations in diabetes patients. The more you can understand about your customers, the better you can serve them, and the more you will sell.  That’s the foundation behind marketing personalisation. Perhaps you’ve had the experience in which you visit an online store and look at a product but don’t buy it — and then see digital ads across the web for that exact product for days afterward. That kind of marketing personalization is just the tip of the iceberg. Companies can personalize which emails a customer receives, which direct mailings or coupons, which offers they see, which products show up as “recommended” and so on, all designed to lead the consumer more reliably towards a sale. Machine learning is getting better and better at spotting potential cases of fraud across many different fields. PayPal, for example, is using machine learning to fight money laundering. The company has tools that compare millions of transactions and can precisely distinguish between legitimate and fraudulent transactions between buyers and sellers. You’re probably familiar with this use if you use services like Amazon or Netflix. Intelligent machine learning algorithms analyze your activity and compare it to the millions of other users to determine what you might like to buy or binge watch next. These recommendations are getting smarter all the time, recognizing, for example, that you might purchase certain things as gifts (and not want the item yourself) or that there might be different family members who have different TV preferences. Perhaps the most famous use of machine learning, Google and its competitors are constantly improving what the search engine understands. Every time you execute a search on Google, the program watches how you respond to the results. If you click the top result and stay on that web page, we can assume you got the information you were looking for and the search was a success.  If, on the other hand, you click to the second page of results, or type in a new search string without clicking any of the results, we can surmise that the search engine didn’t serve up the results you wanted — and the program can learn from that mistake to deliver a better result in the future. NLP is being used in all sorts of exciting applications across disciplines. Machine learning algorithms with natural language can stand in for customer service agents and more quickly route customers to the information they need. It’s being used to translate obscure legalese in contracts into plain language and help attorneys sort through large volumes of information to prepare for a case. IBM recently surveyed top auto executives, and 74% expected that we would see smart cars on the road by 2025. A smart car would not only integrate into the Internet of Things, but also learn about its owner and its environment. It might adjust the internal settings — temperature, audio, seat position, etc. — automatically based on the driver, report and even fix problems itself, drive itself, and offer real time advice about traffic and road conditions. Bernard Marr is a best-selling author & keynote speaker on business, technology and big data. His new book is Data Strategy. To read his future posts simply join his network here."
962bbeede8,This Pen Can Diagnose Cancer in 10 Seconds | Time,"When it comes to treating cancer, surgeons want to get rid of as much cancerous tissue as possible during tumor removal. Now a new technology—the size of a pen—is attempting to make that easier by distinguishing between tumors and healthy tissue in just 10 seconds.
The MasSpec Pen is a real-time diagnostic tool created by researchers at the University of Texas at Austin. In a new study published Wednesday in the journal Science Translational Medicine, the researchers report that their handheld device (which is not yet FDA-approved) uses tiny droplets of water to analyze human tissue samples for cancer with 96% accuracy. “It’s a gentle, simple chemical process,” says study author Livia Schiavinato Eberlin, an assistant professor of chemistry at UT Austin. “It’s highly specific and highly sensitive. The fact that it’s non-destructive brings a new approach to cancer diagnosis.” MORE: Researchers Find a Way to Light Up Cancer Cells Getting rid of all cancerous tissue while also preventing any harm to healthy tissue is a delicate process. When operating on a woman with breast cancer, for example, a doctor needs to remove the tumor and other affected tissues while maintaining the rest of the breast. Currently there are other tools available to surgeons for tissue diagnosis, but many use gases or solvents that can be harmful for the human body. In 2016, researchers in Massachusetts reported that they developed a probe that can find and light up cancer cells, making them easier for surgeons to see. But other methods currently available to surgeons today are slower than the MasSpec Pen, the study authors say, in some cases by 30 minutes or more. Human cells produce a variety of small molecules, and cancer creates a unique set of them that can be used for pattern identification. The MasSpec Pen produces a small drop of water that extracts molecules from a person’s cells during surgery. Through machine learning, the MasSpec Pen is able to determine what molecular fingerprint is normal and what is cancer, Eberlin says. In the study, the researchers tested 253 human tissue samples from lung, ovary, thyroid and breast cancer tumors and compared them to samples of healthy tissues. The device was 96% accurate at identifying cancerous tissues. The researchers also tested the MasSpec Pen in live mice with tumors and found that the device was able to identify the presence of cancer without harming healthy surrounding tissues. The device can also identify different subtypes of lung and thyroid cancer, and the team hopes to make it more specified for other types of cancer, too. The researchers say they need to continue validating their work and that they plan to start clinical testing in humans in 2018. Until then, it’s unclear how exactly the device will work when integrated into surgery. While the pen-sized device that the surgeon would use is small, the device is connected to a large mass spectrometer, which helps the process of analyzing individual molecules. That large machine would need to be wheeled in and out of a surgery room for each procedure. The pen is disposable, so surgeons would replace it with each surgery.
“This is a good example of a tool that empowers our transition to precision medicine where the treatment can be done with much higher levels of confidence,” says study author Thomas Milner, professor of biomedical engineering in UT Austin’s Cockrell School of Engineering. “Treatment can be planned and given where the outcomes are known. This is one tool along that path.”"
6bcdf6b00b,Algorithmia now helps businesses manage and deploy their machine learning models  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Algorithmia started out as an online marketplace for — can you guess it? — algorithms. Many of these algorithms that developers offered on the service focused on machine learning (think face detection, sentiment analysis, etc.). Today, with the boom in ML/AI, that’s obviously a big draw and Algorithmia is now taking its next step in this direction with the launch of a new service that helps data scientists manage and deploy their machine learning models — and share them with others inside their companies. This basically means that the company is turning some of the infrastructure and services it built to run these models itself into a new product. “Tensorflow is open-source, but scaling it is not,” said Kenny Daniel, co-founder and CTO of Algorithmia, in today’s announcement. “Almost all R&D has focused on collecting and cleaning data, and building models. Algorithmia has spent the last five years building the infrastructure that will put those models to work.”
With this new service, data scientists can create their models in the languages and with the frameworks of their choice and then host them in the Algorithmia cloud (the CODEX platform) or using the company’s on-prem architecture. To do this, the company offers two versions of its service: the Serverless AI Layer for hosting models in its cloud and the Enterprise AI Layer for hosting the service in any public or private cloud. Both the hosted and on-prem versions of the service offer enterprises the ability to use git to add models, share them with others inside of an organization, and to handle permissioning and authorization. The service also handles all of the DevOps necessary to host and deploy models.
In recent months, the company started signing up enterprise customers to beta test this service, including a number of government agencies who want to use its service for hosting their models. “Algorithmia empowers U.S. government agencies to rapidly deploy new capabilities to the AI layer,” said Katie Gray, Principal of Investments at In-Q-Tel, the CIA’s investment arm. “The platform delivers security, scalability and discoverability so data scientists can focus on problem solving.” Earlier this year, Algorithmia announced a $10.5 million Series A funding round that was led by Google’s new AI venture fund.
Latest headlines delivered to you daily"
74d4e239cb,Apple Says 'Hey Siri' Detection Briefly Becomes Extra Sensitive If Your First Try Doesn't Work - Mac Rumors,"New in OS X: Get MacRumors Push Notifications on your Mac
There's a lot going on behind the scenes with Siri. I don't think we give her enough credit. My biggest problem with Siri is not any of the silly one-off bugs that get memed to death on the internet.
My biggest problem was described beautifully in a recent article I read somewhere. About how a voice assistant with 10 possible working commands is great. And one with unlimited working commands is great. But one with hundreds of working commands is terrible, because the user will never know what all those commands are. They will just use the core few that they know. And if they try a command and it isn't one of those hundreds, it causes confusion and doubt. Siri-based speaker unveiled at WWDC, launching early 2018.   MacRumors attracts a broad audience
of both consumers and professionals interested in
the latest technologies and products. We also boast an active community focused on
purchasing decisions and technical aspects of the iPhone, iPod, iPad, and Mac platforms."
b28faee930,Why Google’s AI Can Write Beautiful Songs but Still Can’t Tell a Joke - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Creating noodling piano tunes and endless configurations of cat drawings with AI may not sound like an obvious project for Google, but it makes a lot of sense to Douglas Eck. Eck has spent about 15 years studying AI and music, and these days he’s a research scientist on the Google Brain team, leading Magenta—Google’s open-source research project that’s aimed at making art and music with machine learning. He spoke to MIT Technology Review about how Google is producing new sounds with deep neural networks, where Magenta is taking AI music, and why computers suck at telling jokes. Below is an edited excerpt of the interview. Premium MIT Technology Review subscribers can listen to the full interview. Using AI to make art isn’t new, so what’s unique about Google’s approach? We’re exploring this very specific direction having to do with deep neural networks and recurrent neural networks and other kinds of machine learning. And we’re also trying really hard to engage both the artistic community and creative coders and open-source developers at the same time, so we’ve made it an open-source project. A lot of Magenta is focused on music. Why is AI good for making and augmenting music?
To be honest, it’s just a bias of mine. My whole research career has been about music and audio. I think the scope of Magenta has always been about art in general, storytelling, music, narrative, imagery, and trying to understand how to use AI as a creative tool. But you have to start somewhere. And I think if you make serious progress on something as complicated as music, and as important to us as music, then my hope is that some of that will map over into other domains as well. Can we listen to some music that’s been made with Magenta?
This is some music from a model called Performance RNN. Listen and just pay attention to the texture and everything there. This is a kind of music composition but it’s also at the same time a music performance, because the model is not only generating quarter notes—it’s deciding how fast they’re going to be played, how loudly they’re going to be played, and in fact it’s reproducing what it was trained on, which was a bunch of piano performances done as part of a piano competition. As that piece shows, music that’s been created thus far with Magenta is essentially improvisation. Can AI be used to create a coherent piece of music with structure? We’re working on that. So one of the major future research directions for us and, frankly, for the whole field of generative models—by that I mean machine-learning models that can try to generate something new—is learning structure. And that shows up in music here. You hear that there’s no overarching model that’s kind of deciding where things should go. If we wanted to give it chord changes, even the symbols of the chord change, and learn contextually how to take advantage of those chord changes, we could do that. We could even have a separate model that generates chord changes. Our goal is to come up with this end-to-end model that figures out all of these levels of structure on its own. Tell me about Sketch-RNN, which is a recent Magenta experiment that lets you draw with a recurrent neural network—basically, you start drawing a pineapple and then Sketch-RNN takes over and completes it, over and over, in many different styles. We were able to use a bunch of drawings done by people playing Pictionary against a machine-learning algorithm—this was [data from another Google AI drawing experiment made by Google Creative Lab,] Quick, Draw! There are limits on the data. There’s only so much you’re going to get out of these tiny little 20-second drawings. But I think the work done by the main [Sketch-RNN] researcher, David Ha, was really beautiful. He basically trained a recurrent neural network to learn how to reproduce these drawings. He sort of forced the model to learn what’s important. The model wasn’t powerful enough to memorize the entire drawing. Because it can’t memorize all the strokes it’s seeing, its job is just to reproduce lots of cats or whatever, it’s forced to learn what’s important about cats—what are the shared aspects of cat drawings across millions of cat drawings? And so when you play with this model you can ask it to generate new cats out of thin air. It generates really interesting looking cats that look, I think, uncannily like how people would draw cats. I read that you’re working with Magenta to teach computers to tell jokes. What kind of jokes do computers generate? (That was not itself the first line of a joke.) The project was very preliminary, very exploratory, asking the question: can we understand that component of joke telling which is about surprise? Especially punch-line-related jokes, and puns, there’s clearly a point where everything’s running along as normal, I think I know what’s going on with this sentence, and then, boom! Right? And also I think, intuitively, there’s a geometry to the punch line. It’s surprising if the building collapses on your head; [a punch line is] not that kind of surprise. It’s, like, oh, right, I get it! You know? And that sense of “I get it” is, I think, a kind of backtracking you’re forced to do to get it. So we were looking at particular kinds of machine-learning models that can generate these things called truth vectors that are trying to understand what’s happening semantically in a sentence and then, can we actively manipulate those to get a different effect? And the kind of joke we were hearing about was … “The magician was so angry she pulled her hare out.” And the pun of hare and hair, and rabbit—you get it, right? Yeah. But you have to know a lot about words and language to understand it. Yeah, you have to know a lot. Not only did this model not tell any jokes, funny or not, but we didn’t actually get the code to converge. What are you in the middle of trying to figure out with Magenta right now?
Trying to understand more of the long-term structure with music and also trying to branch out into another interesting question, which is: can we learn from the feedback, not from an artist, but from an audience? This is looking at the artistic process as kind of iterative. The Beatles had 12 albums and every one of them was different. And they were all showing that these musicians are learning from feedback they’re getting from peers and from crowds, but also other things that are happening with other artists. They’re really tied in with culture. Artists are not static. And this very simple idea: can you have someone making something with a generative model, putting it out there, but then taking advantage of the fact that the feedback they get? “Oh, that was good, that was bad.” That feedback that we get, the artist can learn from that in one way, but maybe the machine-learning model can learn from it as well, and say, “Oh, I see, here are all the people and here’s what they think of what I’m doing, and I have these parameters.” And we can set those parameters vis-à-vis the feedback, using reinforcement learning, and we’re working on that, too. As I listen to music created with Magenta, I wonder: if you’re using data to train artificial intelligence, can the AI then create anything truly original, or will it just be derivative of what it’s been trained on, whether that’s Madonna songs or impressionist paintings, or both? I think it depends on what we mean by original. I think it’s unlikely to me that a machine-learning algorithm is going to come along and generate some transformative new way of doing art. I think a person working with this technology might be able to do that. And I think we’re just so, so, so far from this AI having a sense of what the world is really like. Like it’s just so, so far away. At the same time, I think that a lot of art is original in another sense. Like, I do one more cool EDM song with the drop at the right place, that’s fun to dance to and is new, but maybe is not, like, creating a completely new genre. And I think that kind of creativity is really interesting anyway. That by and large most of what we do is sitting in a genre we kind of understand, and we’re trying new things, and that kind of creativity I think the AI that we have now can play a huge role in. It’s not reproducing the data set, right? It’s mixing things up. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on AI.
Google,
Google Brain,
AI,
artificial intelligence,
deep learning,
machine learning,
Magenta,
open source,
Douglas Eck,
music,
art,
creativity,
EmTech MIT 2017,
MIT Technology Review Events
Rachel Metz Senior Editor, Mobile As MIT Technology Review’s senior editor for mobile, I cover a wide variety of startups and write gadget reviews out of our San Francisco office. I’m curious about tech innovation, and I’m always on the lookout for the next big thing. Before… More arriving at MIT Technology Review in early 2012, I spent five years as a technology reporter at the Associated Press, covering companies including Apple, Amazon, and eBay, and penning reviews.
Please read our commenting guidelines.
More videos
Connectivity
Connectivity
Connectivity
Connectivity
What it means to be constantly connected with each other and vast sources of information.
The company’s cryptocurrency has also seen an incredible run-up in value, but investors may have gotten the wrong idea. by
Mike Orcutt
High-resolution nano/micro 3-D printing meets modern demands for precision component manufacturing. by
MIT Technology Review Insights
Researchers have found worrying security holes in apps companies use to control industrial processes. by
Martin Giles
More from Connectivity
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
7c1f2cbf05,Deep Neural Networks for Face Detection Explained on Apple's Machine Learning Journal - Mac Rumors,"But bahgawd, the technology Apple is pulling off really is falling squarely into the realm of magical. Anyone else just wowed by the amount of technology embedded into this new iPhone?
Our phones are learning more about us then we ever knew Before. Siri-based speaker unveiled at WWDC, launching early 2018.   MacRumors attracts a broad audience
of both consumers and professionals interested in
the latest technologies and products. We also boast an active community focused on
purchasing decisions and technical aspects of the iPhone, iPod, iPad, and Mac platforms."
9f972c14e5,How DCU machine translation experts are transforming communication,"Weekend takeaway: Cosy up with 10 great sci-tech reads Moral licensing in the world of design The countdown is on to Ireland’s sci-tech extravaganza, Inspirefest 2017 Time running out to get your hands on Inspirefest early bird tickets Construction begins on €500m Limerick Twenty Thirty development Dave, technology and the power of people Facebook’s major update: 5 things you need to think about YouTube cuts business ties with Logan Paul after video furore Mixed results for Apple iOS but China remains a market bright spot Apple responds to concerns over ‘smartphone addiction’ in children Google consolidating payment services as ‘Google Pay’ brand France launches investigation into Apple ‘planned obsolescence’ Just how will PSD2 unleash an open banking revolution in Europe? Ecuador gives Julian Assange citizenship after five-year embassy stay Fourth industrial revolution: Can algorithms replace government? A positive attitude is a crucial element for GDPR compliance Carphone Warehouse hit with £400,000 fine over data breach Chinese spying concerns hamper Huawei US retail expansion Telecoms giant AT&T plans to roll out 5G in 12 US cities this year Planting the green shoots of fibre and 5G ComReg fines Three €575,000 over contract changes 9 pioneers joining the dots of the connected world Virgin switches on free public Wi-Fi in Drogheda and Gorey BT and Sky reach a very sporting TV deal Enterprise Ireland reveals first start-up fund of 2018, worth €750,000 The importance of trademarks when choosing a domain name HBAN sets goal of raising €7m in new angel investment Irish start-ups with women founders secured €79m out of €580m in 2017 Startup Banking champions new early-stage funding model for European founders Dublin’s Digital Hub grows to 85 companies, employing 732 people Sci-Tech 100 2018: The Game-Changers of 2018 8 cool gift ideas for a grown-up geek 15 inspiring gift ideas for clever and curious kids Weekend takeaway: Let your ideas fly into the sci-tech sky Challenge yourself with these 10 Maths Week brainteasers An Cosán co-founder Dr Ann Louise Gilligan dies after short illness India celebrates milestone of 100th satellite in space Meet the woman running the show at BT Young Scientist BT Young Scientist: Just how filthy are pedestrian-crossing buttons? Ireland expected to introduce microbeads ban by end of 2018 BT Young Scientist: Positive thinking about robots, nature and lawnmowers ‘No way, that can’t be right’: Possible answer to strange fast radio bursts Rural regions could get massive energy boost with algae breakthrough 5 disruptive technologies we can embrace in healthcare Samsung signs €4m deal with Insight Centre to build next-generation AI Russia admits airbase came under attack from 13 armed drones Intel reveals quantum computing breakthrough at CES 2018 New IoT breakthrough could finally usher in widespread graphene sensors How many people could live on Earth? Forget a climate shift in centuries, more like decades, climatologists claim WeForest doubles crowdfunding target to empower Indian village World awaits birth of ‘baby dragons’ in Slovenia Gold and platinum discovered in south-east Irish streams The bees are still in trouble, so we are too Office clichés: The definitive guide to stereotypes in the workplace Investment in skills is the key to future-proofing businesses Ireland has Europe’s fastest-growing tech worker population New year, new opportunities: The latest from the Careers section Computer science will now be a Leaving Cert subject Gender wage gaps are now officially illegal in Iceland How to unplug from the office after work An email template for freelancers chasing payments Everything you need to know about online networking What to do if you want to take a sick day from work for your mental health How to fight against loneliness in the workplace 7 bad habits to change at work ‘Active listening skills have allowed me to become a better leader’ ‘Irish people are the friendliest in the world!’ Cork is attracting the best and brightest in tech talent to its doorstep Arpit Jain had no second thoughts when he got an offer from PTC Ireland Kemp Technologies is success-driven and at the forefront of innovation After years abroad, PwC welcomed Alana McMahon home with open arms 6 top international companies hiring in data right now How do companies ensure diversity in their workforce? 6 companies hiring in fintech right now 7 US companies hiring in Ireland right now 7 of the coolest science jobs in the world Thinking about a career in marketing? An analytical mind is helpful What are the essential skills for a software engineer? NK Communications reveals 25 new positions in Limerick and Cork Almost 20,000 hired by FDI companies in Ireland in 2017, says IDA 10 tech job markets that will explode in 2018 Across Ireland, 14,834 roles in 152 jobs announcements in past year Derry digital firm Wurkhouse to create 30 new jobs Are Irish people becoming addicted to their smartphones? WhatsApp debuts feature that allows users to delete accidental messages Inspirefest: The Podcast makes its debut with stellar first episode Irish teachers and parents don’t feel equipped to discuss online safety Rents in Dublin have ballooned by 37pc since 2012, report shows Game of Thrones fans can now plan trips to Northern Irish filming locations with new digital map Google launches three new photography apps for iOS and Android Facebook launches Messenger Kids app for under-13s Twitter is testing a feature allowing users to bookmark tweets Warning: Illegal streaming boxes are a fire hazard Instagram now allows users to add any video or image to Stories Google launches Family Link internet safety app in Ireland League of Ireland teams establish major national e-sports tournament Just what Pinterest users were pining for: boards within boards It feels cinematic: YouTube rolls out a major redesign across mobile and desktop S-Town and My Dad Wrote a Porno on way to Dublin for podcast fest Google adds SOS alert capabilities to Search and Maps Google Maps brings Street View to the International Space Station Trump meets Ophelia: Ireland’s year in Google searches Conor McGregor and a rogue bat: Ireland’s top YouTube videos of the year Manchester tragedy and McGregor fight topped talk on Facebook in 2017 #RepealThe8th storms past #Ophelia in top Irish Twitter trends of 2017 The best Black Friday advice and 5 deals worth a look Who are Ireland’s most-followed people on Twitter? Engineering inclusion in the tech industry using data Nilofer Merchant explains the concept of ‘onlyness’ Enterprise Ireland figures are promising as a new year of Brexit uncertainty dawns Accenture Open House: AI, Data Science & Big Data Machine learning is the next big shift for business, says Google Inspirefest Pay It Forward youth groups take in a tour of Facebook by Ellen Tannam
13 Nov 2017311 Shares
Prof Andy Way of Dublin City University. Image: DCU Machine translation is helping people work through language barriers, and creating a world that will make communication easier, even in times of crisis.
Prof Andy Way is a machine translation (MT) expert. He has been working in this area for the past 30 years, and currently leads the Adapt Centre’s MT team at Dublin City University (DCU). Way explained how MT was a good career choice for him: “My first degree was French, German and linguistics, and then I did a master’s in computing, so doing MT was a perfect marriage of the two disciplines.” After completing a master’s, Way went on to undertake a PhD in MT. Way explained that there have been some significant changes in the approaches to MT over the years. “From the late ’80s to about 2015, the dominant approach to MT was statistical (SMT). We needed large amounts of parallel data, ie source sentences and their human-provided translations, to build our statistical translation models, which essentially would suggest target-language words and phrases, which the model believed to be translations of the source sentence. “Following this process, it was then the job of the target language model – built from a large collection of monolingual data – to rearrange these words and phrases to produce the most fluent output according to the model.” Essentially, SMT worked by identifying patterns in large collections of text, “which could be brought to bear when processing new, previously unseen texts”. The last three years in MT have also seen neural MT (NMT) come to the fore. With NMT, all a research team needs is parallel data. The dominant model encodes the source sentence into a numerical vector representation, “which is in turn sent en bloc to the target-language decoder, whose job it is to generate the most likely target text from that vector”. Way explained that NMT typically outperforms SMT and could be considered the “new state of the art”, citing more fluent translations and better word order as results. NMT does require much bigger training datasets, and models generally also take longer to train. In terms of the application of MT at the Adapt Centre, Way and his team tackle language barriers that are “key challenges in enabling content to flow fluently across the globe”. He explained that language shouldn’t be an obstacle when it comes to accessing information online, and machine translation is “becoming ever more important in facilitating access to content using dynamic transformation techniques”. Way also explained how Adapt’s industry partners reap many benefits by engaging with the MT team at DCU. “[They] increase their competitiveness and reach across a wider set of industry vertical sectors.” The team also works on developing “robust, high-quality Irish-English MT systems”, which are being used on a daily basis by Government departments, providing increasingly important societal and economic benefits to people in Ireland, especially as we approach the expiration of the Irish language derogation afforded by the European Commission. The work done by the Adapt MT team, led by Way, is a wonderful example of the real-life benefits of advances in computer science, which are already producing amazing results. Interact is a project coordinated by Prof Sharon O’Brien at DCU and has partners such as Translators Without Borders, Microsoft, Unbabel, UCL and ASU. The project is concerned with the provision of reliable MT services in crisis scenarios. Way points to the example of the 2010 Haiti earthquake, when “the world’s relief services arrived only to find that the locals only spoke Haitian Creole, so communication was impossible”. At the time, Microsoft put together an MT translation system, and the work Interact is doing runs along similar lines. Way explained how the process works: “We’re trying to do much the same here, for a range of non-major language pairs, by pivoting through a well-resourced language; for example, there isn’t much Greek-Arabic parallel data around, but there is plenty of Greek-English and English-Arabic data, so we will be able to build a Greek-English-Arabic system with English as the pivot language. Other pivot set-ups include German-English-Arabic and French-English-Swahili.” Another MT project that benefits millions is the creation of a software translation tool created by Adapt Centre graduate and Microsoft researcher Dr Sandipan Dandapat. Bangla is the seventh most spoken language in the world. The development of the tool that can help 215m Bangla speakers began while Dandapat was at Adapt during a Microsoft internship. Dandapat said: “As Bangla is also my native language, it was especially rewarding to have the opportunity to bring this project to completion.” Helming a team that works on such major projects, Way is in a good position to examine the future impact MT could have on the world and how we all communicate. “MT quality is now good enough that it has been demonstrated to be a key enabler in the translation pipeline for many use cases in different domains and for a range of end users. For example, Google Translate alone translates around 150bn words per day, every day.” Way cited the emergence of more and more use cases, such as the instantaneous translation of massive volumes of tweets and other user-generated content. He said this area is where the “Adapt MT team is playing a leading role, where MT is the only hope. Human translators cannot work fast enough to meet such a huge translation demand and, in any case, the MT output is ‘good enough’ to allow users to understand the original content.” In 2014, Adapt’s Brazilator project translated 83m words for 26 language pairs in real time, in tweets related to the FIFA World Cup. “By our estimation, around 1,300 human translators would have had to work full-time to do the same job in the time available, at a cost of over €3m. Clearly that would have been unfeasible, so there are opportunities for MT to offer new services where there currently is no provision.” So, you might ask, where does the MT boom leave real-life, human translators? Way explained that it’s not a case that MT will ever replace what an individual expert can do. “This is no threat to human translators; it is estimated that only around 5pc of all content that could be translated actually is, as there simply aren’t enough human translators to meet this demand.” He also emphasised that there would always be a need for a “human in the loop” to try and catch errors that MT models could make, and post-edit them. Although MT quality is now invariably good, Way said: “It will always make some mistakes, which are often difficult to predict. “For example, NMT now produces some excellent translations but, all of a sudden, it’ll produce a translation which has very little to do with the sentence being translated. “Given that a lot of the ‘knowledge’ is encoded in the system’s hidden layers, although quality has definitely improved, our understanding of what’s going on has not.” He also made the point that the MT model training data itself comes from human translators. “We use some of that data for automatic testing of our systems. We need human expertise to tell us what errors our systems are making.” Like all machine learning endeavours, there are ethical concerns, ones that the experts are constantly cognisant of, said Way. For example, who owns the parallel translation data (and any derivatives that ensue) used to train the MT systems? Who is liable if someone suffers (in all meanings of the word) from an MT error? Way also stressed the need for fair rates of pay for translators and post-editors who do vital work. For him, the Adapt team, and machine translation experts in general, “the human in the loop will always be the most important link in the chain. We MT developers are just trying to make technology-savvy translators better, not replace them.” It’s a fascinating field, and one that is metamorphosising at a rapid rate. MT models are helping all of us – from when we use Google Translate to find the meaning of a Spanish word on holidays, to creating an environment where more lives can be saved through clear communications in the aftermath of natural disasters. Way and the Adapt team are at the coalface of major and exciting developments that will improve how we all connect and engage with one another. In-Depth: Science Week, More on Dublin City University Related: Dublin City University, science, AI, computer science, Science Foundation Ireland
Ellen Tannam is a writer covering all manner of business and tech subjects
editorial@siliconrepublic.com
2 days ago52 Shares
Join us to create the technology of tomorrow The largest employer in northwest Ireland Keeps files safe, synced and easy to share Help develop the future of sales and trading 2016 Graduate Employer of the Year
5 Dec 201751 Shares
1 Dec 2017104 Shares
17 Nov 2017442 Shares
17 Nov 201770 Shares
18 Dec 2017490 Shares
35 minutes ago
45 minutes ago
1 hour ago
1 hour ago
2 hours ago
17 hours ago
17 hours ago
18 hours ago
19 hours ago
20 hours ago
21 hours ago
22 hours ago
22 hours ago
23 hours ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
1 day ago
All content copyright 2002-2018 Silicon Republic Knowledge & Events Management Ltd. Reproduction without explicit permission is prohibited. All rights reserved.
Designed by Zero-G and Square1.io
Our Website uses cookies to improve your experience.
Please visit our Privacy Policy page for more information about cookies and how we use them."
1f8f1cd3d3,How Machine Learning Fuels Your Netflix Addiction - RTInsights,"Every recommended video you see on Netflix was selected by an algorithm. The company estimates its algorithms produce $1 billion a year in value from customer retention. When intuition fails, data from machine learning can win, according to a recent paper describing Netflix’s  recommendations system. On a Netflix screen, a user is presented with about 40 rows of video categories, with each row containing up to 75 videos, according to the paper, which was published in the Dec. 2015 issue of ACM Transactions on Management Information Systems (TMIS). While one would think such a large selection would keep viewers browsing, too much choice can be counterproductive. Typically a viewer loses interest after perhaps 60 to 90 seconds of trying to find a video to watch, write authors Carlos A. Gomez-Uribe and Neil Hunt of Netflix. Like any digital business, or even a supermarket with rows of breakfast cereal, Netflix has little time to catch the customer’s attention. The training and evaluation of algorithms takes place offline, while A/B testing takes place online. Image adapted from ACM Historically, Netflix relied heavily on customer ratings of videos when shipping DVDs by mail. Now Netflix has access to a much broader set of data—what each member watches, when they watch, the place on the Netflix screen the customer found the video, recommendations the customer didn’t choose, and the popularity of videos in the catalog. All of this data gets fed into several algorithms powered by statistical and machine-learning techniques. Approaches use both supervised (classification, regression) and unsupervised (dimensionality reduction through clustering or compression) approaches, according to the authors. A video-to-video similarity algorithm, or Sims, makes recommendations in the “Because You Watched” row. A personalized video ranker algorithm, or PVR, selects the order of videos in genre rows, using an arbitrary subset of the Netflix catalog. As Gomez-Uribe told Wired, “The closer to the first position on a row a title is, the more likely it will get played.” But PVR works better when it’s mixed with “unpersonalized popularity,” he and Hunt wrote in their paper. As an example, the authors describe recommendations for shows similar to “House of Cards.” While one might think that political or business dramas such as “The West Wing” or “Mad Men” would increase customer engagement, it turns out that popular but outside-of-genre titles such as “Parks and Recreation” and “Orange Is the New Black” fared better. The authors call this a case of “intuition failure.” Another algorithm is the “Top N ranker,” which makes recommendations in the “Top Picks” row. A “Trending Now” row uses short-term trends, such as an interest in holiday movies or films driven by weather events. These short-term trends are “powerful predictors of videos that our members will watch, especially when combined with the right dose of personalization,” the authors write.
The authors write that over the years, the recommendation system has decreased customer churn by several percentage points and saves the company about $1 billion a year. Personalized content also helps “find an audience even for relatively niche videos that would not make sense for broadcast TV models because their audiences would be too small to support significant advertising revenue, or to occupy a broadcast or cable channel time slot,” according to the paper. “This is very evident in our data, which show that our recommender system spreads viewing across many more videos much more evenly than would an unpersonalized system.” The paper also describes how Netflix uses “evidence” algorithms, which focus on what information to show a viewer about a movie (such as whether it won an Oscar); search algorithms; how Netflix performs A/B testing on its algorithms, and opportunities for improvement in testing. A copy of the paper can be found here.   Chris Raphael (full bio)
covers fast data technologies and business use cases for real-time analytics. Follow him on Twitter at raphaelc44."
16eac4968f,How newsrooms are using machine learning to make journalists' lives easier | Poynter,"By Gurman Bhatia · August 5, 2015
Tags: Much of what many journalists do every day doesn't involve gathering news.
Just consider the typical process for publishing a story: The reporter reports, writes (or produces) the content, and the editor makes suggestions for revision. Then comes fact-checking and proofreading and other processes focused on polishing the copy. After that are processes geared toward presenting and distributing the story: Selecting a photo, designing art, creating interactives and crafting headlines for social media and the Web that are attuned to search-engine optimization.
But this isn't the only way. Using ""machine learning,"" technologists at news outlets around the world are helping newsrooms eliminate extra time-consuming tasks and giving humans more time to do what they do best: reporting the news. And it's a good thing, too: As significant cuts hit the industry, the need for automation has become more urgent.
The New York Times Research and Development Lab and BBC News Labs are two such places. At both organizations, technologists are using machine learning tools and automation to make journalism less tedious and more valuable. Their aim is to make the information more ""structured"" by using tags within HTML (a.k.a. Web code or markup) or by adding additional metadata to non-textual content such as audio, photo or video. Categorizing content with these tags and metadata pays dividends further down the line, saving journalists time and effort.
How much time? These technologies could potentially eliminate the many browser tabs journalists use during a breaking news event to run searches on Facebook, Twitter, federal databases, Google news or archives.
And that's not all. The New York Times Research and Development Lab announced an experimental project, Editor, last week. The tool sorts through a story in real time looking for people, places, organizations and events and categorizes the text accordingly. In this story, for instance, the tool would have recognized two organizations that have been already been mentioned, namely The New York Times and the BBC.
The first step in this process is recognizing a term that can be categorized, or ""tagged."" The second step is linking that entity to existing databases (internally or externally) or microservices. The third step is making information from those databases accessible to journalists.
Editor currently recognizes locations, people, concepts and organizations as the reporter types in his or her story. It also has a context menu that allows the writer to mark up the title, the byline, pull quotes and key points of the story. Mike Dewar, a data scientist at the lab, calls these semantic tags.
When Dewar uses the term ""semantic,"" he is referring to how machine learning can be used to decipher meaningful connections and relationships within a piece of text.
""I think there could be all different kinds of microservices that service journalism,"" said Alexis Lloyd, creative director of the lab. ""You could imagine microservices that could do things like try to identify quotes from people and ones that could try to find relationships between people and organizations in the text.""
The possibilities are huge. Imagine, if you were quoting from a previous story, the technology helped you verify or link to the source. If you link a microservice with a campaign finance database, you could also access and attach information about a politician's biggest donor as you type in his or her name.
The difficulty comes when Editor is trying to recognize these entities. The computer needs to be able to differentiate between instances that seem similar but are actually different, such as the actor Denzel Washington, the state of Washington or the The Washington Post. Depending on the type of entity, Editor can then apply tags to access different services or tools throughout the system.
Editor, an experiment in publishing from NYT R&D on Vimeo.
Thousands of miles away from The New York Times, the BBC is also developing technology that can be used to make its journalism easier and more valuable.
Until last year, Jacqui Maher was building tools at The New York Times Research and Development Lab. But in January, she moved to London to work at BBC News Labs.
Instead of working on futuristic projects at The Times that were always five years out, she started building things that could be produced and were used on a short-term basis, she said. The maximum time span for her projects is now a year.
Most recently, Maher's team at the BBC has devised something called a structured journalism manifesto. The idea behind it is to use technology and machine learning to scale otherwise cumbersome tasks undertaken by humans.
Structured journalism, though a broad term, could be broken down to two domains: The reporter side, where automation helps improve a journalist's reporting and make it less cumbersome, and the audience side, where the tools help scale things that can improve the reader's experience. While tools such as Editor are built for the journalist, features like The Washington Post's Knowledge Map are outward-facing. When it comes to processes that scale large amounts of work, the latter usually requires the former.
Though Maher's team builds tools for both purposes, many of them come with a hardcore newsroom application. One of them, is called ""Juicer.""
Juicer, a tool that was instituted at the BBC in 2012, is at its core is an aggregator. It recognizes entities similar to the way The New York Times' Editor does, but it doesn't work in real-time and is trained on news sources outside of the BBC as well. The primary aim of of building Juicer was to refine the company's entity extraction abilities, Maher said.
Juicer recognizes tags in stories, something the team refers to as ""semantic entity extraction,"" and uses the company's ""Linked Data"" technology to categorize people, places and organizations.
Here is a demo of the tool in action:
The team has created different prototypes to use Juicer. An experimental news map project uses data from the tool and puts it on a global map. Click on a country and see the latest news that happened there.
The BBC has also integrated some of the Linked Data technology behind Juicer into their in-house content management system, CPS. Vivo, a tool built on top of CPS, lets writers discover recent content on the site under a particular topic.
Imagine a mashup of that with The New York Times' Editor. As you type in a name, say ""Ted Cruz,"" the tool would show you recent articles written about that particular person quickly, providing context in a short amount of time. Lloyd says The New York Times is capable of building such a tool in a future update to Editor.
While most organizations use these tags and structured information within text, the approach can also be used to categorize video or audio. Since the BBC is both a broadcast and digital news organization, staffers there have been using object recognition to classify and tag massive video and audio archives owned by the organization.
Maher says that BBC could make contextual information pop up as readers hover or click over keywords (as with The Washington Post's Knowledge Map,) but its focus is broader than that.
""But where it gets really interesting for the BBC is in our much more massive video and audio output,"" said Maher. ""Imagine as the different topics would deem appropriate – styling and interactions on video. On mobile even. And as for audio, we're just starting to explore what the experience could be.""
By way of example, she cites ""Pop-Up Video,"" a show on VH1 that took music videos and added context and fun facts with quote bubble overlays. Staffers at BBC News Labs have already been running object recognition software on their video archives and tagging instances and elements that are part of a video to build a database of different instances.
""The Editor project does contain key ideas that are now being articulated under the umbrella of 'structured journalism' — namely that having more comprehensive and fine-grained structured data about our reporting will enable us to create all kinds of new tools for journalists and experiences for readers,"" Lloyd said. ""These ideas have been central to our work at the lab for the past couple of years and have been explored in other ways through projects like Lazarus, Madison, and Kepler.""
She sees the Editor project as the future of computational journalism and artificial intelligence. The future of news, she says, is one where newsrooms have collaborative systems between people and machines, ""Where people can do the things that they are uniquely good at and computers can do the things that they are uniquely good at.""
""The industry continues to face significant cuts,"" Maher said. ""We want to do more. We have to do more. We have to reach where people are. It is a complicated task. We want to reach them on radio waves, we want to reach them on their TV sets.""
Although she agrees that the most efficient way is to hire more editors, that's not exactly practical.
""We have to embrace automation where we can,"" she said.
CareerLeadership
Career
I report, write and produce interactives for Poynter.org as the institute's 2015 Google Journalism Fellow. Tweet me @gurmanbhatia or email at gbhatia@poynter.org.
View the discussion thread.
ⓒ All Rights Reserved Poynter Institute 2018"
f37ab52bf5,An AI Found Dozens of Gravitational Lens Galaxies,"The algorithm accomplished in a few weeks what it has taken astronomers decades to do. There are roughly 100 billion galaxies in our universe, and it would take many lifetimes for humans to look at them all. Even if our scientists only focused on a tiny sliver of the night sky, there's still enough space to keep any person busy for decades, if not centuries.
Among the many things astronomers could spend decades searching for are gravitational lenses. When one galaxy sits in front of another one, the light from the more distant galaxy in the background can curve around the nearer one due to its gravity, causing a magnification effect. But in a universe of 100 billion galaxies, finding these arrangements can be tough, so astronomers are increasingly turning to artificial intelligence to find gravitational lenses for them. One group of astronomers, from the universities of Groningen, Naples and Bonn, have used just such an AI to find 56 gravitational lens candidates. Their research was recently published in the most recent issue of the Monthly Notices of the Royal Astronomical Society. Finding gravitational lenses isn't easy. Two galaxies have to be almost perfectly aligned for a gravitational lens to occur, which makes them very rare. Further, even if a possible candidate is found, it requires scientists to analyze it closely to make sure that it is in fact a lens instead of just an irregularly shaped galaxy or other interference. The result is that scientists have spent decades combing through galaxy images and only found a few dozen gravitational lenses in that time.
Study author Carlo Enrico Petrillo was growing frustrated and overwhelmed by the sheer amount of data to comb through. ""We don't have enough astronomers to cope with this,"" he says. To fix the problem, he and his colleagues developed a convolutional neural network, similar to the image-recognition algorithms used by Google and other companies. They trained the algorithm to spot gravitational lenses, then set it loose on data from the Kilo-Degree Survey. Analyzing 255 square degrees of the night sky, the algorithm found 761 gravitational lens candidates, which the team reviewed and narrowed down to 56. While these candidates still have to be confirmed, they should dramatically increase the number of known gravitational lenses. Once even more gravitational lenses are discovered, astronomers can use other artificial intelligence systems to analyze them. A few months ago, a different team of astronomers at the SLAC National Accelerator Laboratory and Stanford University developed another AI to study gravitational lenses. The system is designed to analyze images of gravitational lenses to work out what the more distant galaxy looks like and some of the properties of the lensing galaxy in the foreground. With the combination of these two machine learning systems, and the increasing use of artificial intelligence in astronomy, the wonders that will be discovered in the deep universe are anyone's guess.
Source: Netherlands Research School for Astronomy"
6560ab184d,Machine Learning Is Making it Difficult to Tell Humans and Computers Apart,"by
Dom Galeon
on October 30, 2017
8888
If you’ve ever been browsing the internet, spotted something interesting like a video or an article that you wanted to read, only to discover that before you can view it, the website asks you to prove that you’re human. You’re usually instructed to complete a simple task, such as typing out a word that’s hidden under a bunch of squiggly lines, or to correctly identify which image matches a certain description. While such tasks can be annoying, the test is a necessary component of cybersecurity in the modern age; at least in terms of telling human users from bots. But advances in machine learning may mean that’s soon to change. This is called a Completely Automated Public Turing test — better known as a CAPTCHA. New research from artificial intelligence (AI) company Vicarious has found that while these have become a cybersecurity staple, it may only be a matter of time before bots can outsmart them. Vicarious developed a machine learning algorithm that’s able to mimic the human brain, which uses a computer vision model they dubbed Recursive Cortical Network (RCN) to simulate what we call “common sense.” “For common sense to be effective it needs to be amenable to answer a variety of hypotheticals — a faculty that we call imagination,” according to a blog posted by Vicarious. Details of this AI model have been published in the journal Science. Essentially, what Vicarious’ RCN uses a techniques derived from human reasoning to parse a text, like recognizing the letter A, by building its own version of a neural network. Vicarious’ RCN was able to answer BotDetect system CAPTCHAs, with a 57 percent accuracy. RCN can understand CAPTCHAs faster than other deep learning algorithms. While the goal of this research wasn’t to break down CAPTCHAs — it did. That begs the question: does this mean computer systems are more vulnerable to cybersecurity threats? If machines can crack CAPTCHAs like humans, such cybersecurity measures would be rendered ineffective and obsolete. CAPTCHAs, while quite common, aren’t the only layer of security computer systems employ. AI is making it difficult — if not nearly impossible — for today’s cybersecurity measures to differentiate between a human and a machine. It won’t be surprising, then, if AIs become useful to hackers. Indeed, the U.S. Department of Defense research arm, DARPA, wants to develop an AI hacker. On the flip side, AI can also be used to fight back against AI hacking. For example, scientists at the European Laboratory for Particle Physics (CERN) have been training a machine learning system to protect their data from cyber threats. A key challenge is teaching these intelligent algorithms to identify malicious network activity. Perhaps the next step could be identifying machines pretending to be humans? Computers are becoming more effective at replicating how the human brain works, although AI is arguably still far from actually becoming as smart as we are. Still, with an AI like the RCN, the lines are becoming more blurred. Machines can dupe other machines into thinking it’s a human —  albeit with limitations. As Vicarious CEO Dileep George previously told NPR, the development of such human-like capabilities by AI is the way technology is moving forward.
References:
Digital Trends, Vicarious, Science, NPR
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
18f9b0b69b,Deep Blue (chess computer) - Wikipedia,"Deep Blue was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls. Deep Blue won its first game against a world champion on 10 February 1996, when it defeated Garry Kasparov in game one of a six-game match. However, Kasparov won three and drew two of the following five games, defeating Deep Blue by a score of 4–2. Deep Blue was then heavily upgraded, and played Kasparov again in May 1997.[1] Deep Blue won game six, therefore winning the six-game rematch 3½–2½ and becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls.[2] Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.[3] Development for Deep Blue began in 1985 with the ChipTest project at Carnegie Mellon University. This project eventually evolved into Deep Thought, at which point the development team was hired by IBM.[4] The project evolved once more with the new name Deep Blue in 1989. Grandmaster Joel Benjamin was also part of the development team.
The project was started as ChipTest at Carnegie Mellon University by Feng-hsiung Hsu, followed by its successor, Deep Thought. After their graduation from Carnegie Mellon, Hsu, Thomas Anantharaman, and Murray Campbell from the Deep Thought team were hired by IBM Research to continue their quest to build a chess machine that could defeat the world champion.[5] Hsu and Campbell joined IBM in autumn 1989, with Anantharaman following later.[6] Anantharaman subsequently left IBM for Wall Street and Arthur Joseph Hoane joined the team to perform programming tasks.[7] Jerry Brody, a long-time employee of IBM Research, was recruited for the team in 1990.[8] The team was managed first by Randy Moulic, followed by Chung-Jen (C J) Tan.[9] After Deep Thought's 1989 match against Kasparov, IBM held a contest to rename the chess machine and it became ""Deep Blue"", a play on IBM's nickname, ""Big Blue"".[10] After a scaled-down version of Deep Blue, Deep Blue Jr., played Grandmaster Joel Benjamin, Hsu and Campbell decided that Benjamin was the expert they were looking for to develop Deep Blue's opening book, and Benjamin was signed by IBM Research to assist with the preparations for Deep Blue's matches against Garry Kasparov.[11] In 1995 ""Deep Blue prototype"" (actually Deep Thought II, renamed for PR reasons) played in the 8th World Computer Chess Championship. Deep Blue prototype played the computer program Wchess to a draw while Wchess was running on a personal computer. In round 5 Deep Blue prototype had the white pieces and lost to the computer program Fritz 3 in 39 moves while Fritz was running on an Intel Pentium 90 MHz personal computer. In the end of the championship Deep Blue prototype was tied for second place with the computer program Junior while Junior was running on a personal computer.[12] Deep Blue and Kasparov played each other on two occasions. The first match began on 10 February 1996, in which Deep Blue became the first machine to win a chess game against a reigning world champion (Garry Kasparov) under regular time controls. However, Kasparov won three and drew two of the following five games, beating Deep Blue by a score of 4–2 (wins count 1 point, draws count ½ point). The match concluded on 17 February 1996. Deep Blue was then heavily upgraded (unofficially nicknamed ""Deeper Blue"")[13] and played Kasparov again in May 1997, winning the six-game rematch 3½–2½, ending on 11 May. Deep Blue won the deciding game six after Kasparov made a mistake in the opening, becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls. The system derived its playing strength mainly from brute force computing power. It was a massively parallel, RS/6000 SP Thin P2SC-based system with 30 nodes, with each node containing a 120 MHz P2SC microprocessor, enhanced with 480 special purpose VLSI chess chips. Its chess playing program was written in C and ran under the AIX operating system. It was capable of evaluating 200 million positions per second, twice as fast as the 1996 version. In June 1997, Deep Blue was the 259th most powerful supercomputer according to the TOP500 list, achieving 11.38 GFLOPS on the High-Performance LINPACK benchmark.[14] The Deep Blue chess computer that defeated Kasparov in 1997 would typically search to a depth of between six and eight moves to a maximum of twenty or even more moves in some situations.[15] David Levy and Monty Newborn estimate that one additional ply (half-move) increases the playing strength 50 to 70 Elo points.[16] Deep Blue's evaluation function was initially written in a generalized form, with many to-be-determined parameters (e.g. how important is a safe king position compared to a space advantage in the center, etc.). The optimal values for these parameters were then determined by the system itself, by analyzing thousands of master games. The evaluation function had been split into 8,000 parts, many of them designed for special positions. In the opening book there were over 4,000 positions and 700,000 grandmaster games. The endgame database contained many six piece endgames and five or fewer piece positions. Before the second match, the chess knowledge of the program was fine tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian[17]. When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused. However, Kasparov did study many popular PC games to become familiar with computer game play in general.[citation needed] Writer Nate Silver suggests that a bug in Deep Blue's software led to a seemingly random move (the 44th in the first game) which Kasparov misattributed to ""superior intelligence"".[18][19] Subsequently, Kasparov experienced a drop in performance due to anxiety in the following game.[19] After the loss, Kasparov said that he sometimes saw deep intelligence and creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine, which would be a violation of the rules. IBM denied that it cheated, saying the only human intervention occurred between games. The rules provided for the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files but IBM refused, although the company later published the logs on the Internet.[20] Kasparov demanded a rematch, but IBM refused and dismantled Deep Blue.[21] Owing to an insufficient sample of games between Deep Blue and officially rated chess players, a chess rating for Deep Blue was not established.[citation needed] In 2003 a documentary film was made that explored these claims. Entitled Game Over: Kasparov and the Machine, the film interviewed some people who suggest that Deep Blue's victory was a ploy by IBM to boost its stock value.[21] One of the cultural impacts of Deep Blue was the creation of a new game called Arimaa designed to be much more difficult for computers than chess.[22] One of the two racks that made up Deep Blue is on display at the National Museum of American History in their exhibit about the Information Age;[citation needed] the other rack appears at the Computer History Museum in the ""Artificial Intelligence and Robotics"" gallery of the Revolution exhibit.[23] Reports that Deep Blue was sold to United Airlines appear to originate from confusion between Deep Blue itself and other RS6000/SP2 systems.[24] Feng-hsiung Hsu later claimed in his book Behind Deep Blue that he had the rights to use the Deep Blue design to build a bigger machine independently of IBM to take Kasparov's rematch offer, but Kasparov refused a rematch.[25] Deep Blue, with its capability of evaluating 200 million positions per second, was the fastest computer to face a world chess champion. Today, in computer chess research and matches of world class players against computers, the focus of play has often shifted to software chess programs, rather than using dedicated chess hardware. Modern chess programs like Houdini, Rybka, Deep Fritz or Deep Junior are more efficient than the programs during Deep Blue's era. In a November 2006 match between Deep Fritz and world chess champion Vladimir Kramnik, the program ran on a computer system containing a dual-core Intel Xeon 5160 CPU, capable of evaluating only 8 million positions per second, but searching to an average depth of 17 to 18 plies in the middlegame thanks to heuristics; it won 4–2.[26][27] Bibliography"
a4f8e5bf4c,Bringing neural networks to cellphones | MIT News,"Login
or
Subscribe Newsletter
MIT researchers have designed new methods for paring down neural networks so that they’ll run more efficiently on handheld devices.
Image: Jose-Luis Olivares/MIT Method for modeling neural networks’ power consumption could help make the systems portable.
Larry Hardesty | MIT News Office
July 18, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
In recent years, the best-performing artificial-intelligence systems — in areas such as autonomous driving, speech recognition, computer vision, and automatic translation — have come courtesy of software systems known as neural networks. But neural networks take up a lot of memory and consume a lot of power, so they usually run on servers in the cloud, which receive data from desktop or mobile devices and then send back their analyses. Last year, MIT associate professor of electrical engineering and computer science Vivienne Sze and colleagues unveiled a new, energy-efficient computer chip optimized for neural networks, which could enable powerful artificial-intelligence systems to run locally on mobile devices. Now, Sze and her colleagues have approached the same problem from the opposite direction, with a battery of techniques for designing more energy-efficient neural networks. First, they developed an analytic method that can determine how much power a neural network will consume when run on a particular type of hardware. Then they used the method to evaluate new techniques for paring down neural networks so that they’ll run more efficiently on handheld devices. The researchers describe the work in a paper they’re presenting next week at the Computer Vision and Pattern Recognition Conference. In the paper, they report that the methods offered as much as a 73 percent reduction in power consumption over the standard implementation of neural networks, and as much as a 43 percent reduction over the best previous method for paring the networks down. Energy evaluator Loosely based on the anatomy of the brain, neural networks consist of thousands or even millions of simple but densely interconnected information-processing nodes, usually organized into layers. Different types of networks vary according to their number of layers, the number of connections between the nodes, and the number of nodes in each layer. The connections between nodes have “weights” associated with them, which determine how much a given node’s output will contribute to the next node’s computation. During training, in which the network is presented with examples of the computation it’s learning to perform, those weights are continually readjusted, until the output of the network’s last layer consistently corresponds with the result of the computation. “The first thing we did was develop an energy-modeling tool that accounts for data movement, transactions, and data flow,” Sze says. “If you give it a network architecture and the value of its weights, it will tell you how much energy this neural network will take. One of the questions that people had is ‘Is it more energy efficient to have a shallow network and more weights or a deeper network with fewer weights?’ This tool gives us better intuition as to where the energy is going, so that an algorithm designer could have a better understanding and use this as feedback. The second thing we did is that, now that we know where the energy is actually going, we started to use this model to drive our design of energy-efficient neural networks.” In the past, Sze explains, researchers attempting to reduce neural networks’ power consumption used a technique called “pruning.” Low-weight connections between nodes contribute very little to a neural network’s final output, so many of them can be safely eliminated, or pruned. Principled pruning With the aid of their energy model, Sze and her colleagues — first author Tien-Ju Yang and Yu-Hsin Chen, both graduate students in electrical engineering and computer science — varied this approach. Although cutting even a large number of low-weight connections can have little effect on a neural net’s output, cutting all of them probably would, so pruning techniques must have some mechanism for deciding when to stop. The MIT researchers thus begin pruning those layers of the network that consume the most energy. That way, the cuts translate to the greatest possible energy savings. They call this method “energy-aware pruning.” Weights in a neural network can be either positive or negative, so the researchers’ method also looks for cases in which connections with weights of opposite sign tend to cancel each other out. The inputs to a given node are the outputs of nodes in the layer below, multiplied by the weights of their connections. So the researchers’ method looks not only at the weights but also at the way the associated nodes handle training data. Only if groups of connections with positive and negative weights consistently offset each other can they be safely cut. This leads to more efficient networks with fewer connections than earlier pruning methods did. ""Recently, much activity in the deep-learning community has been directed toward development of efficient neural-network architectures for computationally constrained platforms,” says Hartwig Adam, the team lead for mobile vision at Google. “However, most of this research is focused on either reducing model size or computation, while for smartphones and many other devices energy consumption is of utmost importance because of battery usage and heat restrictions. This work is taking an innovative approach to CNN [convolutional neural net] architecture optimization that is directly guided by minimization of power consumption using a sophisticated new energy estimation tool, and it demonstrates large performance gains over computation-focused methods. I hope other researchers in the field will follow suit and adopt this general methodology to neural-network-model architecture design."" Topics: Research, School of Engineering, Artificial intelligence, Computer modeling, Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Research Laboratory of Electronics, Software, Efficiency This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
2855a06be7,Google’s WaveNet machine learning-based speech synthesis comes to Assistant  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Last year, Google showed off WaveNet, a new way of generating speech that didn’t rely on a bulky library of word bits or cheap shortcuts that result in stilted speech. WaveNet used machine learning to build a voice sample by sample, and the results were, as I put it then, “eerily convincing.” Previously bound to the lab, the tech has now been deployed in the latest version of Google Assistant. The general idea behind the tech was to recreate words and sentences not by coding grammatical and tonal rules manually, but allowing a machine learning system to see those patterns in speech and generate them sample by sample. A sample, in this case, being the tone generated every 1/16,000th of a second. At the time of its first release, WaveNet was extremely computationally expensive, taking a full second to generate 0.02 seconds of sound — so a two-second clip like “turn right at Cedar street” would take nearly two minutes to generate. As such, it was poorly suited to actual use (you’d have missed your turn by then) — which is why Google engineers set about improving it. The new, improved WaveNet generates sound at 20x real time — generating the same two-second clip in a tenth of a second. And it even creates sound at a higher sample rate: 24,000 samples per second, and at 16 versus 8 bits. Not that high-fidelity sound can really be appreciated in a smartphone speaker, but given today’s announcements, we can expect Assistant to appear in many more places soon. The voices generated by WaveNet sound considerably better than the state of the art concatenative systems used previously: Old and busted: New and hot: (More samples are available at the Deep Mind blog post, though presumably the Assistant will also sound like this soon.) WaveNet also has the admirable quality of being extremely easy to scale to other languages and accents. If you want it to speak with a Welsh accent, there’s no need to go in and fiddle with the vowel sounds yourself. Just give it a couple dozen hours of a Welsh person speaking and it’ll pick up the nuances itself. That said, the new voice is only available for U.S. English and Japanese right now, with no word on other languages yet. In keeping with the trend of “big tech companies doing what the other big tech companies are doing,” Apple, too, recently revamped its assistant (Siri, don’t you know) with a machine learning-powered speech model. That one’s different, though: it didn’t go so deep into the sound as to recreate it at the sample level, but stopped at the (still quite low) level of half-phones, or fractions of a phoneme. The team behind WaveNet plans to publish its work publicly soon, but for now you’ll have to be satisfied with their promises that it works and performs much better than before. Latest headlines delivered to you daily"
6d96905780,How recommendation algorithms know what you'll like | TechRadar,"By
PC Plus
2012-05-09T10:00:00.129Z
Internet
One of the biggest innovations in online shopping - first introduced by Amazon - was the automatically generated recommendation. You logged onto the site and, right there on the home page, the site would make suggestions for products you could purchase. For example, if you were a JavaScript developer like me, you'd see recommendations for programming books using that language, whereas if you were a mother with young kids, you'd see mentions of toys and children's books.
This personalisation of the home page has a big benefit for the online store compared to just displaying top 10 lists or banner ads: the click-through and conversion rates are far higher. Customers are more likely to view and buy the suggested products. The prediction algorithms then are of huge importance to online stores - the more accurate they are, the more the online store will sell. Consider though the problems that must be solved by such a recommendation algorithm. A large online store like Amazon may have millions of customers and millions of items in stock. New customers will have limited information about their preferences, while more established customers may have too much. The data on which these algorithms work is constantly updated and changed. Customers are browsing the site and the prediction algorithm should take the recently browsed items into consideration, for example - it doesn't help if I'm looking for a toy for my youngest niece and all I get are recommendations for jQuery. The biggest and most important criterion for these systems (apart from accuracy) is speed. The recommendation algorithm must produce suggestions within a second or so. After all, the user is in the process of displaying the store's home page where the recommendations will appear. Traditionally, these recommendation algorithms have worked by finding similar customers in the database. In other words, they work by finding a set of customers who have bought or rated the same items that you have. Throw out the items you've already purchased or commented on, and then recommend the rest. For example, if I've already bought A and B, and the set of such customers also includes purchases for C, then C is recommended to me. One of the earliest such algorithms is known as collaborative filtering. In essence, the algorithm represents each customer as a vector of all items on sale. Each entry in the vector is positive if the customer bought or rated the item, negative if the customer disliked the item, or empty if the customer has not made his or her opinion known. Most of the entries are empty for most of the customers. Some variants factor in the popularity of the items to bump up the significance of items that are less popular or familiar. The algorithm then creates its recommendations by calculating a similarity value between the current customer and everyone else. The most acceptable way to do this is to calculate the angle between the vectors - the simplest method being to calculate the cosine using the dot product divided by the product of the vector lengths. The larger the cosine, the smaller the angle, and therefore the more similar the customers. As you may have surmised, this process is computationally expensive. There are usually a lot of customers and a lot of calculations have to take place very quickly. There are techniques to reduce the computation (by sampling the customer base or ignoring unpopular items, for example), but in general it's always going to be expensive to calculate recommendations this way. Another traditional prediction algorithm involves the use of cluster models. Here the goal is to prepare the customer base by dividing it into clusters and then to assign the current customer to one of the clusters, in theory choosing the cluster with the most similarity. Once the cluster has been identified, the recommendations come from the purchases and ratings from other customers in that particular cluster. Although the choice of cluster works in roughly the same way as the classification algorithm (we assume that we can calculate a characteristic vector that describes the cluster in much the same way that there is a vector per customer), the real meat of the algorithm is in the creation of the clusters. In general, clustering of customer data is done through a heuristic: start off with some number of empty clusters, assign a randomly selected customer to each, and then assign the other customers to the clusters according to similarity. Since the initial clusters are essentially randomly created, sub-algorithms must be used to merge or split clusters as they are being built up. Using cluster models is less computationally intensive at the point where you need to make recommendations quickly for a customer. After all, there's less work to be done to find a similar cluster rather than a similar customer. If you like, most of the work is done up front in the creation of the clusters themselves. Unfortunately, this particular method tends to result in low quality recommendations since the purchases/ratings are averaged out within a cluster. No longer is a particular customer matched to the most similar customer, but instead to the average of a large group of customers. Certainly the number of clusters can be increased to refine the matching, but then you run into the possibility of increasing computation time. The next traditional algorithm is a fairly simple search algorithm. For example, if I buy Our Kind of Traitor by John Le Carré, the search algorithm would query the items database for other books by John Le Carré, spy books by other authors, DVDs of movies made from Le Carré books, and so on so forth. You can see targeted recommendations like these in banner ads as you surf the internet. I recently searched for a messenger bag (and bought one from Ona), and for a good two weeks afterwards, it was as if every website I visited wanted to recommend Timbuktu bags to me. I'm sure this kind of stalker effect has happened to you and, as you'll know, the recommendations offered tend to be low quality. What Amazon did to improve its recommendations was to turn collaborative filtering on its head. Instead of trying to find similar customers, it matched up items. Its version is called item-to-item collaborative filtering. This algorithm matches each of the current customer's purchased and rated items to similar items and then builds a list from those matched items. First of all, then, the web site must build a 'similar items table' by analysing the items customers tend to purchase together. Here's how this works: for each item X in the catalog, find all customers C who purchased X. For each of those customers, find all items Y purchased by C and record that a customer bought X and Y. Then, for all pairs X and Y, calculate the similarity between X and Y in the same manner as for the collaborative filtering algorithm. Although this calculation is fairly computationally expensive, it can be done beforehand. Once the similarity between every pair of items has been determined, the recommendation list follows easily.
The example above shows a simple example where we just have three customers purchasing four products: Alan, Brian, Cory, and Book, DVD, Toy, Jeans. Taking Book first, we see that all the customers purchased it. (We'll assume that 'purchased it' equals 1, and 'didn't purchase it' equals 0.) The similarity between Book and DVD is 0.58, between Book and Toy or Jeans is 0.82. For DVD, only Alan bought it, and he also bought Book and Jeans. The similarity between DVD and Book or Jeans is therefore 1.0. Notice that similarity is not necessarily associative, but in general, over many products and customers, it will be close. From this, if David lands on the page for Book, we can recommend the product Toy or Jeans to him equally, then DVD. If he lands on DVD, we'll suggest Book and Jeans equally, and so on.
The above example shows the situation if, instead of simply buy/didn't buy, we use the customers' ratings instead. (If a customer bought but didn't rate the product, we could assign a neutral rating like 3 to the purchase.) I calculate that the similarity between Book and DVD is 0.56, between Book and Toy 0.82, between Book and Jeans 0.70. Therefore David, on visiting Book, would be recommended Toy, Jeans and DVD in that order. Netflix uses a home-brewed recommendation algorithm called CineMatch. Back in 2006, the film rental company announced a competition with a million dollar prize to see if anyone could improve on the recommendations provided by CineMatch. The goal was a 10 per cent improvement on CineMatch's score on a test subset of the vast Netflix database. The winner, after nearly three years, was a group that called itself BellKor. The competitors who accepted the challenge were given a huge dataset of 100 million ratings, with each rating (between one and five stars) containing the date of the rating, the title and release year of the movie, and an anonymous ID representing the user. They were also provided a qualifying dataset of a selection of 2.8 million ratings with the same information, but without the actual rating. The object was to devise an algorithm from the large dataset, apply it to the qualifying dataset to guess the ratings, and then Netflix would see how close the guessed ratings were to the actual ratings. It's fascinating to see the strategies BellKor used in order to tune its algorithm. It must be stressed that BellKor's solution is not a single algorithm per se, but can be viewed instead as a series of algorithmic knobs that can be twiddled to produce the best answer. The first strategy was to create a set of baseline predictors. These describe a user's ratings on the average. Suppose that the average rating over all movies is 3.5. As an example of a specific film, Star Wars might have an overall rating of 4, which would be 0.5 better than the average movie. Our hypothetical user though tends to rate movies lower than the mean: say his average over all movies was 3.2. Since he averages 0.3 lower than the mean, our initial guess for how he might rate Star Wars would be 4 – 0.3, or 3.7. The second strategy used was the realisation that time plays a big part in people's ratings. Firstly, a movie's popularity will change over time. For example, a movie may start big and then be forgotten, whereas others may start small and then become cult films. Popularity is also affected by the star or director when they release a better (or worse) movie subsequently, and by their appearance, good or bad, in the media. A user's overall ratings tend to drift over time. This could be because the 'user' is actually a whole household, and so the person making the ratings may change, or it could be because of the psychological effect that after the user has made a run of good ratings, their next rating may be better than would otherwise be warranted (or vice versa: after a run of bad movies, the next good film may be rated lower than expected). The next strategy could also be described as partially temporal: the user may 'batch' up and enter ratings for a set of recently-viewed movies on one day. The user would tend to let the ratings influence each other unconsciously (if most movies were good, the bad movies would tend to get better than expected ratings, or vice versa), rather than considering them all independently. This strategy is known as frequency. The BellKor development team then described these strategies mathematically and statistically to provide them parameters to the model that could be tweaked. Taking a large subset of the training data, it repeatedly ran the model, changing the parameters bit by bit, until it predicted the remaining smaller subset's ratings. At that point it was able to submit its guesses for the qualifying subset. From all this, I'm sure that you can see that prediction algorithms are certainly not exact. But then again, providing they are fast and generally accurate enough, it doesn't matter. For Amazon, the recommendation engine is a differentiating factor, and for Netflix it's a primary reason for keeping customers in their subscriptions - after all, once a user has watched Star Wars and its collection of sequels/prequels, they're going to want suggestions for other things or they'll cancel. Get the best tech deals, reviews, product advice, competitions, unmissable tech news and more! TechRadar is part of Future plc, an international media group and leading digital publisher. Visit our corporate site. ©
Future Publishing Limited
Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
a2827fbf5d,"Model evaluation, model selection, and algorithm selection in machine learning","Jun 11, 2016 by Sebastian Raschka Machine learning has become a central part of our life – as consumers, customers, and hopefully as researchers and practitioners! Whether we are applying predictive modeling techniques to our research or business problems, I believe we have one thing in common: We want to make “good” predictions! Fitting a model to our training data is one thing, but how do we know that it generalizes well to unseen data? How do we know that it doesn’t simply memorize the data we fed it and fails to make good predictions on future samples, samples that it hasn’t seen before? And how do we select a good model in the first place? Maybe a different learning algorithm could be better-suited for the problem at hand?
Model evaluation is certainly not just the end point of our machine learning pipeline. Before we handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this article, we will go over a selection of these techniques, and we will see how they fit into the bigger picture, a typical machine learning workflow. Let’s start this section with a simple Q&A: Q: “How do we estimate the performance of a machine learning model?” A: “First, we feed the training data to our learning algorithm to learn a model. Second, we predict the labels of our test labels. Third, we count the number of wrong predictions on the test dataset to compute the model’s prediction accuracy.” Not so fast! Depending on our goal, estimating the performance of a model is not that trivial, unfortunately. Maybe we should address the previous question from another angle: “Why do we care about performance estimates at all?” Ideally, the estimated performance of a model tells how well it performs on unseen data – making predictions on future data is often the main problem we want to solve in applications of machine learning or the development of novel algorithms. Typically, machine learning involves a lot of experimentation, though — for example, the tuning of the internal knobs of a learning algorithm, the so-called hyperparameters. Running a learning algorithm over a training dataset with different hyperparameter settings will result in different models. Since we are typically interested in selecting the best-performing model from this set, we need to find a way to estimate their respective performances in order to rank them against each other. Going one step beyond mere algorithm fine-tuning, we are usually not only experimenting with the one single algorithm that we think would be the “best solution” under the given circumstances. More often than not, we want to compare different algorithms to each other, oftentimes in terms of predictive and computational performance. Let us summarize the main points why we evaluate the predictive performance of a model: Although these three sub-tasks listed above have all in common that we want to estimate the performance of a model, they all require different approaches. We will discuss some of the different methods for tackling these sub-tasks in this article. Of course, we want to estimate the future performance of a model as accurately as possible. However, if there’s one key take-away message from this article, it is that biased performance estimates are perfectly okay in model selection and algorithm selection if the bias affects all models equally. If we rank different models or algorithms against each other in order to select the best-performing one, we only need to know the “relative” performance. For example, if all our performance estimates are pessimistically biased, and we underestimate their performances by 10%, it wouldn’t affect the ranking order. More concretely, if we have three models with prediction accuracy estimates such as M2: 75% > M1: 70% > M3: 65%, we would still rank them the same way if we add a 10% pessimistic bias: M2: 65% > M1: 60% > M3: 55%. On the contrary, if we report the future prediction accuracy of the best ranked model (M2) to be 65%, this would obviously be quite inaccurate. Estimating the absolute performance of a model is probably one of the most challenging tasks in machine learning. Model evaluation is certainly a complex topic. To make sure that we don’t diverge too much from the core message, let us make certain assumptions and go over some of the technical terms that we will use throughout this article. i.i.d. We assume that our samples are i.i.d (independent and identically distributed), which means that all samples have been drawn from the same probability distribution and are statistically independent from each other. A scenario where samples are not independent would be working with temporal data or time-series data. Supervised learning and classification This article will focus on supervised learning, a subcategory of machine learning where our target values are known in our available dataset. Although many concepts also apply to regression analysis, we will focus on classification, the assignment of categorical target labels to the samples. 0-1 loss and prediction accuracy In the following article, we will focus on the prediction accuracy, which is defined as the number of all correct predictions divided by the number of samples. We compute the prediction accuracy as the number of correct predictions divided by the number of samples n. Or in more formal terms, we define the prediction accuracy ACC as where the prediction error ERR is computed as the expected value of the 0-1 loss over n samples in a dataset S: The 0-1 loss
is defined as where
is the ith true class label and
the ith predicted class label, respectively. Our objective is to learn a model h that has a good generalization performance. Such a model maximizes the prediction accuracy or, vice versa, minimizes the probability, C(h), of making a wrong prediction where
is the generating distribution our data has been drawn from,
is the feature vector of a sample with class label . Lastly, since we will mostly refer to the prediction accuracy (instead of the error) throughout this series of articles, we will use Dirac’s Delta function so that
if
and
if
(Please note that we use “accuracy” as a performance metric to keep the discussion general and simple, without digressing into a discussion about different performance metrics. Depending on your application, you may want to consider different performance metrics.) Bias When we use the term bias in this article, we refer to the statistical bias (in contrast to the bias in a machine learning system). In general terms, the bias of an estimator
is the difference between its expected value
and the true value of a parameter
being estimated. So, if , then
is an unbiased estimator of . More concretely, we compute the prediction bias as the difference between the expected prediction accuracy of our model and the true prediction accuracy. For example, if we compute the prediction accuracy on the training set, this would be an optimistically biased estimate of the absolute accuracy of our model since it would overestimate the true accuracy. Variance The variance is simply the statistical variance of the estimator
and its expected value
The variance is a measure of the variability of our model’s predictions if we repeat the learning process multiple times with small fluctuations in the training set. The more sensitive the model-building process is towards these fluctuations, the higher the variance. Finally, let us disambiguate the terms model, hypothesis, classifier, learning algorithms, and parameters: Target function: In predictive modeling, we are typically interested in modeling a particular process; we want to learn or approximate a specific, unknown function. The target function
is the true function
that we want to model. Hypothesis: A hypothesis is a certain function that we believe (or hope) is similar to the true function, the target function that we want to model. In context of spam classification, it would be a classification rule we came up with that allows us to separate spam from non-spam emails. Model: In the machine learning field, the terms hypothesis and model are often used interchangeably. In other sciences, they can have different meanings: A hypothesis could be the “educated guess” by the scientist, and the model would be the manifestation of this guess to test this hypothesis. Learning algorithm: Again, our goal is to find or approximate the target function, and the learning algorithm is a set of instructions that tries to model the target function using our training dataset. A learning algorithm comes with a hypothesis space, the set of possible hypotheses it explores to model the unknown target function by formulating the final hypothesis. Classifier: A classifier is a special case of a hypothesis (nowadays, often learned by a machine learning algorithm). A classifier is a hypothesis or discrete-valued function that is used to assign (categorical) class labels to particular data points. In an email classification example, this classifier could be a hypothesis for labeling emails as spam or non-spam. Yet, a hypothesis must not necessarily be synonymous to the term classifier. In a different application, our hypothesis could be a function for mapping study time and educational backgrounds of students to their future, continuous-valued, SAT scores – a continuous target variable, suited for regression analysis. Hyperparameters: Hyperparameters are the tuning parameters of a machine learning algorithm — for example, the regularization strength of an L2 penalty in the mean squared error cost function of linear regression, or a value for setting the maximum depth of a decision tree. In contrast, model parameters are the parameters that a learning algorithm fits to the training data – the parameters of the model itself. For example, the weight coefficients (or slope) of a linear regression line and its bias (or y-axis intercept) term are model parameters. The holdout method is inarguably the simplest model evaluation technique. We take our labeled dataset and split it into two parts: A training set and a test set. Then, we fit a model to the training data and predict the labels of the test set. And the fraction of correct predictions constitutes our estimate of the prediction accuracy — we withhold the known test labels during prediction, of course. We really don’t want to train and evaluate our model on the same training dataset (this is called resubstitution evaluation), since it would introduce a very optimistic bias due to overfitting. In other words, we can’t tell whether the model simply memorized the training data or not, or whether it generalizes well to new, unseen data. (On a side note, we can estimate this so called optimism bias as the difference between the training accuracy and the test accuracy.) Typically, the splitting of a dataset into training and test sets is a simple process of random subsampling. We assume that all our data has been drawn from the same probability distribution (with respect to each class). And we randomly choose ~2/3 of these samples for our training set and ~1/3 of the samples for our test set. Notice the two problems here? We have to keep in mind that our dataset represents a random sample drawn from a probability distribution; and we typically assume that this sample is representative of the true population – more or less. Now, further subsampling without replacement alters the statistic (mean, proportion, and variance) of the sample. The degree to which subsampling without replacement affects the statistic of a sample is inversely proportional to the size of the sample. Let’s have a look at an example using the Iris dataset, which we randomly divide into 2/3 training data and 1/3 test data:
(The source code can be found here.) When we randomly divide the dataset into training and test sets, we violate the assumption of statistical independence. The Iris dataset consists of 50 Setosa, 50 Versicolor, and 50 Virginica flowers; the flower species are distributed uniformly: If our random function assigns 2/3 of the flowers (100) to the training set and 1/3 of the flowers (50) to the test set, it may yield the following: Assuming that the Iris dataset is representative of the true population (for instance, assuming that flowers are distributed uniformly in nature), we just created two imbalanced datasets with non-uniform class distributions. The class ratio that the learning algorithm uses to learn the model is “38.0% / 28.0% / 34.0%”. Then, we evaluate a model on a dataset with a class ratio that is imbalanced in the “opposite” direction: “24.0% / 44.0% / 32.0%”. Unless our learning algorithm is completely insensitive to these small perturbations, this is certainly not ideal. The problem becomes even worse if our dataset has a high class imbalance upfront. In the worst-case scenario, the test set may not contain any instance of a minority class at all. Thus, the common practice is to divide the dataset in a stratified fashion. Stratification simply means that we randomly split the dataset so that each class is correctly represented in the resulting subsets — the training and the test set. Random subsampling in non-stratified fashion is usually not a big concern if we are working with relatively large and balanced datasets. However, in my opinion, stratified resampling is usually (only) beneficial in machine learning applications. Moreover, stratified sampling is incredibly easy to implement, and Ron Kohavi provides empirical evidence (Kohavi 1995) that stratification has a positive effect on the variance and bias of the estimate in k-fold cross-validation, a technique we will discuss later in this article. Before we dive deeper into the pros and cons of the holdout validation method, let’s take a look at a visual summary of the whole process:
① In the first step, we randomly divide our available data into two subsets: a training and a test set. Setting test data aside is our work-around for dealing with the imperfections of a non-ideal world, such as limited data and resources, and the inability to collect more data from the generating distribution. Here, the test set shall represent new, unseen data to our learning algorithm; it’s important that we only touch the test set once to make sure we don’t introduce any bias when we estimate the generalization accuracy. Typically, we assign 2/3 to the training set, and 1/3 of the data to the test set. Other common training/test splits are 60/40, 70/30, 80/20, or even 90/10.
② After we set our test samples aside, we pick a learning algorithm that we think could be appropriate for the given problem. Now, what about the Hyperparameter Values depicted in the figure above? As a quick reminder, hyperparameters are the parameters of our learning algorithm, or meta-parameters if you will. And we have to specify these hyperparameter values manually – the learning algorithm doesn’t learn them from the training data in contrast to the actual model parameters. Since hyperparameters are not learned during model fitting, we need some sort of “extra procedure” or “external loop” to optimize them separately – this holdout approach is ill-suited for the task. So, for now, we have to go with some fixed hyperparameter values – we could use our intuition or the default parameters of an off-the-shelf algorithm if we are using an existing machine learning library.
③ Our learning algorithm fit a model in the previous step. The next question is: How “good” is the model that it came up with? That’s where our test set comes into play. Since our learning algorithm hasn’t “seen” this test set before, it should give us a pretty unbiased estimate of its performance on new, unseen data! So, what we do is to take this test set and use the model to predict the class labels. Then, we take the predicted class labels and compare it to the “ground truth,” the correct class labels to estimate its generalization accuracy.
④ Finally, we have an estimate of how well our model performs on unseen data. So, there is no reason for with-holding it from the algorithm any longer. Since we assume that our samples are i.i.d., there is no reason to assume the model would perform worse after feeding it all the available data. As a rule of thumb, the model will have a better generalization performance if the algorithms uses more informative data – given that it hasn’t reached its capacity, yet.
Remember that we mentioned two problems when we talked about the test and training split earlier? The first problem was the violation of independence and the changing class proportions upon subsampling. We touched upon the second problem when we walked through the holdout illustration. And in step four, we talked about the capacity of the model, and whether additional data could be useful or not. To follow up on the capacity issue: If our model has NOT reached its capacity, our performance estimate would be pessimistically biased. Assuming that the algorithm could learn a better model from more data, we withheld valuable data that we set aside for estimating the generalization performance (i.e., the test dataset). In the fourth step, we fit a model to the complete dataset, though; however, we can’t estimate its generalization performance, since we’ve now “burned” the test dataset. It’s a dilemma that we cannot really avoid in real-world application, but we should be aware that our estimate of the generalization performance may be pessimistically biased. Using the holdout method as described above, we computed a point estimate of the generalization accuracy of our model. Certainly, a confidence interval around this estimate would not only be more informative and desirable in certain applications, but our point estimate could be quite sensitive to the particular training/test split (i.e., suffering from high variance). Another, “simpler,” approach, which is often used in practice (although, I do not recommend it), may be using the familiar equation assuming a Normal Distribution to compute the confidence interval on the mean on a single training-test split under the central limit theorem. In probability theory, the central limit theorem (CLT) states that, given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed, regardless of the underlying distribution. [Source: https://en.wikipedia.org/wiki/Central_limit_theorem] This rather naive approach is the so-called “Normal approximation interval. “How does that work? Remember, we compute the prediction accuracy as follows where
is a 0-1 loss function and
is the number of samples in the test set;
is the predicted class label and
is the actual class label of the ith sample, respectively. So, we could now consider each prediction as a Bernoulli trial, and the number of correct predictions
is following a binomial distribution
with
samples and
trials, where : for , where (Remember,
is the probability of success, and
is the probability of failure – a wrong prediction.) Now, the expected number of successes is computed as , or more concretely,
if the estimator has 50% success rate, we expect 20 out of 40 predictions to be correct. The estimate has a variance of
and a standard deviation of Since we are interested in the average number of successes, not its absolute value, we compute the variance of the accuracy estimate as and the respective standard deviation Under the normal approximation, we can then compute the confidence interval as where
is the error quantile and
is the
quantile of a standard normal distribution. For a typical confidence interval of 95% (alpha=5%), we have z=1.96. In practice, however, I’d rather recommend repeating the training-test split multiple times to compute the confidence interval on the mean estimate (i.e., averaging the individual runs). In any case, one interesting take-away for now is that having fewer samples in the test set increases the variance (see n in the denominator above) and thus widens the confidence interval. Since the whole article turned out to be quite lengthy, I decided to split it into multiple parts. In the following parts, we will talk about © 2013-2018 Sebastian Raschka"
895ef686ab,Coffee Is Good for You: Drinking Java Linked to Prevention of Heart Disease and Stroke,"Coffee addicts have long clung to their habit with reassurances that the beverage had powerful antioxidants in addition to stimulatory effects—and a new study offers more evidence that java is good for you. Preliminary data indicated that drinking coffee could help reduce heart disease and stroke risk, according to a release. The study was discussed at the American Heart Association's Scientific Sessions, where researchers and doctors gathered to learn about the latest heart-health advancements. Related: Eating at Night Might Be Killing You? Research Links Late Meals to Heart Disease
Keep up with this story and more by subscribing now
A coffee shop in Colombia. Research indicates that drinking coffee regularly could prevent heart disease.
JOAQUIN SARMIENTO/AFP/Getty Images
Using machine learning, scientists studied data from a prior long-term study that looked at diets and cardiovascular health. Those findings indicated that drinking coffee was linked to a 7 percent decrease in heart failure and an 8 percent decrease in stroke for every cup drunk per week. That was in comparison to drinking no coffee. That type of study design demonstrated an observed association but did not prove cause and effect. To test their results, the team then compared their findings from machine learning to research using traditional analysis. They found that for two different but similar studies, the results were consistent with the machine-learning findings. Using the same machine-learning analysis, they also found a link between eating red meat and decreased risk of heart failure and stroke, which is somewhat surprising considering past research that suggested meat-based diets could increase disease risk. However, the team wrote, the relationship was less clear, and the studies used for the analysis had various definitions of red meat.
Research on the health benefits of coffee has been murky: Researchers have difficulty identifying what it is that could make the drink healthy.
Sean Gallup/Getty Images
Coffee is a rich source of antioxidants, but the data on whether it might be a healthy drink was unclear. A study from 2012 revealed that individuals who regularly drank at least three cups of coffee per day were less likely to die from heart disease, stroke and diabetes, among other conditions. In an article for the
National Institutes of Health, lead study author Neal Freedman said the reason was difficult to nail down. “The mechanism by which coffee protects against risk of death—if indeed the finding reflects a causal relationship—is not clear, because coffee contains more than 1,000 compounds that might potentially affect health,” he said. And while coffee does have potentially healthy compounds, it has some scary ones, too. As we previously
reported, coffee contains acrylamide, thought to be a carcinogenic. Found naturally in foods, the compound is triggered by extremely high temperatures, and in the case of coffee, occurs in the roasting process. It’s important to note that for all those studies there is no indication of causation, so it could all just be a big coincidence. But that likely won’t stop extreme caffeine addicts from sleeping better at night.
Weekly magazine, delivered
Daily Newsletter
Website access
Weekly magazine, delivered
Daily Newsletter
Website access
Free access to 40+ digital editions
Website access
Daily Newsletter © 2018 Newsweek LLC"
07be126a35,The Machine Vision Algorithm Beating Art Historians at Their Own Game - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Few areas of academic inquiry have escaped the influence of computer science and machine learning. But one of them is the history of art. The challenge of analyzing paintings, recognizing their artists, and identifying their style and content has always been beyond the capability of even the most advanced algorithms. That is now changing thanks to recent advances in machine learning based on approaches such as deep convolutional neural networks. In just a few years, computer scientists have created machines capable of matching and sometimes outperforming humans in all kinds of pattern recognition tasks. Today, we see just how advanced these approaches have become in the hands of Babak Saleh and Ahmed Elgammal at Rutgers University in New Jersey. These guys have used these new machine learning techniques to train algorithms to recognize the artist and style of a fine-art painting with an accuracy that has never been achieved before. What’s more, the results reveal connections between artists, and between entire painting styles, that art historians have labored for years to understand. Saleh and Elgammal begin with a database of images of more than 80,000 paintings by more than a 1,000 artists spanning 15 centuries. These paintings cover 27 different styles, each with more than 1,500 examples. The researchers also classify the works by genre, such as interior, cityscape, landscape, and so on. They then take a subset of the images and use them to train various kinds of state-of-the-art machine-learning algorithms to pick out certain features. These include general, low-level features such as the overall color, as well as more advanced features that describe the objects in the image, such as a horse and a cross. The end result is a vector-like description of each painting that contains 400 different dimensions. The researchers then test the algorithm on a set of paintings it has not yet seen. And the results are impressive. Their new approach can accurately identify the artist in over 60 percent of the paintings it sees and identify the style in 45 percent of them. But crucially, the machine-learning approach provides an insight into the nature of fine art that is otherwise hard even for humans to develop. This comes from analyzing the paintings that the algorithm finds difficult to classify. For example, Saleh and Elgammal say their new approach finds it hard to distinguish between works painted by Camille Pissarro and Claude Monet. But a little research on these artists quickly reveals both were active in France in the late 19th and early 20th centuries and that both attended the Académie Suisse in Paris. An expert might also know that Pissarro and Monet were good friends and shared many experiences that informed their art. So the fact that their work is similar is no surprise. As another example, the new approach confuses works by Claude Monet and the American impressionist Childe Hassam, who, it turns out, was strongly influenced by the French impressionists and Monet in particular.  These are links that might take a human some time to discover. The algorithmic approach also finds links between certain artistic styles. For instance, it often confuses examples of abstract expressionism and action paintings, in which artists drip or fling paint and step on the canvas. Saleh and Elgammal again say that this kind of mix-up would be entirely understandable for a human viewer. “’Action painting’ is a type or subgenre of “abstract expressionism,’” they point out. The algorithm picks out numerous other similarities. It links expressionism and fauvism, which might be expected given that the latter movement is often thought of as a type of expressionism. It links the mannerist and Renaissance styles, which clearly reflects that fact that mannerism is a form of early Renaissance painting. And it finds other more straightforward links, for example between Renaissance and early Renaissance paintings, between impressionism and post-impressionism, and between cubism and its later manifestation, synthetic cubism. These connections are well known to art historians, but only because of many decades, or indeed centuries, of scholarship. By contrast, the machine-learning approach spans just a few months. That has important implications for the study of art history. One application of the new algorithms is to pick out paintings with similar characteristics (see images). That provides a new and powerful tool for historians to look for influences between artists that may never have been aware of. It also allows a new form of art exploration, jumping from one image to another similar one, in a process that is visually equivalent to finding synonyms.   Fascinating stuff! Ref: arxiv.org/abs/1505.00855 : Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature Want to go ad free?
No ad blockers needed. Emerging Technology from the arXiv
Emerging Technology from the arXiv covers the latest ideas and technologies that appear on the Physics arXiv preprint server. It is part of the Physics arXiv Blog.
Email:… More KentuckyFC@arxivblog.comSubscribe to the Physics arXiv Blog RSS Feed.
Please read our commenting guidelines.
More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
{! insider.display.menuOptionsLabel !}
Unlimited online access including articles and video, plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
70f5d5c9a4,"Scientists enlist supercomputers, machine learning to automatically identify brain tumors","Primary brain tumors encompass a wide range of tumors depending on the cell type, the aggressiveness, and stage of tumor. Quickly and accurately characterizing the tumor is a critical aspect of treatment planning. It is a task currently reserved for trained radiologists, but in the future, computing, and in particular high-performance computing, will play a supportive role.
George Biros, professor of mechanical engineering and leader of the ICES Parallel Algorithms for Data Analysis and Simulation Group at The University of Texas at Austin, has worked for nearly a decade to create accurate and efficient computing algorithms that can characterize gliomas, the most common and aggressive type of primary brain tumor. At the 20th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2017), Biros and collaborators from the University of Pennsylvania (led by Professor Christos Davatzikos), University of Houston (led by Professor Andreas Mang) and University of Stuttgart (led by Professor Miriam Mehl), presented results of a new, fully automatic method that combines biophysical models of tumor growth with machine learning algorithms for the analysis of Magnetic Resonance (MR) imaging data of glioma patients. All the components of the new method were enabled by supercomputers at the Texas Advanced Computing Center (TACC). Biros' team tested their new method in the Multimodal Brain Tumor Segmentation Challenge 2017 (BRaTS'17), an annual competition where research groups from around the world present methods and results for computer-aided identification and classification of brain tumors, as well as different types of cancerous regions, using pre-operative MR scans. Their system scored in the top 25 percent in the challenge and were near the top for whole tumor segmentation. ""The competition is related to the characterization of abnormal tissue on patients who suffer from glioma tumors, the most prevalent form of primary brain tumor,"" Biros said. ""Our goal is to take an image and delineate it automatically and identify different types of abnormal tissue - edema, enhancing tumor (areas with very aggressive tumors), and necrotic tissue. It's similar to taking a picture of one's family and doing facial recognition to identify each member, but here you do tissue recognition, and all this has to be done automatically."" Training and testing the prediction pipeline For the challenge, Biros and his team of more than a dozen students and researchers, were provided in advance with 300 sets of brain images on which all teams calibrated their methods (what is called ""training"" in machine learning parlance). In the final part of the challenge, groups were given data from 140 patients and had to identify the location of tumors and segment them into different tissue types over the course of just two days. ""In that 48-hour window, we needed all the processing power we could get,"" Biros explained. The image processing, analysis and prediction pipeline that Biros and his team used has two main steps: a supervised machine learning step where the computer creates a probability map for the target classes (""whole tumor,"" ""edema,"" ""tumor core""); and a second step where they combine these probabilities with a biophysical model that represents how tumors grow in mathematical terms, which imposes limits on the analyses and helps find correlations. TACC computing resources enabled Biros' team to use large-scale nearest neighbor classifiers (a machine learning method). For every voxel, or three-dimensional pixel, in a MR brain image, the system tries to find all the similar voxels in the brains it has already seen to determine if the area represents a tumor or a non-tumor. With 1.5 million voxels per brain and 300 brains to assess, that means the computer must look at half billion voxels for every new voxel of the 140 unknown brains that it analyzes, deciding for each whether the voxel represents a tumor or healthy tissue. ""We used fast algorithms and approximations to make this possible, but we still needed supercomputers,"" Biros said. Each of the several steps in the analysis pipeline used separate TACC computing systems. The nearest neighbor machine learning classification component simultaneously used 60 nodes (each consisting of 68 processors) on Stampede2, TACC's latest supercomputer and one of the most powerful systems in the world. (Biros was among the first researchers to gain access to the Stampede2 supercomputer in the spring and was able to test and tune his algorithm for the new processors there.) They used Lonestar 5 to run the biophysical models and Maverick to combine the segmentations. Most teams had to limit the amount of training data they used or apply more simplified classifier algorithms on the whole training set, but priority access to TACC's ecosystem of supercomputers meant Biros' team could explore more complex methods. ""George came to us before the BRaTS Challenge and asked if they could get priority access to Stampede2, Lonestar5, and Maverick to ensure that their jobs got through in time to complete the challenge,"" said Bill Barth, TACC's Director of High Performance Computing. ""We decided that just increasing their priority probably wouldn't cut it, so we decided to give them a reservation on each system to cover their needs for the 48 hours of the challenge."" As it turned out, Biros and his team were able to run their analysis pipeline on 140 brains in less than 4 hours and correctly characterized the testing data with nearly 90 percent accuracy, with is comparable to human radiologists. Their method is fully automatic, Biros said, and needed only a small number of initial algorithmic parameters to assess the image data and classify tumors without any hands-on effort. Integrating diverse research The team's scalable, biophysics-based image analysis system was the culmination of 10 years of research into a variety of computational problems, according to Biros. ""In our group and our collaborators' groups, we have multiple research threads on image analysis, scalable machine learning and numerical algorithms,"" he explained. ""But this was the first time we put everything together for an application to make our method work for a really challenging problem. It's not easy, but it's very fulfilling."" The BRaTS competition thus represents a turning point in his research, Biros said. ""We have all the tools and basic ideas, now we polish it and see how we can improve it."" The image segmentation classifier is set to be deployed at the University of Pennsylvania by the end of the year in partnership with his collaborator, Christos Davatzikos, director of the Center for Biomedical Image Computing and Analytics and a professor of Radiology there. It won't be a substitute for radiologists and surgeons, but it will improve the reproducibility of assessments and potentially speed up diagnoses. The methods that the team developed go beyond brain tumor identification. They are applicable to many problems in medicine as well as in physics, including semiconductor design and plasma dynamics. Said Biros: ""Having access to TACC supercomputers makes our life infinitely easier, makes us more productive and is a real advantage.""
Explore further:
Machine learning to help physicians
Provided by:
University of Texas at Austin
Physicians have long used visual judgment of medical images to determine the course of cancer treatment. A new program package from Fraunhofer researchers reveals changes in images and facilitates this task using deep learning. ... How do search engines generate lists of relevant links? Depression affects more than 15 million American adults, or about 6.7 percent of the U.S. population, each year. It is the leading cause of disability for those between the ages of 15 and 44. Today, the Texas Advanced Computing Center (TACC) dedicated Stampede2, the largest supercomputer at any U.S. university, and one of the most powerful systems in the world in a ceremony at The University of Texas at Austin's ... (Tech Xplore)—A team of researchers from several institutions in China has applied deep learning by a computer to the problem of reading visual imagery in the brain and then reproduced it in a 2-D format. A paper describing ... Columbia University's imaging software that facilitates 3-D lung tumor segmentation, licensed to Varian Medical Systems, has been incorporated into the Smart Segmentation module of Varian's Eclipse treatment planning system ... A new security flaw has been found in Intel hardware which could enable hackers to access corporate laptops remotely, Finnish cybersecurity specialist F-Secure said on Friday. Vision impairment is a major global issue. More than 2 billion people worldwide don't have access to corrective lenses. A new design of algae-powered fuel cells that is five times more efficient than existing plant and algal models, as well as being potentially more cost-effective to produce and practical to use, has been developed by researchers ... In a study involving 19 European Union nations, researchers have found that future climate change will make power outages more costly for European households. Henrik Fisker unveiled his $129,000 electric luxury car Tuesday, in a rebooted effort by the renowned auto designer to take on Tesla and other luxury automakers. Netherlands-based startup Travis is out to make people understood no matter what language they speak.
Please sign in to add a comment.
Registration is free, and takes less than a minute.
Read more
Enter your Science X account credentials Connect © Phys.org 2003 - 2018, Science X network"
def92351cb, Machine learning and big data: a new dimension in online banking security,"Home » Technology » Services Machine learning and big data are having a positive impact on customer experience, as well as producing extensive benefits for banks With authentication and risk assessment tailored to the unique demands of each end user and each online transaction, an invaluable new asset is finally within reach of banks: the power and potential to bring a truly personal dimension to their digital offer Imagine a digital banking experience where we can identify ourselves with absolute certainty, simply by being ourselves. Or an online journey where the authentication process is tailored precisely to the risk posed by the transaction itself. For consumers trapped in a seemingly endless cycle of usernames, passwords and additional security questions – not to mention blocks imposed on perfectly legitimate payments and transfers – it’s clearly an attractive proposition. Banks too should find the vision compelling; digitalisation has transformed their marketplace. But, to date, a successful marriage between seamless customer experiences and robust cyber security has proven elusive as traditional risk assessment and authentication solutions have failed to keep pace with the sheer volume of online banking transactions and the scale and sophistication of hacking attacks. >See also: Online banking and financial services: is the end-user protected? Historically, additional security layers imposed in response to perceived threats have come at the expense of greater friction for the end user, or an increase in denied transactions. Fortunately, in an environment created and defined by innovative technologies, the arrival of a new generation of solutions built around machine learning and big data finally promises a way out of this particular paradox. By continuously analysing the vast array of data being generated by digital banking ecosystems, it has become possible to create a unique footprint for every single customer. Furthermore, effective deployment of machine learning and big data can support sophisticated real-time assessment of the risk inherent in every single online transaction. For each online transaction, banks must pose an apparently simple question: are you a trusted customer or a cybercriminal? In business terms, this matters. Online shopping cart abandonment rates are currently averaging nearly 70% – a staggering figure. And every transaction unnecessarily blocked, or ditched by a frustrated customer, comes at a price. But at the same time, the commercial impact of any successful online fraud can be devastating, not just in terms of the direct cost, but also reputational damage and loss of trust. >See also: Cyber crime and the banking sector Fresh thinking is urgently needed. Above all else, banks must recogniSe that by harnessing the rich array of information now at their disposal, a huge leap forward in the convenience, effectiveness and cost-efficiency of authentication strategies is possible. Combined with machine learning it can be used to identify and profile customers through a host of personal and device characteristics. Furthermore, this can all be done in real time, without any need for conscious input by the end user. Deviations and abnormalities that might indicate a risk can be highlighted and challenged with a far greater degree of speed, subtlety and precision. The good news for banks is that solutions powered by big data and machine learning are now ready to deploy. Behavioural biometrics can be used to monitor a user’s keystroke dynamics, touchpad and mouse movements and analyse the behaviour of both users and devices in minute detail. During an online transaction, these are compared with those recorded during previous interactions with the same user to help distinguish between normal and unusual shopping patterns; the volume, location, frequency and velocity of transactions are all tracked. >See also: 1 in 4 banks struggle with online customer verification Analysis of device characteristics is equally sophisticated, including the ability to detect the use of cloaking services to hide an IP address, for example. With all these tools combined it is possible to automatically spot a vast range of anomalous behaviour. Crucially, these capabilities extend far beyond traditional solutions, which are typically based on a relatively limited and inflexible set of fraud indicators. In practice, the combination of machine learning and big data spells an end to a simple binary approach to risk assessment and implementation of additional security layers. In its place comes something as flexible and dynamic as the digital ecosystems themselves. Based on risk scoring that is determined by a spectrum rather than a simple yes/no response, solutions such as the Gemalto Assurance Hub (GAH) will consistently trigger the most appropriate authentication method, thereby helping to reduce friction in the consumer experience. Consequently, end users are far less likely to face additional authentication requests where there is no real risk of fraud. Yet the detection of potential cybercrime is far more agile and effective. What’s more, banks enjoy the freedom to adapt their security procedures in line with the expectations of individual end users; whilst many customers will prefer authentication to be as transparent as possible, others will still appreciate the added reassurance provided by clearly visible procedures. Given the digital domain is routinely characterised as anonymous and distant, there is a certain irony to the fact that it can now provide banks with all the data they need to create rich, multi-dimensional profiles of the customers engaged with it. >See also: Securing the future of banking from the cyber threat However, in utiliSing this resource, banks must pay utmost respect to end users’ right to privacy. High end encryption is essential throughout the process to ensure comprehensive protection of sensitive personal information against hacking attacks. Equally, authentication should be treated as a critical element of the relationship-building process. When combined with the analytical capabilities of machine learning, the effective use of big data means that security and convenience need no longer be considered mutually exclusive. Above all else, these new technologies provide an unprecedented opportunity for banks to address simultaneously the era of digitalisation and differing ages and expectations of individuals. With authentication and risk assessment tailored to the unique demands of each end user and each online transaction, an invaluable new asset is finally within reach of banks: the power and potential to bring a truly personal dimension to their digital offer.   Sourced by Philippe Regniers, vice president Digital Banking Solutions at Gemalto   The Women in IT Awards is the technology world’s most prominent and influential diversity program. On 22 March 2018, the event will come to the US for the first time, taking place in one of the world’s most prominent business cities: New York. Nominations are now open for the Women in IT USA Awards 2018. Click here to nominate Since its launch in 1995, Information Age has been regarded as one of the most respected technology titles in the B2B realm. More than 20 years on from its inception, the publication stands as the UK’s number one business-technology magazine, holding a strong influence over its prestigious readership of IT leaders.
Vitesse Media Plc,
14 Bonhill Street, London EC2A 4BX T. 0207 250 7010 © 2018 Vitesse Media Plc © 2018 Vitesse Media Plc"
c9424252db,Security robot falls into pond after failing to spot stairs or water • The Register    ,"A security robot tasked with patrolling an office building in Washington DC has instead driven itself into a water feature. The bot appears to have been a Knightscope K5, a surveillance bot designed to roll around “parking lots, corporate campuses and hospitals” so it can use its video cameras, anomaly detection, live video streaming and real-time notifications capabilities to inform you if you need to send a human security guard to sort something out. Knightscope says the bot can find a way through “even the most complex environments”. But not, evidently, the water feature of The Washington Harbour, an office and retail complex in Washington, DC. As the Tweet below shows, the K5 decided to take a dip in said water feature. Our D.C. office building got a security robot. It drowned itself.
We were promised flying cars, instead we got suicidal robots. pic.twitter.com/rGLTAWZMjn Knightscope told The Washington Post that the incident was “an isolated event”. The company's also been good-humoured about the incident, responding to social media jibes with a riposte that the K5 is a security robot, not a submarine robot. ®
Sponsored:
Continuous Lifecycle London 2018 - Early Bird Tickets Now Available
The Register - Independent news and views for the tech community. Part of Situation Publishing Join our daily or weekly newsletters, subscribe to a specific section or set News alerts Biting the hand that feeds IT © 1998–2018"
160f3b22e0,"CAPTCHAs may be a thing of the past, thanks to new machine learning research","CAPTCHA is an acronym for Completely Automated Public Turing test to tell Computers and Humans Apart. The term was coined in 2003, when the use of automated bots was becoming commonplace, and it refers to those annoying squiggly distorted letters that you have to type in when creating an online account. Although some companies have found ways around them, CAPTCHAs are still ubiquitous online. Researchers at the AI company Vicarious may have just made CAPTCHAs obsolete, however, by creating a machine-learning algorithm that mimics the human brain. To simulate the human capacity for what is often described as “common sense,” the scientists built a computer vision model dubbed the Recursive Cortical Network. “For common sense to be effective it needs to be amenable to answer a variety of hypotheticals — a faculty that we call imagination,” they noted in a post at their blog. The ability to decipher CAPTCHAs has become something of a benchmark for artificial intelligence research. The new Vicarious model, published in the journal Science, cracks the fundamentals of the CATCHPA code by parsing the text using techniques that are derived from human reasoning. We can easily recognize the letter A for example, even if it’s partly obscured or turned upside down. As Dileep George, the co-founder of the company explained to NPR, the RCN takes far less training and repetition to learn to recognize characters by building its own version of a neural network. “So if you expose it to As and Bs and different characters, it will build its own internal model of what those characters are supposed to look like,” he said. “So it would say, these are the contours of the letter, this is the interior of the letter, this is the background, etc.” These various features get put into groups, creating a hierarchal “tree” of related features. After several passes, the data is given a score for evaluation. CAPTCHAs can be identified with a high degree of accuracy. The RCN was able to crack the BotDetect system with 57 percent accuracy with far less training than conventional “deep learning” algorithms, which rely more on brute force and require tens of thousands of images before they can understand CAPTCHAs with any degree of accuracy. Solving CATCHPAs is not the goal of the research, but it provides insight into how our brains work and how computers can replicate it, NYU’s Brenden Lake told Axios. “It’s an application that not everybody needs,” he said. “Whereas object recognition is something that our minds do every second of every day.” “Biology has put a scaffolding in our brain that is suitable for working with this world. It makes the brain learn quickly,” George said. “So we copy those insights from nature and put it in our model. Similar things can be done in neural networks.”"
2649f7c002,Computer system predicts products of chemical reactions | MIT News,"Login
or
Subscribe Newsletter
A new computer system predicts the products of chemical reactions. “The vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it,” says professor Klavs Jensen.
Image: MIT News Machine learning approach could aid the design of industrial processes for drug manufacturing.
Larry Hardesty | MIT News Office
June 27, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
When organic chemists identify a useful chemical compound — a new drug, for instance — it’s up to chemical engineers to determine how to mass-produce it. There could be 100 different sequences of reactions that yield the same end product. But some of them use cheaper reagents and lower temperatures than others, and perhaps most importantly, some are much easier to run continuously, with technicians occasionally topping up reagents in different reaction chambers. Historically, determining the most efficient and cost-effective way to produce a given molecule has been as much art as science. But MIT researchers are trying to put this process on a more secure empirical footing, with a computer system that’s trained on thousands of examples of experimental reactions and that learns to predict what a reaction’s major products will be. The researchers’ work appears in the American Chemical Society’s journal Central Science. Like all machine-learning systems, theirs presents its results in terms of probabilities. In tests, the system was able to predict a reaction’s major product 72 percent of the time; 87 percent of the time, it ranked the major product among its three most likely results. “There’s clearly a lot understood about reactions today,” says Klavs Jensen, the Warren K. Lewis Professor of Chemical Engineering at MIT and one of four senior authors on the paper, “but it's a highly evolved, acquired skill to look at a molecule and decide how you’re going to synthesize it from starting materials.” With the new work, Jensen says, “the vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it.” With a 72 percent chance of identifying a reaction’s chief product, the system is not yet ready to anchor the type of completely automated chemical synthesis that Jensen envisions. But it could help chemical engineers more quickly converge on the best sequence of reactions — and possibly suggest sequences that they might not otherwise have investigated. Jensen is joined on the paper by first author Connor Coley, a graduate student in chemical engineering; William Green, the Hoyt C. Hottel Professor of Chemical Engineering, who, with Jensen, co-advises Coley; Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science; and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science. Acting locally A single organic molecule can consist of dozens and even hundreds of atoms. But a reaction between two such molecules might involve only two or three atoms, which break their existing chemical bonds and form new ones. Thousands of reactions between hundreds of different reagents will often boil down to a single, shared reaction between the same pair of “reaction sites.” A large organic molecule, however, might have multiple reaction sites, and when it meets another large organic molecule, only one of the several possible reactions between them will actually take place. This is what makes automatic reaction-prediction so tricky. In the past, chemists have built computer models that characterize reactions in terms of interactions at reaction sites. But they frequently require the enumeration of exceptions, which have to be researched independently and coded by hand. The model might declare, for instance, that if molecule A has reaction site X, and molecule B has reaction site Y, then X and Y will react to form group Z — unless molecule A also has reaction sites P, Q, R, S, T, U, or V. It’s not uncommon for a single model to require more than a dozen enumerated exceptions. And discovering these exceptions in the scientific literature and adding them to the models is a laborious task, which has limited the models’ utility. One of the chief goals of the MIT researchers’ new system is to circumvent this arduous process. Coley and his co-authors began with 15,000 empirically observed reactions reported in U.S. patent filings. However, because the machine-learning system had to learn what reactions wouldn’t occur, as well as those that would, examples of successful reactions weren’t enough. Negative examples So for every pair of molecules in one of the listed reactions, Coley also generated a battery of additional possible products, based on the molecules’ reaction sites. He then fed descriptions of reactions, together with his artificially expanded lists of possible products, to an artificial intelligence system known as a neural network, which was tasked with ranking the possible products in order of likelihood. From this training, the network essentially learned a hierarchy of reactions — which interactions at what reaction sites tend to take precedence over which others — without the laborious human annotation. Other characteristics of a molecule can affect its reactivity. The atoms at a given reaction site may, for instance, have different charge distributions, depending on what other atoms are around them. And the physical shape of a molecule can render a reaction site difficult to access. So the MIT researchers’ model also includes numerical measures of both these features. According to Richard Robinson, a chemical-technologies researcher at the drug company Novartis, the MIT researchers’ system “offers a different approach to machine learning within the field of targeted synthesis, which in the future could transform the practice of experimental design to targeted molecules.” “Currently we rely heavily on our own retrosynthetic training, which is aligned with our own personal experiences and augmented with reaction-database search engines,” Robinson says. “This serves us well but often still results in a significant failure rate. Even highly experienced chemists are often surprised. If you were to add up all the cumulative synthesis failures as an industry, this would likely relate to a significant time and cost investment. What if we could improve our success rate?” The MIT researchers, Robinson says, “have cleverly demonstrated a novel approach to achieve higher predictive reaction performance over conventional approaches. By augmenting the reported literature with negative reaction examples, the data set has more value.” Topics: Research, School of Engineering, Artificial intelligence, Chemical engineering, Chemistry, Computer modeling, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Manufacturing This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
bb5bb6b3f8,Deal or no deal? Training AI bots to negotiate | Engineering Blog | Facebook Code | Facebook,"From the moment we wake up, our days are filled with a constant flow of negotiations. These scenarios range from discussing what TV channel to watch to convincing your kids to eat their vegetables or trying to get a better price on something. What these all have in common is that they require complex communication and reasoning skills, which are attributes not inherently found in computers. To date, existing work on chatbots has led to systems that can hold short conversations and perform simple tasks such as booking a restaurant. But building machines that can hold meaningful conversations with people is challenging because it requires a bot to combine its understanding of the conversation with its knowledge of the world, and then produce a new sentence that helps it achieve its goals. Today, researchers at Facebook Artificial Intelligence Research (FAIR) have open-sourced code and published research introducing dialog agents with a new capability — the ability to negotiate.
Similar to how people have differing goals, run into conflicts, and then negotiate to come to an agreed-upon compromise, the researchers have shown that it’s possible for dialog agents with differing goals (implemented as end-to-end-trained neural networks) to engage in start-to-finish negotiations with other bots or people while arriving at common decisions or outcomes. The FAIR researchers studied negotiation on a multi-issue bargaining task. Two agents are both shown the same collection of items (say two books, one hat, three balls) and are instructed to divide them between themselves by negotiating a split of the items. Each agent is provided its own value function, which represents how much it cares about each type of item (say each ball is worth 3 points to agent 1). As in life, neither agent knows the other agent's value function and must infer it from the dialog (you say you want the ball, so you must value it highly). FAIR researchers created many such negotiation scenarios, always ensuring that it is impossible for both agents to get the best deal simultaneously. Furthermore, walking away from the negotiation (or not agreeing on a deal after 10 rounds of dialog) resulted in 0 points for both agents. Simply put, negotiation is essential, and good negotiation results in better performance. Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realized. Such dialogs contain both cooperative and adversarial elements, requiring agents to understand and formulate long-term plans and generate utterances to achieve their goals.
The FAIR researchers' key technical innovation in building such long-term planning dialog agents is an idea called dialog rollouts. When chatbots can build mental models of their interlocutors and “think ahead” or anticipate directions a conversation is going to take in the future, they can choose to steer away from uninformative, confusing, or frustrating exchanges toward successful ones. Specifically, FAIR has developed dialog rollouts as a novel technique where an agent simulates a future conversation by rolling out a dialog model to the end of the conversation, so that an utterance with the maximum expected future reward can be chosen. Similar ideas have been used for planning in game environments but have never been applied to language because the number of possible actions is much higher. To improve efficiency, the researchers first generated a smaller set of candidate utterances to say, and then for each of these, they repeatedly simulated the complete future of the dialog in order to estimate how successful they were. The prediction accuracy of this model is high enough that the technique dramatically improved negotiation tactics in the following areas: Negotiating harder: The new agents held longer conversations with humans, in turn accepting deals less quickly. While people can sometimes walk away with no deal, the model in this experiment negotiates until it achieves a successful outcome. Intelligent maneuvers: There were cases where agents initially feigned interest in a valueless item, only to later “compromise” by conceding it — an effective negotiating tactic that people use regularly. This behavior was not programmed by the researchers but was discovered by the bot as a method for trying to achieve its goals. Producing novel sentences: Although neural models are prone to repeating sentences from training data, this work showed the models are capable of generalizing when necessary. In order to train negotiation agents and conduct large-scale quantitative evaluations, the FAIR team crowdsourced a collection of negotiations between pairs of people. The individuals were shown a collection of objects and a value for each, and asked to agree how to divide the objects between them. The researchers then trained a recurrent neural network to negotiate by teaching it to imitate people’s actions. At any point in a dialog, the model tries to guess what a human would say in that situation. Unlike previous work on goal-orientated dialog, the models were trained “end to end” purely from the language and decisions that humans made, meaning that the approach can easily be adapted to other tasks. To go beyond simply trying to imitate people, the FAIR researchers instead allowed the model to achieve the goals of the negotiation. To train the model to achieve its goals, the researchers had the model practice thousands of negotiations against itself, and used reinforcement learning to reward the model when it achieved a good outcome. To prevent the algorithm from developing its own language, it was simultaneously trained to produce humanlike language. To evaluate the negotiation agents, FAIR tested them online in conversations with people. Most previous work has avoided dialogs with real people or worked in less challenging domains, because of the difficulties of learning models that can respond to the variety of language that people can say. Interestingly, in the FAIR experiments, most people did not realize they were talking to a bot rather than another person — showing that the bots had learned to hold fluent conversations in English in this domain. The performance of FAIR’s best negotiation agent, which makes use of reinforcement learning and dialog rollouts, matched that of human negotiators. It achieved better deals about as often as worse deals, demonstrating that FAIR's bots not only can speak English but also think intelligently about what to say. Supervised learning aims to imitate the actions of human users, but it does not explicitly attempt to achieve an agent's goals. Taking a different approach, the FAIR team explored pre-training with supervised learning, and then fine-tuned the model against the evaluation metric using reinforcement learning. In effect, they used supervised learning to learn how to map between language and meaning, but used reinforcement learning to help determine which utterance to say. During reinforcement learning, the agent attempts to improve its parameters from conversations with another agent. While the other agent could be a human, FAIR used a fixed supervised model that was trained to imitate humans. The second model is fixed, because the researchers found that updating the parameters of both agents led to divergence from human language as the agents developed their own language for negotiating. At the end of every dialog, the agent is given a reward based on the deal it agreed on. This reward was then back-propagated through every word that the agent output, using policy gradients, to increase the probability of actions that lead to high rewards. This work represents an important step for the research community and bot developers toward creating chatbots that can reason, converse, and negotiate, all key steps in building a personalized digital assistant. Working with the community gives us an opportunity to share our work and the challenges we’re aiming to solve, and encourages talented people to contribute their ideas and efforts to move the field forward.
Stay up-to-date via RSS with the latest open source project releases from Facebook, news from our Engineering teams, and upcoming events."
23bc6e8300,Tweets with replies by TayTweets (@TayandYou) | Twitter,"The official account of Tay, Microsoft's A.I. fam from the internet that's got zero chill! The more you talk the smarter Tay gets Only confirmed followers have access to @TayandYou's Tweets and complete profile. Click the ""Follow"" button to send a follow request.
You can add location information to your Tweets, such as your city or precise location, from the web and via third-party applications. You always have the option to delete your Tweet location history.
Learn more
Here's the URL for this Tweet. Copy it to easily share with friends. Add this Tweet to your website by copying the code below. Learn more Add this video to your website by copying the code below. Learn more Hmm, there was a problem reaching the server. Try again? By embedding Twitter content in your website or app, you are agreeing to the Twitter Developer Agreement and Developer Policy. This timeline is where you’ll spend most of your time, getting instant updates about what matters to you.
Hover over the profile pic and click the Following button to unfollow any account.
When you see a Tweet you love, tap the heart — it lets
the person who wrote it know you shared the love.
The fastest way to share someone else’s Tweet with your followers is with a Retweet. Tap the icon to send it instantly.
Add your thoughts about any Tweet with a Reply. Find a topic you’re passionate about, and jump right in.
Get instant insight into what people are talking about now.
Follow more accounts to get instant updates about topics you care about.
See the latest conversations about any topic instantly.
Catch up instantly on the best stories happening as they unfold."
cbdabb7bb9,This site uses machine learning to enhance your low-res photos for free,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Abhimanyu Ghoshal
—
in Apps
We’ve recently seen a number of interesting approaches to improving low-resolution images using machine learning, including ones from researchers at Google, and at the Max Planck Institute for Intelligent Systems. Now, you can try an AI-powered enhancing tool for yourself with just a couple of clicks. Head over to Let’s Enhance, drag and drop a photo onto the page, and give it a few seconds to spit out two different- and surprisingly usable – results. Here’s a sample from a photograph I took and scaled down to a tenth of its original size, and then fed into the app: I tried the tool with a bunch of pictures, and found that the ‘magic’ filter works well with photographs. The ‘boring’ filter can help remove some artifacts in logos and graphic design elements. Plus, both filters upscale your original image to four times the size. Oh, and the third ‘anti-jpeg’ filter doesn’t appear to do much, so it’s best to ignore it. While the results aren’t perfect, they can certainly come in handy for things like tracing vector shapes and preserving old photos. Hopefully, the developers will eventually add an API or a way to upload a bunch of photos at once. Try it out with your own images by visiting this page.
Read next:
Google is mapping out air pollution levels on Google Earth
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
903622b01c,Even a mask won't hide you from the latest face recognition tech | New Scientist,"Daily news
7 September 2017
John Powell/REX/Shutterstock By Matt Reynolds Ditch the hat and scarf – it’s not fooling anyone. Face recognition software can now see through your cunning disguise – even you are wearing a mask. Amarjot Singh at the University of Cambridge and his colleagues trained a machine learning algorithm to locate 14 key facial points. These are the points the human brain pays most attention to when we look at someone’s face. The researchers then hand-labelled 2000 photos of people wearing hats, glasses, scarves and fake beards to indicate the location of those same key points, even if they couldn’t be seen. The algorithm looked at a subset of these images to learn how the disguised faces corresponded with the undisguised faces.
The system accurately identified people a wearing scarf 77 per cent of the time – a cap and scarf 69 per cent of the time and a cap, scarf and glasses 55 per cent of the time. This isn’t as good as systems that recognise undisguised human faces, but it is the best at seeing through disguises, says Singh. The system only needs to be able to see a fraction of facial key points – most of which are around the eyes and mouth – to be able to guess where the other points are likely to be. Based on that guess, it can identify the person if it has already been shown a map of their key points. “In effect, it is able to see through your mask,” says Singh. You can also probably say goodbye to CV Dazzle, the vaunted face recognition camouflage makeup that has been mooted as the way to stay anonymous in a world of face recognition. “This will work very well for this type of camouflage because it works on key points of the face,” he says. He will present his findings at the International Conference on Computer Vision in Italy in late October. Singh has plans to take this research even further and see if it’s possible to design an algorithm that can identify someone wearing a rigid plastic mask, like the V for Vendetta masks that are popular at some protests. The system could be used to identify criminals who are trying to hide their identities, says Singh. But he admits it could also be used by authoritarian governments to identify protesters. “It kind of impinges on the privacy of people,” he says. Automatic face recognition software is catching on with law enforcement across the world. In August, the UK government said it planned to spend £4.6 million on upgrading face recognition software so algorithms could automatically spot suspects from live video footage. “There’s always a trade-off between security and privacy,” says Anil Jain at Michigan State University. But he says that people in public spaces are already under constant surveillance by security cameras, so they shouldn’t be too worried about every improvement in the technology. For now, the system is far from perfect. The fewer facial key points it can see, the worse the software is at recognising a person in a photo. It’s also thrown off by busy backgrounds, so can only identify a person wearing a cap, glasses and scarf 43 per cent of the time if they’re standing in front of a complicated background. It’s also not clear how well this system would work in the real world, Jain says. The algorithm was only trained on photos of 25 people, which he says isn’t enough to really determine its efficacy. And anti-surveillance tricks are keeping pace with improvements. Last year, a team of researchers from Carnegie Mellon University found they could trick face recognition software by wearing specially designed glasses. Now might be the time to trade in your fake beard for a pair of jazzy specs. Reference: https://arxiv.org/pdf/1708.09317.pdf     More on these topics:
A shorter version of this article was published in New Scientist magazine on 16 September 2017"
7e5994c6ff,Deep Neural Networks for Face Detection Explained on Apple's Machine Learning Journal - Mac Rumors,"But bahgawd, the technology Apple is pulling off really is falling squarely into the realm of magical. Anyone else just wowed by the amount of technology embedded into this new iPhone?
Our phones are learning more about us then we ever knew Before. Siri-based speaker unveiled at WWDC, launching early 2018.   MacRumors attracts a broad audience
of both consumers and professionals interested in
the latest technologies and products. We also boast an active community focused on
purchasing decisions and technical aspects of the iPhone, iPod, iPad, and Mac platforms."
610f69d634,Skype launches Photo Effects – sticker suggestions powered by machine learning  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Not content with merely launching its own take on Instagram and Snapchat’s Stories, Skype today is adding another copycat-like feature to its app: photo stickers. The company says it’s introducing new “Photo Effects” (as it’s calling these stickers), which include things like face masks, decorative borders, witty captions, and more. However, unlike the photo stickers you’ll find in other social apps today, Skype will actually suggest the stickers to use based on the photo’s content, day of the week, and other options. The new feature is based on technology Microsoft introduced earlier this year in a silly camera app called Sprinkles. The Sprinkles app leverages Microsoft’s machine learning and A.I. capabilities to do things like detect faces in photos, determine the subject’s age and emotion, figure out your celebrity look-a-like, and suggest captions. It then lets you swipe through its suggestions – for example, various props to add to your photo, funny captions, and stickers displaying its guess about your age, among other things. Similarly, Skype will suggest its photo effects automatically with a press of a button. To use the feature, you’ll first snap a photo then tap the magic wand icon at the top of the screen to access the photo effects. As you swipe right through the suggestions, you’ll be prompted to add things to your photo like a smart face sticker, the weather, your location, a caption that references the day (e.g. “turn that frown upside down, it’s taco Tuesday!”), face masks, a celebrity look-a-like, or even a mystery face swap.
Microsoft says these photo effects will change often – like on different days of the week or holidays, for instance. The resulting image can be shared with Skype friends in a conversation or posted to Skype’s new Highlights feature, which is the Instagram/Snapchat Stories clone introduced earlier this year. Like Stories on other platforms, Highlights are somewhat ephemeral. But instead of lasting a day, they’re available for a week. They’re also not shared with your entire Skype network – only those who have opted to follow your Highlights directly. Highlights remains a mobile-only feature for now. When Skype’s revamped interface launched to desktop users in October, Microsoft told us Highlights was not a priority for desktop integration at this time, based on user feedback. However, the company insisted it still aims to bring Highlights to the desktop in a later release. The addition of Photo Effects is arriving on Skype for mobile users in the latest update. Skype’s release notes list Photo Effects as “upcoming” in Android version 8.10.0.4 and iOS 8.10.0.5. This version began rolling out on Monday, but will gradually release to the install base over the next week. Latest headlines delivered to you daily"
ae8b0642eb,Can Facebook's Machine-Learning Algorithms Accurately Predict Suicide? - Scientific American,"We use cookies to provide you with a better onsite experience. By continuing to browse the site you are agreeing to our use of cookies in accordance with our Cookie Policy. The social media giant aims to save lives by quickly flagging and responding to worrying posts When Naika Venant killed herself in January, the Miami-area teen broadcast the event for two hours on Facebook’s popular video live-streaming feature, Facebook Live. A friend of hers saw the video and alerted police but aid did not arrive in time to save the 14-year-old’s life. Other young people have also recently posted suicidal messages on social media platforms including Twitter, Tumblr and Live.me. In a bid to save lives Facebook and other social media giants are now wading into suicide prevention work—creating new alert systems designed to better identify and help at-risk individuals. Last Wednesday Facebook unveiled a new suite of tools including the company’s first pattern recognition algorithms to spot users who may be suicidal or at risk of lesser self-harm. Facebook says the new effort will help it flag concerning posts and connect users with mental health services. It also represents a new front in its machine learning. Suicide is now the 10th-leading cause of death in this country and the second-leading cause of deaths among youth, so social media could be an important intervention point, says psychologist Daniel Reidenberg, executive director of Save.org, one of Facebook’s partner mental health organizations. Facebook currently reports more than a billion daily users around the world. In the U.S. 71 percent of teens between 13 and 17 and 62 percent of adults over 18 have a presence on it, according to two 2015 reports by the Pew Research Center. To reach its at-risk users, Facebook says it is expanding its services that allow friends to report posts containing signs of any suicidal or self-mutilation plans and it provides a menu of options for both those individuals and the friend who reported them. Choices include hotlines to call, prompts to reach out to friends and tips on what to do in moments of crisis. This tool will now be available for Facebook live streams as well. Similar reporting systems exist on a number of social media platforms including Twitter, Pinterest and YouTube. Facebook is now also piloting a program that will let people use Messenger, its instant messaging app, to directly connect with counselors from crisis support organizations including Crisis Text Line and the National Suicide Prevention Lifeline (NSPL).
Facebook also plans to use pattern recognition algorithms to identify people who may be at risk of self-harm, and to provide them with resources to help. The company says its new artificial intelligence program, which will be rolled out a on a limited basis at first, will employ machine learning to identify posts that suggest suicidal thoughts—even if no one on Facebook has yet reported it. William Nevius, a spokesperson at Facebook, says the machine-learning algorithms will use two signals—one from words or phrases that relate to suicide or self-harm in users’ posts and the other from comments added by concerned friends—to determine whether someone is at risk. If the pattern recognition program identifies concerning posts, the “report post” button will appear more prominently to visually encourage users to click it. “The hope is that the artificial intelligence learning will pick up on multiple signals from various points, [put] that together and activate a response, both to the person who might be at risk and to others who [could help],” Reidenberg wrote in an e-mail. If those cues signal a higher level of urgency, the system will automatically alert Facebook’s Community Operations team—a group of staff members who provide technical support and monitor the site for issues such as bullying or hacked accounts. The team will quickly review the post and determine whether the person needs additional support. If so, they make sure the user will see a page of resources appear on his or her news feed. (That page would normally only pop up if a post were reported by a concerned friend.) To help its artificial intelligence learn to flag concerning posts, Facebook mined “tens of thousands of posts that have been reported by friends who are concerned about another friend,” explains John Draper, project director of the NSPL, also a Facebook partner organization. Although the current algorithms are limited to text, Facebook may eventually use AI to identify worrying photos and videos as well. CEO Mark Zuckerberg announced last month that the company has been “researching systems that can look at photos and videos to flag content our team should review” as a part of efforts to assess reported content, including suicides, bullying and harassment. “This is still very early in development but we have started to have it look at some content, and it already generates about one third of all reports to the team that reviews content for our community,” Zuckerberg wrote. Nevius did not provide information about when these additional tools might be applied. Some mental health experts say AI is still limited at identifying suicide risk via language alone. “I think [machine learning] is a step in the right direction,” says Joseph Franklin, a psychologist at Florida State University who studies suicide risk. Franklin and his colleagues recently conducted a meta-analysis of 365 studies from 1965 to 2014. They found that despite decades of research, experts’ ability to detect future suicide attempts have remained no better than chance. “There’s just a tiny predictive signal,” Franklin says. These limitations have spurred him and others to work on developing machine-learning algorithms that help assess risk by analyzing data from electronic health records. “The limitation of health records is that…we can accurately predict [risk] over time, but we don’t know what day they’re going to attempt suicide,” Franklin says. Social media could be very helpful at providing a clearer sense of timing, he adds. But that, too, would still have key limitations: “There's just not a lot you can tell from text, even using more complicated natural language processing, because people can use the words ‘suicide’ or ‘kill myself’ for many different reasons—and you don't know if someone is using it in a particular way.” Some researchers such as Glen Coppersmith, founder and CEO of the mental health analytics company Qntfy, have discovered useful signals in language alone. In a recent examination of publicly available Twitter data, Coppersmith and his colleagues found the emotional content of posts—including text and emojis—could be indicative of risk. He notes, however, these are still “little pieces of the puzzle,” adding, “The other side of it, the sort of nonlanguage signal, is timing.” “Facebook has information about when you’re logging in, when you’re chatting…and what hours are you logging in, [which are] really interesting signals that might be relevant to whether or not you’re at proximal risk for suicide.” Craig Bryan, a researcher at the University of Utah who investigates suicide risk in veteran populations, has started to examine the importance of timing in the path to suicide. “In our newer research, we’ve been looking at the temporal patterns of sequences as they emerge—where [we find] it’s not just having lots of posts about depression or alcohol use, for instance, [but] it’s the order in which you write them,” he says. Another important factor to consider, especially with teens, is how often their language changes, says Megan Moreno, a pediatrician specializing in adolescent medicine at Seattle Children’s Hospital. In a 2016 study Moreno and colleagues discovered that on Instagram (a social media platform for sharing photos and video), once a self-injury–related hashtag was banned or flagged as harmful, numerous spin-off versions would emerge. For example, when Instagram blocked #selfharm, replacements with alternate spelling (#selfharmmm and #selfinjuryy) or slang (#blithe and #cat) emerged. “I continue to think that machine learning is always going to be a few steps behind the way adolescents communicate,” Moreno says. “As much as I admire these efforts, I think we can’t rely on them to be the only way to know whether a kid is struggling.” “The bottom line is that it’s definitely an effort that makes a lot of sense, given the fact that a lot of people connect with social media,” says Jessica Ribeiro, a psychologist applying machine learning to suicide prevention research at Florida State. “At the same time, they’re limited by what the science in this area doesn’t know—and unfortunately, we don't know a lot despite decades of research.” Diana Kwon Diana Kwon is a freelance science writer with a master's degree in neuroscience from McGill University. More of her pieces can be found at www.dianakwon.com. June 1, 2016
—
Nancy Shute November 30, 2016
—
Nathaniel P. Morris October 15, 2016
—
Michelle Carr February 1, 2009
—
Melinda Wenner Neuroscience. Evolution. Health. Chemistry. Physics. Technology. Follow us © 2018 Scientific American, a Division of Nature America, Inc. All Rights Reserved."
d8f4bf5fcb,Algorithmia now helps businesses manage and deploy their machine learning models  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Algorithmia started out as an online marketplace for — can you guess it? — algorithms. Many of these algorithms that developers offered on the service focused on machine learning (think face detection, sentiment analysis, etc.). Today, with the boom in ML/AI, that’s obviously a big draw and Algorithmia is now taking its next step in this direction with the launch of a new service that helps data scientists manage and deploy their machine learning models — and share them with others inside their companies. This basically means that the company is turning some of the infrastructure and services it built to run these models itself into a new product. “Tensorflow is open-source, but scaling it is not,” said Kenny Daniel, co-founder and CTO of Algorithmia, in today’s announcement. “Almost all R&D has focused on collecting and cleaning data, and building models. Algorithmia has spent the last five years building the infrastructure that will put those models to work.”
With this new service, data scientists can create their models in the languages and with the frameworks of their choice and then host them in the Algorithmia cloud (the CODEX platform) or using the company’s on-prem architecture. To do this, the company offers two versions of its service: the Serverless AI Layer for hosting models in its cloud and the Enterprise AI Layer for hosting the service in any public or private cloud. Both the hosted and on-prem versions of the service offer enterprises the ability to use git to add models, share them with others inside of an organization, and to handle permissioning and authorization. The service also handles all of the DevOps necessary to host and deploy models.
In recent months, the company started signing up enterprise customers to beta test this service, including a number of government agencies who want to use its service for hosting their models. “Algorithmia empowers U.S. government agencies to rapidly deploy new capabilities to the AI layer,” said Katie Gray, Principal of Investments at In-Q-Tel, the CIA’s investment arm. “The platform delivers security, scalability and discoverability so data scientists can focus on problem solving.” Earlier this year, Algorithmia announced a $10.5 million Series A funding round that was led by Google’s new AI venture fund.
Latest headlines delivered to you daily"
d355f0244c,Information Engineering Main/Home Page,"Established by Professor Sir Michael Brady in 1985, the Robotics Research Group brought together a group of like-minded engineers working in robotics research and artificial intelligence. Now known as Information Engineering, it is currently composed of seven research groups whose interests range from machine learning to mobile robotics.
Information Engineering is located in the Engineering Science triangle, principally in the purpose-built Information Engineering Building (opened in 2004).
Other Information
Contact Us
Page last modified on October 17, 2017, at 03:23 PM
Skittlish theme adapted by David Gilbert, powered by PmWiki"
3cea4ca145,Inquisitive bot asks questions to test your understanding | New Scientist,"Daily news
8 May 2017
Bettmann/Getty By Matthew Reynolds
Inquisitive artificial intelligence that asks questions about things it reads could be used to quiz students in class. The question-asking ability would also help chatbots with the back and forth of human conversation. AI is usually on the receiving end of queries, says Xinya Du at Cornell University in Ithaca, New York. Du and his colleagues have turned the tables by building a system that has learned to ask questions of its own. This is something that people have been wanting to do for a long time, says Karen Mazidi at the University of Dallas in Texas. Previous attempts by other people using hand-coded rules haven’t been particularly successful.
The machine-learning algorithm can read a passage of text and come up with the kind of questions you might ask to check someone’s understanding of a topic. Du’s team used a neural network – software that loosely mimics the structure of the brain – and trained it on more than 500 Wikipedia articles and 100,000 questions about those articles sourced from crowdworkers. For example, a sentence about different types of crop grown in Africa might be paired with the question “What is grown in the fertile highlands?” The software learned to recognise patterns that linked questions back to their source text, such as that dates in sentences tended to correspond with questions that started with “when”, and sentences about places often led to questions that started with “where”. The team then presented the AI with extracts from Wikipedia articles that it hadn’t yet seen. Given a passage from an article on economic inequality, it asked the question “When did income inequality fall in the US?” An article about a Pakistani political organisation prompted “When was the Jamaat-e-Islami party founded?” Four human volunteers rated the naturalness and difficulty of the questions as higher than those generated by existing systems. Mazidi is impressed with the results and thinks similar algorithms could eventually be used in classrooms to help test students. “If you stop a student while they’re reading something, and ask them a few questions, it can greatly improve their comprehension of what they’ve just read,” she says. Du’s software taught itself using a large amount of real-life data, but Mazidi thinks rule-based approaches shouldn’t be thrown away entirely. “A neural network is going to learn whatever it learns and you have very little control over that,” she says. By adding in a few rules, a computer could be made to formulate questions in a way that seems more human. The current version of the system produces a question for every sentence it reads but Du wants it to asks questions only about sentences that contain statements. “Not all sentences are question-worthy,” he says. Reference: arxiv.org/abs/1705.00106 Read more: Google uses neural networks to translate without transcribing; Kindergarten bots teach language to tots More on these topics:"
f013778d13,DeepMind AI teaches itself about the world by watching videos | New Scientist,"Daily news
10 August 2017
OE KLAMAR/AFP/Gettyvide By Matt Reynolds To an untrained AI, the world is a blur of confusing data streams. Most humans have no problem making sense of the sights and sounds around them, but algorithms tend only to acquire this skill if those sights and sounds are explicitly labelled for them. Now DeepMind has developed an AI that teaches itself to recognise a range of visual and audio concepts just by watching tiny snippets of video. This AI can grasp the concept of lawn mowing or tickling, for example, but it hasn’t been taught the words to describe what it’s hearing or seeing. “We want to build machines that continuously learn about their environment in an autonomous manner,” says Pulkit Agrawal at the University of California, Berkeley. Agrawal, who wasn’t involved with the work, says this project takes us closer to the goal of creating AI that can teach itself by watching and listening to the world around it.
Most computer vision algorithms need to be fed lots of labelled images so it can tell different objects apart. Show an algorithm thousands of cat photos labelled “cat” and soon enough it’ll learn to recognise cats even in images it hasn’t seen before. But this way of teaching algorithms – called supervised learning –  isn’t scalable, says Relja Arandjelović who led the project at DeepMind. Instead of relying on human-labelled datasets, his algorithm learns to recognise images and sounds by matching up what it sees with what it hears. Humans are particularly good at this kind of learning , says Paolo Favaro at the University of Bern in Switzerland. “We don’t have somebody following us around and telling us what everything is,” he says. Arandjelović created his algorithm by starting with two networks – one that specialised in recognising images and another that did a similar job with audio. He showed the image recognition network stills taken from short videos while the audio recognition network was trained on 1-second audio clips taken from the same point in each video. A third network compared still images with audio clips to learn which sounds corresponded with which sights in the videos. In all, the system was trained on 60 million still-audio pairs taken from 400,000 videos. The algorithm learned to recognise audio and visual concepts, including crowds, tap dancing and water, without ever seeing a specific label for a single concept. When shown a photo of someone clapping, for example, most of the time it knew which sound was associated with that image. This kind of co-learning approach could be extended to include senses other than sight and hearing, says Agarwal. “Learning visual and touch features simultaneously can, for example, enable the agent to search for objects in the dark and learn about material properties such as friction,” he says. DeepMind will present the study at the International Conference on Computer Vision which takes place in Venice, Italy, in late October. While the AI in the DeepMind project doesn’t interact with the real world, Agarwal says that perfecting self-supervised learning will eventually let us create AI that can operate in the real world and learn from what it sees and hears. But until we reach that point, self-supervised learning might be a good way of training image and audio recognition algorithms without input from vast amounts of human-labelled data. The DeepMind algorithm can correctly categorise an audio clip nearly 80 per cent of the time, making it better at audio-recognition than many algorithms trained on labelled data. Such promising results suggest that similar algorithms might be able to learn something by crunching through huge unlabelled datasets like YouTube’s millions of online videos. “Most of the data in the world is unlabelled and therefore it makes sense to develop systems that can learn from unlabelled data,” Agrawal says. Journal reference: arxiv.org Read more: Curious AI learns by exploring game worlds and making mistakes More on these topics:
A shorter version of this article was published in New Scientist magazine on 19 August 2017"
0c347f8e94,How machine learning is helping Virgin boost its frequent flyer business | ZDNet,"This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please
view our cookie policy.
Using DataRobot's automated machine learning service, Virgin Australia has been able to cut down the time it takes to build predictive models by up to 90 percent, while boosting accuracy by up to 15 percent.
By Tas Bindi
|
November 21, 2017 -- 03:04 GMT (03:04 GMT)
| Topic: Innovation
Companies that are able to adapt to a world where innovation is increasingly driven by machine learning, or artificial intelligence more broadly, are the ones that will come out the other end of the tunnel and thrive, according to Oliver Rees, GM of Torque Data at Virgin Australia. Rees, whose data analytics consultancy firm Torque Data was acquired by Virgin Australia in 2015, told ZDNet that one of its tasks has been ""reengineering [Virgin's] analytical capability"", ensuring the airline is well-prepared to embrace the opportunities that are offered by machine learning. While not new to machine learning, Virgin Australia has been seeking better methods of developing, applying, and assessing machine learning algorithms, recently turning to Massachusetts-based company DataRobot, which operates on the belief that automated machine learning will not only increase productivity for data scientists, but also open up the world of data science.
Rees told ZDNet that Torque, as the data analytics arm of Virgin, has been investigating ways to improve customer experience for members of Virgin's Velocity Frequent Flyer loyalty program.
""We want people within our program to be able to redeem points for great experiences, and to do that, we want to be able to better predict when is the best time for particular people to redeem points and what should they be redeeming them against,"" Rees said. ""For a given individual or a group of people, we want the ability to be able to better understand what it is that they might be really interested in doing at a certain destination -- from both a business and a leisure point of view -- and how can we serve them better at that destination, and then be able to tailor that to their specific requirements and serve that up to them in a meaningful way. ""I think we need to take it upon ourselves in the industry to build the predictive models that understand what the needs and wants of our customers are, and go through the whole curation process, become their concierge."" Using DataRobot's automated machine learning service, Virgin Australia is looking to build models that can predict the types of people that are more likely to travel, the types of travel people are likely to undertake, the prices that travellers are willing to pay, the importance of accommodation relative to travel, and the importance of experience compared to travel.
A lot of information is provided willingly by customers, Rees said, but the company also uses previous purchases to predict future preferences.
""There's a lot we can impute from what they do already, but also by actually looking at events and the type of events that people like to attend -- that's a really powerful piece of information,"" Rees said.
""It all comes back to using what we understand already about people, and not necessarily as individuals either, but as groups, so we look at tribes and cohorts ... Often people want guidance around what do people like me do and how can I as an individual benefit from the learnings of others.
""We need to not only understand people based on what they tell us themselves, but also what people like them are telling us, what experiences people like them are enjoying ... that's where the competitive battleground is.""
Using technology provided by DataRobot, which raised more than $124 million since its inception in 2012, Rees said Torque has been able to build new predictive models at one-tenth of the time it had previously taken, and the models are up to 15 percent more accurate than previous ones. ""We have the ability to run multiple different statistical techniques against the same dataset in a very short space of time and have this competitive element whereby the models compete against each other for the best outcome,"" Rees said. ""[DataRobot's system is] constantly reviewing whether this particular technique can outperform this other technique, and that's running really in real-time before my eyes, whereas an analyst might spend a number of hours running one particular model. ""The ability for us to then deploy that model is also enhanced. It's direct API link and we can start to operationalise the analytics far more quickly because there's a reduced number of steps in doing that."" Rees also noted that analysts can be biased towards specific statistical techniques, the same way artists prefer to use watercolour, oil, or acrylic paints.
""So it removes that element of bias as well,"" he added. ""I think that's very powerful; it makes us stronger as an organisation."" Additionally, using an automated machine learning service means analysts are able to spend more time on uncovering opportunities to use analytics and less time on grunt work like manipulating data, Rees said.
""We're actually moving really smart people into different roles where we're using their intellect in a really powerful way,"" he said.
""The way to hang on to great analytical teams is to give them leading edge tools to work with and challenging meaningful analytical problems to solve.
""People are very interested in building their understanding around how new technology is going to impact their work. Giving people the opportunity to learn how it works and recognising that we're all on this journey together ... I think it's been a real positive for us."" Rees believes technology, like that offered by DataRobot, will allow Virgin to ""democratise analytics"" throughout the organisation, enabling all business units to be data-driven.
""Analytics is far too important to be left to the analyst. With these sorts of tools ... we can have the whole business being more data-driven because the power to build really great models lies in many more hands,"" he said. ""It doesn't mean, by the way, that we're not going to need the really powerful hardcore analytical teams that we have -- of course we're going to need those sort of people -- but it means that a wider number of people within our business are going to be able to rely on analytics to help them with faster decision-making."" According to Rees, the real challenge for large organisations like Virgin is ""avoiding falling into the trap of doing nothing and not embracing the new technologies available"".
""It can very easily be seen as something that creates extra work, creates extra stress, creates extra pressure. We can't ignore what we need to do on day-to-day basis at the expense of developing new capability,"" he said.
""Being in an organisation that is prepared to be innovative and move forward is a really important. It's a fine balance."" Rees said the application of machine learning is a ""quantum leap forward"" in how data can be used to drive better outcomes for both customers and businesses, and that businesses are barely scratching the surface of what's possible.
Can machine learning bail enterprises out of their data management woes?
Technology vendors are scrambling to create an abstraction layer that can clean up data and make it easier to prep for analytics. The problem is humans haven't been the best stewards of organized data. As a result, machine learning is the next magic bullet for data management issues dating back decades.
AI, robotics, IoT, augmented and virtual reality to bolster ICT spending
According to IDC, spending on new technologies will accelerate over the next five years and boost spending on information and communications technology overall.
Data to analytics to AI: From descriptive to predictive analytics
Artificial Intelligence seems to be the buzzword du jour for organizations, but this is not an obvious or straightforward transition even for those building advanced products and platforms. Many organizations are still struggling with digital transformation to become data-driven, so how should they approach this new challenge? Video: How machine learning creates demand for human workers (TechRepublic) Artificial intelligence and IoT are increasing demand for human workers, says Calabrio CEO Tom Goodmanson, and they're more stressed out than ever. How AI and machine learning will help the rise of quantum computing (TechRepublic) When quantum computers come online, all encryption will fail. James Barratt explains how AI will aid the way for the emergence of quantum computing. Mobility Telstra CEO: 2018 will be big for 5G Innovation Cortana isn't dead. She just needs to get to work. Innovation Robots are helping researchers work to end world hunger Artificial Intelligence CES 2018: Intel-Ferrari AI partnership will fundamentally change sports broadcasts © 2018 CBS Interactive. All rights reserved. Privacy Policy | Cookies | Ad Choice |
Advertise | Terms of Use | Mobile User Agreement"
e24379f59e,"
            Brain Imaging Technology Uses Machine Learning to Identify Suicidal Thoughts - Seeker        ","Researchers were able to identify subjects with suicidal ideation with 90 percent accuracy using a combination of brain imaging and AI algorithms.
A team of US medical researchers has developed a new and decidedly high-tech system for potentially preventing suicide. Using advanced brain-imaging technology and machine learning — a kind of subset of artificial intelligence — the system can flag patients with suicidal thoughts by essentially reading their minds.
It might sound like science fiction, but it's a technically accurate description of the new technique. The system detects and analyzes brain activity when the subject is asked to consider specific keywords and concepts related to suicide, such as “death” or “cruelty.”
When the brain activity data are processed by the system, electrical activity shows up on a map of the brain, with more intense feelings and thoughts generating specific color patterns in particular areas.
That's where the machine learning comes in. Using specifically coded algorithms, the AI system can detect significant pulses and patterns associated with suicidal thoughts. In a series of experiments using the technique, the system was able to accurately identify suicidal individuals with upwards of 90 percent accuracy.
“This type of analysis can assess a number of different component of the neural representation of a concept,” Marcel Just, professor of psychology at Carnegie Mellon University, told Seeker.
“One of the components that is measurable is the presence of various emotions,” Just said. “A classifier was able to distinguish between suicidal ideators and controls based on the how much of each emotion there was in the neural representation.”
The research, funded in part by the National Institutes of Mental Health, was published Oct. 30 in the journal Nature Human Behavior.
RELATED: AI Interviewers Entice Reluctant Soldiers to Report PTSD Symptoms
Researchers led by Just and the University of Pittsburgh's David Brent designed the experiments by preparing lists of keywords —10 related to death, 10 related to positive concepts, and 10 related to negative ideas.
The team presented the keywords to two groups of 17 people each. The first group consisted of patients with known suicidal tendencies. The second group – the control group – was made up of “neurotypical” persons, randomly selected people with no history of mental illness or suicidal behavior.
Both groups were outfitted with advanced brain scanning systems as they considered the list of keywords in various combinations. By analyzing their brain activity during this period, the system was able to distinguish between the suicidal group and the control group with 91 percent accuracy.
In a second set of experiments, the researchers used a similar approach to see if the machine learning system could distinguish between patients who had made a previous suicide attempt and those who had just thought about it. The program was able to identify those who had previously tried to take their own lives with 94 percent accuracy.
The new technique could have practical value for front-line clinicians who might be worried about potentially suicidal patients.
“We hope that information about what has been altered in a neural representation would be useful to a therapist or a therapy designer,” Just said. “Furthermore, one could subsequently assess the effectiveness of the therapy in terms of whether the alteration has been eliminated or reduced.”
RELATED: Brain-Computer Interface Allows Users to Compose Music With Only Their Thoughts
Just and other Carnegie Mellon researchers first developed the imaging system from brain activation signatures. The research has since been extended to identify emotions and multi-concept thoughts from their neural signatures.
For now, the technique requires specially calibrated brain scanning technology in the laboratory, Just said. But he hopes to change that.
“We are working on another project to determine whether suicidal ideation can be identified using [an electroencephalogram], which is a much cheaper and more widely available technology.”
WATCH: Which Countries Have the Highest Suicide Rates?"
0c2758f61b,How Tesla's autopilot learns | Fortune,"While Tesla’s new hands-free driving is drawing a lot of interest this week, it’s the technology behind-the-scenes of the company’s newly-enabled autopilot service that should be getting more attention. At an event on Wednesday Tesla’s CEO Elon Musk explained that the company’s new autopilot service is constantly learning and improving thanks to machine learning algorithms, the car’s wireless connection, and detailed mapping and sensor data that Tesla collects. Tesla’s cars in general have long been using data, and over-the-air software updates, to improve the way they operate. Machine learning algorithms are the latest in computer science where computers can take a large data set, analyze it and use it to make increasingly accurate predictions. In short, they are learning. Companies like Google (GOOG), Facebook (FB) and now Tesla (TSLA) are using machine learning as a way to train software to help customers or sell them new services. Machine learning is the way that computers can become artificially intelligent, and the technology is a form of AI. While Musk has taken a sort of alarmist stance against the dangers of AI, he clarified during the event on Wednesday that he’s only concerned with artificial intelligence that is meant for nefarious purposes. When a reporter asked Musk during the media Q&A what made his company’s autopilot service different than other computer-based driving assistance features that competing big auto makers are working on, Musk emphasized learning. “The whole Tesla fleet operates as a network. When one car learns something, they all learn it. That is beyond what other car companies are doing,” said Musk. When it comes to the autopilot software, Musk explained that each driver using the autopilot system essentially becomes an “expert trainer for how the autopilot should work.” While most car companies might not be building learning systems, Google’s self-driving cars operate in a similar manner. In that way, Tesla’s cars are more similar to smart connected gadgets like Nest’s learning thermostat (now owned by Google’s Alphabet), than they are to traditional cars. Nest’s thermostat, using sensors and algorithms, learns its owner’s behavior over time, and through software updates offers increasingly useful services, or even informs Nest’s decisions about its next-generation of hardware. So, how does Tesla’s autopilot system, and its cars in general, learn? It all starts with data. Companies building these types of driver-assistance services, as well as full-blown self-driving cars like Google’s, need to teach a computer how to take over key parts (or all) of driving using digital sensor systems instead of a human’s senses. To do that companies generally start out by training algorithms using a large amount of data. You can think of it how a child learns through constant experiences and replication, explained Nvidia’s Senior Director of Automotive, Danny Shapiro in an interview with Fortune. Nvidia (NVDA) sells high performance chips that enable computers to process large amounts of data, and more recently started selling a computing system, called Drive PX, for self-driving cars and driver-assist applications. To create a self-driving car, companies feed hundreds of thousands, or even millions, of miles of driving videos and data into a computer’s data model to basically create a massive vocabulary around driving. The algorithms use visual techniques to break down the videos and to understand them. The goal is that when something unexpected happens — a ball rolls into the street — the car can recognize the pattern and react accordingly (slow down because a child could be running into the street after it). For Nvidia, the company loads this “driving dictionary,” as Shapiro calls it, onto powerful but compact computing hardware that can be used on the car. After that, companies like Google and Tesla add various types of data from different sources to continue to inform the model over time. Companies try to gather as much data as possible to help a car’s computer make smarter and better decisions on the roads. This includes data from customers driving, data from GPS and maps, and data from company employees driving research cars. The data from Tesla drivers was enabled by the hardware choices that Tesla has made. All Tesla cars built in the past year have 12 sensors on the bottom of the vehicle, a front-facing camera next to the rear-view mirror, and a radar system under the nose. These sensing systems are constantly collecting data to help the autopilot work on the road today, but also to amass data that can make Tesla’s operate better in the future. Because all of Tesla’s cars have an always-on wireless connection, data from driving and using autopilot is collected, sent to the cloud, and analyzed with software. For autopilot, Tesla takes the data from cars using the new automated steering or lane change system, and uses it to train its algorithms. Tesla then takes these algorithms, tests them out and incorporates them into upcoming software. Companies will rely on different types of data depending on what they’re trying to do with the cars. For example, Google has used large and expensive LIDAR (light-based radar) sensors on its self-driving cars. But Tesla’s Musk said that LIDAR was basically overkill for what Tesla’s autopilot cars need. But Musk said that Tesla wanted much more detailed high-precision mapping data for its automated steering and lane change applications than was available through the standard navigation tech. To meet its needs, Tesla has started to build high-precision maps —that have 100 times the level of granularity compared to standard navigation systems — using mostly data from Tesla cars driving on roads, but also some data from Tesla employees driving research cars. These new services could provide unexpected business models for companies. Musk said that Tesla might be interested in selling the mapping data to other car companies down the road. Tesla isn’t the only car maker working on driver-assist and self-driving car tech. Google is blazing ahead on its futuristic tech, while Audi has traffic jam assist software. Nvidia’s Shapiro says that most automakers are investigating these technologies. Nvidia started shipping Drive PX this summer, and Shapiro says that it’s engaged with over 50 companies and researchers. Tesla uses Nvidia chips in the 17-inch screen and the instrument cluster for its Model S and there has been speculation around whether Tesla might use the Drive PX system in future versions of the Model X SUV. Shapiro wouldn’t discuss the specifics of its relationships with Tesla or Audi, which uses Nvidia’s tech in its traffic jam system. Shapiro cautioned that despite some companies already deploying these technologies, it’s still early days for self-driving car tech. “A huge amount of work will be done on this over the next decade,” he said. To learn more about Tesla’s autopilot tech watch this Fortune video:"
420d38cb43,Machine Learning: Helping Determine How a Drug Affects the Brain | Technology Networks,"News   Nov 16, 2017
| Original story from University College London
Gaze recovery high-dimensional classifier weights. Represented as 3D cubic glyphs varying in colour and scale are the weights of a transductive linear support vector machine classifier trained to relate the high-dimensional pattern of damage to gaze outcome, achieving k-fold cross-validation performance of 78.33% (SE = 1.70%) sensitivity and 82.78% (SE = 0.56%) specificity for distinguishing between patients who recovered from a leftward deviation of gaze and those who did not. Positive weights (dark blue to cyan) favour recovery, negative weights (dark red to yellow) persistence of symptoms. Though hemispheric asymmetry is prominent, note the distribution of weights is highly complex, as one would expect from the complexity of the functional and lesional architectures that generate the critical pattern. Credit: https://doi.org/10.1093/brain/awx288
Machine learning could improve our ability to determine whether a new drug works in the brain, potentially enabling researchers to detect drug effects that would be missed entirely by conventional statistical tests, finds a new UCL study published in Brain.""Current statistical models are too simple. They fail to capture complex biological variations across people, discarding them as mere noise. We suspected this could partly explain why so many drug trials work in simple animals but fail in the complex brains of humans. If so, machine learning capable of modelling the human brain in its full complexity may uncover treatment effects that would otherwise be missed,"" said the study's lead author, Dr Parashkev Nachev (UCL Institute of Neurology).To test the concept, the research team looked at large-scale data from patients with stroke, extracting the complex anatomical pattern of brain damage caused by the stroke in each patient, creating in the process the largest collection of anatomically registered images of stroke ever assembled. As an index of the impact of stroke, they used gaze direction, objectively measured from the eyes as seen on head CT scans upon hospital admission, and from MRI scans typically done 1-3 days later.They then simulated a large-scale meta-analysis of a set of hypothetical drugs, to see if treatment effects of different magnitudes that would have been missed by conventional statistical analysis could be identified with machine learning. For example, given a drug treatment that shrinks a brain lesion by 70%, they tested for a significant effect using conventional (low-dimensional) statistical tests as well as by using high-dimensional machine learning methods.The machine learning technique took into account the presence or absence of damage across the entire brain, treating the stroke as a complex ""fingerprint"", described by a multitude of variables.""Stroke trials tend to use relatively few, crude variables, such as the size of the lesion, ignoring whether the lesion is centred on a critical area or at the edge of it. Our algorithm learned the entire pattern of damage across the brain instead, employing thousands of variables at high anatomical resolution. By illuminating the complex relationship between anatomy and clinical outcome, it enabled us to detect therapeutic effects with far greater sensitivity than conventional techniques,"" explained the study's first author, Tianbo Xu (UCL Institute of Neurology).The advantage of the machine learning approach was particularly strong when looking at interventions that reduce the volume of the lesion itself. With conventional low-dimensional models, the intervention would need to shrink the lesion by 78.4% of its volume for the effect to be detected in a trial more often than not, while the high-dimensional model would more than likely detect an effect when the lesion was shrunk by only 55%.""Conventional statistical models will miss an effect even if the drug typically reduces the size of the lesion by half, or more, simply because the complexity of the brain's functional anatomy--when left unaccounted for--introduces so much individual variability in measured clinical outcomes. Yet saving 50% of the affected brain area is meaningful even if it doesn't have a clear impact on behaviour. There's no such thing as redundant brain,"" said Dr Nachev.The researchers say their findings demonstrate that machine learning could be invaluable to medical science, especially when the system under study--such as the brain--is highly complex.""The real value of machine learning lies not so much in automating things we find easy to do naturally, but formalising very complex decisions. Machine learning can combine the intuitive flexibility of a clinician with the formality of the statistics that drive evidence-based medicine. Models that pull together 1000s of variables can still be rigorous and mathematically sound. We can now capture the complex relationship between anatomy and outcome with high precision,"" said Dr Nachev.""We hope that researchers and clinicians begin using our methods the next time they need to run a clinical trial,"" said co-author Professor Geraint Rees (Dean, UCL Faculty of Life Sciences). This article has been republished from materials provided by University College London. Note: material may have been edited for length and content. For further information, please contact the cited source.ReferenceXu, T., Jäger, H. R., Husain, M., Rees, G., & Nachev, P. (2017). High-dimensional therapeutic inference in the focally damaged human brain. Brain. doi:10.1093/brain/awx288
A Virus-Like Protein is Important for Cognition and Memory
A protein involved in cognition and storing long-term memories looks and acts like a protein from viruses.
Genetic ‘Switches’ that Guide Human Brain Development Mapped
Mapping the gene regulation in human neurogenesis reveals factors that govern the growth of our brains and, in some cases, set the stage for several brain disorders that appear later in life.
Sensory Interneurons Created from Stem Cells
Researchers at the Eli and Edythe Broad Center of Regenerative Medicine and Stem Cell Research at UCLA have, for the first time, coaxed human stem cells to become sensory interneurons — the cells that give us our sense of touch.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 12, 2018 News   Jan 12, 2018 Poster Article News   Jan 11, 2018 News   Jan 11, 2018"
302b04a462,How Machine Learning Is Helping Morgan Stanley Better Understand Client Needs,"Systems that provide automated investment advice from financial firms have been referred to as “robo-advisers.” However, the enhanced human advising process — augmented by machine learning — that was recently announced by Morgan Stanley goes well beyond the robo label, and may help to finally kill off the term. The “next best action” system at Morgan Stanley is focused on three separate objectives — only one of which is common in the robo-adviser market. It includes providing operational alerts, as well as content on life events. If, for example, a client had a child with a certain illness, the system could recommend the best local hospitals, schools, and financial strategies for dealing with the illness. This service has the potential to help create a trusting and value-adding relationship between clients and financial advisers. Systems that provide automated investment advice from financial firms have been referred to as robo-advisers. While no one in the industry is particularly fond of the term, it has caught on nonetheless. However, the enhanced human advising process — augmented by machine learning — that was recently announced by Morgan Stanley goes well beyond the robo label, and may help to finally kill off the term. New York–based Morgan Stanley, in business since 1935, has been known as one of the more human-centric firms in the retail investing industry. It has 16,000 financial advisors (FAs), who historically have maintained strong relationships with their investor clients through such traditional channels as face-to-face meetings and phone calls. However, the firm knows that these labor-intensive channels limit the number of possible relationships and appeal primarily to older investors (according to a Deloitte study, the average wealth management client in the U.S. across the industry is over 60). So Morgan Stanley’s wealth management business unit has been working for several years on a “next best action” system that FAs could use to make their advice both more efficient and more effective. The first version of the system, which used rule-based approaches to suggesting investment options, is being replaced by a system that employs machine learning to match investment possibilities to client preferences. There are far too many investing options today for FAs to keep track of them all and present them to clients. And if something momentous happens in the marketplace — for example, the Brexit vote and the resulting decline in UK-based stocks — it’s impossible for FAs to reach out personally to all their clients in a short timeframe. The next best action system at Morgan Stanley, then, is focused on three separate objectives — only one of which is common in the robo-adviser market. There is, of course, a set of investment insights and choices for clients. In most existing machine advice, the recommended investments are strictly passive, that is, mutual funds or exchange-traded funds. The Morgan Stanley system can offer those if the client prefers them, but can also present individual stocks or bonds based on the firm’s research. The FA is given several ideas to offer the client and can use their own judgment as to whether to pass along any or all of them. The second aspect of the system is to provide operational alerts. These might include margin calls, low-cash-balance alerts, or notifications of significant increases or decreases in the client’s portfolio. They might also include noteworthy events in financial markets, such as the aforementioned Brexit vote. FAs can combine personalized text with the alert and send it out over a variety of communications channels. Finally, the Morgan Stanley system includes content on life events. If, for example, a client had a child with a certain illness, the system could recommend the best local hospitals, schools, and financial strategies for dealing with the illness. That life-event content isn’t found in other machine advisor systems, and has the potential to help create a trusting and value-adding relationship between clients and FAs. The features and functions of the system are important, of course, but the rollout is just as critical to its success. Morgan Stanley is being careful, observant, and open to change in the rollout process. Several FAs were involved in the design of the system. The development of the system is complete, it is being tested now, and initial rollout to 500 FAs will take place in September. The creators of the system — the Analytics and Data Organization within Wealth Management, headed by Jeff McMillan, the Chief Data and Analytics Officer — know that getting FAs to adopt the system is an enormous change management project. The FAs have traditionally relied on their experience, and at first they won’t understand how the system works. Initially, the next best action system will primarily be mediated through FAs, but clients can get access to new online information as well. Morgan Stanley plans to eventually release a digital-only version with managed portfolios. It will be offered at a lower cost level to clients who prefer digital-only channels (many of whom will presumably be in the Millennial generation). To assist these clients, and to work with FAs as they adopt the system, Morgan Stanley is hiring a cadre of digital adviser associates, who will work out of call centers and provide expert advise on the use of the system. McMillan emphasizes the continuing human role in wealth management and finds the “robo-adviser” term particularly distasteful. He told us over the phone: For the foreseeable future, systems like these are complements to the human relationship between advisers and clients. Throughout the industry, the “hybrid” human/machine offerings have been much more successful. Humans can understand the context, deal with client emotions, and process disparate data sets. They still have a very important role to play in financial advising. McMillan and his colleagues have considerable work to do in order to make all of the firm’s investing knowledge available through the system. They found, for example, that there was no artificial intelligence system available today that could take the knowledge embedded in investment analyst reports and make it available to support the choices presented to clients. So McMillan is working with the firm’s research department to try to make the knowledge in reports more structured and consumable by machines. This is a change management challenge that is at least equal to getting FAs to use the next best action system effectively. Certainly, the robo aspects of this new system and process are a small part of the total. Neither Morgan Stanley’s business model nor its culture would fit well with an entirely machine-based solution for wealth management that provided no human support. Most other firms in the industry, we believe, will discover the same truth. Thomas H. Davenport is the President’s Distinguished Professor in Management and Information Technology at Babson College, a research fellow at the MIT Initiative on the Digital Economy, and a senior adviser at Deloitte Analytics. Author of over a dozen management books, his latest is Only Humans Need Apply: Winners and Losers in the Age of Smart Machines.  Randy Bean is CEO and managing partner of consultancy NewVantage Partners.  You can follow him at @RandyBeanNVP."
d982578a8b,Computer system predicts products of chemical reactions | MIT News,"Login
or
Subscribe Newsletter
A new computer system predicts the products of chemical reactions. “The vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it,” says professor Klavs Jensen.
Image: MIT News Machine learning approach could aid the design of industrial processes for drug manufacturing.
Larry Hardesty | MIT News Office
June 27, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
When organic chemists identify a useful chemical compound — a new drug, for instance — it’s up to chemical engineers to determine how to mass-produce it. There could be 100 different sequences of reactions that yield the same end product. But some of them use cheaper reagents and lower temperatures than others, and perhaps most importantly, some are much easier to run continuously, with technicians occasionally topping up reagents in different reaction chambers. Historically, determining the most efficient and cost-effective way to produce a given molecule has been as much art as science. But MIT researchers are trying to put this process on a more secure empirical footing, with a computer system that’s trained on thousands of examples of experimental reactions and that learns to predict what a reaction’s major products will be. The researchers’ work appears in the American Chemical Society’s journal Central Science. Like all machine-learning systems, theirs presents its results in terms of probabilities. In tests, the system was able to predict a reaction’s major product 72 percent of the time; 87 percent of the time, it ranked the major product among its three most likely results. “There’s clearly a lot understood about reactions today,” says Klavs Jensen, the Warren K. Lewis Professor of Chemical Engineering at MIT and one of four senior authors on the paper, “but it's a highly evolved, acquired skill to look at a molecule and decide how you’re going to synthesize it from starting materials.” With the new work, Jensen says, “the vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it.” With a 72 percent chance of identifying a reaction’s chief product, the system is not yet ready to anchor the type of completely automated chemical synthesis that Jensen envisions. But it could help chemical engineers more quickly converge on the best sequence of reactions — and possibly suggest sequences that they might not otherwise have investigated. Jensen is joined on the paper by first author Connor Coley, a graduate student in chemical engineering; William Green, the Hoyt C. Hottel Professor of Chemical Engineering, who, with Jensen, co-advises Coley; Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science; and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science. Acting locally A single organic molecule can consist of dozens and even hundreds of atoms. But a reaction between two such molecules might involve only two or three atoms, which break their existing chemical bonds and form new ones. Thousands of reactions between hundreds of different reagents will often boil down to a single, shared reaction between the same pair of “reaction sites.” A large organic molecule, however, might have multiple reaction sites, and when it meets another large organic molecule, only one of the several possible reactions between them will actually take place. This is what makes automatic reaction-prediction so tricky. In the past, chemists have built computer models that characterize reactions in terms of interactions at reaction sites. But they frequently require the enumeration of exceptions, which have to be researched independently and coded by hand. The model might declare, for instance, that if molecule A has reaction site X, and molecule B has reaction site Y, then X and Y will react to form group Z — unless molecule A also has reaction sites P, Q, R, S, T, U, or V. It’s not uncommon for a single model to require more than a dozen enumerated exceptions. And discovering these exceptions in the scientific literature and adding them to the models is a laborious task, which has limited the models’ utility. One of the chief goals of the MIT researchers’ new system is to circumvent this arduous process. Coley and his co-authors began with 15,000 empirically observed reactions reported in U.S. patent filings. However, because the machine-learning system had to learn what reactions wouldn’t occur, as well as those that would, examples of successful reactions weren’t enough. Negative examples So for every pair of molecules in one of the listed reactions, Coley also generated a battery of additional possible products, based on the molecules’ reaction sites. He then fed descriptions of reactions, together with his artificially expanded lists of possible products, to an artificial intelligence system known as a neural network, which was tasked with ranking the possible products in order of likelihood. From this training, the network essentially learned a hierarchy of reactions — which interactions at what reaction sites tend to take precedence over which others — without the laborious human annotation. Other characteristics of a molecule can affect its reactivity. The atoms at a given reaction site may, for instance, have different charge distributions, depending on what other atoms are around them. And the physical shape of a molecule can render a reaction site difficult to access. So the MIT researchers’ model also includes numerical measures of both these features. According to Richard Robinson, a chemical-technologies researcher at the drug company Novartis, the MIT researchers’ system “offers a different approach to machine learning within the field of targeted synthesis, which in the future could transform the practice of experimental design to targeted molecules.” “Currently we rely heavily on our own retrosynthetic training, which is aligned with our own personal experiences and augmented with reaction-database search engines,” Robinson says. “This serves us well but often still results in a significant failure rate. Even highly experienced chemists are often surprised. If you were to add up all the cumulative synthesis failures as an industry, this would likely relate to a significant time and cost investment. What if we could improve our success rate?” The MIT researchers, Robinson says, “have cleverly demonstrated a novel approach to achieve higher predictive reaction performance over conventional approaches. By augmenting the reported literature with negative reaction examples, the data set has more value.” Topics: Research, School of Engineering, Artificial intelligence, Chemical engineering, Chemistry, Computer modeling, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Manufacturing This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
b0c5f0180f,Neural networks can add natural animation to video games,"We've seen procedurally generated worlds and weapons in video games before, but piecing together believable animations from a pool of variables is pretty tough. Previous attempts at it have looked janky and disjointed. It's okay in something like Ubisoft's experimental and quirky Grow Home, but big-budget AAA blockbusters akin to Uncharted 4 carry a different set of expectations. New research out of the University of Edinburgh is a bit different, and might help video games get away from one-size-fits-most pre-scripted animations, though. As you can see in the video below, the results are pretty impressive. The neural net blends pre-scripted animations into incredibly lifelike locomotion over a variety of terrain. Jumping over obstructions, ducking and even the avatar putting his arms out for balance when crossing a narrow path are all calculated on an as-needed basis. Or, in technical terms, ""Our system takes as input user controls, the previous state of the character, the geometry of the scene and automatically produces high-quality motions that achieve the desired user control."" The other limitation is that currently, it only works for relatively simple things like running around an environment. ""Like many other methods in this field, our technique cannot deal well with complex interactions with the environment -- in particular if they include precise hand movements such as climbing up walls or interacting with other objects in the scene,"" the paper reads. ""Such a system may allow the character to stably walk and run over different terrains in different physical conditions such a slippery floors or unstable rope bridges."" Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
879ec21dd9,Artificial intelligence suggests recipes based on food photos | MIT News,"Login
or
Subscribe Newsletter
Pic2Recipe, an artificial intelligence system developed at MIT, can take a photo of an entree and suggest a similar recipe to it. Photo: Jason Dorfman/MIT CSAIL Pic2Recipe predicts recipes from photos using a neural network, a way to achieve machine learning in which a computer learns to perform some task by analyzing examples.
Photo: Jason Dorfman/MIT CSAIL Given a still image of a dish filled with food, CSAIL team's deep-learning algorithm recommends ingredients and recipes.
Watch Video
Adam Conner-Simons | Rachel Gordon | CSAIL
July 20, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab There are few things social media users love more than flooding their feeds with photos of food. Yet we seldom use these images for much more than a quick scroll on our cellphones. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) believe that analyzing photos like these could help us learn recipes and better understand people's eating habits. In a new paper with the Qatar Computing Research Institute (QCRI), the team trained an artificial intelligence system called Pic2Recipe to look at a photo of food and be able to predict the ingredients and suggest similar recipes. “In computer vision, food is mostly neglected because we don’t have the large-scale datasets needed to make predictions,” says Yusuf Aytar, an MIT postdoc who co-wrote a paper about the system with MIT Professor Antonio Torralba. “But seemingly useless photos on social media can actually provide valuable insight into health habits and dietary preferences.” The paper will be presented later this month at the Computer Vision and Pattern Recognition conference in Honolulu. CSAIL graduate student Nick Hynes was lead author alongside Amaia Salvador of the Polytechnic University of Catalonia in Spain. Co-authors include CSAIL postdoc Javier Marin, as well as scientist Ferda Ofli and research director Ingmar Weber of QCRI. How it works The web has spurred a huge growth of research in the area of classifying food data, but the majority of it has used much smaller datasets, which often leads to major gaps in labeling foods. In 2014 Swiss researchers created the “Food-101” dataset and used it to develop an algorithm that could recognize images of food with 50 percent accuracy. Future iterations only improved accuracy to about 80 percent, suggesting that the size of the dataset may be a limiting factor. Even the larger datasets have often been somewhat limited in how well they generalize across populations. A database from the City University in Hong Kong has over 110,000 images and 65,000 recipes, each with ingredient lists and instructions, but only contains Chinese cuisine. The CSAIL team’s project aims to build off of this work but dramatically expand in scope. Researchers combed websites like All Recipes and Food.com to develop “Recipe1M,” a database of over 1 million recipes that were annotated with information about the ingredients in a wide range of dishes. They then used that data to train a neural network to find patterns and make connections between the food images and the corresponding ingredients and recipes. Given a photo of a food item, Pic2Recipe could identify ingredients like flour, eggs, and butter, and then suggest several recipes that it determined to be similar to images from the database. (The team has an online demo where people can upload their own food photos to test it out.) “You can imagine people using this to track their daily nutrition, or to photograph their meal at a restaurant and know what’s needed to cook it at home later,” says Christoph Trattner, an assistant professor at MODUL University Vienna in the New Media Technology Department who was not involved in the paper. “The team’s approach works at a similar level to human judgement, which is remarkable.”   The system did particularly well with desserts like cookies or muffins, since that was a main theme in the database. However, it had difficulty determining ingredients for more ambiguous foods, like sushi rolls and smoothies. It was also often stumped when there were similar recipes for the same dishes. For example, there are dozens of ways to make lasagna, so the team needed to make sure that system wouldn’t “penalize” recipes that are similar when trying to separate those that are different. (One way to solve this was by seeing if the ingredients in each are generally similar before comparing the recipes themselves). In the future, the team hopes to be able to improve the system so that it can understand food in even more detail. This could mean being able to infer how a food is prepared (i.e. stewed versus diced) or distinguish different variations of foods, like mushrooms or onions. The researchers are also interested in potentially developing the system into a “dinner aide” that could figure out what to cook given a dietary preference and a list of items in the fridge. “This could potentially help people figure out what’s in their food when they don’t have explicit nutritional information,” says Hynes. “For example, if you know what ingredients went into a dish but not the amount, you can take a photo, enter the ingredients, and run the model to find a similar recipe with known quantities, and then use that information to approximate your own meal.” The project was funded, in part, by QCRI, as well as the European Regional Development Fund (ERDF) and the Spanish Ministry of Economy, Industry, and Competitiveness. Topics: Research, Algorithms, Machine learning, Behavior, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer vision, Networks, Data, Artificial intelligence, School of Engineering, Electrical Engineering & Computer Science (eecs), Food CSAIL researchers have developed an artificial neural network that generates recipes from pictures of food, reports Laurel Dalrymple for NPR. The researchers input recipes into an AI system, which learned patterns “connections between the ingredients in the recipes and the photos of food,” explains Dalrymple. In this video for USA Today, Sean Dowling highlights Pic2Recipe, the artificial intelligence system developed by CSAIL researchers that can predict recipes based off images of food. The researchers hope the app could one day be used to help, “people track daily nutrition by seeing what’s in their food.” Researchers at MIT have developed an algorithm that can identify recipes based on a photo, writes BBC News reporter Zoe Kleinman. The algorithm, which was trained using a database of over one million photos, could be developed to show “how a food is prepared and could also be adapted to provide nutritional information,” writes Kleinman. MIT researchers have developed a new machine learning algorithm that can look at photos of food and suggest a recipe to create the pictured dish, reports Matt Reynolds for New Scientist. Reynolds explains that, “eventually people could use an improved version of the algorithm to help them track their diet throughout the day.” CSAIL researchers have trained an AI system to look at images of food, predict the ingredients used, and even suggest recipes, writes Matt Burgess for Wired. The system could also analyze meals to determine their nutritional value or “manipulate an existing recipe to be healthier or to conform to certain dietary restrictions,"" explains graduate student Nick Hynes. This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
3b15b24324,Even a mask won't hide you from the latest face recognition tech | New Scientist,"Daily news
7 September 2017
John Powell/REX/Shutterstock By Matt Reynolds Ditch the hat and scarf – it’s not fooling anyone. Face recognition software can now see through your cunning disguise – even you are wearing a mask. Amarjot Singh at the University of Cambridge and his colleagues trained a machine learning algorithm to locate 14 key facial points. These are the points the human brain pays most attention to when we look at someone’s face. The researchers then hand-labelled 2000 photos of people wearing hats, glasses, scarves and fake beards to indicate the location of those same key points, even if they couldn’t be seen. The algorithm looked at a subset of these images to learn how the disguised faces corresponded with the undisguised faces.
The system accurately identified people a wearing scarf 77 per cent of the time – a cap and scarf 69 per cent of the time and a cap, scarf and glasses 55 per cent of the time. This isn’t as good as systems that recognise undisguised human faces, but it is the best at seeing through disguises, says Singh. The system only needs to be able to see a fraction of facial key points – most of which are around the eyes and mouth – to be able to guess where the other points are likely to be. Based on that guess, it can identify the person if it has already been shown a map of their key points. “In effect, it is able to see through your mask,” says Singh. You can also probably say goodbye to CV Dazzle, the vaunted face recognition camouflage makeup that has been mooted as the way to stay anonymous in a world of face recognition. “This will work very well for this type of camouflage because it works on key points of the face,” he says. He will present his findings at the International Conference on Computer Vision in Italy in late October. Singh has plans to take this research even further and see if it’s possible to design an algorithm that can identify someone wearing a rigid plastic mask, like the V for Vendetta masks that are popular at some protests. The system could be used to identify criminals who are trying to hide their identities, says Singh. But he admits it could also be used by authoritarian governments to identify protesters. “It kind of impinges on the privacy of people,” he says. Automatic face recognition software is catching on with law enforcement across the world. In August, the UK government said it planned to spend £4.6 million on upgrading face recognition software so algorithms could automatically spot suspects from live video footage. “There’s always a trade-off between security and privacy,” says Anil Jain at Michigan State University. But he says that people in public spaces are already under constant surveillance by security cameras, so they shouldn’t be too worried about every improvement in the technology. For now, the system is far from perfect. The fewer facial key points it can see, the worse the software is at recognising a person in a photo. It’s also thrown off by busy backgrounds, so can only identify a person wearing a cap, glasses and scarf 43 per cent of the time if they’re standing in front of a complicated background. It’s also not clear how well this system would work in the real world, Jain says. The algorithm was only trained on photos of 25 people, which he says isn’t enough to really determine its efficacy. And anti-surveillance tricks are keeping pace with improvements. Last year, a team of researchers from Carnegie Mellon University found they could trick face recognition software by wearing specially designed glasses. Now might be the time to trade in your fake beard for a pair of jazzy specs. Reference: https://arxiv.org/pdf/1708.09317.pdf     More on these topics:
A shorter version of this article was published in New Scientist magazine on 16 September 2017"
b10eea976d,AI spots Alzheimer’s brain changes years before symptoms emerge | New Scientist,"Daily news
14 September 2017
baranozdemir/Getty By Anil Ananthaswamy Artificial intelligence can identify changes in the brains of people likely to get Alzheimer’s disease almost a decade before doctors can diagnose the disease from symptoms alone. The technique uses non-invasive MRI scans to identify alterations in how regions of the brain are connected. Alzheimer’s is a neurodegenerative disease that is the leading cause of dementia for the elderly, eventually leading to loss of memory and cognitive functions.
The race is on to diagnose the disease as early as possible. Although there is no cure, drugs in development are likely to work better the earlier they are given. An early diagnosis can also allow people to start making lifestyle changes to help slow the progression of the disease. In an effort to enable earlier diagnosis, Nicola Amoroso and Marianna La Rocca at the University of Bari in Italy and their colleagues developed a machine-learning algorithm to discern structural changes in the brain caused by Alzheimer’s disease. First, they trained the algorithm using 67 MRI scans, 38 of which were from people who had Alzheimer’s and 29 from healthy controls. The scans came from the Alzheimer’s Disease Neuroimaging Initiative database at the University of Southern California in Los Angeles. The idea was to teach the algorithm to correctly classify and discriminate between diseased and healthy brains. The researchers divided each brain scan into small regions and analysed the neuronal connectivity between them, without making any assumptions about the ideal size of these regions for diagnosis. They found that the algorithm made the most accurate classification of Alzheimer’s when the brain regions being compared were about 2250 to 3200 cubic millimetres. This was intriguing, says La Rocca, since this is similar to the size of the anatomical structures connected with the disease, such as the amygdala and hippocampus. The team then tested the algorithm on a second set of scans from 148 subjects. Of these, 52 were healthy, 48 had Alzheimer’s disease and 48 had mild cognitive impairment (MCI) but were known to have developed Alzheimer’s disease 2.5 to nine years later. The AI distinguished between a healthy brain and one with Alzheimer’s with an accuracy of 86 per cent. Crucially, it could also tell the difference between healthy brains and those with MCI with an accuracy of 84 per cent. This shows that the algorithm could identify changes in the brain that lead to Alzheimer’s almost a decade before clinical symptoms appear. The researchers were limited by the scans available from the database, so they weren’t able to test whether the algorithm could predict the onset of disease even earlier. Alzheimer’s disease has been linked to the formation of sticky beta-amyloid plaques and neurofibrillary tau tangles in the brain. “Nowadays, cerebrospinal fluid analyses and brain imaging using radioactive tracers can tell us to what extent the brain is covered with plaques and tangles, and are able to predict relatively accurately who is at high risk of developing Alzheimer’s 10 years later,” says La Rocca. “However, these methods are very invasive, expensive and only available at highly specialised centres.” In contrast, the new technique can distinguish with similar accuracy between brains that are normal and the brains of people with MCI who will go on to develop Alzheimer’s disease in about a decade – but using a simpler, cheaper and non-invasive technique. More work will be needed to distinguish between people with MCI whose brains go on to age normally, or who might develop other kinds of dementia. Blood tests that look for biomarkers of Alzheimer’s could be even cheaper and simpler than the new technique, but none are on the market yet. “There are no blood tests for Alzheimer’s disease,” says Goran Šimić at the University of Zagreb in Croatia. “There have been some attempts, but without much success yet.” Patrick Hof at the Icahn School of Medicine at Mount Sinai in New York is intrigued by the new test. He says that a method that might predict the disease a decade before it is fully expressed would be “incredibly valuable” should preventative therapeutics emerge. La Rocca says her team now intends to extend the technique to help with the early diagnosis of other neurodegenerative conditions such as Parkinson’s disease. “It’s a method that is very versatile,” she says. Reference: arXiv, 1709.02369
More on these topics:
A shorter version of this article was published in New Scientist magazine on 23 September 2017"
5ef904324c,"AI Can Help Apple Watch Predict High Blood Pressure, Sleep Apnea","The world’s most valuable company crammed a lot into the tablespoon-sized volume of an Apple Watch. There’s GPS, a heart-rate sensor, cellular connectivity, and computing resources that not long ago would have filled a desk-dwelling beige box. The wonder gadget doesn’t have a sphygmomanometer for measuring blood pressure or polysomnographic equipment found in a sleep lab---but thanks to machine learning, it might be able to help with their work. Research presented at the American Heart Association meeting in Anaheim Monday claims that, when paired with the right machine-learning algorithms, the Apple Watch’s heart-rate sensor and step counter can make a fair prediction of whether a person has high blood pressure or sleep apnea, in which breathing stops and starts repeatedly through the night. Both are common---and commonly undiagnosed---conditions associated with life-threatening problems, including stroke and heart attack. The new study adds to evidence that the right algorithms might transform the Apple Watch from personal trainer to personal physician. Apple said in September that it is working on a study with Stanford that will test whether the gadget can detect atrial fibrillation, or irregular heartbeat, which can lead to stroke or heart failure. A study independent of Apple presented in May has already suggested the answer is yes. And health insurer Aetna said last week that it is partnering with Apple to give Apple Watches to members to try to reduce health costs. The Apple Watch’s potential to predict high blood pressure and sleep apnea was revealed by a collaboration between University of California San Francisco and a startup called Cardiogram. The company offers an app for organizing heart-rate data from an Apple Watch, and other devices with heart-rate sensors. UCSF provided data from more than 6,000 Apple Watch users enrolled in a study on mobile health. Cardiogram’s founders drew on their previous experience as Google employees, working on speech recognition for Android phones and the Google Assistant. Cardiogram’s engineers took the kind of artificial neural networks that Google and others use to turn our speech into text and adapted them to interpret heart-rate and step count data. (Like speech, they are signals that vary over time.) The system, dubbed DeepHeart, is given strings of heart-rate and step data from multiple people, and information about their health conditions. In May, the company and UCSF released results showing that DeepHeart could figure out how to predict atrial fibrillation from a person's Apple Watch data. The study presented Monday shows that with one week of data on a wearer, the algorithms can predict hypertension with roughly 80 percent accuracy, and sleep apnea with about 90 percent accuracy. https://www.wired.com How the Apple Watch Will Help You Take Charge of Your Health Apple's much-anticipated watch is arriving in April, and through third party apps, it could make you live a healthier life. Controversial Brain Imaging Uses AI to Take Aim at Suicide Prevention Researchers are training algorithms to spot tell-tale signs of self-harm in brain scans. But there’s probably better data to use. Doctors don’t---and probably couldn’t---diagnose high blood pressure or sleep apnea just by eyeballing a week’s worth of data from your smartwatch. They diagnose hypertension by putting that familiar cuff on your arm. Sleep apnea requires a visit to a sleep clinic, or use of home monitoring equipment. So how do Cardiogram’s algorithms make good guesses without directly measuring a person's blood pressure or breathing? We only sort of know. Breathing, heart rate, and blood pressure are all connected to our autonomic nervous system, which regulates the unconscious bodily functions that keep us alive. Past research has shown how hypertension and sleep apnea alter the dynamics of heart rate. For example, heart rate variability is lower in people with sleep apnea. But Brandon Ballinger, a Cardiogram cofounder, admits that he doesn’t know all the patterns in a person's heart rate that his algorithms use to make predictions. “They’re kind of a foreign form of intelligence,” says Ballinger. Ballinger says that, with the right testing, that doesn't prevent his alien intelligence from having business potential. Cardiogram’s app for Apple Watch and other devices is free today. But the startup’s business plan is to one day add features that advise a user to be checked for atrial fibrillation, high blood pressure, or sleep apnea. To stay on the right side of the FDA, the app would have to advise a person to get tested, and not suggest the person has a particular condition. Cardiogram would make money by offering to ship the necessary equipment for a home test, and billing a person's health insurer. The app could also provide advice after a diagnosis, or link people to medical practitioners or health coaches, Ballinger says. He predicts some of these features will appear within months. That plan is plausible, but needs to be proved out. Leslie Saxon, a cardiologist and executive director of the Center for Body Computing at the University of Southern California, says the idea of inferring conditions indirectly from heart rate and step counts needs more testing. “The study is seeing a correlation and that’s important to know, but the value is still unproven for medicine,” she says. Saxon also notes that the Apple Watch's heart data varies in accuracy depending on how a person wears the device. Cardiogram says it has more research underway, and expects accuracy to improve. There are now about 30,000 people enrolled in Cardiogram's study with UCSF. That’s big for a medical study—and perhaps a reflection of people’s readiness for wearables like the Apple Watch to act as medical advisers. Saxon says studies at USC have shown that patients eagerly engage with apps capable of medical-grade measurements. If people are properly educated about what they can do alone, their health care is better managed as a result, she says. Her center's projects include testing a mobile heart sensor that pairs with a phone or watch made by startup AliveCor. “Patients would much rather self-manage than deal with you, the physician,” says Saxon. “And they’re already on their phone 200 times a day.” If Cardiogram and Saxon are right, medical-grade notifications may soon nestle among those for our Snaps, likes, and texts. Will be used in accordance with our Privacy Policy Skip the Apple Watch, Go for the Casio Timepiece Calculator
How Meltdown and Spectre Were Independently Discovered By Four Research Teams At Once Andy Greenberg James Damore's Lawsuit Is Designed to Embarrass Google Nitasha Tiku Why One Man Has Spent Years Building a Boeing 777 Out of Paper Patrick Farrell What Is MicroLED and When am I Going to Get It? Brian Barrett Byton, Tesla's Latest Chinese Competitor, Takes Screens to an Extreme Jack Stewart
Use of this site constitutes acceptance of our user agreement (effective 3/21/12) and privacy policy (effective 3/21/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast."
b6602ca86a,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
eedfde6a6b,Using Amazon Machine Learning to Predict the Weather – Network Technology,"Amazon recently launched their Machine Learning service, so I thought I’d take it for a spin. Machine Learning (ML) is all about predicting future data based on patterns in existing data. As an experiment I wanted to see if machine learning would be able to predict the weather of tomorrow based on weather observations. Weather systems travel large distances on a time scale of hours and days, so recent weather observations from around the country can be used to predict the future weather of one specific site. Meteorological institutes do this every day by running complex weather models on hundreds of nodes in large HPC clusters. I don’t expect machine learning to produce quite as good results as those models do, but thought it would be fun to see how close ML could get. The Amazon Machine Learning service makes it easy to get started and reduces the time it takes to get actionable insights from data. The service comes with tutorials, developer guides, a very useful explanation of machine learning concepts and enough tips to guide anyone through their first steps in the world of machine learning. There is a sample dataset you can use to create your first prediction model but if you want, you can follow along my journey in this post with my dataset instead. The source code to generate it is on Github and all you need to generate CSV files with weather observations is a free API key from Wunderground.com. Before diving into coding and machine learning, it’s important to define the use case as clearly as possible. To test whether Machine Learning is a viable approach to weather forecasting is the overall goal. To test this, I choose to predict the temperature tomorrow at 12:00 UTC in Oslo, the capital of Norway. The dataset I’ve chosen is weather observations from five cities in Norway, scattered around the southern half of the country. The weather in Oslo usually comes from the west, so I include observations from cities like Stavanger and Bergen in the dataset. The layout of the dataset is important. Amazon Machine Learning treats every line in the dataset (CSV file) as a separate record and processes them randomly. For each record, it tries to predict the target value (temperature in Oslo the next day) from the variables present in that record. This means that you can’t rely on any connections between records, for example that the temperature measured at 10 AM may be similar to the temperature at 11 AM. To create a dataset with enough data in each record to be able to predict the target value, I append all weather observations with the same timestamp, regardless of location, to the same record. This means that for any given timestamp there will for instance be five temperatures, five wind measurements and so on. To be able to distinguish between different cities in the dataset I named each column (each variable) with the first letter of the city name, forming variable names like “o_tempm” when the original observational data had a variable “tempm” containing the temperature for Oslo. Machine Learning works by creating a model from a training dataset where the target value to predict is already known. Since I want to predict a numerical value, Amazon ML defaults to a linear regression model. That is, it tries to build a formula which can output the target value, using individual weights for each variable in a record that tells the model how that variable is related to the target value. Some variables get weight zero, meaning they are not related to the target value at all, and others get positive weights between 0 and 1. To be able to determine weights for variables, there must be a sufficient amount of training data. To create a sufficiently large training dataset, I needed weather observations for some time, at least 14 days. Fortunately, Wunderground.com has a JSON API that is really easy to use, and their history endpoint can provide weather observations for both the current date and dates back in time. I use that to collect observations for the last two weeks for all five cities. Since the free tier of their API restricts the use to 500 calls a day and maximum of 10 calls per minute, the script I made to generate the dataset has to wait some seconds between each API call. To limit the API usage and be able to rerun the script I cache weather observations on disk, because I don’t expect past weather observations to change. A weather observation returned by their API has the following syntax: As you can see, this is the observation for May 29th at 12:00 UTC and the temperature was 12 degrees Celsius (“tempm”, where m stands for the metric system). The API returns a list of observations for a given place and date. The list contains observation data for at least every hour, some places even three times per hour. For each date and time, I combine the observations from all five places into one long record. If there isn’t data from all five places for that timestamp, I skip it, to make sure that all machine learning records have a sufficient amount of data. Lastly, for all records belonging to the training set, I append the target value (the next day’s temperature) as the last field. That way, I get a file “dataset.csv” with known target values that can be used to train the model. Here is an example record with the last number (8) being the target to predict: This way, I get a dataset where each timestamp is a separate record and all timestamps belonging to one date gets the next day’s temperature in Oslo at 12 UTC as the target value. In total 844 records for 14 days of observations. In addition, the script outputs a file “testset.csv” with the last day of observations where the target value is unknown and should be predicted by the model. Upload both CSV files to Amazon S3 before continuing, as Amazon Machine Learning is only able to use input data from S3 (or Redshift, but in that case Amazon exports it to S3 before using it). Be sure to select the “US Standard” region of S3, as the Machine Learning service is only available in their North Virginia location at the moment. To reduce costs it is important that the S3 datasets are in the same region as the Machine Learning service. The datasource is an object used by the Machine Learning model to access the data in S3. Datasource objects contain a schema that tells the model what type of field each variable is (numeric, binary, categorical, text). This schema is autodetected when you create a datasource, but may need some review before continuing to make sure all field types were classified correctly. To create a datasource, first log into AWS to get access to Amazon Machine Learning. Select Create New -> Datasource. You are then asked for the path to the dataset in S3 and to give it a name, for instance “Weather observations”:
The dataset is verified and the schema is auto-generated. The next step is to make any adjustments to the schema if needed. Remember to say Yes to “Does the first line in your CSV contain the column names?”. I found that most fields containing actual data were correctly categorized, but fields with little or no data were not. There are some observation fields, like for instance “tornado”, that are normally zero for all five places and all times in my dataset. That field is binary but often autodetected as numeric (probably not an issue, since the field has no relevant data). The field “precipm” is numeric, but as it’s often blank (no precipitation detected) it can be mislabeled as categorical. Remember to go through all variables to check for misdetections like these, in my dataset there is 97 variables to check. The third step is to define a target, which is the variable “target_o_tempm” in my dataset. When selected, Amazon Machine Learning informs you that “ML models trained from this datasource will use Numerical regression.” The last step in datasource creation is to define what field will be used as row identifier, in this case “datetime_utc”. The row identifier will be used to label output from the machine learning model, so it’s handy to use a value that’s unique to each record. The field selected as row identifier will be classified as Categorical. Review the settings and click Finish. It may take some minutes for the datasource to get status Completed, since Amazon does quite a bit of data analysis in the background. For each variable, the range of values is detected and scores like mean and median are computed. At this point, I suggest you go ahead and create another datasource for the testset while you wait. The testset needs to have the same schema as the dataset used to train the model, which means that every field must have the same classification in both datasources. Therefore it’s smart to create the testset datasource right away when you still remember how each variable should be classified. In my case I got 30 binary attributes (variables), 11 categorical attributes and 56 numeric attributes. The review page for datasource creation lists that information. Make sure the numbers match for the dataset and testset:
So, now you’ve got two datasources and can initialize model training. Most of the hard work in creating a dataset and the necessary datasources is already done at this point, so it’s time to put all this data to work and initialize creation of the model that is going to generate predictions. Select Create New -> ML Model on the Machine Learning Dashboard page. The first step is to select the datasource you created for the dataset (use the “I already created a datasource pointing to my S3 data” option). The second step is the model settings, where the defaults are just fine for our use. Amazon splits the training dataset into two parts (a 70-30 split) to be able to both train the model and evaluate its performance. Evaluation is done using the last 30% of the dataset. The last step is to review the settings, and again, the defaults are fine. The advanced settings include options like how many passes ML should do over the dataset (default 10) and how it should do regularization. More on that below, just click Finish now to create the model. This will again take some time, as Amazon performs a lot of computations behind the scenes to train the model and evaluate its performance. Regularization is a technique used to avoid overfitting of the model to the training dataset. Machine learning models are prone to both underfitting and overfitting problems. Underfitting means the model has failed at capturing the relation between input variables and target variable, so it is poor at predicting the target value. Overfitting also gives poor predictions and is a state where the model follows the training dataset too closely. The model remembers the training data instead of capturing the generic relations between variables. If for instance the target value in the training data fluctuates back and forth, the model output also fluctuates in the same manner. The result is errors and noise in the model output. When used on real data where the target value is unknown, the model will not be able to predict the value in a consistent way. This image helps explain the issue in a beautiful way: So to avoid overfitting, regularization is used. Regularization can be performed in multiple ways. Common techniques involve penalizing extreme variable weights and setting small weights to zero, to make the model less dependent on the training data and better at predicting unknown target values. Remember that Amazon did an automatic split of the training data, using 70% of the records to train the model and the remaining 30% to evaluate the model. The Machine Learning service computes a couple of interesting performance metrics to inspect. Go to the Dashboard page and select the evaluation in the list of objects. The first metric shown is the RMSE, Root Mean Square Error, of the evaluation training data. The RMSE should be as low as possible, meaning the mean error is small and the predicted output is close to the actual target value. Amazon computes a baseline RMSE based from the model training data and compares the RMSE of the evaluation training data to that. In my testing I archieved an RMSE of about 2.5 in my first tests and near 2.0 after refining the dataset a bit. The biggest optimization I did was to change the value of invalid weather observations from the default value of -999 or -9999 to be empty. That way the range of values for each field got more close to the truth and did not include those very low numbers. By selecting “Explore model performance”, you get access to an histogram showing the residuals of the model. Residuals are differences between predicted target and actual value. Here’s the plot for my model: The histogram tells us that my model has a tendency to over-predict the temperature, and that a residual of 1 to 2 degrees Celcius is the most likely outcome. This is called a positive bias. To lower the bias it is possible to re-train the ML model with more data. Before we use the model to predict temperatures, I’d like to show some of the interesting parts of the results of model training. Click the datasource for the training dataset on the Dashboard page to load the Data Report. Select Categorical under Attributes in the left menu. Sort the table by “Correlations to target” by clicking on that column. You’ll get a view that looks like this:
This table tells you how much weight each field has in determining the target value, so it is a good source of information on what the most important weather observation data are. Of the categorical attributes, the wind direction in Stavanger is the most important attribute for how the temperature is going to be in Oslo the next day. That makes sense since Stavanger is west of Oslo, so weather that hits Stavanger first is likely to arrive to Oslo later. Wind direction in Kristiansand is also important and in third place on this ranking we find the conditions in Trondheim. The most commonly observed values is shown along with a small view of the distribution for each variable. Click Numeric to see a similar ranking for those variables, revealing that dew point temperature for Trondheim and atmospheric pressure in Stavanger are the two most important numeric variables. For numeric variables, the range and mean of observed values is shown. It’s interesting to see that none of the mentioned important variables are weather observations from Oslo. The last step is to actually use the Amazon Machine Learning service to predict tomorrow’s temperature, by feeding the testset to the model. The testset contains one day of observations and an empty target value field. Go back to the Dashboard and select the model in the object list. Click “Generate batch predictions” at the bottom of the model info page. The first step is to locate the testset datasource, which you’ve already created. When you click Verify Amazon checks that the schema of the testset matches the training dataset, which it should given that all variables have the same classification in both datasources. It is possible to create the testset datasource in this step instead of choosing an existing datasource. However, when I tried that, the schema of the testset was auto-generated with no option for customizing field classifications, so therefore it failed verification since the schema didn’t match the training dataset. That’s why I recommended you create a datasource for the testset too, not just for the training dataset. After selecting the testset datasource, the last step in the wizard before review is choice of output S3 location. Amazon Machine Learning will create a subfolder in the location you supply. Review and Finish to launch the prediction process. Just like some of the previous steps, this may take some minutes to finish. The results from the prediction process is saved to S3 as a gzip file. To see the results you need to locate the file in the S3 Console, download it and unpack it. The unpacked file is a CSV file, but lacks the “.csv” suffix, so you might need to add that to get the OS to recognize it properly. Results look like this: The “score” field is the predicted value, in this case the predicted temperature in Celsius. The tag reveals what observations resulted in the prediction, so for instance the observations from the five places for timestamp 02:00 resulted in a predicted temperature in Oslo the next day at 12:00 UTC of 12.7 degrees Celsius. As you might have noticed, we’ve now got a bunch of predictions for the same temperature, not just one prediction. The strength in that is that we can inspect the distribution of predicted values and even how the predicted value changes according to observation timestamp (for example to check if weather observations from daytime give better predictions than those from the night).
Even though the individual predictions differ, there seems to be strong indications that the value is between 13.0 and 14.5 degrees Celsius. Computing the mean over all predicted temperatures gives 13.6 degrees Celsius.
This plot shows the development in the predicted value as the observation time progresses. There does not seem to be any significant trend in how the different observation times perform. At this point I’m sure you wonder what the real temperature value ended up being. The value I tried to predict using Amazon Machine Learning turned out to be: 12:00 UTC on May 31, 2015:  12 degrees Celsius Taking into account the positive bias of 1 to 2 degrees and a prediction mean value of 13.6 degrees Celsius, I am very satisfied with these results. To improve the model performance further I could try to reduce the bias. To do that I’d have to re-train the model with more training data, since two weeks of data isn’t much. To get a model which could be used all year around, I’d have to include training data from a relevant subset of days throughout the year (cold days, snowy days, heavy rain, hot summer days and so on). Another possible optimization is to include more places in the dataset. The dataset lacks places to the east of Oslo, which is an obvious flaw. In addition to more data, I could explore if the dataset might need to be organized differently. It is for instance possible to append all observations from an entire day into one record, instead of creating a separate record for each observation timestamp. That would give the model more variables to use for prediction but then only one record to predict the value from. It has been very interesting to get started with Amazon Machine Learning and test it with a real-life dataset. The power of the tool combined with the ease of use that Amazon has built into the service makes it a great offering. I’m sure I’ll use the service for new experiments in the future! ~ Arne ~ […] https://arnesund.com/2015/05/31/using-amazon-machine-learning-to-predict-the-weather/ […] LikeLike […] Eu ainda tenho que preparar um post sobre esses workhorses produzidos pela Amazon, Microsoft, e Google; mas fica a leitura. […] LikeLike Does Amazon ML support sentiment analysis? If yes, then are there any tutorials/blog posts for the same? LikeLike Amazon Machine Learning only does supervised ML, so you’d need a training dataset with sentiments filled in. In addition, the training dataset and test data (to use for prediction) must have the same schema, which means they must have the same amount of columns and column types that match. So I’m not sure how well suited the service is for that task. For sentiment analysis of texts of variable size you’d need to preprocess the dataset so the schema matches the training data, for example by transforming each text into a vector based on a vocabulary and a bag-of-words representation. Then I suppose you could use Amazon ML to run a standard multi-class classification to determine the sentiment that fits each text best. I’m not aware of any tutorials using Amazon ML for sentiment analysis, but let me know if you find any. For more details about Amazon Machine Learning in general there is an official FAQ, an introduction to the concepts and a Developer Guide at their site. LikeLike Hi Arne,
tnx for this great review. I am evaluating AML at the moment, too, and I like the simplicity of the usage. But there is one thing I could not figure out: if AML uses a regression model for prediction: do you know where to find the equation (http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_Regression/paste_image40.jpg)?
Best,
Stefan LikeLike Hi Stefan! Glad you liked it! I found Amazon ML easy to get started with and powerful enough to be usable in a wide range of use cases. There is even real-time predictions available using their API. I didn’t find the exact equation either, but you can get valuable insights into the most important variables by exploring the datasource used for the training dataset. Look at the column “Correlations to target” in the table of variables to see the weight each variable has in the equation (there is a screenshot in my tutorial). I guess you could build the entire equation manually by inspecting the weight of every variable there. LikeLike Thanks again, Arne. I already had a look at the correlations but – even for the sample data set provided by amzn – the correlations are so low that you cannot infer which independant variable (or which combination of them) has the highest impact. LikeLike […] Source: Using Amazon Machine Learning to Predict the Weather | Network Technology […] LikeLike Reblogged this on jozvison. LikeLike Thanks for the tutorial, it’s great ! But I don’t get a point:
– how is target_o_temp computed ? I know that it’s the target temperature to predict in Olso at 12 UTC the next day but if so, where does each timestamp’s value for target_o_tempm come from ?
– If these values correspond to the temperature the day before in Oslo, at each corresponding timestamp, what does the testset corresponds to ? It’s not really clear in my mind the difference between target_o_tempm and the testset. Thanks for your kind help, Florian LikeLike The testset was the part of the data where target_o_tempm is unknown, which means that the testset only contained observations from May 30th (the day I ran the prediction model). When creating the dataset I pulled data for the last 14 days, so the training dataset was 13 days of data and the testset was the last day. For those 13 days of training data the temperature at 12:00 the next day is known, and I therefore added it as “target_o_tempm” (the last column) to each weather observation. So in effect, all weather observations for all timestamps of any given day had the same target_o_tempm value, namely the observed temperature at 12:00 the day after.
I hope this helps to clarify the dataset structure a bit. Let me know if there are more unclear parts of this. I appreciate your interest in my tutorial and am happy to help! Regards,
Arne LikeLike I tried to predict a column value called yield but i am unable to get the correct output. Even after running the experiment, i get these values as null values. Please could you help me with this LikeLike There could be many reasons you don’t get the desired output. Please include more details about what you try to archieve, how the input data is structured (data types, observed value ranges) and type and range of the yield column in the training data set. AWS also have an instructive tutorial at http://docs.aws.amazon.com/machine-learning/latest/dg/tutorial.html which you could try to follow to see if that gives you the desired results. LikeLike Very interesting post and work you have done here. Have you tried running this over a longer period (a few months maybe) and compare the skill to the skill of NWP model? I don’t think it is possible to really conclude anything about the prediction skill here. It will actually be interesting to see how it performs against a “poor-mans” predictor, that is, defining the baseline as “the weather tomorrow is the same as today”. Regards, Kasper LikeLike That’s an interesting idea for a follow-up post! The objective of this post was both to see if it could be done, and to talk about the (then) newly introduced Amazon ML tools.
The baseline you suggest is very relevant. Testing the prediction model for some months and comparing to that baseline would give an important hint about the validity of the ML approach.
For a comparison with a standard NWP model the predicted values should however be verified using weather observations, as that is the common way to evaluate NWP models. Calculating bias and RMSE when comparing with observations should be done as well. I’d love to see all these types of different evaluations of weather machine learning models, so if anyone want to do it they are more than welcome to build on my introduction post here. LikeLike Hey Arne ….Amazing Tutorial for a beginner like me!!!!! I want to predict future room temperature in a particular room and set the thermostat to that temperature which will help me reduce the energy consumption. I have the last months data set of room containing room temperature at each time stamp for that room . How can I go about this ? I just want to start with basic sample predicting on the basis of past internal room temperature data available for 6 months and then add variations like outside temperature ,humidity , the current weather season etc to
improve the model. LikeLike Hi Sachin! That sounds like a fun project 🙂 You should start by organising your dataset so you can use it for training a model. That means for each row of data, you have to append the target value to predict as an additional element. The value to predict is the temperature some hours into the future I’d guess. The model also needs enough past data to predict the future value, so if you currently have one temperature per time step, you should try augmenting the dataset to make each row contain all temperature measurements for the last 12 hours up to the current timestamp for example. Later you can add new columns to the dataset with additional data types, as you mention. When you’ve organised the dataset, just follow the steps to create the model in Amazon ML. Good luck! LikeLiked by 1 person Fill in your details below or click an icon to log in:
You are commenting using your WordPress.com account. ( Log Out / Change )
You are commenting using your Twitter account. ( Log Out / Change )
You are commenting using your Facebook account. ( Log Out / Change )
You are commenting using your Google+ account. ( Log Out / Change ) Connecting to %s
Notify me of new comments via email."
144848ad4d,Skype launches Photo Effects – sticker suggestions powered by machine learning  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Not content with merely launching its own take on Instagram and Snapchat’s Stories, Skype today is adding another copycat-like feature to its app: photo stickers. The company says it’s introducing new “Photo Effects” (as it’s calling these stickers), which include things like face masks, decorative borders, witty captions, and more. However, unlike the photo stickers you’ll find in other social apps today, Skype will actually suggest the stickers to use based on the photo’s content, day of the week, and other options. The new feature is based on technology Microsoft introduced earlier this year in a silly camera app called Sprinkles. The Sprinkles app leverages Microsoft’s machine learning and A.I. capabilities to do things like detect faces in photos, determine the subject’s age and emotion, figure out your celebrity look-a-like, and suggest captions. It then lets you swipe through its suggestions – for example, various props to add to your photo, funny captions, and stickers displaying its guess about your age, among other things. Similarly, Skype will suggest its photo effects automatically with a press of a button. To use the feature, you’ll first snap a photo then tap the magic wand icon at the top of the screen to access the photo effects. As you swipe right through the suggestions, you’ll be prompted to add things to your photo like a smart face sticker, the weather, your location, a caption that references the day (e.g. “turn that frown upside down, it’s taco Tuesday!”), face masks, a celebrity look-a-like, or even a mystery face swap.
Microsoft says these photo effects will change often – like on different days of the week or holidays, for instance. The resulting image can be shared with Skype friends in a conversation or posted to Skype’s new Highlights feature, which is the Instagram/Snapchat Stories clone introduced earlier this year. Like Stories on other platforms, Highlights are somewhat ephemeral. But instead of lasting a day, they’re available for a week. They’re also not shared with your entire Skype network – only those who have opted to follow your Highlights directly. Highlights remains a mobile-only feature for now. When Skype’s revamped interface launched to desktop users in October, Microsoft told us Highlights was not a priority for desktop integration at this time, based on user feedback. However, the company insisted it still aims to bring Highlights to the desktop in a later release. The addition of Photo Effects is arriving on Skype for mobile users in the latest update. Skype’s release notes list Photo Effects as “upcoming” in Android version 8.10.0.4 and iOS 8.10.0.5. This version began rolling out on Monday, but will gradually release to the install base over the next week. Latest headlines delivered to you daily"
86b5b568aa,Using machine learning to improve patient care  | MIT News,"Login
or
Subscribe Newsletter
Image: Shutterstock New CSAIL research employs many types of medical data, including electronic health records, to predict outcomes in hospitals.
Rachel Gordon | CSAIL
August 21, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Doctors are often deluged by signals from charts, test results, and other metrics to keep track of. It can be difficult to integrate and monitor all of these data for multiple patients while making real-time treatment decisions, especially when data is documented inconsistently across hospitals. In a new pair of papers, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) explore ways for computers to help doctors make better medical decisions. One team created a machine-learning approach called “ICU Intervene” that takes large amounts of intensive-care-unit (ICU) data, from vitals and labs to notes and demographics, to determine what kinds of treatments are needed for different symptoms. The system uses “deep learning” to make real-time predictions, learning from past ICU cases to make suggestions for critical care, while also explaining the reasoning behind these decisions. “The system could potentially be an aid for doctors in the ICU, which is a high-stress, high-demand environment,” says PhD student Harini Suresh, lead author on the paper about ICU Intervene. “The goal is to leverage data from medical records to improve health care and predict actionable interventions.” Another team developed an approach called “EHR Model Transfer” that can facilitate the application of predictive models on an electronic health record (EHR) system, despite being trained on data from a different EHR system. Specifically, using this approach the team showed that predictive models for mortality and prolonged length of stay can be trained on one EHR system and used to make predictions in another. ICU Intervene was co-developed by Suresh, undergraduate student Nathan Hunt, postdoc Alistair Johnson, researcher Leo Anthony Celi, MIT Professor Peter Szolovits, and PhD student Marzyeh Ghassemi. It was presented this month at the Machine Learning for Healthcare Conference in Boston. EHR Model Transfer was co-developed by lead authors Jen Gong and Tristan Naumann, both PhD students at CSAIL, as well as Szolovits and John Guttag, who is the Dugald C. Jackson Professor in Electrical Engineering. It was presented at the ACM’s Special Interest Group on Knowledge Discovery and Data Mining in Halifax, Canada. Both models were trained using data from the critical care database MIMIC, which includes de-identified data from roughly 40,000 critical care patients and was developed by the MIT Lab for Computational Physiology. ICU Intervene Integrated ICU data is vital to automating the process of predicting patients’ health outcomes. “Much of the previous work in clinical decision-making has focused on outcomes such as mortality (likelihood of death), while this work predicts actionable treatments,” Suresh says. “In addition, the system is able to use a single model to predict many outcomes.” ICU Intervene focuses on hourly prediction of five different interventions that cover a wide variety of critical care needs, such as breathing assistance, improving cardiovascular function, lowering blood pressure, and fluid therapy. At each hour, the system extracts values from the data that represent vital signs, as well as clinical notes and other data points. All of the data are represented with values that indicate how far off a patient is from the average (to then evaluate further treatment). Importantly, ICU Intervene can make predictions far into the future. For example, the model can predict whether a patient will need a ventilator six hours later rather than just 30 minutes or an hour later. The team also focused on providing reasoning for the model’s predictions, giving physicians more insight. “Deep neural-network-based predictive models in medicine are often criticized for their black-box nature,” says Nigam Shah, an associate professor of medicine at Stanford University who was not involved in the paper. “However, these authors predict the start and end of medical interventions with high accuracy, and are able to demonstrate interpretability for the predictions they make.” The team found that the system outperformed previous work in predicting interventions, and was especially good at predicting the need for vasopressors, a medication that tightens blood vessels and raises blood pressure. In the future, the researchers will be trying to improve ICU Intervene to be able to give more individualized care and provide more advanced reasoning for decisions, such as why one patient might be able to taper off steroids, or why another might need a procedure like an endoscopy. EHR Model Transfer Another important consideration for leveraging ICU data is how it’s stored and what happens when that storage method gets changed. Existing machine-learning models need data to be encoded in a consistent way, so the fact that hospitals often change their EHR systems can create major problems for data analysis and prediction. That’s where EHR Model Transfer comes in. The approach works across different versions of EHR platforms, using natural language processing to identify clinical concepts that are encoded differently across systems and then mapping them to a common set of clinical concepts (such as “blood pressure” and “heart rate”). For example, a patient in one EHR platform could be switching hospitals and would need their data transferred to a different type of platform. EHR Model Transfer aims to ensure that the model could still predict aspects of that patient’s ICU visit, such as their likelihood of a prolonged stay or even of dying in the unit. “Machine-learning models in health care often suffer from low external validity, and poor portability across sites,” says Shah. “The authors devise a nifty strategy for using prior knowledge in medical ontologies to derive a shared representation across two sites that allows models trained at one site to perform well at another site. I am excited to see such creative use of codified medical knowledge in improving portability of predictive models.” With EHR Model Transfer, the team tested their model’s ability to predict two outcomes: mortality and the need for a prolonged stay. They trained it on one EHR platform and then tested its predictions on a different platform. EHR Model Transfer was found to outperform baseline approaches and demonstrated better transfer of predictive models across EHR versions compared to using EHR-specific events alone. In the future, the EHR Model Transfer team plans to evaluate the system on data and EHR systems from other hospitals and care settings. Both papers were supported, in part, by the Intel Science and Technology Center for Big Data and the National Library of Medicine. The paper detailing EHR Model Transfer was additionally supported by the National Science Foundation and Quanta Computer, Inc. Topics: Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical Engineering & Computer Science (eecs), Artificial intelligence, Data, Research, School of Engineering, Institute for Medical Engineering and Science (IMES), Computer science and technology, Health care, Health sciences and technology, National Science Foundation (NSF) Two new papers from CSAIL researchers “aim to help doctors make better use of the digital information they’re presented with,” writes Adi Gaskell for The Huffington Post. One examines a tool that uses ICU data to choose the best treatment option based on a range of symptoms, while the other facilitates “predictive models based upon an electronic health record system.”
This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
b652795fe5,Counterfeiters are using AI and machine learning to make better fakes,"It's terrifyingly easy to just make stuff up online these days, such is life in the post-truth era. But recent advancements in machine learning (ML) and artificial intelligence (AI) have compounded the issue exponentially. It's not just the news that's fake anymore but all sorts of media and consumer goods can now be knocked off thanks to AI. From audio tracks and video clips to financial transactions and counterfeit products -- even your own handwriting can be mimicked with startling levels of accuracy. But what if we could leverage the same computer systems that created these fakes to reveal them just as easily? People have been falling for trickery and hoaxes since forever. Human history is filled with false prophets, demagogues, snake-oil peddlers, grifters and con men. The problem is that these days, any two-bit huckster with a conspiracy theory and a supplement brand can hop on YouTube and instantly reach a global audience. And while the definition of ""facts"" now depends on who you're talking to, one thing that most people agreed to prior to January 20th this year is the veracity of hard evidence. Video and audio recordings have long been considered reliable sources of evidence but that's changing thanks to recent advances in AI. In July 2016, researchers at the University of Washington developed a machine learning system that not only accurately synthesizes a person's voice and vocal mannerisms but lip syncs their words onto a video. Essentially, you can fake anybody's voice and create a video of them saying whatever you want. Take the team's demo video, for example. They trained the ML system using footage of President Obama's weekly address. The recurrent neural network learned to associate various audio features with their respective mouth shapes. From there, the team generated CGI mouth movements, and with the help of 3D pose matching, ported the animated lips onto a separate video of the president. Basically, they're able to generate a photorealistic video using only its associated audio track. While the team took an outsized amount of blowback over the potential misuses of such technology, they had far more mundane uses for it in mind. ""The ability to generate high-quality video from audio could signicantly reduce the amount of bandwidth needed in video coding/transmission (which makes up a large percentage of current internet bandwidth),"" they suggested in their study, Synthesizing Obama: Learning Lip Sync from Audio. ""For hearing-impaired people, video synthesis could enable lip-reading from over-the-phone audio. And digital humans are central to entertainment applications like film special effects and games."" UW isn't the only facility looking into this sort of technology. Last year, a team from Stanford debuted the Face2Face system. Unlike UW's technology, which generates video from audio, Face2Face generates video from other video. It uses a regular webcam to capture the user's facial expressions and mouth shapes, then uses that information to deform the target YouTube video to best match the user's expressions and speech -- all in real time. AI-based audio-video transcription is a two-way street. Just as UW's system managed to generate video from an audio feed, a team from MIT's CSAIL figured out how to create audio from a silent video reel. And do it well enough to fool human audiences. ""When you run your finger across a wine glass, the sound it makes reflects how much liquid is in it,"" Andrew Owens, the paper's lead author told MIT News. ""An algorithm that simulates such sounds can reveal key information about objects' shapes and material types, as well as the force and motion of their interactions with the world."" The MIT's deep learning system was trained over the course of a few months using 1,000 videos containing some 46,000 sounds resulting from different objects being poked, struck or scraped with a drumstick. Like the UW algorithm, MIT's learned to associate different audio properties with specific onscreen actions and synthesize those sounds as the video played. When tested online against a video with authentic sound, people actually chose the fake audio over the real twice as often as the baseline algorithm. The MIT team figures that they can leverage this technology to help give robots better situational awareness. ""A robot could look at a sidewalk and instinctively know that the cement is hard and the grass is soft, and therefore know what would happen if they stepped on either of them,"" Owens said. ""Being able to predict sound is an important first step toward being able to predict the consequences of physical interactions with the world."" Research into audio synthesization isn't limited to universities; a number of major corporations are investigating the technology as well. Google, for example, has developed Wavenet, a ""deep generative model of raw audio waveforms."" Among the first iterations of computer-generated text-to-speech (TTS) systems is ""concatenative"" TTS. That's where a single person records a variety speech fragments, those are fed into a database and then reconstructed by a computer to form words and sentences. The problem is that the output sounds more like the MovieFone guy (ask your parents) than a real person. Waveform, on the other hand, is trained on waveforms of people speaking. The system samples those recordings for data points up to 16,000 times per second. To output sound, Waveform uses a model to predict what the next sound will be based on the sounds that came before it. The process is computationally expensive but does produce superior audio quality compared to the conventional TTS methods. In the future, robots could potentially forge your signature on official documents, if this AI-based handwriting mimic developed at the University College London is ever misused. Dubbed the ""My Text in Your Handwriting"" program, this system can accurately recreate a subject's handwriting with as little as a paragraph's input. The program is based on ""glyphs,"" essentially the unique traits of each person's handwriting. By measuring various aspects like horizontal and vertical spacing, connectors between letters and writing texture, the program can readily copy the style. ""Our software has lots of valuable applications. Stroke victims, for example, may be able to formulate letters without the concern of illegibility, or someone sending flowers as a gift could include a handwritten note without even going into the florist,"" Dr. Tom Haines, UCL Computer Science and lead author of the study, told UCL News. ""It could also be used in comic books where a piece of handwritten text can be translated into different languages without losing the author's original style."" And while this technology could be used to create forgeries, it can just as easily be leveraged to spot them as well. ""Forgery and forensic handwriting analysis are still almost entirely manual processes,"" Dr. Gabriel Brostow, of the UCL computer science department, said. ""But by taking the novel approach of viewing handwriting as texture-synthesis we can use our software to characterise handwriting to quantify the odds that something was forged."" Forgeries and faked products don't stop at the the bounds of the internet. Recent estimates by the Organisation for Economic Co-operation and Development put the global market for counterfeit goods at around $460 billion annually. And that's where the Entrupy authentication system comes in. ""In an ideal world, we shouldn't exist,"" Entrupy CEO Vidyuth Srinivasan lamented. ""The more we can instill trustworthiness in the market, the better it will be for commerce in general."" The company first imaged a wide variety of luxury goods and uses that database to help its customers -- generally those in secondary retail markets like vintage clothing stores or eBay sellers -- authenticate products with around 98.5 percent accuracy. Customers receive a handheld microscope and take various images of the product in question, such as the exterior, logo or interior lining. These photos are then fed into a mobile app and transmitted to the company's servers where a classification algorithm goes to work, differentiating between legitimate goods and counterfeits. If the product is real, the Entrupy will provide a certificate of authenticity. Although the company's product database is varied, there are limits to the system's current capabilities. Because it's optical, reflective or transparent items are no good, nor is anything without surface texture. Some things that it does not work on include porcelain, diamonds and glass, pure plastic and bare metal. Unlike the other AI-based systems discussed here, there's little chance of the Entrupy system being corrupted or gamed. ""We have had [counterfeiters] pose as real customers and legitimate businesses to try and buy [the system] and we're fine with it,"" Srinivasan explained. That's because the system doesn't actually tell the user which of the images they're taking are actually being used to verify the product's authenticity. ""We ask our customers to take images of different parts of the item because it's not just pure material [being used for verification]...,"" he continued. ""It's a holistic view of the different aspects of the item -- from the workmanship to the material used to the wear"" as well as a number of other contextual bits of metadata. What's more, the system is continually updated with new data, not just from the company's internal efforts of posing as secret buyers to acquire counterfeit goods, but also from the users themselves. Images taken during the authentication process -- whether the item turns out to be real or not -- are incorporated into the company's database, further improving the system's accuracy. ""In the near to medium future, I think that AI and ML will, as a counterfeiting solution, will definitely raise the bar,"" he concluded. ""It's a spy versus spy game, cat versus mouse."" Increasing our ability to spot fakes will force counterfeiters to up their game and start using better quality materials and better workmanship. That, however, will increase the production cost of these products, hopefully to a price that is no longer economically viable. ""The MO of any counterfeiter is to make something that they can sell a lot of, that can be easily produced and that does not cost a lot to produce a fake of,"" Srinivasan sid. ""Otherwise there's no profitability."" Similar measures have been adopted by Paypal, one of the the internet's top financial service providers, for cases of account fraud. ""Say my account was accessed today from San Francisco, tomorrow from NYC, and some other IP the day after,"" Hui Wang, Paypal's senior director of global risk sciences, told Engadget. This sort of activity is indicative of some kind of account takeover. ""In order to detect these kinds of fraud,"" she explained, ""we track the IP we track the machine and we track the network."" The company created an algorithm that looks at both the IP and the geolocation of that IP, then compares them to your account history to see if this matches up with previous actions. Paypal developed a proprietary technology that compares this IP location patten with other users, to see if there is a larger effect at work or there's a reasonable explanation for the movement -- i.e., perhaps you're flying through New York on business and buy a souvenir at the airport gift shop before continuing on the trip. The company's AI system also attempts to identify each previous IP, whether it's a hotel's secured ethernet connection or the public WiFi at the airport. ""[The algorithm] is retrieving tons of data from your account history and going beyond your account to look at the traffic on your network, like the other people using the same IP,"" Wang said. From this raw information, the algorithm selects specific data points and uses those to estimate whether the transaction is legitimate. Most of these actions and their subsequent decisions -- such as verifying or denying a payment -- are performed autonomously. However, if the algorithm's confidence value is too low, human investigators from the operations center will investigate the transaction manually. ""We are also in the process of ensuring that human intelligence can be fed back into the automated system,"" she continued, so that the ML system continually learns, improves and increases its accuracy. These sorts of systems, both those designed to generate fakes and those trained to uncover them, are still in their infancy. But in the coming decades, artificial intelligence and machine learning techniques will continue to improve, often in ways that we have yet to envision. There is a very real danger in technologies that can create uncannily convincing lies, hoaxes and fakes -- in front of our very eyes, no less. But, like movable type, radio and internet that came before it, AI systems like these, ones capable of generating photorealistic content, will only be as dangerous as the intentions of the people using it. And that's a terrifying thought. Andrew has lived in San Francisco since 1982 and has been writing clever things about technology since 2011. When not arguing the finer points of portable vaporizers and military defense systems with strangers on the internet, he enjoys tooling around his garden, knitting and binge watching anime. Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
304b52dfec,Danske Bank uses Machine Learning and AI to understand customers - Business Insider Nordic,"*Copyright © 2016 Business Insider Inc / Bonnier Business Media Sweden AB. All rights reserved. Registration on or use of this site constitutes acceptance of our Legal Fine Print.   Salesforce just bought a startup for 'tens of millions of dollars' adding to its buying spree Amazon, Google, and Microsoft might be going to war to win Uber's cloud business The future is here Elon Musk wants people to live on Mars by 2030 — and this isolated Hawaiian base camp is the closest we've come to that vision yet Legendary hedge fund manager Jim Simons made $1.7 billion last year ― here's how he went from cracking codes to making billions A hedge fund manager you've never heard of made $300 million last year Warren Buffett once proposed a 4th Law of Motion to explain the pitfalls of active management The former CEO of troubled startup Zenefits made more than $10 million from the company ...
and still owns stock Billion-dollar-broker Fredrik Eklund reveals his 10 best tips to seal any deal 'Empty labor' is a huge crisis in offices — and the solution is to work less 17 psychological tricks to make people like you immediately 'I interviewed over 100 people at Goldman Sachs, and this was the biggest mistake job candidates made' A 37-year-old who created his dream job traveling the world gives his best advice on making it happen A stunning glacier in Iceland shows exactly how much the climate has changed The 10 most popular summer travel destinations, according to TripAdvisor This remote-controlled camera that sticks to walls could replace selfie sticks 16 essential terms every beginning watch collector should know 15 successful startup founders who can include the title of 'Mom' on their resumes 17 apps that Apple thinks moms and dads should download 'Help! My boss hates me and I'm afraid it will ruin my career' Thousands to receive basic income in Finland: a trial that could lead to the greatest societal transformation of our time Sweden's latest six-hour work day experiment was a complete failure 41 powerful photos of Hillary Clinton's storied career Finland: To beat Sweden we need to work longer - for lower wages 15 tips to start earning money doing what you love, from people who have done it Goldman Sachs thinks everyone should read these 12 books this fall Spotify's former recruiter reveals the top qualities you need to work at the world's hottest start-ups This 31-year-old who couldn't afford a Rolex has built a $180 million watch empire in less than 5 years How does a brick-and-mortar company of age create disruptive innovations? Danske Bank, the largest bank in Denmark, showed the way in 2013, by introducing MobilePay, a new and convenient way to transfer money. Today, the MobilePay is the Nordic's leading money transfer app with 3.8 million users in its core market, Denmark. Now, Danske Bank is applying the same approach as with MobilePay - but instead of payments, it focuses on customer behavior and analytics. The bank has created an in-hose startup, Advanced Analytics, whose sole purpose is to shake things up with bleeding edge AI and Machine Learning technology. ”Basically, we got the mandate to tear everything apart in to understand our customers better. Today, we are 24 people in the team, and we have completed more than 30 projects,” says Bjørn Büchmann-Slorup, 35, the head of Advanced Analytics at Danske Bank. His unit was established two years ago to utilize Machine Learning (ML), where AI (Artificial Intelligence) gives the computer the ability to learn without additional programming. The team is now using ML for predictive models to asses customer behavior and preferences on a personal level. ”By analyzing customer data we were able to identify the customer's preferred means of communication, such as phone, letter or email. [This sort of valuable info] has helped improve our marketing campaign hit rate by a factor of four,” Büchmann-Slorup says. Read More: 'Machine learning' is a revolution as big as the internet or personal computers The technology can also be used to predict the customer's needs. ”Based on online behavior we identified customers in a specific situation where financial advice is needed. For instance, when a person changes jobs with a new salary and pension plan."" Danske Bank used these situations to contact customers proactively, and achieved 62% better results than in their  traditional campaigns, Büchmann-Slorup explains. But Advanced Analytics is not just about knowing the customers; Danske Bank also wants to empower them on their own terms – by using peer-based models á la Tripadvisor. ”We introduced a concept called 'Others Like You', which enables us to show a customer the choice of people with a similar profile, [for instance when choosing] pension plans,” Büchmann-Slorup says. Bjørn Büchmann-Slorup's job is not all about technology. His job also includes a fair amount of education an persuasion. ”We still encounter skepticism and resistance to change in the organization. Which is perfectly understandable,"" he says, adding some historical context:  ""When people were introduced to the calculator 60 years ago, they did not trust the device which so they still did calculations on paper to verify. That is exactly the same reaction we are experiencing with ML and Big Data today.”  The bank's data experts have faced some skepticism regarding their startup approach of the Advanced Analytics team, and the tools they are introducing across the organization. ”A lot of people [..] expect instant ROI (Return on Investment) which is not realistic. You need resources to develop the right ML tools and [you need to] work with the users in the organization. That is why we in some ways consider ourselves a startup within Danske Bank."" ""We got some initial funding, and we succeeded with proof of concept. Now we can invest further in new tools and skills.” Jacob Rees-Mogg challenged to work in a food bank ... Jacob Rees-Mogg says the surge in food banks is ac... Inside a human brain bank, where frozen tubs prese... Inside a human brain bank, where frozen tubs prese... Mervyn King wanted to keep Northern Rock bailout a... The Bank of England gave banks their starkest warn... One of retail's most troubled brands is slated to ... An invisible banking reform that 'could fundamenta... Citi will set up a private banking hub in Luxembou... DEUTSCHE BANK: Italy's 3 big problems could trigge... *Copyright © 2016 Business Insider Inc / Bonnier Business Media Sweden AB. All rights reserved. Registration on or use of this site constitutes acceptance of our Legal Fine Print."
b2a5128503,Robot learns to follow orders like Alexa | MIT News,"Login
or
Subscribe Newsletter
ComText allows robots to understand contextual commands such as, “Pick up the box I put down.” Photo: Tom Buehler/MIT CSAIL The Baxter robot picks up a block using the ComText system. Photo: Tom Buehler/MIT CSAIL ComText, from the Computer Science and Artificial Intelligence Laboratory, allows robots to understand contextual commands.
Adam Conner-Simons | Rachel Gordon | CSAIL
August 30, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Despite what you might see in movies, today’s robots are still very limited in what they can do. They can be great for many repetitive tasks, but their inability to understand the nuances of human language makes them mostly useless for more complicated requests. For example, if you put a specific tool in a toolbox and ask a robot to “pick it up,” it would be completely lost. Picking it up means being able to see and identify objects, understand commands, recognize that the “it” in question is the tool you put down, go back in time to remember the moment when you put down the tool, and distinguish the tool you put down from other ones of similar shapes and sizes. Recently researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have gotten closer to making this type of request easier: In a new paper, they present an Alexa-like system that allows robots to understand a wide range of commands that require contextual knowledge about objects and their environments. They've dubbed the system “ComText,” for “commands in context.” The toolbox situation above was among the types of tasks that ComText can handle. If you tell the system that “the tool I put down is my tool,” it adds that fact to its knowledge base. You can then update the robot with more information about other objects and have it execute a range of tasks like picking up different sets of objects based on different commands. “Where humans understand the world as a collection of objects and people and abstract concepts, machines view it as pixels, point-clouds, and 3-D maps generated from sensors,” says CSAIL postdoc Rohan Paul, one of the lead authors of the paper. “This semantic gap means that, for robots to understand what we want them to do, they need a much richer representation of what we do and say.” The team tested ComText on Baxter, a two-armed humanoid robot developed for Rethink Robotics by former CSAIL director Rodney Brooks. The project was co-led by research scientist Andrei Barbu, alongside research scientist Sue Felshin, senior research scientist Boris Katz, and Professor Nicholas Roy. They presented the paper at last week’s International Joint Conference on Artificial Intelligence (IJCAI) in Australia. How it works Things like dates, birthdays, and facts are forms of “declarative memory.” There are two kinds of declarative memory: semantic memory, which is based on general facts like the “sky is blue,” and episodic memory, which is based on personal facts, like remembering what happened at a party. Most approaches to robot learning have focused only on semantic memory, which obviously leaves a big knowledge gap about events or facts that may be relevant context for future actions. ComText, meanwhile, can observe a range of visuals and natural language to glean “episodic memory” about an object’s size, shape, position, type and even if it belongs to somebody. From this knowledge base, it can then reason, infer meaning and respond to commands. “The main contribution is this idea that robots should have different kinds of memory, just like people,” says Barbu. “We have the first mathematical formulation to address this issue, and we’re exploring how these two types of memory play and work off of each other.” With ComText, Baxter was successful in executing the right command about 90 percent of the time. In the future, the team hopes to enable robots to understand more complicated information, such as multi-step commands, the intent of actions, and using properties about objects to interact with them more naturally. For example, if you tell a robot that one box on a table has crackers, and one box has sugar, and then ask the robot to “pick up the snack,” the hope is that the robot could deduce that sugar is a raw material and therefore unlikely to be somebody’s “snack.” By creating much less constrained interactions, this line of research could enable better communications for a range of robotic systems, from self-driving cars to household helpers. “This work is a nice step towards building robots that can interact much more naturally with people,” says Luke Zettlemoyer, an associate professor of computer science at the University of Washington who was not involved in the research. “In particular, it will help robots better understand the names that are used to identify objects in the world, and interpret instructions that use those names to better do what users ask.” The work was funded, in part, by the Toyota Research Institute, the National Science Foundation, the Robotics Collaborative Technology Alliance of the U.S. Army, and the Air Force Research Laboratory. Topics: Research, School of Engineering, Artificial intelligence, Data, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Linguistics, Machine learning, Robotics, National Science Foundation (NSF) This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
bcae49dcdb,Machine learning shows exactly when to zap brain to boost memory | New Scientist,"Daily news
20 April 2017
ZEPHYR/Science Photo Library By Lefteris Apostolakis Struggling to remember something? An electrical jolt deep in the brain might help – if it is given at the right time. To discover the effect of electrical stimulation on memory, Michael Kahana and colleagues at the University of Pennsylvania turned to 150 volunteers who had previously had electrodes implanted in their brains to help control severe epilepsy. These electrodes can record the brain’s electrical signals, giving the team a window into each person’s neural processes. They can also deliver electricity to the brain. First, the team recorded the brain signals of the volunteers while they learned items from a list, and later as they tried to recall those items. They then applied machine learning methods to this brain signal data, enabling them to predict if a person’s efforts to commit something to memory would later prove successful, based on the state of their brain at the time.
The team next ran further recall tests, during which they delivered random jolts of electricity to the participants while they were trying to memorise test items. They compared the effects of jolting someone during two different brain states – the pattern of signals linked to being likely to later remember something, and the pattern linked to being more likely to have a memory lapse. They found that giving electrical stimulation when a person’s brain signals suggested they would later forget the current item made that person 13 per cent more likely to recall it. “You get significant enhancement,” says Kahana. Timing was key, however. A jolt of electricity during a pattern of brain activity linked to later recall went on to reduce a person’s likelihood of remembering an item by 18 per cent. The study is the latest of many probing the question of whether zaps of electricity can improve memory. So far, many studies have conflicted with each other on the effects of deep brain stimulation and recall. “Electrical brain stimulation is controversial,” says Inês Violante at Imperial College London. “The majority of studies have a very low number of participants. A study of this size is much more reliable.” Kahana is now working on a device that could tell when the brain would benefit from an induced memory boost. “You could build a technology that could trigger stimulation at moments when you’re predicted to have poor memory, thus enhancing memory of an individual wearing such a device,” says Kahana. Such a device may be useful for people who have memory loss, but first we need to understand which parts of the brain benefit the most from this kind of stimulation. While deep brain stimulation already helps people with untreatable epilepsy or Parkinson’s disease, it’s an extreme treatment that carries the risk of infection. Experimental approaches that stimulate the brain externally may be a more desirable option. Journal reference: Current Biology , DOI: 10.1016/j.cub.2017.03.028 Read more: Alzheimer’s damage reversed by deep brain stimulation; Deep brain stimulation: A wonder treatment pushed too far? More on these topics:
A shorter version of this article was published in New Scientist magazine on 29 April 2017"
42dd1b3759,An On-device Deep Neural Network for Face Detection - Apple,"Vol. 1, Issue 7 ∙
November 2017
November Two Thousand Seventeen
by Computer Vision Machine Learning Team
Apple started using deep learning for face detection in iOS 10. With the release of the Vision framework, developers can now use this technology and many other computer vision algorithms in their apps. We faced significant challenges in developing the framework so that we could preserve user privacy and run efficiently on-device. This article discusses these challenges and describes the face detection algorithm.
Apple first released face detection in a public API in the Core Image framework through the CIDetector class. This API was also used internally by Apple apps, such as Photos. The earliest release of CIDetector used a method based on the Viola-Jones detection algorithm [1]. We based subsequent improvements to CIDetector on advances in traditional computer vision. With the advent of deep learning, and its application to computer vision problems, the state-of-the-art in face detection accuracy took an enormous leap forward. We had to completely rethink our approach so that we could take advantage of this paradigm shift. Compared to traditional computer vision, the learned models in deep learning require orders of magnitude more memory, much more disk storage, and more computational resources. As capable as today’s mobile phones are, the typical high-end mobile phone was not a viable platform for deep-learning vision models. Most of the industry got around this problem by providing deep-learning solutions through a cloud-based API. In a cloud-based solution, images are sent to a server for analysis using deep learning inference to detect faces. Cloud-based services typically use powerful desktop-class GPUs with large amounts of memory available. Very large network models, and potentially ensembles of large models, can run on the server side, allowing clients (which could be mobile phones) to take advantage of large deep learning architectures that would be impractical to run locally. Apple’s iCloud Photo Library is a cloud-based solution for photo and video storage. However, due to Apple’s strong commitment to user privacy, we couldn’t use iCloud servers for computer vision computations. Every photo and video sent to iCloud Photo Library is encrypted on the device before it is sent to cloud storage, and can only be decrypted by devices that are registered with the iCloud account. Therefore, to bring deep learning based computer vision solutions to our customers, we had to address directly the challenges of getting deep learning algorithms running on iPhone. We faced several challenges. The deep-learning models need to be shipped as part of the operating system, taking up valuable NAND storage space. They also need to be loaded into RAM and require significant computational time on the GPU and/or CPU. Unlike cloud-based services, whose resources can be dedicated solely to a vision problem, on-device computation must take place while sharing these system resources with other running applications. Finally, the computation must be efficient enough to process a large Photos library in a reasonably short amount of time, but without significant power usage or thermal increase. The rest of this article discusses our algorithmic approach to deep-learning-based face detection, and how we successfully met the challenges to achieve state-of-the-art accuracy. We discuss: In 2014, when we began working on a deep learning approach to detecting faces in images, deep convolutional networks (DCN) were just beginning to yield promising results on object detection tasks. Most prominent among these was an approach called “OverFeat” [2] which popularized some simple ideas that showed DCNs to be quite efficient at scanning an image for an object. OverFeat drew the equivalence between fully connected layers of a neural network and convolutional layers with valid convolutions of filters of the same spatial dimensions as the input. This work made clear that a binary classification network of a fixed receptive field (for example 32x32, with a natural stride of 16 pixels) could be efficiently applied to an arbitrary sized image (for example, 320x320) to produce an appropriately sized output map (20x20 in this example). The OverFeat paper also provided clever recipes to produce denser output maps by effectively reducing the network stride. We built our initial architecture based on some of the insights from the OverFeat paper, resulting in a fully convolutional network (see Figure 1) with a multitask objective comprising of: We experimented with several ways of training such a network. For example, a simple procedure for training is to create a large dataset of image tiles of a fixed size corresponding to the smallest valid input to the network such that each tile produces a single output from the network. The training dataset is ideally balanced, so that half of the tiles contain a face (positive class) and the other half do not contain a face (negative class). For each positive tile, we provide the true location (x, y, w, h) of the face. We train the network to optimize the multitask objective described previously. Once trained, the network is able to predict whether a tile contains a face, and if so, it also provides the coordinates and scale of the face in the tile. Since the network is fully convolutional, it can efficiently process an arbitrary sized image and produce a 2D output map. Each point on the map corresponds to a tile in the input image and contains the prediction from the network regarding the presence of or absence of a face in that title and its location/scale within the input tile (see inputs and outputs of DCN in Figure 1). Given such a network, we could then build a fairly standard processing pipeline to perform face detection, consisting of a multi-scale image pyramid, the face detector network, and a post-processing module. We needed a multi-scale pyramid to handle faces across a wide range of sizes. We apply the network to each level of the pyramid and candidate detections are collected from each layer. (See Figure 2.) The post processing module then combines these candidate detections across scales to produce a list of bounding boxes that correspond to the network’s final prediction of the faces in the image. This strategy brought us closer to running a deep convolutional network on device to exhaustively scan an image. But network complexity and size remained key bottlenecks to performance. Overcoming this challenge meant not only limiting the network to a simple topology, but also restricting the number of layers of the network, the number of channels per layer, and the kernel size of the convolutional filters. These restrictions raised a crucial problem: our networks that were producing acceptable accuracy were anything but simple, most going over 20 layers and consisting of several network-in-network [3] modules. Using such networks in the image scanning framework described previously would be completely infeasible. They led to unacceptable performance and power usage. In fact, we would not even be able to load the network into memory. The challenge then was how to train a simple and compact network that could mimic the behavior of the accurate but highly complex networks. We decided to leverage an approach, informally called “teacher-student” training[4]. This approach provided us a mechanism to train a second thin-and-deep network (the “student”), in such a way that it matched very closely the outputs of the big complex network (the “teacher”) that we had trained as described previously. The student network was composed of a simple repeating structure of 3x3 convolutions and pooling layers and its architecture was heavily tailored to best leverage our neural network inference engine. (See Figure 1.) Now, finally, we had an algorithm for a deep neural network for face detection that was feasible for on-device execution. We iterated through several rounds of training to obtain a network model that was accurate enough to enable the desired applications. While this network was accurate and feasible, a tremendous amount of work still remained to make it practical for deploying on millions of user devices. Practical considerations around deep learning factored heavily into our design choices for an easy-to-use framework for developers, which we call Vision. It became quickly apparent that great algorithms are not enough for creating a great framework. We had to have a highly optimized imaging pipeline. We did not want developers to think about scaling, color conversions, or image sources. Face detection should work well whether used in live camera capture streams, video processing, or processing of images from disc or the web. It should work regardless of image representation and format. We were concerned with power consumption and memory usage, especially for streaming and image capture. We worried about memory footprint, such as the large one needed for a 64 Megapixel panorama. We addressed these concerns by using techniques of partial subsampled decoding and automatic tiling to perform computer vision tasks on large images even with non-typical aspect ratios. Another challenge was colorspace matching. Apple has a broad set of colorspace APIs but we did not want to burden developers with the task of color matching. The Vision framework handles color matching, thus lowering the threshold for a successful adoption of computer vision into any app. Vision also optimizes by efficient handling and reuse of intermediates. Face detection, face landmark detection, and a few other computer vision tasks work from the same scaled intermediate image. By abstracting the interface to the algorithms and finding a place of ownership for the image or buffer to be processed, Vision can create and cache intermediate images to improve performance for multiple computer vision tasks without the need for the developer to do any work. The flip side was also true. From the central interface perspective, we could drive the algorithm development into directions that allow for better reusing or sharing of intermediates. Vision hosts several different, and independent, computer vision algorithms. For the various algorithms to work well together, implementations use input resolutions and color spaces that are shared across as many algorithms as possible The joy of ease-of-use would quickly dissipate if our face detection API were not able to be used both in real time apps and in background system processes. Users want face detection to run smoothly when processing their photo libraries for face recognition, or analyzing a picture immediately after a shot. They don’t want the battery to drain or the performance of the system to slow to a crawl. Apple’s mobile devices are multitasking devices. Background computer vision processing therefore shouldn’t significantly impact the rest of the system’s features. We implement several strategies to minimize memory footprint and GPU usage. To reduce memory footprint, we allocate the intermediate layers of our neural networks by analyzing the compute graph. This allows us to alias multiple layers to the same buffer. While being fully deterministic, this technique reduces memory footprint without impacting the performance or allocations fragmentation, and can be used on either the CPU or GPU. For Vision, the detector runs 5 networks (one for each image pyramid scale as shown in Figure 2). These 5 networks share the same weights and parameters, but have different shapes for their input, output, and intermediate layers. To reduce footprint even further, we run the liveness-based memory optimization algorithm on the joint graph composed by those 5 networks, significantly reducing the footprint. Also, the multiple networks reuse the same weight and parameter buffers, thus reducing memory needs. To achieve better performance, we exploit the fully convolutional nature of the network: All the scales are dynamically resized to match the resolution of the input image. Compared to fitting the image in square network retinas (padded by void bands), fitting the network to the size of the image allows us to reduce drastically the number of total operations. Because the topology of the operation is not changed by the reshape and the high performance of the rest of the allocator, dynamic reshaping does not introduce performance overhead related to allocation. To ensure UI responsiveness and fluidity while deep neural networks run in background, we split GPU work items for each layer of the network until each individual time is less than a millisecond. This allows the driver to switch contexts to higher priority tasks in a timely manner, such as UI animations, thus reducing and sometimes eliminating frame drop. Combined, all these strategies ensure that our users can enjoy local, low-latency, private deep learning inference without being aware that their phone is running neural networks at several hundreds of gigaflops per second. Did we accomplish what we set as our goal of developing a performant, easy-to-use, face detection API? You can try the Vision framework and judge for yourself. Here’s how to get started: [1] Viola, P. and Jones, M.J. Robust Real-time Object Detection Using a Boosted Cascade of Simple Features. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2001. [2] Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks. arXiv:1312.6229 [Cs], December, 2013. [3] Lin, Min, Qiang Chen, and Shuicheng Yan. Network In Network. arXiv:1312.4400 [Cs], December, 2013. [4] Romero, Adriana, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for Thin Deep Nets. arXiv:1412.6550 [Cs], December, 2014. [5] Tam, A. Core ML and Vision: Machine learning in iOS Tutorial. Retrieved from https://www.raywenderlich.com, September, 2017. Send questions or feedback via email Apply now for a career at Apple Apple Developer Program"
f9aa29224b,An AI has learned how to pick a single voice out of a crowd | New Scientist,"News & Technology
24 October 2017
Christopher Anderson/Magnum Photos By Richard Gray Devices like Amazon’s Echo and Google Home can usually deal with requests from a lone person, but like us they struggle in situations such as a noisy cocktail party, where several people are speaking at once. Now an AI that is able to separate the voices of multiple speakers in real time promises to give automatic speech recognition a big boost, and could soon find its way into an elevator near you. The technology, developed by researchers at the Mitsubishi Electric Research Laboratory in Cambridge, Massachusetts, was demonstrated in public for the first time at this month’s Combined Exhibition of Advanced Technologies show in Tokyo.
It uses a machine learning technique the team calls “deep clustering” to identifies unique features in the “voiceprint” of multiple speakers. It then groups the distinct features from each speaker’s voice together, allowing it to disentangle multiple voices and then reconstruct what each person was saying. “It was trained using 100 English speakers, but it can separate voices even if a speaker is Japanese,” says Niels Meinke, a spokesperson for Mitsubishi Electric. Meinke says the system can separate and reconstruct the speech of two people speaking into a single microphone with up to 90 per cent accuracy. If there are three speakers the accuracy dips, but is still up to 80 per cent. In both cases, this was with speakers the system had never encountered before. Conventional approaches to this problem – such as using two microphones to replicate the position of a listener’s ears – have only managed 51 per cent accuracy. In overcoming the “cocktail party effect” that has dogged AI research for decades, the new technology could help smart assistants in homes and cars work better. It could also improve automatic speech transcription, and be used to help law enforcement agencies reconstruct recordings of conversations that had been muddied by music, for example. In preliminary tests the system was able to separate the voices of up to five people at once. “The system could be used to separate speech in a range of products including lifts, air-conditioning units and household products,” says Meinke. Indeed, Mitsubishi is now in the process of building its voice recognition technology into lifts and air-conditioners, among other products. Reference: arxiv.org/abs/1508.04306 More on these topics:
A shorter version of this article was published in New Scientist magazine on 28 October 2017"
488841c4e6,An ever-learning mineral exploration platform - Mining Technology,"We use them to give you the best experience. If you continue using our website, we'll assume that you are happy to receive all cookies on this website. Trending: Earth AI is a new mineral exploration platform, generating an ever-growing, ever-learning geological map that could help companies find the best reserves quickly. Could this be the future of mineral exploration?
The system analyses the input of geophysical, satellite and geochemical data, to predict the mineral composition of areas. Credit: Courtesy of Earth AI
Developed at Sydney University by PhD student Roman Teslyuk with help from his friend, web designer Igor Gerechko, Earth AI is an online mineral exploration platform. Credit: Courtesy of Earth AI ""It helps to find a link between raw, remote sensed physical data and geochemical validation, basically."" Credit: Courtesy of Earth AI Earth AI is a new mineral exploration platform, generating an ever-growing, ever-learning geological map that could help companies find the best reserves quickly. Could this be the future of mineral exploration? Developed by a Sydney University PhD student Roman Teslyuk with help from his friend, web designer Igor Gerechko, Earth AI is an online mineral exploration platform. The system analyses the input of geophysical, satellite and geochemical data, to predict the mineral composition of areas and allows users to upload their own information so that the training data pool is constantly growing. Earth AI hopes the platform will improve mining companies’ capabilities to predict where future deposits lie, facilitating the comparison of formations and signatures. Airtree and Blackbird Ventures, two of Australia’s biggest technology venture capital funds, are two of the most high-profile recent investors in Earth AI. In a recent funding round, the firms invested A$500,000 to help build the platform. The company has already grown to a team of five and is working with more than 60 mining firms on both trial and paid contracts. Here, founder and CEO Roman Teslyuk talks about the origins of Earth AI. Roman Teslyuk (RT): I started my PhD in Sydney University in 2015, and we were working with a lot of geochemical data. We collected 300 samples within the project and analysed them for 45 elements. When we started interpreting what the data means, it became confusing very quickly because you have a lot of papers saying different things; one says this geochemical signature is a specific subduction signature and another paper will say it’s plume or crystal contamination-related. At the time I didn’t have any machine learning skills, so instead I went to the global GEOROC database and downloaded available two million data points. I then narrowed these down to just the rocks alike mine, which were high-magnesium and low-titanium rocks, and I just mainly classified it. This took me three weeks to finish. After that, I presented my results to the people at the university – the worldwide classification of high-magnesium, low-titanium rocks. The response I got was, ‘It’s very subjective, you’re pigeonholeing whatever you want to, whichever rock group you want’. So, I said ok, I’ll do it more objectively. I’ve learned some simple clustering and showed my results again, but my colleagues were not interested. Thus, I couldn’t continue learning about AI within my PhD and had to do it in my own time. RT: First of all, there was a course for PhD students called Inventing the Future, where we were split into teams to work on different innovative problems. For our team, the problem was related to nanosatellites in space; we quickly learned that building a satellite is very expensive and the capabilities are too small to do a viable start-up, despite the fact we thought they might be very useful for better data analysis. After the course I called my old friend and business partner Igor. He’s a web developer and I’m a geologist so it seemed a logical partnership. I didn’t know much about machine learning so I did some analysis at that point and together we created our first prototype of Earth AI. We went to the International Joint Conference on Artificial Intelligence in Melbourne last November. I thought that a platform like this would probably be something that already existed and I just hadn’t heard about it. But we asked everybody and they said that no one was actually doing it. It was then that we started to apply to the INCUBATE programme at Sydney University. We were accepted and started working with the mentors, got some office space and a small amount of funding, $5,000. We needed to find companies to work with, so I just decided to cold call everybody and from that we got our first customer in February called ActivEX in Brisbane. By the end of March we already had ten more customers, so then we could start raising the money we needed to grow. We’ve continued to attend different conferences, meeting more companies, and now we probably have 65 customers. RT: For the trial, we have a database of Australia that uses remote sensor data. We use satellites to collect this, working with joint data from NASA, some data from Japan and also some Australian servers. There are satellites sensing data all over the world, so we use that data together with all the open source data from Geoscience Australia and other geological servers in Australia, these provide geochemical data. First of all, we can apply unsupervised machine learning to create a data-driven geological map. Each rock has a unique signature, so if you identify these you can then put those signatures into a geological map. We’re using our network to categorise this data in a new way. There’s all this geological data and the geochemistry, and together they make a big difference to exploration. It helps to find a link between raw, remote sensed physical data and geochemical validation, basically. So we have a lot of geological data already, why not learn from that and start to find out what particular signatures are linked to all the samples? RT: I don’t know; from talking to people it seems there have been a lot of attempts, especially from larger companies that have done case studies. But they didn’t have a team themselves so they always had to contract somebody to do the job for them. First of all, you need a lot of domain knowledge to make it work. I kind of know what makes sense and what things to use because I’m a geologist. So, rather than me having to explain it to somebody else who has the skills of data science and coding skills, I can bridge the gap without loss of information. So they struggled with it for a while, but if you have to consistently work on it to make it work, it’s not really successful. The last thing, I guess, is actually validating the data. You might try a lot and get different analyses; it’s very dependent on the database that you’ve got and also to the area that you’re predicting. You have to be a geologist working on the ground to be able to differentiate what one makes sense, compared to the other ones which aren’t super correct. RT: Every day. We have had a lot of computational challenges; some databases are up to 3TB so we’ve have problems with how long they take. It could be a week to get the relevant information and then after a week, you look at it and it’s completely wrong. So you’ve wasted a week, just like that. We’ve had a lot of problems just with errors on the technical side. RT: Yes, and we have several other ambitions. We want you to be able to supervise where you can improve the geological map by constantly giving your own observations, so changing our approach into more of a group one with our customers. There is no option to travel everywhere and to every site so, instead, we want our analysis to just send you to the areas where there is the highest chance of discovering something. But also, if we could build a vehicle later, an autonomous machine that could go and analyse soil samples, that would be really helpful for exploration companies because they could focus on specific areas without travel. They could get a lot of the data and understanding of the ore body and the surrounding area autonomously. NSS to Exhibit at PDAC 2018 Flexco Releases Free Paper on Diversion Technology Calder Griffon Electrofusion Welder Becomes Top-Selling Product in Australia Breaking the Billion Variable Barrier in Real-World Optimization Using a Customized Evolutionary Algorithm © Copyright 2018 Kable, a trading division of Kable Intelligence Limited.
Go Top"
40097c4a87,Artificial intelligence aids materials fabrication | MIT News,"Login
or
Subscribe Newsletter
A team of researchers at MIT, the University of Massachusetts at Amherst, and the University of California at Berkeley hope to close the materials-science automation gap, with a new artificial-intelligence system that would pore through research papers to deduce “recipes” for producing particular materials.
Image: Chelsea Turner/MIT System could pore through millions of research papers to extract “recipes” for producing materials.
Larry Hardesty | MIT News Office
November 5, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
In recent years, research efforts such as the Materials Genome Initiative and the Materials Project have produced a wealth of computational tools for designing new materials useful for a range of applications, from energy and electronics to aeronautics and civil engineering. But developing processes for producing those materials has continued to depend on a combination of experience, intuition, and manual literature reviews. A team of researchers at MIT, the University of Massachusetts at Amherst, and the University of California at Berkeley hope to close that materials-science automation gap, with a new artificial-intelligence system that would pore through research papers to deduce “recipes” for producing particular materials. “Computational materials scientists have made a lot of progress in the ‘what’ to make — what material to design based on desired properties,” says Elsa Olivetti, the Atlantic Richfield Assistant Professor of Energy Studies in MIT’s Department of Materials Science and Engineering (DMSE). “But because of that success, the bottleneck has shifted to, ‘Okay, now how do I make it?’” The researchers envision a database that contains materials recipes extracted from millions of papers. Scientists and engineers could enter the name of a target material and any other criteria — precursor materials, reaction conditions, fabrication processes — and pull up suggested recipes. As a step toward realizing that vision, Olivetti and her colleagues have developed a machine-learning system that can analyze a research paper, deduce which of its paragraphs contain materials recipes, and classify the words in those paragraphs according to their roles within the recipes: names of target materials, numeric quantities, names of pieces of equipment, operating conditions, descriptive adjectives, and the like. In a paper appearing in the latest issue of the journal Chemistry of Materials, they also demonstrate that a machine-learning system can analyze the extracted data to infer general characteristics of classes of materials — such as the different temperature ranges that their synthesis requires — or particular characteristics of individual materials — such as the different physical forms they will take when their fabrication conditions vary. Olivetti is the senior author on the paper, and she’s joined by Edward Kim, an MIT graduate student in DMSE; Kevin Huang, a DMSE postdoc; Adam Saunders and Andrew McCallum, computer scientists at UMass Amherst; and Gerbrand Ceder, a Chancellor’s Professor in the Department of Materials Science and Engineering at Berkeley. Filling in the gaps The researchers trained their system using a combination of supervised and unsupervised machine-learning techniques. “Supervised” means that the training data fed to the system is first annotated by humans; the system tries to find correlations between the raw data and the annotations. “Unsupervised” means that the training data is unannotated, and the system instead learns to cluster data together according to structural similarities. Because materials-recipe extraction is a new area of research, Olivetti and her colleagues didn’t have the luxury of large, annotated data sets accumulated over years by diverse teams of researchers. Instead, they had to annotate their data themselves — ultimately, about 100 papers. By machine-learning standards, that’s a pretty small data set. To improve it, they used an algorithm developed at Google called Word2vec. Word2vec looks at the contexts in which words occur — the words’ syntactic roles within sentences and the other words around them — and groups together words that tend to have similar contexts. So, for instance, if one paper contained the sentence “We heated the titanium tetracholoride to 500 C,” and another contained the sentence “The sodium hydroxide was heated to 500 C,” Word2vec would group “titanium tetracholoride” and “sodium hydroxide” together. With Word2vec, the researchers were able to greatly expand their training set, since the machine-learning system could infer that a label attached to any given word was likely to apply to other words clustered with it. Instead of 100 papers, the researchers could thus train their system on around 640,000 papers. Tip of the iceberg To test the system’s accuracy, however, they had to rely on the labeled data, since they had no criterion for evaluating its performance on the unlabeled data. In those tests, the system was able to identify with 99 percent accuracy the paragraphs that contained recipes and to label with 86 percent accuracy the words within those paragraphs. The researchers hope that further work will improve the system’s accuracy, and in ongoing work they are exploring a battery of deep learning techniques that can make further generalizations about the structure of materials recipes, with the goal of automatically devising recipes for materials not considered in the existing literature. Much of Olivetti’s prior research has concentrated on finding more cost-effective and environmentally responsible ways to produce useful materials, and she hopes that a database of materials recipes could abet that project. “This is landmark work,"" says Ram Seshadri, the Fred and Linda R. Wudl Professor of Materials Science at the University of California at Santa Barbara. “The authors have taken on the difficult and ambitious challenge of capturing, through AI methods, strategies employed for the preparation of new materials. The work demonstrates the power of machine learning, but it would be accurate to say that the eventual judge of success or failure would require convincing practitioners that the utility of such methods can enable them to abandon their more instinctual approaches."" This research was supporting by the National Science Foundation, Office of Naval Research, the Department of Energy, and seed support through the MIT Energy Initiative. Kim was partially supported by Natural Sciences and Engineering Research Council of Canada. Topics: Research, School of Engineering, Computer science and technology, Energy, Environment, Machine learning, Manufacturing, Materials Science and Engineering, DMSE, Artificial intelligence This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
6798d1862c,Tesla Autopilot and artificial intelligence: The unfair advantage,"Serial tech entrepreneur and Tesla CEO Elon Musk has had a longstanding fear of artificial intelligence, but his company’s investments in artificial intelligence have been noted as an attempt to keep track of developments in the field of AI. In an interview for Vanity Fair in April 2017, he outright expressed his concerns with AI and claimed that one of the reasons for the development of SpaceX was that it could be an interplanetary escape route for humanity if artificial intelligence goes rogue. However, even Musk realizes the importance of AI in real-world applications, specifically for self-driving cars. At the end of June, Musk hired Andrej Karpathy as the new Director of Artificial Intelligence at Tesla, and MIT Technology Review claims it is the start of a plan to rethink automated driving at Tesla. Karpathy comes from OpenAI, a non-profit company founded by Musk that focuses on “discovering and enacting the path to safe artificial general intelligence.” Afterwards, he moved on to intern at DeepMind, a place that spotlighted reinforcement learning with AI. Karpathy’s previous research focuses are on image understanding and recognition, which directly translates into applying proven image recognitions algorithms in Tesla’s Autopilot.
Recently, the popular question of morality was brought up in context to AI learning in Autopilot cars. It’s very interesting to consider how to teach technology to respond to an innately human moral problem. The Moral Machine, hosted by Massachusetts Institute of Technology, is a platform built to “gather human perspectives on moral decisions made by machine intelligence, such as self-driving cars.” It questions how the machine would act in human decisions such as whether to crash the driver or keep driving into a pedestrian that is crossing the street where there are no traffic regulators. How exactly do you teach a logical machine the mechanisms of ethical decision-making? Although Musk and Tesla are the leaders in the self-driving field, a number of other companies are also entering into the competition sphere. Google, Uber, and Intel’s Mobileye have all been considering the application of reinforcement learning in the context of self-driving cars. Uber, Waymo, GM (Cruise Automation), Mobileye (camera supplier), Mercedes and Velodyne (LiDAR Supplier) could be potential competitors in the realm of self-driving vehicles. However, most of the technology does not encompass full self-driving, which is Musk’s aim. While other companies are investing heavily in autonomous fleets, Tesla far outpaces them in terms of data collection and release of finished product. Historically, Musk has focused on “narrow AI” which can enable the car to make decisions without driver interference. The vehicles would increasingly rely on radar as well as ultrasonic technology for sensing and data-gathering to form the basis for Tesla’s Autopilot algorithms. A technology that isn’t derived from LiDAR, the combination of radar and camera system said to outperform LiDAR especially in adverse weather conditions such as fog. With the introduction of Autopilot 2.0 and Tesla’s “Vision” system, and billions of miles real-world driving data collected by Model S and Model X drivers, Tesla continues to create a detailed 3D map of the world that has increasingly finer resolution as more vehicles are purchased, delivered and placed onto roadways. The addition of GPS allows Tesla to put together a visual driving map for AI vehicles to follow, paving the path for newer and more advanced vehicles.
The addition of Karpathy will be a notable asset for Tesla’s Autopilot team. In specific, the team will be able to apply Karpathy’s deep knowledge of reinforcement learning systems. Reinforcement learning for AI is similar to teaching animals via repetition of a behavior until a positive outcome is yielded. This type of machine learning will allow Tesla Autopilot to navigate complex and challenging scenarios. For example, AI will allow cars to determine in real-time how to navigate a four-way stop, a busy intersection or other difficult situations present on city streets. By making cars smarter with the way they navigate drivers, Tesla will put itself ahead of the curve with a fully-thinking, fully self-driving car. Tesla is expected to demonstrate a fully autonomous cross-country drive from California to New York by the end of this year as a showcase for its upcoming Full Self-driving Capability. If you’re buying a Tesla Model 3, or an existing Model S or Model X owner, just know that you’re contributing to a self-driving future, mile by mile. Interested in solar? Get a solar cost estimate and find out how much a solar system would cost for your home or business. ""I’m setup at Canaveral National Seashore on the top of my vehicle for a better vantage point. There’s a lot of buzz here, large crowd! I’m stoked!! I'm broadcasting live on Instagram Stories, so be sure to stay along for the ride by following and leaving a comment for me!"" News from Palo Alto... Expect additional reservations...
Michael RussoYeah, several advantages associated with the LRB, Johnny.... the right choice if you ca... johnnyboywow 60% of people want the extended battery?!
i didn't think it would be that many pe... Tesla news, rumors and reviews on all things Tesla. Connecting owners and enthusiasts via @TeslaratiApp available on iOS and Android Copyright © 2017 TESLARATI. All rights reserved."
99b326abd1,"Gaussian Processes for Timeseries Modelling | Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences","Institution: National University of Ireland, Maynooth In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches. If we are to take full advantage of the richness of scientific data available to us, we must consider a principled framework under which we may reason and infer. To fail to do this is to ignore uncertainty and risk false analysis, decision-making and forecasting. What we regard as a prerequisite for intelligent data analysis is ultimately concerned with the problem of computing in the presence of uncertainty. Considering data analysis under the mathematics of modern probability theory allows us to exploit a profound framework under which information, uncertainty and risk for actions, events and outcomes may be readily defined. Much recent research hence focuses on the principled handling of uncertainty for modelling in environments that are dynamic, noisy, observation costly and time sensitive. The machinery of probabilistic inference brings to the field of time-series analysis and monitoring robust, stable, computationally practical and principled approaches that naturally accommodate these real-world challenges. As a framework for reasoning in the presence of uncertain, incomplete and delayed information, we appeal to Bayesian inference. This allows us to perform robust modelling even in highly uncertain situations, and has a long pedigree in inference. Being able to include measures of uncertainty allows, for example, us to actively select where and when we would like to observe samples and offers approaches by which we may readily combine information from multiple noisy sources. This paper favours the conceptual over the mathematical (of course, the mathematical details are important and elegant but would obscure the aims of this paper; the interested reader is encouraged to read the cited material and a canonical text such as Rasmussen & Williams [1]). We start §2 with a short overview of why Bayesian modelling is important in time-series analysis, culminating in arguments that provoke us to use non-parametric models. Section 3 presents a conceptual overview of a particular flavour of non-parametric model, the Gaussian process (GP), which is well suited to time-series modelling [1]. We discuss in more detail the role of covariance functions, the influence they have on our models and explore, by example, how the (apparently subjective) function choices we make are in fact motivated by domain knowledge. Section 5 presents real-world time-series examples, from sensor networks, changepoint data and astronomy, to highlight the practical application of GP models. The more mathematical framework of inference is detailed in §4. We start by casting time-series analysis into the format of a regression problem, of the form y(x)=f(x)+η, in which f() is a (typically) unknown function and η is a (typically white) additive noise process. The goal of inference in such problems is twofold: firstly to evaluate the putative form of f() and secondly to evaluate the probability distribution of y* for some x*, i.e. p(y*|x*). To enable us to perform this inference, we assume the existence of a dataset of observations, typically obtained as input–output pairs,
for example. For the purposes of this study, we make the tacit assumption that the inputs xi (representing, e.g. time locations of samples) are known precisely, i.e. there is no input noise, but that observation noise is present on the yi. When we come to analyse time-series data, there are two approaches we might consider. The first function mapping and the second curve fitting. The mapping approach considers inference of a function f that maps some observed x to an outcome variable y without explicit reference to the (time) ordering of the data. For example, if we choose x to be a datum in a time series, and y to be the next datum, then inferring f(x) models the relationship between one datum and its successor. Problematically, the mapping is (typically) static, so poorly models non-stationary time series, and there is difficulty in incorporating time-series domain knowledge, such as beliefs about smoothness and continuity. Furthermore, if the periods between samples are uneven, this approach fails to accommodate this knowledge with ease. Curve fitting, on the other hand, makes the tacit assumption that y is ordered by x, the latter normally taken to be the time variable, with inference proceeding by fitting a curve to the set of x,y points. Prediction, for example, is thence achieved by extrapolating the curve that models the observed past data. The relationship between x and y is hence not fixed, but conditioned on observed data that (typically) lies close, in time, to the point we are investigating. In this study, we make the decision to concentrate on this approach, as we believe it offers a more profound model for much of the time-series data we are concerned with. As a simple example to introduce the canonical concepts of Bayesian modelling, we consider a small set of data samples, located at x=0,1,2, and associated observed target values. Least-squares regression on this data using a simple model (based on polynomial splines) gives rise to the curve shown as the line in figure 1a. We see that, naturally, this curve fits our observed data very well. What about the credibility of the model in regions where we see no data, importantly x>2? If we look at a larger set of example curves from the same model, we obtain a family of curves that explains the observed data identically yet differ very significantly in regions where we have no observations, both interpolating between sample points, and in extrapolation. This simple example leads naturally to us considering a distribution of curves. Working with some distribution over the curves, each of which offers an explanation for the observed data, is central to Bayesian modelling. We note that curves which lie towards the edges of this distribution have higher average curvature than those which lie close to the middle. In the simple example under consideration, there is an intimate relationship between curvature, complexity and Bayesian inference, leading naturally to posterior beliefs over models being a combination of how well observed data are explained and how complex the explanatory functions are. This elegant formalism encodes in a single mathematical framework such ideas as Occam’s razor, such that simple explanations of observed data are favoured.
A simple example of curve fitting. (a) The least-squares fit of a simple spline to the observed data (circles). (b) Example curves with identical fit to the data as the least-squares spline. These curves have high similarity close to the data yet high variability in regions of no observations, both interpolating and, importantly for time series, as we extrapolate beyond x=2. (Online version in colour.) The simple example above showed that there are many functions which can equally well explain data that we have observed. How should we choose from the bewildering array of mathematical functions that give rise to such explanatory curves? If we have strong prior knowledge regarding a system, then this (infinite-dimensional) function space may be reduced to a single family; perhaps the family of quartic polynomials may be the right choice. Such models are considered to be parametric, in the sense that a finite number of unknown parameters (in our polynomial example, these are the coefficients of the model) need to be inferred as part of the data modelling process. Although there is a very large literature (rightly so) on such parametric modelling methods, there are many scenarios in which we have little, or no, prior knowledge regarding appropriate models to use. We may, however, have seemingly less specific domain knowledge; for example, we may know that our observations are visible examples from an underlying process that is smooth, continuous and variations in the function take place over characteristic time scales (not too slowly yet not so fast) and have typical amplitude. Surprisingly, we may work mathematically with the infinite space of all functions that have these characteristics. Furthermore, we may even contemplate probability distributions over this function space, such that the work of modelling, explaining and forecasting data is performed by refining these distributions, so focusing on regions of the function space that are excellent contenders to model our data. As these functions are not characterized with explicit sets of parameters to be inferred (unlike our simple polynomial example, in which sets of coefficients need to be evaluated), this approach is referred to as a branch of non-parametric modelling.1 As the dominant machinery for working with these models is that of probability theory, they are often referred to as Bayesian non-parametric models. We now focus on a particular member, namely the GP. We start this introduction to GPs by considering a simple two-variable Gaussian distribution, which is defined for variables x1,x2 say, by a mean and a 2×2 covariance matrix, which we may visualize as a covariance ellipse corresponding to equal probability contours of the joint distribution p(x1,x2). Figure 2 shows an example of a two-dimensional distribution as a series of elliptical contours. The corresponding marginal distributions p(x1) and p(x2) are shown as ‘projections’ of this along the x1 and x2 axes (solid black lines). We now consider the effect of observing one of the variables such that, for example, we observe x1 at the location of the dashed vertical line in the figure. The resultant conditional distribution p(x2|x1=known), indicated by the dash-dotted curve, now deviates significantly from the marginal p(x2). Because of the relationship between the variables implied by the covariance, knowledge of one shrinks our uncertainty in the other.
The conceptual basis of GPs starts with an appeal to simple multi-variate Gaussian distributions. A joint distribution (covariance ellipse) forms marginal distributions p(x1),p(x2) that are vague (black solid line). Observing x1 at a value indicated by the vertical dashed line changes our beliefs about x2, giving rise to a conditional distribution (black dashed-dot line). Knowledge of the covariance lets us shrink uncertainty in one variable, based on observation of the other. (Online version in colour.) To see the intimate link between this simple example and time-series analysis, we represent the same effect in a different format. Figure 3 shows the mean (black line) and ±σ (grey-shaded region) for p(x1) and p(x2). Figure 3a depicts our initial state of ignorance and figure 3b after we observe x1. Note how the observation changes the location and uncertainty of the distribution over x2. Why stop at only two variables? We can extend this example to arbitrarily large numbers of variables, the relationships between which are defined by an ever larger covariance. Figure 4 shows the posterior distribution for a 10 day example in which observations are made at locations 2, 6 and 8. Figure 4a shows the posterior mean and ±σ as in our previous examples. Figure 4b extends the posterior distribution evaluation densely in the same interval (here, we evaluate the distribution over several hundred points). We note that the ‘discrete’ distribution is now rather continuous. In principle, we can extend this procedure to the limit in which the locations of the xi are infinitely dense (here, on the real line) and so the infinite joint distribution over them all is equivalent to a distribution over a function space. In practice, we will not need to work with such infinite spaces, it is sufficient that we can choose to evaluate the probability distribution over the function at any location on the real line and that we incorporate any observations we may have at any other points. We note, crucially, that the locations of observations and points we wish to investigate the function are not constrained to lie on any predefined sample points; hence, we are working in continuous time with a GP.
The change in distributions on x1 and x2 is here presented in a form more familiar to time-series analysis. (a) The initial, vague, distributions (the black line showing the mean and the grey shading ±σ) and (b) subsequent to observing x1. The distribution over x2 has become less uncertain and the most-likely ‘forecast’ of x2 has also shifted. (a) The posterior distribution (the black line showing the mean and the grey shading ±σ) for a 10 day example, with observations made at locations 2, 6 and 8. (b) Evaluates the posterior densely in the interval [1,10] showing how arbitrarily dense evaluation gives rise to a ‘continuous’ posterior distribution with time. As we have seen, the covariance forms the beating heart of GP inference. How do we formulate a covariance over arbitrarily large sets? The answer lies in defining a covariance kernel function, k(xi,xj), which provides the covariance element between any two (arbitrary) sample locations, xi and xj say. For a set of locations x={x1,x2,…,xn}, we hence may define the covariance matrix as
3.1
This means that the entire function evaluation, associated with the points in x, is a draw from a multi-variate Gaussian distribution,
3.2
where y={y1,y2,…,yn} are the dependent function values, evaluated at locations x1,…,xn, and μ is a mean function, again evaluated at the locations of the x variables (which we will briefly revisit later). If we believe that there is noise associated with the observed function values, yi, then we may fold this noise term into the covariance. As we expect noise to be uncorrelated from sample to sample in our data, so the noise term adds only to the diagonal of K, giving a modified covariance for noisy observations of the form
3.3
where I is the identity matrix and σ2 is a hyperparameter representing the noise variance. How do we evaluate the GP posterior distribution at some test datum, x* say? We start with considering the joint distribution of the observed data
(consisting of x and associated values y) augmented by x* and y*,
3.4
where K(x,x*) is the column vector formed from k(x1,x*),…,k(xn,x*) and K(x*,x) is its transpose. We find, after some manipulation, that the posterior distribution over y* is Gaussian with mean and variance given by
3.5
and
3.6
We may readily extend this to infer the GP at a set of locations outside our observations, at x* say, to evaluate the posterior distribution of y(x*). The latter is readily obtained once more by extending the above equations and using standard results for multi-variate Gaussians. We obtain a posterior mean and variance given by
3.7
where
3.8
and
3.9
in which we use the shorthand notation for the covariance, K(a,b), defined as
3.10
If we believe (and in most situations we do) that the observed data are corrupted by a noise process, we would replace the K(x,x) term above with, for example, V(x,x) from equation (3.3) above. What should the functional form of the kernel function k(xi,xj) be? To answer this, we will start by considering what the covariance elements indicate. In our simple two-dimensional example, the off-diagonal elements define the correlation between the two variables. By considering time series in which we believe the informativeness of past observations, in explaining current data, is a function of how long ago we observed them, we then obtain stationary covariance functions that are dependent on |xi−xj|. Such covariance functions can be represented as the Fourier transform of a normalized probability density function (via Bochner’s theorem [1]); this density can be interpreted as the spectral density of the process. The most widely used covariance function of this class is arguably the squared exponential (SE) function, given by
3.11
In equation (3.11), we see two more hyperparameters, namely h,λ, which respectively govern the output scale of our function and the input, or time, scale. The role of inference in GP models is to refine vague distributions over many, very different curves, to more precise distributions that are focused on curves that explain our observed data. As the form of these curves is uniquely controlled by the hyperparameters, so, in practice, inference proceeds by refining distributions over them. As h controls the gain, or magnitude, of the curves, we set this to h=1 to generate figure 5, which shows curves drawn from a GP (with an SE covariance function) with varying λ=0.1,1,10 (figure 5a–c). The important question of how we infer the hyperparameters is left until later in this paper, in §4. We note that to be a valid covariance function, k(), implies only that the resultant covariance matrix, generated using the function, is guaranteed to be positive (semi-)definite. As a simple example, figure 6a shows a small sample of six observed data points, shown as dots, along with error bars associated with each. The seventh datum, with ‘?’ beneath it, is unobserved. We fit a GP with the SE covariance kernel (equation (3.11)). Figure 6b shows the GP posterior mean (black curve) along with ±2σ (the posterior standard deviation). Although only a few samples are observed, corresponding to the set of x,y of equations (3.8) and (3.9), we here evaluate the function on a fine set of points, evaluating the corresponding y* posterior mean and variance using these equations and hence providing interpolation between the noisy observations (this explains the past) and extrapolation for x*>0 that predicts the future. In this simple example, we have used a ‘simple’ covariance function. As the sum of valid covariance functions is itself a valid covariance function (more on this in §3c(i) later) so we may entertain more complex covariance structures, corresponding to our prior belief regarding the data. Figure 7 shows GP modelling of observed (noisy) data for which we use slightly more complex covariances. Figure 7a shows data modelled using a sum of SE covariances, one with a bias towards shorter characteristic time scales than the other. We see how this combination elegantly allows us to model a system with both long- and short-term dynamics. Figure 7b uses an SE kernel, with bias towards longer time-scale dynamics, along with a periodic component kernel (which we will discuss in more detail in §3c(i)). Note here how extrapolation outside the data indicates a strong posterior belief regarding the continuance of periodicity.
(a–c) Functions drawn from a GP with a squared exponential covariance function with output scale h=1 and length scales λ=0.1 (a), 1 (b), 10 (c). (Online version in colour.) (a) Given six noisy data points (error bars are indicated with vertical lines), we are interested in estimating a seventh at x*=0.2. (b) The solid line indicates an estimation of y* for x* across the range of the plot. Along with the posterior mean, the posterior uncertainty, ±2σ, is shaded. (Online version in colour.) (a) Estimation of y* (solid line) and ±2σ posterior deviance for a function with short-term and long-term dynamics, and (b) long-term dynamics and a periodic component. Observations are shown as pluses. As in the previous example, we evaluate the posterior GP over an extended range to show both interpolation and extrapolation. (Online version in colour.) We start by considering a simple example, shown in figure 8. Figure 8a shows a set of data points and the GP posterior distribution excluding observation of the right-most datum (darker shaded point). Figure 8b depicts the same inference including this last datum. We see how the posterior variance shrinks as we make the observation. The previous example showed how making an observation, even of a noisy time series, shrinks our uncertainty associated with beliefs about the function local to the observation. We can see this even more clearly if we successively extrapolate until we see another datum, as shown in figure 9. Rather than observations coming on a fixed time-interval grid, we can imagine a scenario in which observations are costly to acquire, and we wish to find a natural balance between sampling and reducing uncertainty in the functions of interest. This concept leads us naturally in two directions. Firstly, for the active requesting of observations when our uncertainty has grown beyond acceptable limits (of course these limits are related to the cost of sampling and observation and the manner in which uncertainty in the time series can be balanced against this cost) and secondly to dropping previously observed samples from our model. The computational cost of GPs is dominated by the inversion of a covariance matrix (as in equation (3.9)) and hence scales with the cube of the number of retained samples. This leads to an adaptive sample retention. Once more, the balance is problem specific, in that it relies on the trade-off between computational speed and (for example) forecasting uncertainty. The interested reader is pointed to Osborne et al. [2] for more detailed discussions. We provide some examples of active data selection in operation in real problem domains later in this study.
A simple example of a GP applied sequentially. (a) The posterior mean and ±2σ prior to observing the right-most datum (darker shaded) and (b) after observation. (Online version in colour.) The GP is run sequentially making forecasts until a new datum is observed. Once we make an observation, the posterior uncertainty drops to zero (assuming noiseless observations). (Online version in colour.) The prior mean of a GP represents whatever we expect for our function before seeing any data. The covariance function of a GP specifies the correlation between any pair of outputs. This can then be used to generate a covariance matrix over our set of observations and predictants. Fortunately, there exist a wide variety of functions that can serve in this purpose [3,4], which can then be combined and modified in a further multitude of ways. This gives us a great deal of flexibility in our modelling of functions, with covariance functions available to model periodicity, delay, noise and long-term drifts and other phenomena. In the following section, we briefly describe commonly used kernels. We start with simple white noise, and then consider common stationary covariances, both uni- and multi-dimensional. We finish this section with periodic and quasi-periodic kernel functions. The interested reader is referred to Rasmussen & Williams [1] for more details. Although the following list is not exclusive by any means, it provides details for most of the covariance functions suitable for analysis of time series. We note once more that sums (and products) of valid covariance kernels give valid covariance functions (i.e. the resultant covariance matrices are positive (semi-)definite) and so we may entertain with ease multiple explanatory hypotheses. The price we pay lies in the extra complexity of handling the increased number of hyperparameters.  White noise with variance σ2 is represented by
3.12
This kernel allows us to entertain uncertainty in our observed data and is so typically found added to other kernels (as we saw in equation (3.3)).  The SE kernel is given by
3.13
where h is an output-scale amplitude and λ is an input (length, or time) scale. This gives rather smooth variations with a typical time scale of λ and admits functions drawn from the GP that are infinitely differentiable.  The rational quadratic (RQ) kernel is given by
3.14
where α is known as the index. Rasmussen & Williams [1] show that this is equivalent to a scale mixture of SE kernels with different length scales, the latter distributed according to a Beta distribution with parameters α and λ−2. This gives variations with a range of time scales, the distribution peaking around λ but extending to significantly longer period (but remaining rather smooth). When , the RQ kernel reduces to the SE kernel with length scale λ.  The Matérn class of covariance functions is defined by
3.15
where h is the output scale, λ is the input scale, Γ() is the standard Gamma function and
is the modified Bessel function of second order. The additional hyperparameter ν controls the degree of differentiability of the resultant functions modelled by a GP with a Matérn covariance function, such that they are only
times differentiable. As
so the functions become infinitely differentiable and the Matérn kernel becomes the SE one. Taking
gives the exponential kernel
3.16
which results in functions that are only once differentiable, and correspond to the Ornstein–Ulenbeck process, the continuous time equivalent of a first-order autoregressive model, AR(1). Indeed, as discussed in Rasmussen & Williams [1], time-series models corresponding to AR(p) processes are discrete time equivalents of GP models with Matérn covariance functions with .  Multiple inputs and outputs. The simple distance metric, |x1−x2|, used thus far clearly allows only for the simplest case of a one-dimensional input x, which we have hitherto tacitly assumed to represent a time measure. In general, however, we assume our input space has finite dimension and write x(e) for the value of the eth element in x and denote
as the value of the eth element at the ith index point. In such scenarios, we entertain multiple exogenous variables. Fortunately, it is not difficult to extend covariance functions to allow for these multiple input dimensions. Perhaps the simplest approach is to take a covariance function that is the product of one-dimensional covariances over each input (the product correlation rule [5]),
3.17
where k(e) is a valid covariance function over the eth input. As the product of covariances is a covariance, so equation (3.17) defines a valid covariance over the multi-dimensional input space. We can also introduce distance functions appropriate for multiple inputs, such as the Mahalanobis distance,
3.18
where Σ is a covariance matrix over the input variable vector x. Note that this is a matrix which represents hyperparameters of the model, and should not be confused with covariances formed from covariance functions (which are always denoted by K in this study). If Σ is a diagonal matrix, its role in equation (3.18) is simply to provide an individual input scale
for the eth dimension. However, by introducing off-diagonal elements, we can allow for correlations among the input dimensions. To form the multi-dimensional kernel, we simply replace the scaled distance measure |xi−xj|/λ of, e.g. equation (3.13) with d(M)(x1,x2) from equation (3.18) above. For multi-dimensional outputs, we consider a multi-dimensional space consisting of a set of time series along with a label l, which indexes the time series, and x denoting time. Together, these thence form the two-dimensional set of [l,x]. We will then exploit the fact that a product of covariance functions is a covariance function in its own right, and write
taking covariance function terms over both time and time-series label. If the number of time series is not too large, we can arbitrarily represent the covariance matrix over the labels, using the spherical decomposition [6]. This allows us to represent any covariance structure over the labels. More details of this approach, which enables the dependencies between time series to be modelled, are found in Roberts et al. [7], and we use this as the focus of one of our examples in §5.  Periodic and quasi-periodic kernels. Note that a valid covariance function under any arbitrary (smooth) map remains a valid covariance function [8,1]. For any function , a covariance function k() defined over the range of x gives rise to a valid covariance k′() over the domain of u. Hence, we can use simple, stationary covariances in order to construct more complex (possibly non-stationary) covariances. A particularly relevant example of this,
3.19
allows us to modify our simple covariance functions above to model periodic functions. We can now take this covariance over u as a valid covariance over x. As a result, we have the covariance function, for the example of the SE (3.13),
3.20
In this case, the output scale h serves as the amplitude and T is the period. The hyperparameter w is a ‘roughness’ parameter that serves a role similar to the input scale λ in stationary covariances. With this formulation, we can perform inference about functions of arbitrary roughness and with arbitrary period. Indeed a periodic covariance function can be constructed from any kernel involving the squared distance (xi−xj)2 by replacing the latter with , where T is the period. The length scale w is now relative to the period, and letting
gives sinusoidal variations, while increasingly small values of w give periodic variations with increasingly complex harmonic content. Similar periodic functions could be constructed from any kernel. Other periodic functions could also be used, so long as they give rise to a symmetric, positive definite covariance matrix –
is merely the simplest. As described in Rasmussen & Williams [1], valid covariance functions can be constructed by adding or multiplying simpler covariance functions. Thus, we can obtain a quasi-periodic kernel simply by multiplying a periodic kernel with one of the basic stationary kernels described earlier. The latter then specifies the rate of evolution of the periodic signal. For example, we can multiply equation (3.20) with an SE kernel,
3.21
to model a quasi-periodic signal with a single evolutionary time scale λ. Examples of functions drawn from these kernels are shown in figure 10. There are many more types of covariance functions in use, including some (such as the Matérn family above) that are better suited to model rougher, less smooth variations. However, the SE and RQ kernels already offer a great degree of freedom with relatively few hyperparameters, and covariance functions based on these are widely used to model time-series data.
Random draws from GPs with different kernels. (a) Shows the SE kernel (equation (3.13), with h=1, λ=1), (b) the RQ (equation (3.14), with h=1, λ=1 and α=0.5) and (c) a periodic kernel based on the SE (equation (3.20), with h=1, T=2 and w=0.5). (d) Shows a quasi-periodic kernel constructed by multiplying the periodic kernel of equation (3.13) (with h=1, T=2, w=1) with the RQ kernel of equation (3.14) (with λ=4 and α=0.5). (e,f) Show noisy versions of this kernel obtained by adding, respectively, a white noise term (equation (3.13), with σ=0.2) and an SE term (equation (3.13), with h=0.1, λ=0.1). Each line consists of equally spaced samples over the interval [−5,5], and is offset from the previous one by 3 for clarity. The random number generated was initiated with the same seed before generating the samples shown in each panel. (Online version in colour.)  Changepoints. We now describe how to construct appropriate covariance functions for functions that experience sudden changes in their characteristics. This section is meant to be expository; the covariance functions we describe are intended as examples rather than an exhaustive list of possibilities. To ease exposition, we assume the (single) input variable of interest, x, represents time. If additional features are available, they may be readily incorporated into the derived covariances [1]. A drastic change in covariance: we start by considering a function of interest as well behaved, except for a drastic change at the point xc, which separates the function into two regions with associated covariance functions k1(⋅,⋅;θ1) before xc and k2(⋅,⋅;θ2) after, where θ1 and θ2 represent the values of any hyperparameters associated with k1 and k2, respectively. The change is so drastic that the observations before xc are completely uninformative about the observations after the changepoint. The full set of hyperparameters for this covariance function are hence the hyperparameters of the two covariance functions as well as the location of the changepoint, xc. This covariance function is easily seen to be semi-positive definite and hence admissible [9,10]. The covariance function, and an example draw from the GP associated with it, are presented in the left-most plots of figure 11.
Example covariance functions (a) for the modelling of data with changepoints and associated draws (b) from the resultant GPs, indicating what kind of data that they might be appropriate for. Each changepoint covariance function is drawn as a bold line, with the standard SE kernel shown as kSE for comparison (thin line). For ease of comparison, we fix the location of the changepoint hyperparameter to xc=500 and plot the functions over the interval from 460≤x≤560. (Online version in colour.) A drastic change in covariance with constraints: suppose a continuous function of interest is best modelled by different covariance functions, before and after a changepoint xc. The function values after the changepoint are conditionally independent of the function values before, given the value at the changepoint itself. This represents an extension to the drastic covariance described earlier; our two regions can be drastically different, but we can still enforce continuity and smoothness constraints across the boundary between them. We call this covariance function the continuous conditionally independent covariance function. This covariance function can be extended to multiple changepoints, boundaries in multi-dimensional spaces, and also to cases where function derivatives are continuous at the changepoint. For proofs and details of this covariance function, the reader is invited to see Osborne et al. [10] and Reece et al. [11]. A sudden change in input scale: suppose a function of interest is well behaved, except for a drastic change in the input scale λ at time xc, which separates the function into two regions with different degrees of long-term dependence. We let λ1 and λ2 represent the input scale of the function before and after the changepoint at xc, respectively. The hyperparameters of this covariance consist of the two input scales, λ1,λ2 along with a common output scale, h, and the changepoint location, xc. The second panel in figure 11 shows an example covariance function of this form (figure 11a) and an example function (figure 11b). A sudden change in output scale: we now consider a function of interest as well behaved, except for a drastic change in the output scale h at time xc, which separates the function into two regions. As before, we let h1 and h2 represent the output scales before and after the changepoint at xc. The full set of hyperparameters of this model consists of the two output scales, h1,h2, a common input scale, λ, and the location of the changepoint, xc. The third panel of figure 11 shows an example covariance and associated example function. We note that we may readily combine changes in input and output scale into a single changepoint covariance (an example of which is shown in the right-most plots of figure 11). A change in observation likelihood: hitherto, we have taken the observation likelihood as being defined by a single GP. We now consider other possible observation models, motivated by fault detection and removal [11,12]. For example, a sensor fault implies that the relationship between the underlying process model and the observed values is temporarily corrupted. In situations where a model of the fault is known, the faulty observations need not be discarded; they may still contain valuable information about the plant process. The interested reader is directed to Reece et al. [11,12], in which covariances for biased readings, stuck sensors and sensor drifts are discussed. As the mean function will dominate our forecasts in regions far from the data, the choice of the prior mean function can have a profound impact on our predictions and must be chosen with this in mind. In the majority of cases in the literature, we find vague (i.e. high uncertainty) flat mean functions used. This choice is reinforced by considering the prior mean function as the expectation function, prior to any observed data, of our domain beliefs. In the vast majority of situations, the symmetry of our ignorance (i.e. we are equally unsure that a trend is up or down) leads to flat, often zero-offset, mean functions. As a simple example, we may have domain knowledge that our functions have a linear drift term, but we do not know the magnitude or direction. Whatever prior we place over the gradient of the drift will be necessarily symmetric and leads to a zero mean with variance defined by the vagueness of our priors. If we do have such domain knowledge, then we are free to incorporate this into our GP models. For example, consider the case in which we know that the observed time series consists of a deterministic component and an unknown additive component. Draws from our GP are hence
3.22
in which the mean function, m, has hyperparameters θm that encode domain knowledge regarding the deterministic component and the covariance matrix K has hyperparameters θC. For example, we may know that our observations are obtained from an underlying exponential decay with an unknown additive function along with coloured noise. Our mean function will hence be of the form
where A,a are unknown hyperparameters. Figure 12a shows a standard SE covariance GP used to model a small set of noisy data samples (dots) drawn from a function with an underlying exponential decay. The GP models the observed data well, but long-term predictions are naturally dominated by a flat prior mean function. In figure 12b, a GP with identical covariance is used, but the mean function is that of an exponential decay with unknown hyperparameters. Even a few data points are sufficient for the exponential function hyperparameters to be inferred, leading to long-term forecasts that are dominated by a (albeit uncertain) decay function.
The effect of including a simple mean function. (a) A GP model with a flat prior mean and SE covariance function. The noisy observations are indicated by dots. The posterior from the GP is shown, along with ±2σ. (b) The same covariance function is used, but now the mean function has extra hyperparameters corresponding to an exponential decay with unknown time constant and scale. We see that the long-term forecasts in this example encode our prior belief in the decay function. (Online version in colour.) GP models have a number of hyperparameters (owing to both the covariance and mean functions) that we must marginalize2 in order to perform inference. That is, we must first assign a prior p(θ) to these hyperparameters, informed by our domain knowledge. For example, in assigning a prior to the period of a tidal signal (as in §5a), we would use a prior that expressed that the most important period was of the order of days, rather than nanoseconds or millenia. In the absence of hard domain knowledge, these priors are chosen to be diffuse: for example, a Gaussian with high variance. Then, the quantity we are interested in is
4.1
which requires two integrals to be evaluated. These are both typically non-analytic, owing to the complex form of the likelihood
when considered as a function of hyperparameters θ. As such, we are forced to resort to approximate techniques. Approximating an integral requires two problems to be solved. First, we need to make observations of the integrand, to explore it, and then those observations need to be used to construct an estimate for the integral. There are a number of approaches to both problems. Optimizing an integrand (figure 13) is one fairly effective means of exploring it: we will take samples around the maxima of the integrand, which are likely to describe the majority of the mass comprising the integral. A local optimizer, such as a gradient ascent algorithm, will sample the integrand around the peak local to the start point, giving us information pertinent to at least that part of the integrand. If we use a global optimizer, our attempts to find the global extremum will ultimately result in all the integrand being explored, as desired.
Samples (dots) obtained by optimizing the log-likelihood (grey curve) using a global optimizer, and the maximum-likelihood approximation (vertical line) of the likelihood surface. (Online version in colour.) Maximizing an integrand is most common when performing maximum likelihood. The integrands in (4.1) are proportional to the likelihood : if the prior p(θ) is relatively flat, the likelihood will explain most of the variation of the integrands as a function of θ. Maximizing the likelihood hence gives a reasonable means of integrand exploration, as above. Maximum likelihood, however, specifies a generally unreasonable means of integral estimation: the likelihood is approximated as a Dirac delta function located at the θ that maximized the likelihood. As per figure 13, this completely ignores the width of the integrands, leading to potentially problematic features [13]. This approximation finds use when the likelihood is very peaked, as is the case when we have a great deal of data. A slightly more sophisticated approach to integral estimation is to take a Laplace approximation, which fits a Gaussian around the maximum-likelihood peak. This gives at least some representation of the width of the integrands. Yet further sophistication is displayed by the methods of variational Bayes [14], which treat the fitting of probability distributions to the problematic terms in our integrands as an optimization problem. Monte Carlo techniques represent a very popular means of exploring an integrand. Simple Monte Carlo techniques draw random samples from the prior p(ϕ), to which our integrands are proportional. Note that (4.1) can be rewritten as
4.2
More sophisticated Markov chain Monte Carlo techniques [15] attempt to generate samples from the hyperparameter posterior
4.3
to which (4.2) is proportional (figure 14 illustrates samples drawn using such a method). Sampling in this way ensures that we have many samples where the prior/posterior is large, and hence, where our integrands are likely to be large. This is a particular concern for multi-dimensional integrals, where the problem is complicated by the ‘curse of dimensionality’ [16]. Essentially, the volume of space that could potentially be explored is exponential in its dimension. However, a probability distribution, which must always have a total probability mass of one, will be highly concentrated in this space, ensuring our samples are likewise concentrated is a great boon. Moreover, Monte Carlo sampling ensures a non-zero probability of obtaining samples from any region where the prior is non-zero. This means that we can achieve some measure of broader exploration of our integrands.
Samples obtained by taking draws from the posterior using a Markov chain Monte Carlo method. Monte Carlo, does not, however, provide a very satisfactory means of integral estimation: it simply approximates the integral as the average over the obtained samples. As discussed by O’Hagan [17], this ignores the information content contained in the locations of the samples, leading to unsatisfactory behaviour. For example, imagine that we had three samples, two of which were identical: θ1=θ2. In this case, the identical value will receive two-thirds of the weight, whereas the equally useful other value will receive only one-third. This is illustrated in figure 15.
A set of samples that would lead to unsatisfactory behaviour from simple Monte Carlo. In an attempt to address these issues, Bayesian quadrature [18,19] provides a model-based means of integral estimation. This approach assumes GPs over the integrands, using the obtained samples to determine a distribution for the integrals (figure 16). This probabilistic approach means that we can use the obtained variance in the integral as a measure of our confidence in its estimate. Of course, we still need to determine the hyperparameters for the GPs over the integrands. This problem is solved by adopting simple covariance functions for these GPs and using maximum likelihood to fit their hyperparameters (the maximum-likelihood output scale even has a closed-form solution). This renders the approach computationally tractable, to complement its superior accuracy.
Bayesian quadrature fits a GP to the integrand, and thereby performs inference about the integral. (Online version in colour.) In the examples to follow, we will exclusively use Bayesian quadrature to marginalize the hyperparameters of our GP models. Where desired, similar techniques are also used to calculate the posteriors for such hyperparameters
4.4
Where the posterior for a hyperparameter is highly concentrated around a particular value, we will informally describe the hyperparameter as having been learned as having that value. In the following examples, we briefly illustrate the GP approach to practical time-series analysis, highlighting the use of a variety of covariance and mean functions. The first example we provide is based on real-time data that are collected by a set of weather, sea state and environment sensors on the south coast of the UK (see Roberts et al. [7] for more details). The network (Bramblemet) consists of four sensors (named Bramblemet, Sotonmet, Cambermet and Chimet), each of which measures a range of environmental variables (including wind speed and direction, air temperature, sea temperature and tide height) and makes up-to-date sensor measurements. We have two data streams for each variable at our disposal. The first is the real-time, but sporadic, measurements of the environmental variables; it is these that are presented as a multi-dimensional time series to the GP. Second we have access, retrospectively, to finer-grained data. We use this latter dataset for assessment only. Figure 17 illustrates the efficacy our GP prediction for a tide height dataset. In order to manage the four outputs of our tide function (one for each sensor), we rewrite so that we have a single output and inputs t, time, and l, a sensor label, as discussed in §3a and in §3c.
Prediction and regression of tide height data for (a) independent and (b) multi-output GPs. (Online version in colour.) Note that our covariance over time is the sum of a periodic term and a disturbance term. Both are of the Matérn form with . This form is a consequence of our expectation that the tides would be well modelled by the superposition of a simple periodic signal and an occasional disturbance signal due to exceptional conditions. Of course, for a better fit over the course of, say, a year, it would be possible to additionally incorporate longer term drifts and periods. The period T of the periodic covariance term was unsurprisingly learnt as being about half a day, whereas for the disturbance term, the time scale w was found to be about two and a half hours. Note that this latter result is concordant with our expectations for the time scales of the weather events we intend our disturbance term to model. Our algorithm learned that all four sensors were very strongly correlated, with spherical decomposition of the inferred correlation elements all very close to one. The hyperparameter matrix Σ of equation (3.18) (which defines relationships between variables) additionally gives an appropriate length scale for each sensor. From this inference, we determined weather events to have induced changes in tide height of the order of 20 per cent. We also make allowances for the prospect of relative latency among the sensors by incorporating delay variables, introduced by a vector of delays in time observations [7]. We found that the tide signals at the Cambermet and Chimet stations were delayed by about 10 minutes relative to the other two. This makes physical sense—the Bramblemet and Sotonmet stations are located to the west of the Cambermet and Chimet stations, and the timing of high tide increases from west to east within the English channel. Note the performance of our multi-output GP formalism when the Bramblemet sensor drops out at t=1.45 days. In this case, the independent GP quite reasonably predicts that the tide will repeat the same periodic signal it has observed in the past. However, the GP can achieve better results if it is allowed to benefit from the knowledge of the other sensors’ readings during this interval of missing data. Thus, in the case of the multi-output GP, by t=1.45 days, the GP has successfully determined that the sensors are all very strongly correlated. Hence, when it sees an unexpected low tide in the Chimet sensor data (caused in this case by the strong northerly wind), these correlations lead it to infer a similarly low tide in the Bramblemet reading. Hence, the multi-output GP produces significantly more accurate predictions during the missing data interval, with associated smaller error bars. Exactly the same effect is seen in the later predictions of the Chimet tide height, where the multi-output GP predictions use observations from the other sensors to better predict the high tide height at t=2.45 days. Note also that there are two brief intervals of missing data for all sensors just after both of the first two peak tides. During the second interval, the GP’s predictions for the tide are notably better than for the first—the greater quantity of data it has observed allows it to produce more accurate predictions. With time, the GP is able to build successively better models for the series. The predictive performances for our various algorithms over this dataset can be found in table 1. For the Kalman filter comparison, a history length of 16 observations was used to generate each prediction because this gave rise to the best predictive ability for the Kalman model on out-of-sample data. However, note that our multi-output GP, which exploits correlations between the sensors, and the periodicity in each individual sensors’ measurements, significantly outperforms both the Kalman filter and the independent GP [7]. The naive result is obtained by repeating the last observed sensor value as a forecast and is included as a baseline only.
Predictive performances for 5 day Bramblemet tide height dataset. We note the superior performance of the GP compared with a more standard Kalman filter model. Error metrics shown are root mean square error (r.m.s.e) and normalized mean square error (n.m.s.e.), which is presented on a logarithmic, decibel scale. We now demonstrate our active data selection algorithm. Using the fine-grained data (downloaded directly from the Bramblemet weather sensors), we can simulate how our GP would have chosen its observations had it been in control. Results from the active selection of observations from all four tide sensors are displayed in figure 18. Again, these plots depict dynamic choices; at time t, the GP must decide when next to observe, and from which sensor, given knowledge only of the observations recorded prior to t, in an attempt to maintain the uncertainty in tide height below 10 cm. The covariance function used was that described in the previous example, namely a sum of two
Matérn covariance functions, one stationary and the other of periodic form. Consider first the case shown in figure 18a, in which separate independent GPs are used to represent each sensor. Note that a large number of observations are taken initially as the dynamics of the sensor readings are learnt, followed by a low but constant rate of observation. By contrast, for the multi-output case shown in figure 18b, the GP is allowed to explicitly represent correlations and delays between the sensors. As mentioned earlier, this dataset is notable for the slight delay of the tide heights at the Chimet and Cambermet sensors relative to the Sotonmet and Bramblemet sensors, due to the nature of tidal flows in the area. Note that after an initial learning phase as the dynamics, correlations and delays are inferred, the GP chooses to sample predominantly from the undelayed Sotonmet and Bramblemet sensors. The dynamics of the tide height at the Sotonmet sensor are more complex than the other sensors owing to the existence of a ‘young flood stand’ and a ‘double high tide’ in Southampton. For this reason, the GP selects Sotonmet as the most informative sensor and samples it most often. Despite no observations of the Chimet sensor being made within the time span plotted, the resulting predictions remain remarkably accurate. Consequently, only 119 observations are required to keep the uncertainty below the specified tolerance, whereas 358 observations were required in the independent case. This represents another clear demonstration of how our prediction is able to benefit from the readings of multiple sensors.
(a) Comparison of active sampling of tide data using independent and (b) multi-output GPs. Note that, in the case of multi-output GPs, one sensor reading (Sotonmet) slightly leads the other readings and is hence sampled much more frequently. In some cases, such as the Cambermet readings, only occasional samples are taken, yet the GP forecasts are excellent. (Online version in colour.) In Garnett et al. [9] and Osborne et al. [10], a fully Bayesian framework was introduced for performing sequential time-series prediction in the presence of changepoints. The position of a particular changepoint becomes a hyperparameter of the model that is marginalized using Bayesian quadrature. If the locations of changepoints in the data are of interest, the full posterior distribution of these hyperparameters can be obtained given the data. The result is a robust time-series prediction algorithm that makes well-informed predictions, even in the presence of sudden changes in the data. If desired, the algorithm additionally performs changepoint and fault detection as a natural by-product of the prediction process. In this section, we briefly present some exemplar datasets and the associated changepoint inference. We first consider a canonical changepoint dataset, the minimum water levels of the Nile river during the period AD 622–1284 [20]. Several authors have found evidence supporting a change in input scale for this data around the year AD 722 [21]. The conjectured reason for this changepoint is the construction in AD 715 of a new device (a ‘nilometer’) on the island of Roda, which affected the nature and accuracy of the measurements. We performed one-step (next datum) lookahead prediction on this dataset using the input-scale changepoint covariance discussed earlier. The results can be seen in figure 19. Figure 19a shows our one-step predictions on the dataset, including the mean and ±σ error bars. Figure 19b shows the posterior distribution of the number of years since the last changepoint. A changepoint around AD 720–722 is clearly visible and agrees with previous results [21].
Prediction for the Nile dataset using input-scale changepoint covariance (a) and the corresponding posterior distribution for time since the changepoint (b). (Online version in colour.) As a second canonical changepoint dataset, we present the series of daily returns of the Dow–Jones industrial average between 3 July 1972 and 30 June 1975 [22]. This period included a number of newsworthy events that had significant macroeconomic influences, as reflected in the Dow–Jones returns. We performed sequential one-step (next datum) prediction on this data using a GP with a diagonal covariance that assumed all measurements were independent and identically distributed (as under the efficient market hypothesis, returns should be uncorrelated). However, the variance of these observations was assumed to undergo changes, and as such, we used a covariance that incorporated such changes in output scale. We had three hyperparameters to marginalize: the variance before the changepoint, the variance after the changepoint and, finally, the location of that changepoint. Our results are plotted in figure 20. Our model clearly identifies the important changepoints that likely correspond to the commencement of the Organization of the Petroleum Exporting Countries embargo on 19 October 1973, and the resignation of Richard Nixon as President of the USA on 9 August 1974. A weaker changepoint is identified early in 1973, which Adams & MacKay [22] speculate is due to the beginning of the Watergate scandal.
Online (sequential) one-step predictions (a) and posterior for the location of changepoint for the Dow–Jones industrial average data using an output-scale changepoint covariance (b). (Online version in colour.) Many Sun-like stars display quasi-periodic brightness variations on time scales of days to weeks, with amplitudes ranging from a few parts per million to a few per cent. These variations are caused by the evolution and rotational modulation of magnetically active regions, which are typically fainter than the surrounding photosphere. In this case, we may expect a range of both periodic covariance scales w and evolutionary time scales λ, corresponding to different active region sizes and lifetimes, respectively. This can be achieved by replacing one or both of the SE kernels in equation (3.13) by RQ kernels (equation (3.14)). Finally, we can also allow for short-term irregular variability or correlated observational noise by including a separate, additive SE or RQ kernel. For example, Pont et al. [23] used a GP with such quasi-periodic kernels to model the total irradiance variations of the Sun in order to predict its radial velocity variations. In figure 21, we show the results of a quasi-periodic GP regression to photometric observations of the well-known planet host star HD 189733, taken from Henry & Winn [24]. The kernel used consists of a periodic SE component (equation (3.21)) multiplied by an RQ term (equation (3.14)) to allow for a range of evolutionary time scales, plus an additive white noise term (equation (3.12)). Inference over the hyperparameters of interest yielded expected values of h=6.68 mmag, T=11.86 days, w=0.91, α=0.23, λ=17.81 days and σ=2.1 mmag, where σ is the amplitude of the white noise term. Our period is in excellent agreement with Henry & Winn [24]. The relatively long periodic length scale w indicates that the variations are dominated by a small number of fairly large active regions. The evolutionary term has a relatively short time scale, λ, but a shallow index, α, which is consistent with the notion that the active regions on this star evolve relatively fast and/or that, as in the Sun, active regions located at different latitudes have different rotation rates (known as differential rotation).
Predictive distribution for a quasi-periodic GP model using a mixed SE and RQ kernel, trained and conditioned on observations made with the 0.8 m Automated Patrol Telescope [24] using the Strömgren b and y filters. The dots represent the observations, the line is the mean of the predictive posterior distribution and the shaded region encompasses the ±σ interval. (Online version in colour.) One of the most successful ways of discovering and characterizing extra-solar planets (i.e. planets not in our solar system) is through observing transit light curves. A transit occurs when a planet periodically passes between its host star and the Earth, blocking a portion of the stellar light, and produces a characteristic dip in the light curve. From this transit, we can measure such physical parameters as the planet-to-star radius ratio and the inclination of the orbit. While transit light curves are readily described by a deterministic parametric function, real observations are corrupted by systematic noise in the detector, external state variables (such as the temperature of the detector, orbital phase, position of the host star on the charge-coupled device array, etc.), as well as the underlying flux variability of the host star. As it is not possible to produce a deterministic model to account for all these systematics, a GP may be used to place a distribution over possible artefact functions, modelling correlated noise as well as subtle changes in observed light curves due to external state variables. We hence encode the transit curve as the mean function of a GP. The covariance function has inputs given by time and external state variables (hence, this is a multi-input, single-output model). By integrating out our uncertainty (see §4) in the hyperparameters of the GP (which model all the systematic artefacts and noise processes), we can gain much more realistic inference of probability distribution of the transit function parameters (the hyperparameters of the mean function). For a detailed discussion of the application of GPs to transit light curves, see Gibson et al. [25], in which the instrumental systematics are represented by a GP with an SE kernel (equation (3.13)) and input parameters representing the external state variables. Robust inference of transit parameters is required to perform detailed studies of transiting systems, including the search for atomic and molecular signatures in the atmospheres of exoplanets. Figure 22 shows this GP model fitting to the time series of observations. More details are found in Gibson et al. [25].
As an example of a complex mean function, we here model data from an exoplanet transit light curve. The data are fitted with a GP with an exoplanet transit mean function and a squared exponential covariance kernel to model the correlated noise process and the effects of external state variables. The shaded regions are at ±1,2σ from the posterior mean. (Online version in colour.) In this paper, we have presented a brief outline of the conceptual and mathematical basis of GP modelling of time series. As ever, a practical implementation of the ideas concerned requires jumping algorithmic rather than theoretical hurdles, which we do not discuss here because of space constraints. Some introductory code may be found at ftp://ftp.robots.ox.ac.uk/pub/outgoing/mebden/misc/GPtut.zip and more general code can be downloaded from http://www.gaussianprocess.org/gpml. Space has not permitted discussion of exciting recent trends in GP modelling that allow for more explicit incorporation of differential equations governing the system dynamics (either observed or not), such as latent force models [26]. Further extensions, using GPs as building blocks in more complex probabilistic models, are of course possible, and recent research has also highlighted the use of GPs for numerical integration, global optimization, mixture-of-experts models, unsupervised learning models and much more. The authors thank Alex Rogers, Roman Garnett, Richard Mann, Tom Evans, Mark Smith and Chris Hart. Part of this work was funded by the UK research councils, whose support is gratefully acknowledged. One contribution of 17 to a Discussion Meeting Issue ‘Signal processing and inference for the physical sciences’. ↵1 This always feels rather disingenuous though, as these models do have hyperparameters, which we discuss later in this paper. These still need to be inferred! They are referred to as hyperparameters, as they govern such things as the scale of a distribution rather than acting explicitly on the functional form of the curves. ↵2 The process of marginalization refers to ‘integrating out’ uncertainty. For example, given p(y,θ)=p(y|θ)p(θ), we may obtain p(y) by marginalizing over the unknown parameter θ, such that . Thank you for your interest in spreading the word on Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. NOTE: We only request your email address so that the person you are recommending the page to knows that you wanted them to see it, and that it is not junk mail. We do not capture any email address. Please log in to add an alert for this article. Article reuse Celebrating 350 years of Philosophical Transactions Anniversary issue with free commentaries, archive material, videos and blogs. Copyright © 2018 The Royal Society"
2abe5e7f15,Bringing neural networks to cellphones | MIT News,"Login
or
Subscribe Newsletter
MIT researchers have designed new methods for paring down neural networks so that they’ll run more efficiently on handheld devices.
Image: Jose-Luis Olivares/MIT Method for modeling neural networks’ power consumption could help make the systems portable.
Larry Hardesty | MIT News Office
July 18, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
In recent years, the best-performing artificial-intelligence systems — in areas such as autonomous driving, speech recognition, computer vision, and automatic translation — have come courtesy of software systems known as neural networks. But neural networks take up a lot of memory and consume a lot of power, so they usually run on servers in the cloud, which receive data from desktop or mobile devices and then send back their analyses. Last year, MIT associate professor of electrical engineering and computer science Vivienne Sze and colleagues unveiled a new, energy-efficient computer chip optimized for neural networks, which could enable powerful artificial-intelligence systems to run locally on mobile devices. Now, Sze and her colleagues have approached the same problem from the opposite direction, with a battery of techniques for designing more energy-efficient neural networks. First, they developed an analytic method that can determine how much power a neural network will consume when run on a particular type of hardware. Then they used the method to evaluate new techniques for paring down neural networks so that they’ll run more efficiently on handheld devices. The researchers describe the work in a paper they’re presenting next week at the Computer Vision and Pattern Recognition Conference. In the paper, they report that the methods offered as much as a 73 percent reduction in power consumption over the standard implementation of neural networks, and as much as a 43 percent reduction over the best previous method for paring the networks down. Energy evaluator Loosely based on the anatomy of the brain, neural networks consist of thousands or even millions of simple but densely interconnected information-processing nodes, usually organized into layers. Different types of networks vary according to their number of layers, the number of connections between the nodes, and the number of nodes in each layer. The connections between nodes have “weights” associated with them, which determine how much a given node’s output will contribute to the next node’s computation. During training, in which the network is presented with examples of the computation it’s learning to perform, those weights are continually readjusted, until the output of the network’s last layer consistently corresponds with the result of the computation. “The first thing we did was develop an energy-modeling tool that accounts for data movement, transactions, and data flow,” Sze says. “If you give it a network architecture and the value of its weights, it will tell you how much energy this neural network will take. One of the questions that people had is ‘Is it more energy efficient to have a shallow network and more weights or a deeper network with fewer weights?’ This tool gives us better intuition as to where the energy is going, so that an algorithm designer could have a better understanding and use this as feedback. The second thing we did is that, now that we know where the energy is actually going, we started to use this model to drive our design of energy-efficient neural networks.” In the past, Sze explains, researchers attempting to reduce neural networks’ power consumption used a technique called “pruning.” Low-weight connections between nodes contribute very little to a neural network’s final output, so many of them can be safely eliminated, or pruned. Principled pruning With the aid of their energy model, Sze and her colleagues — first author Tien-Ju Yang and Yu-Hsin Chen, both graduate students in electrical engineering and computer science — varied this approach. Although cutting even a large number of low-weight connections can have little effect on a neural net’s output, cutting all of them probably would, so pruning techniques must have some mechanism for deciding when to stop. The MIT researchers thus begin pruning those layers of the network that consume the most energy. That way, the cuts translate to the greatest possible energy savings. They call this method “energy-aware pruning.” Weights in a neural network can be either positive or negative, so the researchers’ method also looks for cases in which connections with weights of opposite sign tend to cancel each other out. The inputs to a given node are the outputs of nodes in the layer below, multiplied by the weights of their connections. So the researchers’ method looks not only at the weights but also at the way the associated nodes handle training data. Only if groups of connections with positive and negative weights consistently offset each other can they be safely cut. This leads to more efficient networks with fewer connections than earlier pruning methods did. ""Recently, much activity in the deep-learning community has been directed toward development of efficient neural-network architectures for computationally constrained platforms,” says Hartwig Adam, the team lead for mobile vision at Google. “However, most of this research is focused on either reducing model size or computation, while for smartphones and many other devices energy consumption is of utmost importance because of battery usage and heat restrictions. This work is taking an innovative approach to CNN [convolutional neural net] architecture optimization that is directly guided by minimization of power consumption using a sophisticated new energy estimation tool, and it demonstrates large performance gains over computation-focused methods. I hope other researchers in the field will follow suit and adopt this general methodology to neural-network-model architecture design."" Topics: Research, School of Engineering, Artificial intelligence, Computer modeling, Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Research Laboratory of Electronics, Software, Efficiency This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
aed1d46270,"With commercial satellite imagery, computer learns to quickly find missile sites in China - SpaceNews.com","WASHINGTON — For all the hype and promise around artificial intelligence and machine learning technologies in military applications, it always comes down to what specifically can be done with it. The industry keeps rolling out new gee-whiz artificial intelligence tools but the defense and intelligence communities still are trying to figure out how to use them and whether they really work as promised. According to a new study, there is one area where deep machine learning algorithms can definitely help the government, and that is to analyze satellite imagery. Officials from the National Geospatial Intelligence Agency have called on the private sector to bring forth machine learning tools to automate repetitive and time-consuming image analysis tasks. They want to free up skilled analysts to spend more time on hard intelligence problems that can’t be turned over to a computer. Researchers from the Center for Geospatial Intelligence at the University of Missouri used a deep learning neural network to assist human analysts in visual searches for surface-to-air missile sites over a large area in southeastern China. The results showed that the computer performed an average search time of only 42 minutes for an area of approximately 90,000 square kilometers. By comparison, North Korea is about 120,000 square kilometers. “This was more than 80 times more efficient than a traditional human visual search,” the center’s director and University of Missouri electrical engineering and computer science professor Curt Davis told SpaceNews. The software achieved the same overall statistical accuracy as human analysts — 90 percent — for correctly locating the missile sites. “I’ve been doing this research for almost 20 years, and I do believe the application of deep machine learning technology to satellite imagery reconnaissance is revolutionary,” he said. “I never expected this type of performance that we’ve been able to see both in the lab and the study. The metrics we’re seeing, the applications to larger-scale data sets to me is revolutionary.” Historically, machine learning algorithms haven’t performed well when they have been applied to large satellite imagery data sets, he said. The breakthroughs came in the last couple of years. The computer used in the study searched the 90,000 square kilometer area in less than an hour. Information overload U.S. defense and intelligence agencies are drowning in high-resolution imagery they need to analyze every day to monitor events unfolding around the world. “There is simply not enough manpower to effectively analyze all the image data collected today, and the problem is only getting worse,” Davis said. And the technology is only going to get better, he said. “The ultimate goal is to recognize dozens and hundreds of different types of objects very quickly,” said Davis. “I believe that goal is achievable in the near future.” Researchers will be training networks to search for things military analysts typically look for, including bunkers, aircraft shelters, radar sites, antennas, satellite dishes, launch pads and tank formations. The study used commercially available remote sensing satellite imagery of one-meter resolution. With new generations of satellites soon to be launched by commercial firms, including some with sub-meter resolution, the data deluge will continue. “It has taken a while for the remote sensing community to evaluate these deep machine learning methods,” said Davis. “Most of the studies I’ve seen were only experiments against limited data sets,” he continued. “Now we’ve been able to apply deep learning models to a large data set.” The research was published in the SPIE Journal of Applied Remote Sensing in a special issue on deep learning in remote sensing applications. Readers can search for Chinese surface-to-air missile sites on a demonstration website that uses the same high-resolution satellite imagery and deep learning algorithms used in the study. If and when these tools start replacing human analysts remains to be seen. National Geospatial-Intelligence Agency Director Robert Cardillo said recently he wants to automate 75 percent of the repetitive tasks analysts perform so they can focus on the “25 percent that require the most attention.” Deep learning methods can help do that, said Davis. The tough threat posed by North Korea is a case in point. The computer can find most of the fixed-site missiles but it takes human skills to track Pyongyang’s notoriously elusive mobile ballistic missile launchers. “That’s a harder problem. They can be hiding in a cave, pop out and launch a test missile.” Pentagon interest The Pentagon years ago identified machine learning and artificial intelligence as central elements to the military’s modernization strategy for weapons and information systems. Clearly the industry is progressing quickly, but the Defense Department has not moved as fast in applying the technology. “One of the challenges DoD faces in this area is that we are too often in this position where we discuss something in an abstract or theoretical way,” said Shawn Steene, senior force planner for emerging technologies at the Defense Department. He spoke Oct. 19 at a CNA panel discussion on artificial intelligence. In recognition of the growing role of these technologies in defense, CNA, a federally funded nonprofit think tank in Arlington, Va., announced the opening of a “Center for Artificial Intelligence and Autonomy.” “To some degree we’re limited by our creativity in the application of these capabilities,” Steene said. He recalled when the U.S. Geospatial Intelligence Foundation put out an open-source challenge offering a prize to whoever would come up with the best algorithms to take overhead imagery and identify the buildings in the picture. “The point was to remove from the analysts the first cut layer,” he said. “Just having that program to do that, having the machine doing the first layer, I can pass that to an analyst. And instead of spending time doing basic tasks, now they can do the ‘value added’ work.” Using artificial intelligence for data mining also could help prioritize information so networks are not clogged by data that may not be valuable, said Steene. “Instead of needing a giant pipe, if I have some screening at the front end, I can constrain the data flow,” he said. This technology offers infinite applications but the Defense Department needs to define the problems it is trying to address and “we need to use more creativity.” 2018 Spacenews, Inc. All Rights Reserved"
a039bf20d0,Soil nutrient maps of Sub-Saharan Africa: assessment of soil nutrient content at 250 m spatial resolution using machine learning | SpringerLink,"This service is more advanced with JavaScript available, learn more at http://activatejavascript.org
Nutrient Cycling in Agroecosystems September 2017, Volume 109, Issue 1,
pp 77–102 | Cite as Spatial predictions of soil macro and micro-nutrient content across Sub-Saharan Africa at 250 m spatial resolution and for 0–30 cm depth interval are presented. Predictions were produced for 15 target nutrients: organic carbon (C) and total (organic) nitrogen (N), total phosphorus (P), and extractable—phosphorus (P), potassium (K), calcium (Ca), magnesium (Mg), sulfur (S), sodium (Na), iron (Fe), manganese (Mn), zinc (Zn), copper (Cu), aluminum (Al) and boron (B). Model training was performed using soil samples from ca. 59,000 locations (a compilation of soil samples from the AfSIS, EthioSIS, One Acre Fund, VitalSigns and legacy soil data) and an extensive stack of remote sensing covariates in addition to landform, lithologic and land cover maps. An ensemble model was then created for each nutrient from two machine learning algorithms—random forest and gradient boosting, as implemented in R packages ranger and xgboost—and then used to generate predictions in a fully-optimized computing system. Cross-validation revealed that apart from S, P and B, significant models can be produced for most targeted nutrients (R-square between 40–85%). Further comparison with OFRA field trial database shows that soil nutrients are indeed critical for agricultural development, with Mn, Zn, Al, B and Na, appearing as the most important nutrients for predicting crop yield. A limiting factor for mapping nutrients using the existing point data in Africa appears to be (1) the high spatial clustering of sampling locations, and (2) missing more detailed parent material/geological maps. Logical steps towards improving prediction accuracies include: further collection of input (training) point samples, further harmonization of measurement methods, addition of more detailed covariates specific to Africa, and implementation of a full spatio-temporal statistical modeling framework. Sub-Saharan Africa (SSA) has over 50% of the world’s potential land for cultivation, yet only a small portion of this land satisfies conditions for agricultural production from cropping (Lal 1987; Jayne et al. 2010). Although the proportion of arable land in SSA has been steadily growing since 1950’s, currently only 9% of SSA is arable land and only 1% is permanently cultivated1. Current cropping yields in Sub-Saharan Africa are low, often falling well short of water-limited yield potentials (Jayne et al. 2010). This underperformance is due to number of factors: soil nutrient deficiencies, soil physical constraints, pests and diseases and sub-optimal management. Whilst it is well established that nutrient deficiencies are constraining yields in SSA (Giller et al. 2009), only limited information is available on soil nutrient contents and nutrient availability. Only very general (approximate) maps of soil micro-nutrients are at the moment available for the whole continent (see e.g. Kang and Osiname 1985; Roy et al. 2006 and/or Alloway 2008). The Africa Soil Information Services project has recently developed a gridded Soil Information System of Africa at 250 m resolution showing the spatial distribution of primary soil properties of relatively stable nature, such as depth to bedrock, soil particle size fractions (texture), pH, contents of coarse fragments, organic carbon and exchangeable cations such as Ca, Mg, Na, K and Al and the associated cation exchange capacity (Hengl et al. 2015, 2017). These maps were derived from a compilation of soil profile data collected from current and previous soil surveys. There is now a growing interest in applying similar spatial prediction methods to produce detailed maps of soil nutrients (including micro-nutrients) for SSA, in order to support agricultural development, intensification and monitoring of the soil resource (Kamau and Shepherd 2012; Shepherd et al. 2015; Wild 2016). Detailed maps of soil nutrients, including micro-nutrients, are now possible due to the increasing inflow of soil samples collected at field point locations by various government and/or NGO funded projects: e.g. by projects supported by the National Governments of Ethiopia, Tanzania, Kenya, Uganda, Nigeria, Ghana, Rwanda, Burundi and others; and by organizations such as the Bill and Melinda Gates Foundation (Leenaars 2012; Shepherd et al. 2015; Towett et al. 2015; Vågen et al. 2016) and similar, as well as by the private sector. inputs for pan-continental soil-crop models, inputs for large scale spatial planning projects, inputs for regional agricultural decision support systems, general estimates of total nutrient content against which future human-induced or natural changes may be recognized and measured, and as prior information to guide more detailed soil sampling surveys. We generate predictions of individual nutrients, then look at the possibilities of delineating nutrient management zones using automated cluster analysis. At the end, we analyze whether the produced predictions of soil nutrients (maps) are correlated with field-measured crop yields based on field trials. AfSIS (Africa Soil Information Service) Sentinel Sites: 18,000 soil samples at 9600 locations i.e. 60 sites of 10 by 10 km (Walsh and Vågen 2006; Vågen et al. 2010). Samples were taken in the period 2008–2016 at 0–20 and 20–50 cm soil depth intervals; analyzed by mid-infrared (MIR) diffuse reflectance spectroscopy based on calibration points from 960 samples (10%) analyzed by conventional wet chemistry including Mehlich-3, and thermal oxidation for org. C and total N. Sentinel Sites were designed to cover all of the agro-ecological regions in SSA and therefore should provide a good range of covariates at each location. EthioSIS (Ethiopia Soil Information Service): 15,000 topsoil samples (0–20 cm) from Ethiopia analyzed by conventional wet chemistry including Mehlich-3. The majority of samples was collected in the period 2012–2015. The Africa Soil Profiles database compiled for AfSIS: over 60,000 samples of 18,500 soil profiles collected from on average four depth intervals to on average 125 cm depth in period 1960–2010 (mainly 1980–1990) and 40 countries, with C, N, K, Ca and Mg available for nearly all points, P for one third of the points and micro-nutrients for ca 20% of points (Leenaars 2012). International Fertilizer Development Center (IFDC) projects co-funded by the government of The Netherlands: 3500 topsoil samples (0–20 cm) for Uganda, Rwanda and Burundi also analyzed using soil spectroscopy. Majority of samples was collected in the period 2009–2014. One Acre Fund: some 2400 topsoil samples (0–20 cm) for Uganda and Kenya, collected in the period 2010–2016. University of California, Davis: some 1800 topsoil samples (0–20 cm) for Kenya. VitalSigns: 1374 soil samples from Ghana, Rwanda, Tanzania and Uganda also analyzed using mid-infrared spectroscopy, collected in the period 2013–2016. Combined histograms (at log-scale) for the soil macro-nutrients based on a compilation of soil samples for Sub-Saharan Africa Combined histograms (at log-scale) for the soil micro-nutrients based on a compilation of soil samples for Sub-Saharan Africa We focused on producing spatial predictions for the following 15 nutrients (all concentrations are expressed as mass fractions using mg per kg soil fine earth i.e. ppm): organic carbon (C) and total nitrogen (N), total phosphorus (P), and extractable: phosphorus (P), potassium (K), calcium (Ca), magnesium (Mg), sulfur (S), sodium (Na), iron (Fe), manganese (Mn), zinc (Zn), copper (Cu), aluminum (Al) and boron (B). Although C, Na and Al are commonly not classified as soil nutrients, their spatial distribution can help assessment of soil nutrient constraints. For example, extractable Al can be an important indicator of soil production potential: high exchangeable Al levels can reduce growth of sensitive crops as soil pH (\(\hbox {H}_2\hbox {O}\)) drops below <5.3 and become toxic to the majority of plants <4.5 (White 2009). Histograms of nutrients based on the data compilation are depicted in Figs. 1 and 2. Although the soil data sources used for model calibration are quite diverse, the majority of soil samples had been analyzed using the MIR technology by the (same) soil-plant spectral diagnostics laboratory at the World Agroforestry Centre (ICRAF), Nairobi, and Crop Nutrition Laboratory Services, Nairobi, and are hence highly compatible. the produced spatial predictions presented in this paper might not everywhere reflect current status of nutrients on the field, i.e. they should only be used as long-term, average estimates, and the temporal variation in soil nutrients is ignored—or in other words, dynamics of soil nutrients over the 1980–2016 span is not discussed in this work. Comparison of spatial coverage of sampling locations for four nutrients: ext. P, ext. K, ext. Mg and ext. Fe. Data sources: AfSIS Sentinel Sites soil samples, EthioSIS soil samples, Africa Soil Profiles DB soil samples, IFDC-PBL soil samples, One Acre Fund soil samples, University of California soil samples and Vital Signs soil samples. See text for more details DEM-derived surfaces—slope, profile curvature, Multiresolution Index of Valley Bottom Flatness (VBF), deviation from mean elevation value, valley depth, negative and positive Topographic Openness and SAGA Wetness Index, all derived using SAGA GIS at 250 m resolution (Conrad et al. 2015); Long-term averaged monthly mean and standard deviation of the MODIS Enhanced Vegetation Index (EVI) at 250 m; Long-term averaged monthly mean and standard deviation of the MODIS land surface temperature (daytime and nighttime) based on the 1 km resolution data; Land cover map of the world at 300 m resolution for the year 2010 prepared by the European Space Agency (http://www.esa-landcover-cci.org/); Monthly precipitation images at 1 km spatial resolution based on the CHELSA climate data set obtained from http://chelsa-climate.org (Karger et al. 2016); Global cloud dynamics images at 1 km resolution obtained from http://www.earthenv.org/cloud (Wilson and Jetz 2016); Geologic age of surficial outcrops from the USGS map (at general scale) showing geology, oil and gas fields and geological provinces of Africa (Persits et al. 2002); Kernel density maps based on the Mineral Resources Data System (MRDS) points (McFaul et al. 2000), for mineral resources mentioning Fe, Cu, Mn, Mg, Al and Zn; Groundwater storage map, depth to groundwater and groundwater productivity map provided by the British Geological Survey (MacDonald et al. 2012); Landform classes (breaks/foothills, flat plains, high mountains/deep canyons, hills, low hills, low mountains, smooth plains) based on the USGS Map of Global Ecological Land Units (Sayre et al. 2014); Global Water Table Depth in meters based on Fan et al. (2013); Landsat bands red, NIR, SWIR1 and SWIR2 for years 2000 and 2014 based on the Global Forest Change 2000–2014 data v1.2 obtained from http://earthenginepartners.appspot.com/science-2013-global-forest (Hansen et al. 2013); Global Surface Water dynamics images: occurrence probability, surface water change, and water maximum extent (Pekel et al. 2016), obtained from https://global-surface-water.appspot.com/download; Distribution of Mangroves derived from Landsat images and described in Giri et al. (2011); Predicted soil pH (\(\hbox {H}_2\hbox {O}\)) maps at 250 m produced within the SoilGrids project (https://soilgrids.org); Remote sensing data had been previously downloaded and prepared via ISRIC’s massive storage server for the purpose of the SoilGrids project (Hengl et al. 2017). The majority of covariates cover the time period 2000–2015, i.e. they match the time span for most of the newly collected soil samples. Prior to modeling, all covariates have been stacked to the same spatial grids of 250 m, as the best compromise between computational load and average resolution of all covariates. To downscale climatic images and similar coarser resolution images we used the bicubic spline algorithm as available in the GDAL software (Mitchell and Developers 2014). Model fitting and prediction were undertaken using an ensemble of two Machine Learning algorithms (MLA) (Hengl et al. 2017): ranger (random forest) (Wright and Ziegler 2016) and xgboost (Gradient Boosting Tree) (Chen and Guestrin 2016), as implemented in the R environment for statistical computing. Both random forest and gradient boosting have already proven to be efficient in predicting soil chemical and physical soil properties at the continental and global scale (Hengl et al. 2017). Packages ranger and xgboost were selected also because both are highly suitable for dealing with large data sets and support parallel computing in R. We initially considered running kriging of remaining residuals, but eventually this was not finally considered worth the effort for the following two reasons. First, most of the observation points are far apart so kriging would have had little effect on the output predictions. Second, the variograms of the residuals all had a nugget-sill ratio close to 1, meaning that the residual variation lacked spatial structure and would not benefit spatial interpolation, e.g. by the use of kriging (Hengl et al. 2007). From our work so far on this and other soil related projects, it seems that there is a rule of thumb where once a machine learning model explains over 60% of variation in data, chances are that kriging is not worth the computational effort. To optimize fine-tuning of the Machine Learning model parameters, the caret::train function (Kuhn 2008) was used consistently with all nutrients. This helped especially with fine-tuning of the xgboost model parameters and the mtry parameter used in the random forest models. Optimization and fine-tuning of Machine Learning algorithms was computationally demanding and hence time consuming, but our experience was that it often led to a 5–15% improvement in the overall accuracy. All processing steps and data conversion and visualization functions have been documented via ISRIC’s institutional github account2. Access to legacy data from the Africa soil profiles database and other data sets produced by the AfSIS project is public and to access the other data sets consider contacting the corresponding agencies. In addition to fitting models per nutrient, we also run multivariate and cluster analysis to determine cross-correlations and groupings in the values. First, we analyzed correlation between the nutrients by running principal component analysis. Secondly, we allocated individual sampling locations to clusters using unsupervised classification to determine areas with relatively homogeneous concentrations of nutrients. For this we used the fuzzy k-means algorithm as implemented in the h2o package (Aiello et al. 2016). Both principal component analysis and unsupervised fuzzy k-means clustering were run on transformed variables using the Aitchison compositions as implemented in the compositions package (van den Boogaart and Tolosana-Delgado 2008). Note that transforming the original nutrient values into compositions is important as in its absence, application of statistical methods assuming free Euclidean space (e.g. PCA and unsupervised fuzzy k-means clustering) gives a highly skewed view of the variable space (van den Boogaart and Tolosana-Delgado 2008). Transform all nutrient values from ppm’s to compositions using the compositions package (van den Boogaart and Tolosana-Delgado 2008). Determine the optimal number of classes for clustering using the mclust package (Fraley et al. 2012) i.e. by using the Bayesian Information Criterion for expectation-maximization. Allocate sampling points to clusters using unsupervised classification with fuzzy k-means using the h2o package (Aiello et al. 2016). Fit a spatial prediction model using the ranger package based on clusters at sampling points and the same stack of covariates used to predict nutrients. Predict clusters over the whole area of interest and produce probabilities per cluster. Derive scaled Shannon Entropy Index (SSEI) map and use it to quantify spatial prediction uncertainty. In order to evaluate the importance of these soil nutrient maps for actual agricultural planning, we use the publicly available Optimising Fertilizer Recommendations in Africa (OFRA) field trials database. OFRA, a project led by CABI (Kaizzi et al. 2017), contains 7954 legacy rows from over 600 trials collected in the period 1960–2010. Field trials include crop yields and field conditions for majority of crops including maize, cowpea, sorghum, (lowland, upland) rice, groundnut, bean, millet, soybean, wheat, cassava, pea, climbing bean, barley, sunflower, (sweet, irish and common) potato, cotton, and similar. The OFRA database covers only 13 countries in Sub-Saharan Africa, hence it does not have the ideal representation considering all combinations of climatic and land conditions of crop growing. It is, nevertheless, the most extensive field trial database publicly available for SSA. Here we primarily concentrate on testing whether soil nutrients are important factor controlling crop yield. Note also that fitting one model for all crop types is statistically elegant (one multivariate model to explain all crop yield) also because one can then explore all interactions e.g. between crop types, varieties, treatments etc, and produce predictions for all combinations of crop types, varieties and treatments; this would have been otherwise very difficult if not impossible if we were to fit models per each crop type. Principal component analysis plots generated using sampled data: (left) biplot using first two components, (right) biplot using the third and fourth component. Prior to PCA, original values were transformed to compositions using the compositions package. P is the extractable phosphorus, and P.T is the total phosphorus List of target soil macro- and micro-nutrients of interest and summary results of model fitting and cross-validation Nutrient Method N 1% 50% 99% R-square RMSE org. N total (organic) N extractable by wet oxidation 63,937 0.0 600.0 4200 0.66 558 tot. P total phosphorus
7899
0 132 3047 0.85 284 ext. K extractable by Mehlich 3 104,784 0 130 1407.5 0.64 201 ext. Ca extractable by Mehlich 3 105,173 14 1162 14288 0.69 1950 ext. Mg extractable by Mehlich 3 103,356 1.2 242 2437 0.78 241 ext. Na extractable by Mehlich 3 71,986 0 30.13 2690 0.61 452 ext. S extractable by Mehlich 3 43,666 0.6 9 51
0.11
78 ext. Al extractable by Mehlich 3 30,945 0 874 2120 0.84 171 ext. P extractable by Mehlich 3 42,984 0 6 188
0.12
43 ext. B extractable by Mehlich 3 43,338 0 0.33 2.09 0.41 0.47 ext. Cu extractable by Mehlich 3 45,572 0.001 2.2 10.6 0.54 2.11 ext. Fe extractable by Mehlich 3 18,341 0 121 574 0.68 53 ext. Mn extractable by Mehlich 3 44,689 1.8 124 440 0.53 69 ext. Zn extractable by Mehlich 3 45,626 0.1 2.1 26.03 0.47 4.0 All values are expressed in ppm. N = “Number of samples used for training”, R-square = “Coefficient of determination” (amount of variation explained by the model based on cross-validation) and RMSE = “Root Mean Square Error”. Underlined cells indicate poorer models (or too small sample sizes) Top ten most important covariates per nutrient, reported by the ranger package Nutrient Most important covariates (10) org. N
Depth, LSTD November, TWI (DEM), LSTD October, precipitation November, soil pH, DEM, water vapor January-February, precipitation December, mean annual temperature tot. P Precipitation July, density of mineral exploration sites (Al), precipitation August, September, lithology, precipitation February, LSTD August, mean annual precipitation, water vapor January-February, precipitation June ext. K Soil pH, water vapour July-August, DEM, precipitation January, std. EVI April, precipitation February, water vapor January-February, depth, cloud fraction February, water vapor November-December ext. Ca
Soil pH, water vapour January-February, water vapour November-December, cloud fraction March, DEM, mean EVI May-June, Landsat NIR, std. LSTD November, mean EVI July-August, Landsat SWIR1 ext. Mg Soil pH, water vapor January-February, Landsat NIR, Landsat SWIR1, cloud fraction February, Landsat SWIR2, water vapor November-December, LSTD March, water vapor March-April, Landsat SWIR1 ext. Na
Soil pH, depth, cloud fraction seasonality, cloud fraction March, LSTN December, mean EVI January-February, slope (DEM), std. LSTN April, mean EVI May-June, LSTD July ext. S Lithology, Landsat SWIR2, cloud fraction December, precipitation October, May, TWI (DEM), precipitation November, std. EVI July-August, LSTD November ext. Al Soil pH, LSTD November, precipitation November, TWI, LSTD December, cloud fraction November, DEM, cloud fraction December, precipitation total, precipitation February ext. P Valley depth (DEM), precipitation July, Deviation from mean (DEM), precipitation November, DEM, std. EVI May-June, precipitation January, positive openness (DEM), mean EVI July-August, mean EVI May-June ext. B Precipitation August, January, depth, precipitation November, soil pH, DEM, std. EVI July-August, precipitation September, positive openness (DEM), precipitation December ext. Cu Water vapor May-June, precipitation December, water vapor November-December, July-August, September-October, depth, water vapor January-February, precipitation July, cloud fraction November, precipitation August ext. Fe Water vapor January-February, density of mineral exploration sites (Phosphates), water vapor September-October, July-August, cloud fraction seasonality, water vapor May-June, March-April, depth, DEM, cloud fraction mean annual ext. Mn
Depth, precipitation November, April, cloud fraction January, land cover, DEM, precipitation February, January, water vapor January-February, precipitation December ext. Zn Precipitation January, December, mean EVI May-June, precipitation March, std. EVI March-April, precipitation February, November, April, TWI Explanation of codes: depth = depth from soil surface, LSTD = MODIS mean monthly Land Surface Temperature day-time, LSTN = MODIS mean monthly Land Surface Temperature night-time, EVI = MODIS Enhanced Vegetation Index, TWI = topographic wetness index, DEM = Digital Elevation Model, NIR = Landsat Near Infrared band, SWIR = Landsat Shortwave Infrared band. Underlined covariates indicate distinct importance The fact that Landsat bands also come as important covariate for number of nutrients (Na, Ca, Mg, S) is a promising discovery for those requiring higher resolution maps (Landsat bands are available at resolution of 60–30 m). Nevertheless, for majority of nutrients, the most important covariates are various climatic images, especially precipitation images. Although climatic images are only available at coarse resolution of 1 km or coarser, it seems that climate is the key factor controlling formation and evolution of nutrients in soil. Model fitting results also show that apart from org. C and N, and ext. Mn, Fe, B and P, the majority of nutrient values do not change significantly with depth. For the majority of soil macro- and micro-nutrients, it is probably enough to sample nutrients at a single depth. For C, N, P, Mn, Fe and B, depth is relatively high on the list of important covariates and hence should not be ignored. Final spatial predictions for nutrients with significant models are shown in Figs. 5 and 6. The spatial patterns produced match our expert knowledge and previously mapped soil classes in general, which is true especially for Fe, org. C and N and Ca and Na. Our predictions also indicate that the highest deficiencies for B and Cu are in sub-humid zones, which corresponds to the results of Kang and Osiname (1985). As several of the micro-nutrients have been mapped for the first time for the whole of Sub-Saharan Africa, many produced spatial patterns will need to be validation both locally and regionally. Predicted soil macro-nutrient concentrations (0–30 cm) for Sub-Saharan Africa. All values are expressed in ppm Predicted soil micro-nutrient concentrations (0–3 cm) and extractable Al for Sub-Saharan Africa. All values are expressed in ppm Examples of nutrient deficiency maps based on our results: zoom in on town Bukavu at the border between the eastern Democratic Republic of the Congo (DRC) and Rwanda. Points indicated samples used for model training. The threshold levels are based on Roy et al. (2006, p.78) ranging from very low (<50% expected yield) to medium (80–100% yield) to very high (100% yield). All values are in ppm’s. Background data source: OpenStreetMap Examples of locally defined nutrient deficiency maps based on our results: Eastern Africa. The adopted threshold levels are based on Roy et al. (2006, p.78) ranging from very low (<50% expected yield) to medium (80–100% yield) to very high (100% yield). All values are in ppm’s. Background data source: OpenStreetMap Predictions of all soil nutrients at four depths took approximately 40 h on ISRIC’s dedicated server with 256 GiB RAM and 48 cores (whole of Sub-Saharan Africa is about 7500 by 7000 km, i.e. covers about 23.6 million square kilometers). Fitting of models on the dedicated server running R software is efficient and models can be generated within 1 hour even though there were, on average, >50,000 of measurements per nutrient. With some minor additional investments in computing infrastructure, spatial predictions could be updated in future within 24 hrs (assuming all covariates are ready and harmonization of nutrients data already implemented). Figures 7 and 8 show the level of spatial detail of the output maps and demonstrates how these maps could be used for delineation of areas potentially deficient in key soil nutrients, i.e. a somewhat more useful/interpretable form of summary information to agronomists and ecologists. In this case, determination of deficient and suitable nutrient content was based on soil fertility classes by Roy et al. (2006), ranging from very low (<50% expected yield) to medium (80–100% yield) to very high (100% yield) and assuming soil of medium CEC. Crop specific threshold levels can be set by users to quickly map areas of nutrient deficiency/high potential fertility to spatially target suitable agronomic intervention. Similar, threshold values beyond which crop does not respond to fertilizer nutrient application can be diversified and mapped at regional scale based on the spatial diversity of measured or calculated attainable yield levels. Accuracy assessment plots for all nutrients. Predictions derived using 5–fold cross-validation. All values expressed in ppm and displayed on a log-scale The results of the cluster analysis show that the optimal number of clusters, based on the Bayesian Information Criterion for expectation-maximization, initialized by hierarchical clustering for parameterized Gaussian mixture models, as implemented in the mclust package function Mclust (Fraley et al. 2012), can be set at 20. It appears, however, that optimal number of clusters cannot be set clearly as majority of points were not split into distinct clouds, hence other smaller and larger numbers than 20 could have been derived probably with other similar cluster analysis packages. Class centers for 20 clusters determined using supervised fuzzy k-means clustering Cluster org. C org. N K P P tot. Ca Mg Na S Fe Mn Zn Cu B c1
23,400
1680
247 13.7
874
1570 306 113 13 123 119 3.8 2.2 0.4 c2 1840 280
54.7
12.4 344 321
56
0
49 97 250
47.0
60.0
0.7 c3 2230 286 115 78.7 366 463 114 48 29 134 128 5.3 2.7 0.9 c4 3580 335 109 33.7 333 631 162 69 23 117 120 4.7 2.8 0.6 c5 3090 383 211 27.9 449 1720 282 171
626
99 134 4.2 2.7 2.4 c6 1890 246 76.3 13.4
163
628 166 76 24 86 130 9.5 5.4 0.4 c7 1190 291
58
19.5 237 295
95
46 28 115 121 4.7 2.3
2.6
c8
1
0
62
7.55
290 231 104 36 9
193
55
1.3
1.7
0.1
c9 2840 372 335 21.3 303 3780 669
4270
56
69
116 4.1 2.5 0.7 c10 3870 439 297 27 444 3200 416 267 246 90 166 4.1 2.9 2.1 c11 5780 704
1740
34.4
607 2840 572 474 33 86 117 4.4 2.6 0.9 c12
4
1
112 7.74 278 797 214 41
8
140 74 1.5
1.1
0.1
c13 34 5 79.4
4.36
821 447 133 37
8
114
46
1.1
0.9
0.1
c14 3750 803 101 22.7 465 518 150 59 21 116 113 4.0 2.6 0.6 c15 1620 1040 82.2 26.6 482 332 115 56 28 108 116 3.9 2.8 0.6 c16 4260 514 269 21.8 451 5790
1180
496 41 76 136 4.3 3.2 0.8 c17 3200 393 65.6 21.6 301 357 127 51 22 139 122 5.0 2.4 1.8 c18 2600 330 64.7 15
179
742 145
25
51 101
330
27.5 30.1 0.6 c19 6890 580 133 24.4 413 665 162 56 17 122 114 4.0 2.3 0.5 c20 13,700 1100 416 20.8 756
5820
944 262 17
67
132 2.3 3.2 0.7 Underlined numbers indicate highest values per nutrient; italic indicates top two lowest concentrations per class Predicted spatial distribution of the determined clusters (20) (above), and the corresponding map of scaled Shannon Entropy Index (below). High values in scaled Shannon Entropy Index indicate higher prediction uncertainty. Cluster centers are given in Table 3
Map in Fig. 10 confirms that the produced clusters, in general, match combinations of climate and lithology. A map of the scaled Shannon Entropy Index (SSEI) for produced clusters is also shown in Fig. 10 (right). The differences in uncertainty for different parts of Sub-Saharan Africa are high. Especially large parts of Namibia, Democratic Republic Congo, Botswana, Somalia and Kenya have relatively high scaled Shannon Entropy Index (SSEI), hence higher uncertainty. In general it can be said that the SSEI map closely corresponds to extrapolation effects, i.e. that uncertainty primarily reflects density of points—as we get further away from main sampling locations, the SSEI grows to >80% (high uncertainty). In that sense, further soil sampling campaigns, especially in areas where the SSEI is >80%, could help decrease uncertainty of mapping soil nutrients in Sub-Saharan Africa. Map of SSEI is provided via the download data section. The result of modeling relationship between crop yield and nutrient and climatic maps (Eq. 5) show that a potentially accurate model can be fitted using random forest: this model explains 78% of variation in the crop yield values with an Out-Of-Bag (OOB) RMSE of \(\pm 2.4\,\hbox {t ha}^{-1}\). The variable importance plot (Fig. 11) further shows that the most influential predictors of the crop yield are: crop type, selection of nutrients and micro-nutrients (Mn, Zn, Al, B, Na), and, from climatic data, primarily monthly rainfall for June, October, September, May and July. This proves that producing maps of soil nutrients is indeed valuable for modeling agricultural productivity. Variable importance plot for prediction of the crop yield using the model from Eq. (5). Training points include 7954 legacy rows for 606 trials Examples of predicted potential crop yield for the land mask of SSA (excluding: forests, semi-deserts and deserts, tropical jungles and wetlands). Circles indicate the OFRA field trials database points used to train the model Distribution of the OFRA field trials is clustered and limited to actual 606 trials/locations (Fig. 12), hence probably not representative of the whole SSA. Most of field trials are legacy trials (often over 20 years old) and hence correlating them with the current soil conditions probably increases uncertainty in the models. This model ignores weather conditions for specific years (instead long-term estimates of rainfall, temperatures are used). Matching exact weather conditions per year would probably be more appropriate. In the following section we address some open issues and suggest the approaches to overcome these. This is mainly to emphasize limitations of this work, and to try to announce future research directions. One of the biggest problems of mapping soil nutrients for large areas are the laboratory and field measurement diversity. There is large complexity considering the methods and approaches to measurement of soil nutrients (Barber 1995). At farm-scale, this might not pose a too serious problem, but for pan-continental data modeling efforts it is certainly something that can not be ignored. In principle, many extractable soil nutrient content determination methods are highly correlated and harmonization of values is typically not a problem. For example, Phosphorus can be determined using Bray-1, Olsen and Mehlich-3 methods, which are all highly correlated depending on the pH range considered (Bray-1 and Mehlich-3 could be considered equivalent in fact). Conversion from one method to another however depends also on the soil conditions, such as soil pH and soil types (Roy et al. 2006) and requires data which are not readily available. Some nutrient measurements might come from the X-ray fluorescence method (XRF), especially where plant available nutrient levels relate to total element concentrations (Towett et al. 2015). In this project, we did not invest in harmonization of measurement methods as this was well beyond the project budget. It is, for example, well known that extractable P, K and micro-nutrients do not predict well from MIR, hence there are still many limitations with using nutrient concentrations derived from soil spectroscopy. Improving harmonization, geolocation accuracy of samples and standardizing sufficiently large measurement support sizes (some samples were taken at fixed depths restricted to the topsoil, others were taken per soil horizon over soil depth), could possibly help improve accuracy of predictions. Machine learning methods have already been proven effective in representing complex relationships with large stacks of variables (Strobl et al. 2009; Biau 2012). However, MLA’s can demand excessive computing time. Even though possibly more accurate, more generic algorithms than ranger and xgboost exist, these might require computing time which is beyond the feasibility of this project. For example, we have also tested using the bartMachine (bayesian additive regression trees) (Kapelner and Bleich 2013) and cubist (Kuhn et al. 2013) packages for generating spatial predictions, but due to very excessive computing times (even with full parallelization) we had no choice but to limit prediction modeling to ranger and xgboost. Computing time becomes a limiting factor especially as the number of training points is \(\gg\)10,000 and number of predictions locations goes beyond few million. In our case, whole of Sub-Saharan Africa at 250 m is an image of ca. 29,000 by 28,000 pixels, i.e. about 382 million pixels to represent the land mask. Although the preliminary results presented in this paper are promising and many significant correlations have been detected, for nutrients such as ext. P, S and B we obtained relatively low accuracies. It could very well be that these types of nutrients will be very difficult to map at some significant accuracy using this mapping framework. To address these shortcomings in the near future one could test developing spatial predictions at high spatial detail e.g. at 100 m spatial resolution, and/or test developing spatiotemporal models for mapping the space-time dynamics of soil nutrients over Africa. Drechsel et al. (2001), for example, recognized that much of the soils of Sub-Saharan Africa are actually constantly degrading, hence spatiotemporal modeling of nutrients could probably lead to higher accuracy in many areas. In addition, all soil tests need calibration with crop response trials for different soil types and climates, and future efforts may be better directed at more accurate calibration of crop responses to soil test data. Since this study focussed on predictions of soil nutrients using soil samples from a long period of years (1980–2016), we cannot tell from the current data what the rate of soil nutrient depletion is, nor where it is most serious. As nutrient contents can also be quite dynamic and controlled by the land use system (especially for nitrogen and organic carbon, and potentially phosphorus depending on fertilisation history), spatiotemporal models which take into account changes in land use could help increase mapping accuracy. Although we already have preliminary experience with developing spatiotemporal models for soil data, there are still many methodological challenges that need to be addressed, e.g. especially considering poor representation of time within the given sampling plans. Accuracy of spatial predictions of nutrients could also be improved by investing in new and/or more detailed covariates. Unfortunately, no better parent material i.e. surface lithology map was available to us than the most general map of surface geology provided by USGS (Persits et al. 2002). Kang and Osiname (1985) suggests that the micro-nutrient deficiencies are especially connected with the type of parent material, hence lack of detailed parent material/lithology map of Africa is clearly a problem. Using gamma-radiometrics images in future could likely help increase accuracy of nutrient maps (especially for P and K). In Australia for example, a national agency has collected and publicly released gamma-radiometrics imagery for the whole of the continent (Minty et al. 2009); similar imagery is also available for the whole of conterminous USA (Duval et al. 2005). Although it is not realistic to expect that the African continent would soon have an equivalent, gamma-radiometric imagery could contribute substantially to regional soil nutrient mapping due to its ability to differentiate topsoil mineralogy. The recent initiatives such as the World Bank’s Sustainable Energy, Oil, Gas and Mining Unit (SEGOM) programme “The Billion Dollar Map” (Ovadia 2015), could only help with bridging these gaps. Another opportunity for increasing the accuracy of maps of nutrients is to try to utilize Landsat 8 and Sentinel-2 near and mid-infrared imagery to derive proxies of surface minerology. Several research groups are now working on integrating airborne/satellite sensing with ground-based soil sensing into a single framework (see e.g. work of Stevens et al. (2008) and Ben-Dor et al. (2009)). The newly launched SHALOM Hyperspectral Space Mission (Feingersh and Dor 2016) could be another source of possibly crucial remote sensing data for nutrient mapping and monitoring. Accuracy and value of produced predictions could be improved if more sampling points were added to the training dataset, especially those funded and/or collected by national government agencies and NGO’s. Relevant data from additional soil data sets (not currently available for spatial prediction or with unknown user restrictions) include the AfSIS data recently generated in collaboration with Ethiopia, Tanzania, Ghana and Nigeria, ICRAF (World Agroforestry Centre) and CIAT (The International Center for Tropical Agriculture) institutes additional data generated in collaborative projects, private sector funded data (e.g. MARS in Ivory Coast and others in Ivory Coast, Nigeria), USAID-funded IFDC project (https://ifdc.org/) data from West Africa, CASCAPE project (http://www.cascape.info/) data sets, N2Africa project samples (http://www.n2africa.org/), and data generated by various national initiatives. As gradually more soil samples are added, especially in the (extrapolation) areas with highest spatial prediction error, it is reasonable to expect that the models and derived maps will also gradually become better. If not more accurate, then at least more representative of the main lithologic, climatic and land cover conditions in the SSA. There is a critical need for agricultural and ecological data in Africa, where an expected 3.5–fold population increase this century (Gerland et al. 2014) will place immense demand on soil nutrients that form the basis of food production. Researchers and policy makers have repeatedly called for data and monitoring systems to track the state of the world’s agriculture (Sachs et al. 2010). In response to this need, this soil nutrients data set provides both a useful tool for researchers interested in the role that soil nutrients play in ecological, agricultural and social outcomes in Sub-Saharan Africa, as well as a general estimate of soil nutrient stocks at a time when the continent is facing significant climate and land-use change. As the resolution of maps is relatively detailed, it is possible to spatially identify regional areas (Figs. 7, 8) that are ‘naturally’: (1) deficient, (2) adequate and (3) in excess relative to specific land-use requirements; and pair these with the nutrient-specific agronomic interventions required to achieve critical crop thresholds. Such usage could help optimize the use of soil resource and possibly (major) agronomic interventions across African countries (Vanlauwe et al. 2014). These agronomic interventions could consist of: targeting degraded areas that are suitable for restoration projects, and/or targeting areas for agricultural intensification and investment by modeling crop suitability and yield gaps at the regional scale (Nijbroek and Andelman 2016), and/or assessing the nutrient gaps to predict fertilizer nutrient use efficiency. Although we have only estimated long-term nutrient contents using relatively scarce data, the maps produced could be used to derive various higher-level data products, such as nutrient mass balance maps, when combined with soil bulk density data, Soil Fertility Index maps (Schaetzl et al. 2012) and/or nutrient gap (deficiency) maps. Such maps can be beneficial for non-specialist audiences who are nevertheless interested in spatial distributions of soil nutrients. The maps from this research could also be used as prior estimates that could be updated with more intensive local level sampling. In addition to deriving higher-level products from this data set, combining these soil nutrient data with other continent-wide data sets will also yield insights. For example, data sets on weather (multiple years), farm management and root depth soil water (Leenaars et al. 2015) combined with data sets on crop distribution and yield, both actual and potential, will lead to insights about edaphic and agronomic drivers of yields gaps and associated nutrient gaps, or help policy makers target areas likely to undergo future nutrient depletion through crop removal and prevent areas that would otherwise fall below some critical nutrient level in the near to medium future. Other socio-economic data sets, such as health or income surveys, could be paired with these data to demonstrate how soil nutrient depletion can affect livelihoods and health outcomes, as well as to model the effects of predicted soil nutrient changes. Finally, this dataset could be combined with ecological data, such as biophysical inventories or NDVI data sets to refine our understanding of the role soil nutrients play in the heterogeneous and seemingly stochastically shifting plant community regimes of the semi-arid tropics, the underlying dynamics of which are still poorly understood (Murphy and Bowman 2012). As we have already noted, probably the most serious limitation of this project was the high spatial clustering of points, i.e. under-sampling in countries with security issues or poor road infrastructures (tropical jungles, wetlands and similar). Fitting models with (only) 60 sites could result in many parts of Africa containing only extrapolated areas as topsoil data are predominantly collected/available for Eastern Africa (Ethiopia, Kenya, Uganda, Rwanda, Burundi, Tanzania), with large areas of relatively fertile soils developed from materials of volcanic origin located at relatively high altitude. More sampling points are certainly needed to improve spatial prediction models (and also to make the cross-validation more reliable), especially in West African soils developed in basement complexes (granites, gneisses, schists) and deposits and which are generally very much lower in soil nutrient contents. Because of high spatial clustering of points, and consequent extrapolation problems, the maps presented in this work should be used with caution. In that context, for the purpose of pan-African mapping it would be important to further optimize spreading of the sampling locations especially to increase representation of the geological and particularly the pedological feature space. This would increase sampling costs, but it might be the most efficient way to improve accuracy and usability of maps for the whole continent. Also soil subsoil could be somewhat better represented. As the majority (>90%) of measurements refer to topsoil, unfortunately, we cannot tell if these soil-depth relationships are also valid for subsoil i.e. beyond 50 cm of depth and including the soil C horizon in weathering substrate. So also collecting soil nutrient measurements for depths beyond 50 cm could lead to interesting discoveries, especially when it comes to mapping organic Carbon and Nitrogen, soil alkalinity and similar. Spatial predictions of main macro- and micro-nutrients have been produced for soils of Sub-Saharan Africa using an international compilation of soil samples from various projects. Our focus was mainly on producing spatial prediction of extractable concentrations of soil nutrients (thus relative nutrient content estimates based on Mehlich-3 and compatible methods). For phosphorus we also produced maps of the total P content and for carbon and nitrogen we produced maps of organic component of the two elements. The results of cross-validation showed that, apart from S, P and B, which seemed to be more difficult to model spatially using the given framework, significant models can be produced for most targeted nutrients (R-square between 40–85%; Table 1). Produced maps of soil macro- and micro-nutrients (Figs. 5, 6) could potentially be used for delineating areas of nutrient deficiency/sufficiency relative to nutrient requirements and as an input to crop modeling. Results of cluster analysis indicate that whole of SSA could be represented with ca. 20 classes (Fig. 10), which could potentially serve as the (objectively delineated) nutrient management zones. The finally produced predictions represent a long-term (average) status of soil nutrients for a period from 1960–2016. The training data set could have been subset to more recently collected soil samples (2008–2016) to try to produce baseline estimates of soil nutrients for e.g. 2010. We have decided to use all available nutrient data instead, mainly to avoid huge sampling gaps, but also because our covariates cover longer time spans. A limiting factor for mapping nutrients using the existing point data in Africa is a high spatial clustering of sampling locations with many countries/land cover and land use groups completely unrepresented (based on the Shannon Entropy Index map in Fig. 10). Logical steps towards improving prediction accuracies include: further collection of input (training) point samples, especially in areas that are under-represented or where the models perform poorly, harmonization of observations, addition of more detailed covariates specific to Africa, and implementation of full spatio-temporal statistical modeling frameworks (i.e. matching more exactly in time domain nutrient concentrations, crop yields and weather conditions). Overlaying soil nutrient data with crop yield trials data shows that soil nutrients are indeed important for agricultural development with especially Mn, Zn, Al, B and Na, being listed high as the most important variables for prediction of crop yield (Fig. 11). If both nutrient maps and climatic images of the area are available, crop yields can be predicted with an average error of \(\pm 2.4\,\hbox {t ha}^{-1}\). If a more up-to-date field trial database was available, the model from Eq. (5) could have been used to produce more actual maps of potential yield (as compared to Fig. 12). Because the model from Eq. (5) can be used to produce almost infinite combinations of predictions, it would be also fairly interesting to serve the model as a web-service, i.e. so that users can inspect potential yields on-demand (for arbitrary chosen combination of crop type, variant and application). The gridded maps produced in this work are available under the Open Data Base licenses and can be downloaded from http://data.isric.org. These maps will be gradually incorporated into Web-services for soil nutrient data, so that also users on the field can access the data in real-time (i.e. through mobile phone apps and cloud services).
http://data.worldbank.org/indicator/AG.LND.ARBL.ZS?locations=ZG.
https://github.com/ISRICWorldSoil/AfricaSoilNutrients.
This study has been conducted primarily upon request of the Netherlands Environmental Assessment Agency (PBL). Acknowledgments are due to the various projects and organizations who made soil data collected from various countries available for this study, including projects partially or completely funded by the Bill and Melinda Gates Foundation (BMGF), such as the AfSIS (Africa Soil Information Service) project, which was co-funded by the Alliance for a Green Revolution in Africa (AGRA), the Vital Signs project with interventions in Ghana, Rwanda, Tanzania and Uganda, and the EthioSIS project, funded primarily by the Ethiopian government and co-funded by the the Bill and Melinda Gates Foundation and the Netherlands government through the CASCAPE project. Also co-funded by the Netherlands government are projects of the International Fertilizer Development Center (IFDC) in collaboration with the governments of Burundi, Rwanda and Uganda. The One Acre Fund made the collection of soil samples possible in Rwanda and Kenya and the University of California, Davis, in Kenya. We are grateful to these organizations for providing soil sample data and for commenting on the first drafts of the manuscript. ISRIC—World Soil Information is a non-profit foundation primarily funded by the Dutch government. Authors are thankful to the two anonymous reviewers for thoroughly reading the manuscript and helping with improving various sections of results and discussion. Open AccessThis article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. Over 10 million scientific documents at your fingertips © 2017 Springer International Publishing AG. Part of Springer Nature.
Not logged in
Maynooth University Print Journals, The Library (2000707391) - IRIS - LNCS (3000178775) - IReL Consortium c/o The Library Maynooth University (3000215760) - 1255 IRIS Ireland (3000240761)
149.157.138.202"
06b7da681a,How Machine Learning Is Revolutionizing the Diagnosis of Rare Diseases - NBC News,"by Jane C. Hu
DNA double helix strand viewed with a phone Stanislaw Pytel / Getty Images
Well before the family came in to the Batson Children’s Specialty Clinic in Jackson, Mississippi, they knew something was wrong. Their child was born with multiple birth defects, and didn’t look like any of its kin. A couple of tests for genetic syndromes came back negative, but Omar Abdul-Rahman, Chief of Medical Genetics at the University of Mississippi, had a strong hunch that the child had Mowat-Wilson syndrome, a rare disease associated with challenging life-long symptoms like speech impediments and seizures.
So he pulled out one of his most prized physicians’ tools: his cell phone.
Using an app called Face2Gene, Abdul-Rahman snapped a quick photo of the child’s face. Within a matter of seconds, the app generated a list of potential diagnoses — and corroborated his hunch. “Sure enough, Mowat-Wilson syndrome came up on the list,” Abdul-Rahman recalls.
Related: How Computers are Learning to Predict the Future
Abdul-Rahman is a member of Face2Gene’s scientific advisory board, and he’s one of the many physicians and researchers working with its developers to identify syndromes and diseases using facial recognition software. Like many of the 7,000 known rare diseases out there, Mowat-Wilson syndrome has a specific facial signature: square face, pointed chin, widely set eyes. Face2Gene’s algorithms map points on a patient’s face, compare those points with a database containing points from thousands of other faces, and suggest potential diagnoses.
Doing this all with a single photo is a neat trick, but the real power of the app lies in its long-term potential. Face2Gene’s system uses a machine-learning algorithm, meaning it learns from every new face it scans. The more data it acquires through its use, scientists hope, the more accurate the diagnoses.
This is the latest medical technology that leverages the power of big data to make better diagnoses and more accurate predictions; other artificial intelligence systems analyze patient symptoms or medical scans to predict cancers and crunch numbers to provide personalized treatments. Early diagnosis of rare diseases can help families seek specialized treatments, yet families often wait years for test results and referrals to specialists. That’s because the facial signatures of some rare diseases can be subtle and difficult to diagnose, even for a trained professional.
Related: Is Perfecting Artificial Intelligence This Generations Space Race?
Most experienced physicians have only seen a handful of patients with a specific rare disease — hardly enough to notice distinct patterns. “Only when you have hundreds or thousands of patients can you really say there’s a pattern,” FDNA CEO Dekel Gelbman says. “That’s where the computer power comes in.”
Face2Gene is publicly available, but there’s still a lot to learn, so its developers recommend only experts use it. When you download the app via the iTunes store, you’re asked to certify your credentials as a healthcare professional before you complete signing up. The technology is meant to be a tool to help identify diseases and syndromes; as a result, Face2Gene isn’t yet great at recognizing the faces of typically developing children.
“If you take a picture of a normal individual, it will still try to match them to the closest possible syndrome,” Abdul-Rahman says. “If you’re inexperienced and you use it on a random individual, it will suggest a diagnosis anyway.”
Face2Gene’s predictions could lead an inexperienced user to false positives; it takes an experienced professional to take the app’s suggestions and interpret them in the context of other clinical symptoms, or to order genetic tests.
Still, it could help a wider range of medical professionals make diagnoses. Pediatricians, nurse practioners, and physicians’ assistants may all see patients with genetic disorders, and Face2Gene could help those professionals screen children, says Rich Dineen, genetic counselor at the University of Illinois at Chicago. Dineen has not yet used the app himself, so he’s skeptical it will replace years of medical experience — but he’s hopeful it will help the field. “Any sort of technology we can use to make our job easier is a great thing,” he says.
Related: How the Tech Industry is Tackling the Cancer Moonshot
The technology certainly helped Abdul-Rahman and his patient. After getting Face2Gene’s opinion, he sent in an order for a genetic test, which verified the child’s diagnosis: Mowat-Wilson syndrome. That put an end to the family’s long search for a formal diagnosis, he says. “When the test came back, they were elated to finally have an answer.”
For more of the breakthroughs changing our lives, follow NBC MACH."
3a5dc45cfc,Artificial Intelligence: Find it Right in Your Own Backyard - FICO,"FICO Blog  /  Analytics & Optimization  /  Artificial Intelligence: Find it Right in Your Own Backyard
It seems like you can’t spend more than a minute or two reviewing online articles or social media without stumbling on something about artificial intelligence (AI) or machine learning. It’s the hottest thing since the iPhone. But while AI is heralded as amazing new technology, it is in fact an amazing 30+-year-old, proven technology. Like Dorothy in the “Wizard of Oz,” you don’t really have to look much further than your own backyard to find AI––specifically, in your own wallet, mobile or otherwise. Machine learning and AI are a prominent part of banking and processing payment cards, not just reserved for self-driving cars and computers that can win at Jeopardy. In banking, machine learning not only improves predictions, it is effective for improving subsequent decisions. Machine learning has a presence in many of our analytic models at FICO that are widely used in the financial industry. Here are a few examples. Fighting fraud The FICO® Falcon Fraud Management solution is the industry leader, protecting two thirds of the world’s payment card transactions against fraud. In far less than the blink of an eye (40-60 milliseconds, to be precise) Falcon scores each authorization that a merchant submits for approval. Here, Falcon uses adaptive analytics, a type of machine learning in which self-learning models work with the Falcon consortium models to improve the prediction of future fraudulent behavior based on fraud attacks in production. These models score transactions based on recent known fraud and non-fraud transaction data, dramatically improving the models’ sensitivity to changing fraud patterns, and keeping up with the fraudsters. Making credit decisions Many consumer credit decisions (such as applications for credit cards and loans) are made using FICO® Origination Manager. In this solution, machine learning helps significantly improve the overall predictive power of our models by determining the risk assessment of applicants through AI. The solution then uses those insights to inform an optimal model design and segmentation for a traditional scorecard model. Using this AI derived knowledge, and subsequently constructing traditional scorecard models, allows for strong improvement in model performance while sustaining the traditional advantages of scorecard models including transparency and explainability. FICO Score The FICO® Score is used by 90 of the top 100 largest US lending institutions for their risk assessment needs. A FICO Score is generated using multiple scorecards, with each scorecard tuned to assess risk for a specific consumer segment—for instance, consumers with serious delinquencies. Machine learning is used as a benchmarking tool for the scorecards that are manually developed, allowing the team to garner additional insights much more quickly. These insights help to inform variable generation and segmentation schemes. On a side note, I recently read a fascinating book on the social implications of big data, Weapons of Math Destruction. In it I was very pleased to read what author Cathy O’Neil says of the FICO Score: “Fair and Isaac’s great advance was to ditch the proxies in favor of the relevant financial data, like past behavior with respect to paying bills. They focused their analysis on the individual in question – and not on other people with similar attributes. E-scores, by contrast, march us back in time. They analyze the individual through a veritable blizzard of proxies.” FICO is proud that our history of empirical models based on quantifiable hard behavioral attributes and outcomes. To be recognized for it in this way is extremely gratifying. Marketing offers The FICO® Marketing Solutions Suite uses patented FICO analytics, rules management and optimization technology to enable large-scale personalized offer formulation. For example, one leading North American grocer uses Marketing Solutions Suite to score thousands of potential offers across millions of customers on a weekly basis, to determine the best set of offers for each customer within the loyalty program. Machine learning is used in this solution to bolster predictive results. The automated tree-ensemble models produce scores on the tens of thousands of products to offer to retail clients, and effectively specify the optimal set of offers to provide. The benefit of automated modeling has truly been seen in the time to market for these solutions, reducing the effort from months to days. Where are we going? Machine learning and AI aren’t only for making better predictions. In a time when real-time interactions are prevalent, decisions are being made on the fly. Through the use of machine learning algorithms, decisioning can be done instantaneously in our world of digital channels. We are already focusing on this area through our mobile analytics initiatives. Where I am going? If you’re a New Yorker, this week I’ll be in your backyard. I’m delighted to be keynoting at Predictive Analytics World for Financial in New York City on October 26, and at the Advisen Cyber Risks Insight Conference, also in NYC, on October 27. There I’ll be talking about the new FICO® Enterprise Security Score, which I’m incredibly excited about. Come see me! Or just follow me on Twitter @ScottZoldi. Posted by Scott Zoldi in Analytics & Optimization
Leave a comment Thank you for subscribing Scott Zoldi is chief analytics officer at FICO responsible for the analytic development of FICO's product and technology solutions. Bruce Curry is a senior principal consultant at FICO, helping clients across Europe, the Middle East and Africa improve performance in collections and recovery. Ethan Dornhelm, Vice President, FICO Scores and Predictive Analytics, leads the research and analytic development of FICO® Scores globally.
Copyright Fair Isaac Corporation. All rights reserved. FICO, myFICO, ScoreWatch,
Back To Top"
b2e34f0ec0,"Apple's Face ID For iPhone X Uses Advanced Machine Learning & 30,000 Infrared Dots To Map Faces; Here's How It Works","Apple’s 2017 event for the iPhone is bigger than anything we’ve seen before. For quite a while at least. The last time Apple turned this many heads was with Touch ID. Biometric recognition wasn’t known in the smartphone world then. And it also took Android a long time to catch up. But, as is with everything in the tech world, time passes fast. This time, it’s biometrics that have come to Apple’s rescue after a slow couple of years for the iPhone. Facial recognition is here folks. What a time to be alive. Take a look below for all the details. Last year, Samsung surprised all of us with the Galaxy Note7. In more ways than one really. The smartphone became the first gadget to feature Iris recognition for mainstream devices. But, the feature failed to catch on as the Note7 went up in flames. The Korean tech giant came back with a strong Galaxy S8/S8+ launch. The pair also feature Iris recognition, but so far, we haven’t seen that big of an impact for the feature. Proprietary knowledge is key for survival in the tech world. And right now, it’s Apple that’s got the lead. Make no mistake. It isn’t an easy task to bring facial recognition on a smartphone, especially with the size constraints for the flagship world. Face ID’s launch has put Apple in a good place. But this post is about how the feature works. Facial recognition requires four separate components, dedicated solely for the feature to work together. It’s protected by the Secure Enclave and projects 30,000 dots on your face (more on this below).
The components are a Dot Projector (structured light transmitter), the Infrared Camera (structured light receiver), Flood Illuminator (Ambient sensor) and a proximity sensor. Finally, the iPhone X’s front camera gathers 2D data regarding a user’s face. The Dot Projector starts the cycle, by generating Infrared rays. It’s up to the Infrared Camera to catch them, which then relays this information to the smartphone’s processor. The Flood Illuminator makes sure that color reproduction is on par with temperature and surrounding light. Since these components work at a short distance, the proximity sensor warns the user when he/she is out of reach. Together, not only do they make facial recognition possible, but also add another cool feature. The iPhone X won’t turn off or dim down display when you’re looking at it. The device will also support gesture recognition. Finally, Apple further claims that Facial ID uses machine learning to recognize your face. This means that any changes in appearance will automatically factor in for an updated mug shot. Thoughts? Let us know what you think in the comments section below and stay tuned. We’ll keep you updated on the latest."
37b1dd04c5,Google Maps gets a dose of machine learning - Google Maps for Mobile,"Google Maps is almost indispensable as a service in our lives. Directions, business listings, traffic updates, public transit information - Google Maps is probably that one app we use every time we go out. To constantly update the service with new addresses and information, the company has now decided to lean on some of its deep-learning tech for help. In a blog post, Google has now stated that its implementing deep learning techniques to Street View and Google Maps to help map new addresses. Google will now use the photographic data that Street View cars gather during their trips to extract additional information like street names and numbers. The problem is the photos that the Street View cars take are not clear. There may be discrepancies in the angle of the shot, distortions, obstacles, which make it difficult for a someone to decipher names and numbers. Google’s ReCaptcha solution also hit a roadblock as it’s impossible for humans to manually analyse over 80 billion hi-res photos collected by the Street View Cars. So, the Ground Truth team at Google infused artificial intelligence and deep learning to automatically extract information for the geo-located images. The deep neural net makes the process of reading the content of the images automated. The latest algorithm has achieved an 84.2 per cent accuracy on challenging French Street Name Signs, significantly outperforming the previous state-of-the-art systems. Google has made the algorithm publicly available on GitHub through TensorFlow, Google’s open-source machine learning software library. Google already uses machine learning to obscure faces and car license plates. Now it has started to use the technology to extract information from street signs. Google stated it could improve location data of one-third of the world’s addresses. The company improved the software to take not only the street numbers into account, but also the names. The algorithm can replace abbreviations with full names too and ignore any irrelevant texts in the photos. The same algorithm also helps map new streets and buildings that have not made it into the official maps of cities. It can also read the names of businesses enabling Google Maps to update information on business listings in the app.
A photographer with only a smartphone as his weapon of choice, Subhrojit spends most his time reading about the emerging trends in technology and occasionally dabbles in writing about them. He is seriously considering changing his mother tongue from Bengali to techspeak. He also swears by the Beatles and considers Ringo to be the fab among the Fab Four. More » The company is making a comeback after its failed partnership with LeEco More » The Pixel 2 will be indeed made by HTC and could bring in the HTC ... More » The handset is aimed for the mid-range enthusiasts looking for a phone with powerful specs ... More » Reports says Akash Ambani, Mukesh Ambani's elder son, will be leading the ... Skype will use Signal protocol, which is used in WhatsApp and Facebook ... Methods are employed to provide more accurate friend suggestions It was PSLV's 42nd mission 2018 will see massive adoption of Dolby technology in consumer electronics"
59c1e92411,An On-device Deep Neural Network for Face Detection - Apple,"Vol. 1, Issue 7 ∙
November 2017
November Two Thousand Seventeen
by Computer Vision Machine Learning Team
Apple started using deep learning for face detection in iOS 10. With the release of the Vision framework, developers can now use this technology and many other computer vision algorithms in their apps. We faced significant challenges in developing the framework so that we could preserve user privacy and run efficiently on-device. This article discusses these challenges and describes the face detection algorithm.
Apple first released face detection in a public API in the Core Image framework through the CIDetector class. This API was also used internally by Apple apps, such as Photos. The earliest release of CIDetector used a method based on the Viola-Jones detection algorithm [1]. We based subsequent improvements to CIDetector on advances in traditional computer vision. With the advent of deep learning, and its application to computer vision problems, the state-of-the-art in face detection accuracy took an enormous leap forward. We had to completely rethink our approach so that we could take advantage of this paradigm shift. Compared to traditional computer vision, the learned models in deep learning require orders of magnitude more memory, much more disk storage, and more computational resources. As capable as today’s mobile phones are, the typical high-end mobile phone was not a viable platform for deep-learning vision models. Most of the industry got around this problem by providing deep-learning solutions through a cloud-based API. In a cloud-based solution, images are sent to a server for analysis using deep learning inference to detect faces. Cloud-based services typically use powerful desktop-class GPUs with large amounts of memory available. Very large network models, and potentially ensembles of large models, can run on the server side, allowing clients (which could be mobile phones) to take advantage of large deep learning architectures that would be impractical to run locally. Apple’s iCloud Photo Library is a cloud-based solution for photo and video storage. However, due to Apple’s strong commitment to user privacy, we couldn’t use iCloud servers for computer vision computations. Every photo and video sent to iCloud Photo Library is encrypted on the device before it is sent to cloud storage, and can only be decrypted by devices that are registered with the iCloud account. Therefore, to bring deep learning based computer vision solutions to our customers, we had to address directly the challenges of getting deep learning algorithms running on iPhone. We faced several challenges. The deep-learning models need to be shipped as part of the operating system, taking up valuable NAND storage space. They also need to be loaded into RAM and require significant computational time on the GPU and/or CPU. Unlike cloud-based services, whose resources can be dedicated solely to a vision problem, on-device computation must take place while sharing these system resources with other running applications. Finally, the computation must be efficient enough to process a large Photos library in a reasonably short amount of time, but without significant power usage or thermal increase. The rest of this article discusses our algorithmic approach to deep-learning-based face detection, and how we successfully met the challenges to achieve state-of-the-art accuracy. We discuss: In 2014, when we began working on a deep learning approach to detecting faces in images, deep convolutional networks (DCN) were just beginning to yield promising results on object detection tasks. Most prominent among these was an approach called “OverFeat” [2] which popularized some simple ideas that showed DCNs to be quite efficient at scanning an image for an object. OverFeat drew the equivalence between fully connected layers of a neural network and convolutional layers with valid convolutions of filters of the same spatial dimensions as the input. This work made clear that a binary classification network of a fixed receptive field (for example 32x32, with a natural stride of 16 pixels) could be efficiently applied to an arbitrary sized image (for example, 320x320) to produce an appropriately sized output map (20x20 in this example). The OverFeat paper also provided clever recipes to produce denser output maps by effectively reducing the network stride. We built our initial architecture based on some of the insights from the OverFeat paper, resulting in a fully convolutional network (see Figure 1) with a multitask objective comprising of: We experimented with several ways of training such a network. For example, a simple procedure for training is to create a large dataset of image tiles of a fixed size corresponding to the smallest valid input to the network such that each tile produces a single output from the network. The training dataset is ideally balanced, so that half of the tiles contain a face (positive class) and the other half do not contain a face (negative class). For each positive tile, we provide the true location (x, y, w, h) of the face. We train the network to optimize the multitask objective described previously. Once trained, the network is able to predict whether a tile contains a face, and if so, it also provides the coordinates and scale of the face in the tile. Since the network is fully convolutional, it can efficiently process an arbitrary sized image and produce a 2D output map. Each point on the map corresponds to a tile in the input image and contains the prediction from the network regarding the presence of or absence of a face in that title and its location/scale within the input tile (see inputs and outputs of DCN in Figure 1). Given such a network, we could then build a fairly standard processing pipeline to perform face detection, consisting of a multi-scale image pyramid, the face detector network, and a post-processing module. We needed a multi-scale pyramid to handle faces across a wide range of sizes. We apply the network to each level of the pyramid and candidate detections are collected from each layer. (See Figure 2.) The post processing module then combines these candidate detections across scales to produce a list of bounding boxes that correspond to the network’s final prediction of the faces in the image. This strategy brought us closer to running a deep convolutional network on device to exhaustively scan an image. But network complexity and size remained key bottlenecks to performance. Overcoming this challenge meant not only limiting the network to a simple topology, but also restricting the number of layers of the network, the number of channels per layer, and the kernel size of the convolutional filters. These restrictions raised a crucial problem: our networks that were producing acceptable accuracy were anything but simple, most going over 20 layers and consisting of several network-in-network [3] modules. Using such networks in the image scanning framework described previously would be completely infeasible. They led to unacceptable performance and power usage. In fact, we would not even be able to load the network into memory. The challenge then was how to train a simple and compact network that could mimic the behavior of the accurate but highly complex networks. We decided to leverage an approach, informally called “teacher-student” training[4]. This approach provided us a mechanism to train a second thin-and-deep network (the “student”), in such a way that it matched very closely the outputs of the big complex network (the “teacher”) that we had trained as described previously. The student network was composed of a simple repeating structure of 3x3 convolutions and pooling layers and its architecture was heavily tailored to best leverage our neural network inference engine. (See Figure 1.) Now, finally, we had an algorithm for a deep neural network for face detection that was feasible for on-device execution. We iterated through several rounds of training to obtain a network model that was accurate enough to enable the desired applications. While this network was accurate and feasible, a tremendous amount of work still remained to make it practical for deploying on millions of user devices. Practical considerations around deep learning factored heavily into our design choices for an easy-to-use framework for developers, which we call Vision. It became quickly apparent that great algorithms are not enough for creating a great framework. We had to have a highly optimized imaging pipeline. We did not want developers to think about scaling, color conversions, or image sources. Face detection should work well whether used in live camera capture streams, video processing, or processing of images from disc or the web. It should work regardless of image representation and format. We were concerned with power consumption and memory usage, especially for streaming and image capture. We worried about memory footprint, such as the large one needed for a 64 Megapixel panorama. We addressed these concerns by using techniques of partial subsampled decoding and automatic tiling to perform computer vision tasks on large images even with non-typical aspect ratios. Another challenge was colorspace matching. Apple has a broad set of colorspace APIs but we did not want to burden developers with the task of color matching. The Vision framework handles color matching, thus lowering the threshold for a successful adoption of computer vision into any app. Vision also optimizes by efficient handling and reuse of intermediates. Face detection, face landmark detection, and a few other computer vision tasks work from the same scaled intermediate image. By abstracting the interface to the algorithms and finding a place of ownership for the image or buffer to be processed, Vision can create and cache intermediate images to improve performance for multiple computer vision tasks without the need for the developer to do any work. The flip side was also true. From the central interface perspective, we could drive the algorithm development into directions that allow for better reusing or sharing of intermediates. Vision hosts several different, and independent, computer vision algorithms. For the various algorithms to work well together, implementations use input resolutions and color spaces that are shared across as many algorithms as possible The joy of ease-of-use would quickly dissipate if our face detection API were not able to be used both in real time apps and in background system processes. Users want face detection to run smoothly when processing their photo libraries for face recognition, or analyzing a picture immediately after a shot. They don’t want the battery to drain or the performance of the system to slow to a crawl. Apple’s mobile devices are multitasking devices. Background computer vision processing therefore shouldn’t significantly impact the rest of the system’s features. We implement several strategies to minimize memory footprint and GPU usage. To reduce memory footprint, we allocate the intermediate layers of our neural networks by analyzing the compute graph. This allows us to alias multiple layers to the same buffer. While being fully deterministic, this technique reduces memory footprint without impacting the performance or allocations fragmentation, and can be used on either the CPU or GPU. For Vision, the detector runs 5 networks (one for each image pyramid scale as shown in Figure 2). These 5 networks share the same weights and parameters, but have different shapes for their input, output, and intermediate layers. To reduce footprint even further, we run the liveness-based memory optimization algorithm on the joint graph composed by those 5 networks, significantly reducing the footprint. Also, the multiple networks reuse the same weight and parameter buffers, thus reducing memory needs. To achieve better performance, we exploit the fully convolutional nature of the network: All the scales are dynamically resized to match the resolution of the input image. Compared to fitting the image in square network retinas (padded by void bands), fitting the network to the size of the image allows us to reduce drastically the number of total operations. Because the topology of the operation is not changed by the reshape and the high performance of the rest of the allocator, dynamic reshaping does not introduce performance overhead related to allocation. To ensure UI responsiveness and fluidity while deep neural networks run in background, we split GPU work items for each layer of the network until each individual time is less than a millisecond. This allows the driver to switch contexts to higher priority tasks in a timely manner, such as UI animations, thus reducing and sometimes eliminating frame drop. Combined, all these strategies ensure that our users can enjoy local, low-latency, private deep learning inference without being aware that their phone is running neural networks at several hundreds of gigaflops per second. Did we accomplish what we set as our goal of developing a performant, easy-to-use, face detection API? You can try the Vision framework and judge for yourself. Here’s how to get started: [1] Viola, P. and Jones, M.J. Robust Real-time Object Detection Using a Boosted Cascade of Simple Features. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2001. [2] Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks. arXiv:1312.6229 [Cs], December, 2013. [3] Lin, Min, Qiang Chen, and Shuicheng Yan. Network In Network. arXiv:1312.4400 [Cs], December, 2013. [4] Romero, Adriana, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for Thin Deep Nets. arXiv:1412.6550 [Cs], December, 2014. [5] Tam, A. Core ML and Vision: Machine learning in iOS Tutorial. Retrieved from https://www.raywenderlich.com, September, 2017. Send questions or feedback via email Apply now for a career at Apple Apple Developer Program"
55fe662028,Detecting consumer decisions within messy data | MIT News,"Login
or
Subscribe Newsletter
“In health care, there’s this gigantic world of unstructured data that needs to be translated into useable information,” says Paul Nemirovsky, who co-founded dMetrics with Ariadna Quattoni. Image: MIT News Software analyzes online chatter to predict health care consumers’ behavior.
Rob Matheson | MIT News Office
December 21, 2015
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
Millions of people each month report positive and negative health care feedback across the Web. Some jump into forums to complain about ineffective prescriptions or to discuss which drugs are best to treat illnesses. Others take to blogs to describe symptoms and how to get relief. MIT spinout dMetrics believes this online chatter is an information treasure-trove for the health care industry. “In health care, there’s this gigantic world of unstructured data that needs to be translated into useable information,” says Paul Nemirovsky PhD ’06, who co-founded dMetrics with Ariadna Quattoni PhD ’09. The startup has developed a platform called DecisionEngine that uses machine learning and natural language processing — which helps computers better understand human speech — to mine billions of conversations about drugs, medical devices, and other health care products. These discussions are happening on blogs, Facebook, Twitter, forums, and even in comments accompanying news articles and videos. From those vast stores of messy data, the software reveals insights into consumer decisions, Nemirovsky says: “What people do, don’t do, consider doing, may do, did in the past, as well as what needs, fears, and hopes they have.” Today, Nemirovsky explains, dMetrics has a database that includes every public comment about patient-reported illnesses, solutions, and outcomes, pulled from more than 1 million online sources. This includes information on more than 14,000 health care products. Clients, including Fortune 500 companies and nonprofit organizations, can use dMetrics software to answer specific questions, such as how many patients used a specific medication for a particular reason in certain time frame, or which customers are considering switching from their drug to a competitor’s drug. Although focusing on the health care industry, dMetrics, headquartered in Brooklyn, New York, is also trialing its platform with consumer finance and political organizations. Credit card companies, for instance, can analyze why consumers favor specific credit cards over others. Political scientists could use the software to determine which issues people care about and how strongly they stand behind their opinions. “For all these types of questions, you have to understand not only the words people use but the concepts behind the words,” Nemirovsky says. Decoding language and expression Other software generally relies on ontologies — formal naming and definitions — to sense overall sentiment and popularity of brands, Nemirovsky says. The software may count, for example, the number of mentions of a word (such as the name of a specific drug) to determine if it’s important, or it may detect “positive” or “negative” words. “Language and expression doesn’t work like that,” Nemirovsky says. “We’re a bit more complex as humans.” DecisionEngine, Nemirovsky says, better derives meaning from text because the software — which now consists of around 2 million lines of code — is consistently trained to recognize various words and synonyms, and to interpret syntax and semantics. “Online text is incredibly tough to analyze properly,” he says. “There’s slang, misspellings, run-on sentences, and crazy punctuation. Discussion is messy.” Visualize the software as a three-tiered funnel, Nemirovsky suggests, with more refined analysis happening as the funnel gets narrower. At the top of the funnel, the software mines all mentions of a particular word or phrase associated with a certain health care product, while filtering out “noise” such as fake websites and users, or spam. The next level down involves separating out commenters’ personal experiences over, say, marketing materials and news. The bottom level determines people’s decisions and responses, such as starting to use a product — or even considering doing so, experiencing fear or confusion, or switching to a different medication. To explain, Nemirovsky provides an example comment that could appear in an online forum: “I'm now on Drug A and took 10 mgs of Drug B, and it seemed to sync well. I'm seeing my doc tomorrow to ask about adding Drug C to my current meds. For me personally Drug A is a very tricky drug, only helpful if I'm getting good sleep, eat and exercise well and limit the use to couple times a week.” Other software, he says, may only detect positive and negative words (such as “well” and “good” versus “tricky” and “limit”). DecisionEngine, on the other hand, would identify many more pieces of information, including the use and effectiveness of Drugs A and B combined; the dosage of Drug B; consideration for adopting Drug C; potential dissatisfaction with Drug A, depending on lifestyle choices such as “getting good sleep”; the commenter’s use of three concurrent medications; and plans of visiting a health care professional. These insights allow clients to take action, Nemirovsky says. If consumers are planning to switch drugs, for instance, a pharmaceutical firm may want to ensure that the consumers are using their products properly, and to find a means to address any issues. Recently, Nemirovsky says, a pharmaceutical firm used DecisionEngine to determine if an allergy medication had improved the quality of life for a subgroup of patients. Analyzing specific issues associated with the subgroup, the firm discovered that the drug had an outsized positive impact, more so than several competing brands. The firm used the results in a regulatory submission — a critical stage in bringing any health care product to market. “It’s rare for the regulatory authorities to consider online patient reports as part of the regulatory approval process,” Nemirovsky says. Everyone’s an expert In the late 2000s at MIT, Nemirovsky, who was an MIT Media Lab graduate student, and Quattoni, who was studying at the Computer Science and Artificial Intelligence Laboratory (CSAIL), came together with a lofty goal: Use big data to make everyone experts. The plan was to combine machine learning with natural language processing to decode mountains of unstructured data and provide pertinent information, about anything, to anyone who wanted. “If you give people the right information, at the right time, anyone can be an expert,” Nemirovsky says. In building the software, they discovered that an important topic for most people on a daily basis is health care. “Patients go to the doctor with complex conditions, and sometimes they leave with less certainty they had before,” Nemirovsky says. “Then they go online and say, ‘What on Earth is going on? What do I do?’” Focusing on the health care industry, they turned to MIT’s Venture Mentoring Service, which helped them navigate various startup issues: fundraising, operations, marketing, legal issues, and other things. “Things that sound obvious now, were not obvious to us at all,” Nemirovsky says. “We were helped a lot by the VMS, especially as first-time entrepreneurs.” Soon after Nemirovsky graduated, he and Quattoni launched dMetrics in Boston, before relocating to Brooklyn. Over the years, the startup expanded from two to 16 employees — whose machine learning and natural language processing research has been cited in more than 4,500 academic journals total — and earned four National Science Foundation grants to develop its technology. Moving forward, dMetrics aims to bring its software to more sectors than health care, politics, and consumer finance, with aims of empowering everyone with data. In that way, Nemirovsky says, the dMetrics mission hasn’t changed much from its early MIT days: “It’s our vision that we need to open means of expertise to everyone.” Topics: Innovation and Entrepreneurship (I&E), Startups, Alumni/ae, Big data, Health care, Analytics, Venture Mentoring Service, Media Lab, Computer Science and Artificial Intelligence Laboratory, School of Architecture and Planning, School of Engineering, Computer science and technology
Mike Johnson
December 22, 2015
This is EXTREMELY interesting - pluripotent research- please discuss elliptically
:-) This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
c3e7ef571a,How PayPal beats the bad guys with machine learning | InfoWorld,"By Eric Knorr,
Editor in Chief,
InfoWorld
|
Apr 13, 2015
Use commas to separate multiple email addresses Your message has been sent. There was an error emailing this page. When Amazon Web Services announced a new machine learning service for its cloud last week, it was a sort of mini-milestone. Now all four of the top clouds -- Amazon, Microsoft, Google, and IBM -- will offer developers the means to build machine learning into their cloud applications. What sort of applications? As InfoWorld’s Andrew Oliver has observed, both machine learning and big data will eventually disappear as separate technology categories and insinuate themselves into many, many different aspects of computing. Nonetheless, right now certain uses of machine learning stand out for their immediate payback. Fraud detection is first among them, because it addresses an urgent problem that would be impractical to solve if machine learning didn't exist. To get a sense of how machine learning is combating fraud, I interviewed Dr. Hui Wang, senior director of risk sciences for PayPal. Wang holds a Ph.D. in statistics from UC Berkeley, and prior to her 11 years at PayPal conducted credit scoring research at Fair Isaac. You can easily imagine why PayPal would be concerned about fraud, given the innumerable scams that have targeted PayPal users. As it turns out, however, PayPal has already ventured beyond fraud detection to address other areas of risk management, including “modern machine learning in the credit decision world,” which Wang says is a lot more complex -- in part due to regulatory requirements. According to Wang, PayPal is a pioneer in risk management, although some advanced efforts are just now emerging from the lab. PayPal uses three types of machine learning algorithms for risk management: linear, neural network, and deep learning. Experience has shown PayPal that in many cases, the most effective approach is to use all three at once. Running linear algorithms to detect fraud is an established practice, Wang says, where “we can separate the good from the bad with one straight line.” When this either/or categorization fails, however, things get more interesting: Neural net algorithms were developed decades ago, but today’s modern computing infrastructure -- along with the enormous quantity of data we can now throw at those algorithms -- has increased neural net effectiveness by a magnitude. Wang says these advances have been essential for risk management: Quickly determining trustworthy customers and putting them in the express lane to a transaction is a key objective, Wang explains, using caching mechanisms to run relational queries and linear algorithms, among other techniques. The more sophisticated algorithms apply to customers who may be problematic, which slows down the system a bit as it acquires more data to perform in-depth screening. This downstream process extends all the way to deep learning, which today also powers computer vision, speech recognition, and other applications. When I asked Wang for a layman’s explanation of the difference between neural nets and deep learning, she offered this explanation: Wang emphasizes that you need large quantities of data to support these complex neural network structures. PayPal itself collects gargantuan amounts of data about buyers and sellers, including their network information, machine information, and financial data. The deep learning beast is well fed. But again, PayPal does not use deep learning in isolation. It applies all three together: linear, neural network, and deep learning algorithms. Wang explains why: Wang says she is proud to be managing the data science team at PayPal, which is on the forefront of developing machine learning and data mining technology. Because her team is so advanced, particularly in the practical application of deep learning, I couldn’t resist asking her what she thought about widely publicized warnings from Stephen Hawking, Elon Musk, and Bill Gates regarding the potential dangers of artificial intelligence in the future: There’s a practical lesson in that statement: Now and in the future, machine learning depends not only on big data, but on the right data. Cloud infrastructure and integration presents abundant compute power for deep learning -- as well as access to unthinkably large data sets across a potentially unlimited number of domains. What we refer to as big data and machine learning today will tomorrow simply be integrated into the fabric of computing. As the major cloud providers open up those capabilities to all developers, the stage is set for a new wave of applications that will be much more intelligent than before. Eric Knorr is the editor in chief of IDG Enterprise. Eric has received the Neal and Computer Press Awards for journalistic excellence. Copyright © 2018 IDG Communications, Inc."
46525dbc6f,This neural network has got some fascinating Halloween costume ideas,"After producing more than 100 fascinating craft beer names with the help of artificial intelligence, research scientist Janelle Shane is again offering some help – this time by creating first-of-its-kind machine learning algorithm that gives out some really interesting Halloween costume ideas. According to a report in Business Insider, Shane tried simplifying the costume-selection task by feeding a neural network with over 4,500 names of Halloween costumes. She noted on her blog that the massive dataset was crowdsourced from the internet, which included classics as well as technically challenging names such as, Invisible Pink Unicorn, Whale-frog, Glow Cloud, Lake Michigan, Toaster Oven, and Garnet. The information was then processed by the algorithm, which pulled out a bunch of new and interesting costume ideas. It is important to note that instead of analyzing the meaning of the words, the neural network learned word patterns from the scratch, letter-by-letter, to create new ones. It initially produced some common names, but got better and pulled out names like Punk Tree, Disco Monster, Spartan Gandalf, Starfleet Shark, A masked box, Martian Devil, Panda Clam, Party Scarecrow, Potato man, Dragon of Liberty, Pumpkin King, and more. The complete list of costume ideas can be accessed through her blog post. Detailing the algorithm to Business Insider, Shane said, ""I would argue that the Halloween costume neural network is actually right up there at coming up with creative things that humans love. It can form its own rules about what it's seeing rather than just memorising."" A few months back, Shane's neural network did a tremendous job at giving away a plethora of novel craft beer names. One of those – The Fine Stranger – has actually been given to a beer.
I trained a neural network to generate Halloween costumes. It had trouble w/the sexy costumes, but loved sharks. https://t.co/H1YW5qjckD pic.twitter.com/hEDWKc97gt I need to show you some of the costumes from the bonus names that wouldn't fit in my original post. Because - BOBA FECK pic.twitter.com/MIOV5AJP33"
369b5dcc08,How machine learning is helping Virgin boost its frequent flyer business | ZDNet,"This web site uses cookies to improve your experience. By viewing our content, you are accepting the use of cookies. To find out more and change your cookie settings, please
view our cookie policy.
Using DataRobot's automated machine learning service, Virgin Australia has been able to cut down the time it takes to build predictive models by up to 90 percent, while boosting accuracy by up to 15 percent.
By Tas Bindi
|
November 21, 2017 -- 03:04 GMT (03:04 GMT)
| Topic: Innovation
Companies that are able to adapt to a world where innovation is increasingly driven by machine learning, or artificial intelligence more broadly, are the ones that will come out the other end of the tunnel and thrive, according to Oliver Rees, GM of Torque Data at Virgin Australia. Rees, whose data analytics consultancy firm Torque Data was acquired by Virgin Australia in 2015, told ZDNet that one of its tasks has been ""reengineering [Virgin's] analytical capability"", ensuring the airline is well-prepared to embrace the opportunities that are offered by machine learning. While not new to machine learning, Virgin Australia has been seeking better methods of developing, applying, and assessing machine learning algorithms, recently turning to Massachusetts-based company DataRobot, which operates on the belief that automated machine learning will not only increase productivity for data scientists, but also open up the world of data science.
Rees told ZDNet that Torque, as the data analytics arm of Virgin, has been investigating ways to improve customer experience for members of Virgin's Velocity Frequent Flyer loyalty program.
""We want people within our program to be able to redeem points for great experiences, and to do that, we want to be able to better predict when is the best time for particular people to redeem points and what should they be redeeming them against,"" Rees said. ""For a given individual or a group of people, we want the ability to be able to better understand what it is that they might be really interested in doing at a certain destination -- from both a business and a leisure point of view -- and how can we serve them better at that destination, and then be able to tailor that to their specific requirements and serve that up to them in a meaningful way. ""I think we need to take it upon ourselves in the industry to build the predictive models that understand what the needs and wants of our customers are, and go through the whole curation process, become their concierge."" Using DataRobot's automated machine learning service, Virgin Australia is looking to build models that can predict the types of people that are more likely to travel, the types of travel people are likely to undertake, the prices that travellers are willing to pay, the importance of accommodation relative to travel, and the importance of experience compared to travel.
A lot of information is provided willingly by customers, Rees said, but the company also uses previous purchases to predict future preferences.
""There's a lot we can impute from what they do already, but also by actually looking at events and the type of events that people like to attend -- that's a really powerful piece of information,"" Rees said.
""It all comes back to using what we understand already about people, and not necessarily as individuals either, but as groups, so we look at tribes and cohorts ... Often people want guidance around what do people like me do and how can I as an individual benefit from the learnings of others.
""We need to not only understand people based on what they tell us themselves, but also what people like them are telling us, what experiences people like them are enjoying ... that's where the competitive battleground is.""
Using technology provided by DataRobot, which raised more than $124 million since its inception in 2012, Rees said Torque has been able to build new predictive models at one-tenth of the time it had previously taken, and the models are up to 15 percent more accurate than previous ones. ""We have the ability to run multiple different statistical techniques against the same dataset in a very short space of time and have this competitive element whereby the models compete against each other for the best outcome,"" Rees said. ""[DataRobot's system is] constantly reviewing whether this particular technique can outperform this other technique, and that's running really in real-time before my eyes, whereas an analyst might spend a number of hours running one particular model. ""The ability for us to then deploy that model is also enhanced. It's direct API link and we can start to operationalise the analytics far more quickly because there's a reduced number of steps in doing that."" Rees also noted that analysts can be biased towards specific statistical techniques, the same way artists prefer to use watercolour, oil, or acrylic paints.
""So it removes that element of bias as well,"" he added. ""I think that's very powerful; it makes us stronger as an organisation."" Additionally, using an automated machine learning service means analysts are able to spend more time on uncovering opportunities to use analytics and less time on grunt work like manipulating data, Rees said.
""We're actually moving really smart people into different roles where we're using their intellect in a really powerful way,"" he said.
""The way to hang on to great analytical teams is to give them leading edge tools to work with and challenging meaningful analytical problems to solve.
""People are very interested in building their understanding around how new technology is going to impact their work. Giving people the opportunity to learn how it works and recognising that we're all on this journey together ... I think it's been a real positive for us."" Rees believes technology, like that offered by DataRobot, will allow Virgin to ""democratise analytics"" throughout the organisation, enabling all business units to be data-driven.
""Analytics is far too important to be left to the analyst. With these sorts of tools ... we can have the whole business being more data-driven because the power to build really great models lies in many more hands,"" he said. ""It doesn't mean, by the way, that we're not going to need the really powerful hardcore analytical teams that we have -- of course we're going to need those sort of people -- but it means that a wider number of people within our business are going to be able to rely on analytics to help them with faster decision-making."" According to Rees, the real challenge for large organisations like Virgin is ""avoiding falling into the trap of doing nothing and not embracing the new technologies available"".
""It can very easily be seen as something that creates extra work, creates extra stress, creates extra pressure. We can't ignore what we need to do on day-to-day basis at the expense of developing new capability,"" he said.
""Being in an organisation that is prepared to be innovative and move forward is a really important. It's a fine balance."" Rees said the application of machine learning is a ""quantum leap forward"" in how data can be used to drive better outcomes for both customers and businesses, and that businesses are barely scratching the surface of what's possible.
Can machine learning bail enterprises out of their data management woes?
Technology vendors are scrambling to create an abstraction layer that can clean up data and make it easier to prep for analytics. The problem is humans haven't been the best stewards of organized data. As a result, machine learning is the next magic bullet for data management issues dating back decades.
AI, robotics, IoT, augmented and virtual reality to bolster ICT spending
According to IDC, spending on new technologies will accelerate over the next five years and boost spending on information and communications technology overall.
Data to analytics to AI: From descriptive to predictive analytics
Artificial Intelligence seems to be the buzzword du jour for organizations, but this is not an obvious or straightforward transition even for those building advanced products and platforms. Many organizations are still struggling with digital transformation to become data-driven, so how should they approach this new challenge? Video: How machine learning creates demand for human workers (TechRepublic) Artificial intelligence and IoT are increasing demand for human workers, says Calabrio CEO Tom Goodmanson, and they're more stressed out than ever. How AI and machine learning will help the rise of quantum computing (TechRepublic) When quantum computers come online, all encryption will fail. James Barratt explains how AI will aid the way for the emergence of quantum computing. Mobility Telstra CEO: 2018 will be big for 5G Innovation Cortana isn't dead. She just needs to get to work. Artificial Intelligence CES 2018: Intel-Ferrari AI partnership will fundamentally change sports broadcasts Innovation Robots are helping researchers work to end world hunger © 2018 CBS Interactive. All rights reserved. Privacy Policy | Cookies | Ad Choice |
Advertise | Terms of Use | Mobile User Agreement"
04b1b46477,An On-device Deep Neural Network for Face Detection - Apple,"Vol. 1, Issue 7 ∙
November 2017
November Two Thousand Seventeen
by Computer Vision Machine Learning Team
Apple started using deep learning for face detection in iOS 10. With the release of the Vision framework, developers can now use this technology and many other computer vision algorithms in their apps. We faced significant challenges in developing the framework so that we could preserve user privacy and run efficiently on-device. This article discusses these challenges and describes the face detection algorithm.
Apple first released face detection in a public API in the Core Image framework through the CIDetector class. This API was also used internally by Apple apps, such as Photos. The earliest release of CIDetector used a method based on the Viola-Jones detection algorithm [1]. We based subsequent improvements to CIDetector on advances in traditional computer vision. With the advent of deep learning, and its application to computer vision problems, the state-of-the-art in face detection accuracy took an enormous leap forward. We had to completely rethink our approach so that we could take advantage of this paradigm shift. Compared to traditional computer vision, the learned models in deep learning require orders of magnitude more memory, much more disk storage, and more computational resources. As capable as today’s mobile phones are, the typical high-end mobile phone was not a viable platform for deep-learning vision models. Most of the industry got around this problem by providing deep-learning solutions through a cloud-based API. In a cloud-based solution, images are sent to a server for analysis using deep learning inference to detect faces. Cloud-based services typically use powerful desktop-class GPUs with large amounts of memory available. Very large network models, and potentially ensembles of large models, can run on the server side, allowing clients (which could be mobile phones) to take advantage of large deep learning architectures that would be impractical to run locally. Apple’s iCloud Photo Library is a cloud-based solution for photo and video storage. However, due to Apple’s strong commitment to user privacy, we couldn’t use iCloud servers for computer vision computations. Every photo and video sent to iCloud Photo Library is encrypted on the device before it is sent to cloud storage, and can only be decrypted by devices that are registered with the iCloud account. Therefore, to bring deep learning based computer vision solutions to our customers, we had to address directly the challenges of getting deep learning algorithms running on iPhone. We faced several challenges. The deep-learning models need to be shipped as part of the operating system, taking up valuable NAND storage space. They also need to be loaded into RAM and require significant computational time on the GPU and/or CPU. Unlike cloud-based services, whose resources can be dedicated solely to a vision problem, on-device computation must take place while sharing these system resources with other running applications. Finally, the computation must be efficient enough to process a large Photos library in a reasonably short amount of time, but without significant power usage or thermal increase. The rest of this article discusses our algorithmic approach to deep-learning-based face detection, and how we successfully met the challenges to achieve state-of-the-art accuracy. We discuss: In 2014, when we began working on a deep learning approach to detecting faces in images, deep convolutional networks (DCN) were just beginning to yield promising results on object detection tasks. Most prominent among these was an approach called “OverFeat” [2] which popularized some simple ideas that showed DCNs to be quite efficient at scanning an image for an object. OverFeat drew the equivalence between fully connected layers of a neural network and convolutional layers with valid convolutions of filters of the same spatial dimensions as the input. This work made clear that a binary classification network of a fixed receptive field (for example 32x32, with a natural stride of 16 pixels) could be efficiently applied to an arbitrary sized image (for example, 320x320) to produce an appropriately sized output map (20x20 in this example). The OverFeat paper also provided clever recipes to produce denser output maps by effectively reducing the network stride. We built our initial architecture based on some of the insights from the OverFeat paper, resulting in a fully convolutional network (see Figure 1) with a multitask objective comprising of: We experimented with several ways of training such a network. For example, a simple procedure for training is to create a large dataset of image tiles of a fixed size corresponding to the smallest valid input to the network such that each tile produces a single output from the network. The training dataset is ideally balanced, so that half of the tiles contain a face (positive class) and the other half do not contain a face (negative class). For each positive tile, we provide the true location (x, y, w, h) of the face. We train the network to optimize the multitask objective described previously. Once trained, the network is able to predict whether a tile contains a face, and if so, it also provides the coordinates and scale of the face in the tile. Since the network is fully convolutional, it can efficiently process an arbitrary sized image and produce a 2D output map. Each point on the map corresponds to a tile in the input image and contains the prediction from the network regarding the presence of or absence of a face in that title and its location/scale within the input tile (see inputs and outputs of DCN in Figure 1). Given such a network, we could then build a fairly standard processing pipeline to perform face detection, consisting of a multi-scale image pyramid, the face detector network, and a post-processing module. We needed a multi-scale pyramid to handle faces across a wide range of sizes. We apply the network to each level of the pyramid and candidate detections are collected from each layer. (See Figure 2.) The post processing module then combines these candidate detections across scales to produce a list of bounding boxes that correspond to the network’s final prediction of the faces in the image. This strategy brought us closer to running a deep convolutional network on device to exhaustively scan an image. But network complexity and size remained key bottlenecks to performance. Overcoming this challenge meant not only limiting the network to a simple topology, but also restricting the number of layers of the network, the number of channels per layer, and the kernel size of the convolutional filters. These restrictions raised a crucial problem: our networks that were producing acceptable accuracy were anything but simple, most going over 20 layers and consisting of several network-in-network [3] modules. Using such networks in the image scanning framework described previously would be completely infeasible. They led to unacceptable performance and power usage. In fact, we would not even be able to load the network into memory. The challenge then was how to train a simple and compact network that could mimic the behavior of the accurate but highly complex networks. We decided to leverage an approach, informally called “teacher-student” training[4]. This approach provided us a mechanism to train a second thin-and-deep network (the “student”), in such a way that it matched very closely the outputs of the big complex network (the “teacher”) that we had trained as described previously. The student network was composed of a simple repeating structure of 3x3 convolutions and pooling layers and its architecture was heavily tailored to best leverage our neural network inference engine. (See Figure 1.) Now, finally, we had an algorithm for a deep neural network for face detection that was feasible for on-device execution. We iterated through several rounds of training to obtain a network model that was accurate enough to enable the desired applications. While this network was accurate and feasible, a tremendous amount of work still remained to make it practical for deploying on millions of user devices. Practical considerations around deep learning factored heavily into our design choices for an easy-to-use framework for developers, which we call Vision. It became quickly apparent that great algorithms are not enough for creating a great framework. We had to have a highly optimized imaging pipeline. We did not want developers to think about scaling, color conversions, or image sources. Face detection should work well whether used in live camera capture streams, video processing, or processing of images from disc or the web. It should work regardless of image representation and format. We were concerned with power consumption and memory usage, especially for streaming and image capture. We worried about memory footprint, such as the large one needed for a 64 Megapixel panorama. We addressed these concerns by using techniques of partial subsampled decoding and automatic tiling to perform computer vision tasks on large images even with non-typical aspect ratios. Another challenge was colorspace matching. Apple has a broad set of colorspace APIs but we did not want to burden developers with the task of color matching. The Vision framework handles color matching, thus lowering the threshold for a successful adoption of computer vision into any app. Vision also optimizes by efficient handling and reuse of intermediates. Face detection, face landmark detection, and a few other computer vision tasks work from the same scaled intermediate image. By abstracting the interface to the algorithms and finding a place of ownership for the image or buffer to be processed, Vision can create and cache intermediate images to improve performance for multiple computer vision tasks without the need for the developer to do any work. The flip side was also true. From the central interface perspective, we could drive the algorithm development into directions that allow for better reusing or sharing of intermediates. Vision hosts several different, and independent, computer vision algorithms. For the various algorithms to work well together, implementations use input resolutions and color spaces that are shared across as many algorithms as possible The joy of ease-of-use would quickly dissipate if our face detection API were not able to be used both in real time apps and in background system processes. Users want face detection to run smoothly when processing their photo libraries for face recognition, or analyzing a picture immediately after a shot. They don’t want the battery to drain or the performance of the system to slow to a crawl. Apple’s mobile devices are multitasking devices. Background computer vision processing therefore shouldn’t significantly impact the rest of the system’s features. We implement several strategies to minimize memory footprint and GPU usage. To reduce memory footprint, we allocate the intermediate layers of our neural networks by analyzing the compute graph. This allows us to alias multiple layers to the same buffer. While being fully deterministic, this technique reduces memory footprint without impacting the performance or allocations fragmentation, and can be used on either the CPU or GPU. For Vision, the detector runs 5 networks (one for each image pyramid scale as shown in Figure 2). These 5 networks share the same weights and parameters, but have different shapes for their input, output, and intermediate layers. To reduce footprint even further, we run the liveness-based memory optimization algorithm on the joint graph composed by those 5 networks, significantly reducing the footprint. Also, the multiple networks reuse the same weight and parameter buffers, thus reducing memory needs. To achieve better performance, we exploit the fully convolutional nature of the network: All the scales are dynamically resized to match the resolution of the input image. Compared to fitting the image in square network retinas (padded by void bands), fitting the network to the size of the image allows us to reduce drastically the number of total operations. Because the topology of the operation is not changed by the reshape and the high performance of the rest of the allocator, dynamic reshaping does not introduce performance overhead related to allocation. To ensure UI responsiveness and fluidity while deep neural networks run in background, we split GPU work items for each layer of the network until each individual time is less than a millisecond. This allows the driver to switch contexts to higher priority tasks in a timely manner, such as UI animations, thus reducing and sometimes eliminating frame drop. Combined, all these strategies ensure that our users can enjoy local, low-latency, private deep learning inference without being aware that their phone is running neural networks at several hundreds of gigaflops per second. Did we accomplish what we set as our goal of developing a performant, easy-to-use, face detection API? You can try the Vision framework and judge for yourself. Here’s how to get started: [1] Viola, P. and Jones, M.J. Robust Real-time Object Detection Using a Boosted Cascade of Simple Features. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2001. [2] Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks. arXiv:1312.6229 [Cs], December, 2013. [3] Lin, Min, Qiang Chen, and Shuicheng Yan. Network In Network. arXiv:1312.4400 [Cs], December, 2013. [4] Romero, Adriana, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for Thin Deep Nets. arXiv:1412.6550 [Cs], December, 2014. [5] Tam, A. Core ML and Vision: Machine learning in iOS Tutorial. Retrieved from https://www.raywenderlich.com, September, 2017. Send questions or feedback via email Apply now for a career at Apple Apple Developer Program"
5d3a3b844a,Bug-repair system learns from example | MIT News,"Login
or
Subscribe Newsletter
A new machine-learning system analyzes successful repairs to buggy software and learns how to repair new bugs.
Automatic code-patching system corrects nearly twice as many errors as its predecessors.
Larry Hardesty | MIT News Office
September 28, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office Anyone who’s downloaded an update to a computer program or phone app knows that most commercial software has bugs and security holes that require regular “patching.” Often, those bugs are simple oversights. For example, the program tries to read data that have already been deleted. The patches, too, are often simple — such as a single line of code that verifies that a data object still exists. That simplicity has encouraged computer scientists to explore the possibility of automatic patch generation. Several research groups, including that of Martin Rinard, an MIT professor of electrical engineering and computer science, have developed templates that indicate the general forms that patches tend to take. Algorithms can then use the templates to generate and evaluate a host of candidate patches. Recently, at the Association for Computing Machinery’s Symposium on the Foundations of Software Engineering, Rinard, his student Fan Long, and Peter Amidon of the University of California at San Diego presented a new system that learns its own templates by analyzing successful patches to real software. Where a hand-coded patch-generation system might feature five or 10 templates, the new system created 85, which makes it more diverse but also more precise. Its templates are more narrowly tailored to specific types of real-world patches, so it doesn’t generate as many useless candidates. In tests, the new system, dubbed Genesis, repaired nearly twice as many bugs as the best-performing hand-coded template system. Thinning the herd “You are navigating a tradeoff,” says Long, an MIT graduate student in electrical engineering and computer science and first author on the paper. “On one hand, you want to generate enough candidates that the set you’re looking through actually contains useful patches. On the other hand, you don’t want the set to include so many candidates that you can’t search through it.” Every item in the data set on which Genesis was trained includes two blocks of code: the original, buggy code and the patch that repaired it. Genesis begins by constructing pairs of training examples, such that every item in the data set is paired off with every other item. Genesis then analyzes each pair and creates a generic representation — a draft template — that will enable it to synthesize both patches from both originals. It may synthesize other, useless candidates, too. But the representation has to be general enough that among the candidates are the successful patches. Next, Genesis tests each of its draft templates on all the examples in the training set. Each of the templates is based on only two examples, but it might work for several others. Each template is scored on two criteria: the number of errors that it can correct and the number of useless candidates it generates. For instance, a template that generates 10 candidates, four of which patch errors in the training data, might score higher than one that generates 1,000 candidates and five correct patches. On the basis of those scores, Genesis selects the 500 most promising templates. For each of them, it augments the initial two-example training set with each of the other examples in turn, creating a huge set of three-example training sets. For each of those, it then varies the draft template, to produce a still more general template. Then it performs the same evaluation procedure, extracting the 500 most promising templates. Covering the bases After four rounds of this process, each of the 500 top-ranking templates has been trained on five examples. The final winnowing uses slightly different evaluation criteria, ensuring that every error in the training set that can be corrected will be. That is, there may be a template among the final 500 that patches only one bug, earning a comparatively low score in the preceding round of evaluation. But if it’s the only template that patches that bug, it will make the final cut. In the researchers’ experiments, the final winnowing reduced the number of templates from 500 to 85. Genesis works with programs written in the Java programming language, and the MIT researchers compared its performance with that of the best-performing hand-coded Java patch generator. Genesis correctly patched defects in 21 of 49 test cases drawn from 41 open-source programming projects, while the previous system patched 11. It’s possible that more training data and more computational power — to evaluate more candidate templates — could yield still better results. But a system that allows programmers to spend only half as much time trying to repair bugs in their code would be useful nonetheless. Topics: Research, School of Engineering, Computer science and technology, Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical Engineering & Computer Science (eecs), Machine learning, Software This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
3f5ee86ce0,Artificial intelligence suggests recipes based on food photos | MIT News,"Login
or
Subscribe Newsletter
Pic2Recipe, an artificial intelligence system developed at MIT, can take a photo of an entree and suggest a similar recipe to it. Photo: Jason Dorfman/MIT CSAIL Pic2Recipe predicts recipes from photos using a neural network, a way to achieve machine learning in which a computer learns to perform some task by analyzing examples.
Photo: Jason Dorfman/MIT CSAIL Given a still image of a dish filled with food, CSAIL team's deep-learning algorithm recommends ingredients and recipes.
Watch Video
Adam Conner-Simons | Rachel Gordon | CSAIL
July 20, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab There are few things social media users love more than flooding their feeds with photos of food. Yet we seldom use these images for much more than a quick scroll on our cellphones. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) believe that analyzing photos like these could help us learn recipes and better understand people's eating habits. In a new paper with the Qatar Computing Research Institute (QCRI), the team trained an artificial intelligence system called Pic2Recipe to look at a photo of food and be able to predict the ingredients and suggest similar recipes. “In computer vision, food is mostly neglected because we don’t have the large-scale datasets needed to make predictions,” says Yusuf Aytar, an MIT postdoc who co-wrote a paper about the system with MIT Professor Antonio Torralba. “But seemingly useless photos on social media can actually provide valuable insight into health habits and dietary preferences.” The paper will be presented later this month at the Computer Vision and Pattern Recognition conference in Honolulu. CSAIL graduate student Nick Hynes was lead author alongside Amaia Salvador of the Polytechnic University of Catalonia in Spain. Co-authors include CSAIL postdoc Javier Marin, as well as scientist Ferda Ofli and research director Ingmar Weber of QCRI. How it works The web has spurred a huge growth of research in the area of classifying food data, but the majority of it has used much smaller datasets, which often leads to major gaps in labeling foods. In 2014 Swiss researchers created the “Food-101” dataset and used it to develop an algorithm that could recognize images of food with 50 percent accuracy. Future iterations only improved accuracy to about 80 percent, suggesting that the size of the dataset may be a limiting factor. Even the larger datasets have often been somewhat limited in how well they generalize across populations. A database from the City University in Hong Kong has over 110,000 images and 65,000 recipes, each with ingredient lists and instructions, but only contains Chinese cuisine. The CSAIL team’s project aims to build off of this work but dramatically expand in scope. Researchers combed websites like All Recipes and Food.com to develop “Recipe1M,” a database of over 1 million recipes that were annotated with information about the ingredients in a wide range of dishes. They then used that data to train a neural network to find patterns and make connections between the food images and the corresponding ingredients and recipes. Given a photo of a food item, Pic2Recipe could identify ingredients like flour, eggs, and butter, and then suggest several recipes that it determined to be similar to images from the database. (The team has an online demo where people can upload their own food photos to test it out.) “You can imagine people using this to track their daily nutrition, or to photograph their meal at a restaurant and know what’s needed to cook it at home later,” says Christoph Trattner, an assistant professor at MODUL University Vienna in the New Media Technology Department who was not involved in the paper. “The team’s approach works at a similar level to human judgement, which is remarkable.”   The system did particularly well with desserts like cookies or muffins, since that was a main theme in the database. However, it had difficulty determining ingredients for more ambiguous foods, like sushi rolls and smoothies. It was also often stumped when there were similar recipes for the same dishes. For example, there are dozens of ways to make lasagna, so the team needed to make sure that system wouldn’t “penalize” recipes that are similar when trying to separate those that are different. (One way to solve this was by seeing if the ingredients in each are generally similar before comparing the recipes themselves). In the future, the team hopes to be able to improve the system so that it can understand food in even more detail. This could mean being able to infer how a food is prepared (i.e. stewed versus diced) or distinguish different variations of foods, like mushrooms or onions. The researchers are also interested in potentially developing the system into a “dinner aide” that could figure out what to cook given a dietary preference and a list of items in the fridge. “This could potentially help people figure out what’s in their food when they don’t have explicit nutritional information,” says Hynes. “For example, if you know what ingredients went into a dish but not the amount, you can take a photo, enter the ingredients, and run the model to find a similar recipe with known quantities, and then use that information to approximate your own meal.” The project was funded, in part, by QCRI, as well as the European Regional Development Fund (ERDF) and the Spanish Ministry of Economy, Industry, and Competitiveness. Topics: Research, Algorithms, Machine learning, Behavior, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer vision, Networks, Data, Artificial intelligence, School of Engineering, Electrical Engineering & Computer Science (eecs), Food CSAIL researchers have developed an artificial neural network that generates recipes from pictures of food, reports Laurel Dalrymple for NPR. The researchers input recipes into an AI system, which learned patterns “connections between the ingredients in the recipes and the photos of food,” explains Dalrymple. In this video for USA Today, Sean Dowling highlights Pic2Recipe, the artificial intelligence system developed by CSAIL researchers that can predict recipes based off images of food. The researchers hope the app could one day be used to help, “people track daily nutrition by seeing what’s in their food.” Researchers at MIT have developed an algorithm that can identify recipes based on a photo, writes BBC News reporter Zoe Kleinman. The algorithm, which was trained using a database of over one million photos, could be developed to show “how a food is prepared and could also be adapted to provide nutritional information,” writes Kleinman. MIT researchers have developed a new machine learning algorithm that can look at photos of food and suggest a recipe to create the pictured dish, reports Matt Reynolds for New Scientist. Reynolds explains that, “eventually people could use an improved version of the algorithm to help them track their diet throughout the day.” CSAIL researchers have trained an AI system to look at images of food, predict the ingredients used, and even suggest recipes, writes Matt Burgess for Wired. The system could also analyze meals to determine their nutritional value or “manipulate an existing recipe to be healthier or to conform to certain dietary restrictions,"" explains graduate student Nick Hynes. This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
2f4d51b0ad,New computer programme replicates handwriting,"UCL News
12 August 2016
In a world
increasingly dominated by the QWERTY keyboard, UCL computer scientists have
developed software which may spark the comeback of the handwritten word by
analysing the handwriting of any individual and accurately replicating it.
The scientists
have created ‘My Text in Your Handwriting’, a programme which semi-automatically
examines a sample of a person’s handwriting, which can be as little as one paragraph,
and generates new text saying whatever the user wishes, as if the author had
handwritten it themselves.
First author,
Dr Tom Haines (UCL Computer Science), said: “Our software has lots of valuable
applications. Stroke victims, for example, may be able to formulate letters
without the concern of illegibility, or someone sending flowers as a gift could
include a handwritten note without even going into the florist. It could also
be used in comic books where a piece of handwritten text can be translated into
different languages without losing the author’s original style.”
Published in ACM Transactions on Graphics and
funded by the EPSRC, the machine learning algorithm is built around glyphs – a
specific instance of a character. Authors produce different glyphs to represent
the same element of writing – the way one individual writes an “a” will usually
be different to the way others write an “a”. Although an individual’s writing
has slight variations, every author has a recognisable style that manifests in
their glyphs and their spacing. The software learns what is consistent across
an individual’s style and reproduces this.
To generate an
individual’s handwriting, the programme analyses and replicates an author’s
specific character choices, pen-line texture and colour, inter-character
ligatures (the joining-up between letters), and vertical and horizontal
spacing.
Co-author, Dr Oisin
Mac Aodha (UCL Computer Science), said: “Up until now, the only way to produce
computer-generated text that resembles a specific person’s handwriting would be
to use a relevant font. The problem with such fonts is that it is often clear
that the text has not been penned by hand, which loses the character and
personal touch of a handwritten piece of text. What we’ve developed removes
this problem and so could be used in a wide variety of commercial and personal
circumstances.” The system is
flexible enough that samples from historical documents can be used with little
extra effort. Thus far, the scientists have analysed and replicated the
handwriting of such figures as Abraham Lincoln, Frida Kahlo and Arthur Conan
Doyle. Infamously, Conan Doyle never actually wrote Sherlock Holmes as saying,
“Elementary my dear Watson” but the team have produced evidence to make you
think otherwise. To test the
effectiveness of their software, the team asked people to distinguish between handwritten
envelopes and ones created by their automatic software. People were flummoxed by
many of the envelopes, resulting in them being fooled by the computer generated
writing 40% of the time. Given how
convincing the computer generated handwriting is, some may believe the method could
help in forging documents, but the team explained it works both ways and could
actually help in detecting forgeries. Senior author,
Dr Gabriel Brostow (UCL Computer Science), said: “Forgery and forensic
handwriting analysis are still almost entirely manual processes but by taking
the novel approach of viewing handwriting as texture-synthesis, we can use our
software to characterise handwriting to quantify the odds that something was
forged. For example, we could calculate what ratio of people start their ‘o’s’
at the bottom versus the top and this kind of detailed analysis could reduce
the forensics service’s reliance on heuristics.” UCL Communications & Marketing Intern Tel: +44 (0)20 3108 3846Email: r.caygill [at] ucl.ac.uk"
737f7960cb,Research Team Wins Award for Machine Learning Diagnostic,"A team of scientists hailing from the Sandia National Laboratories and Boston University developed an experimental algorithm that could automatically diagnose problems in supercomputers. There is an array of internal and external issues that could arise with these powerful machines. For instance, factors like physical parts breaking can occur or previous programs performing “zombie processes” that prevent the computer from functioning properly. Furthermore, the repair process for these devices can take an extended period of time, which raises another issue since these computers perform critical tasks like forecasting the weather and ensuring the U.S. nuclear arsenal is safe and reliable without needing to do underground testing. To develop the algorithm, the team took a multi-step approach. First, the engineers created a suite of issues they became familiar with over the time they spent working on various supercomputers, which was then followed by them writing specific codes to re-create these anomalies.  Two supercomputers, one residing at Sandia and a public cloud system that Boston University helps operate, ran a variety of programs with and without the anomaly codes. A large quantity of data points were collected in this process including how much energy, processor power, and memory was used in each node. Next, this trove of information was programmed into several machine learning algorithms which were able to detect anomalies by comparing data from normal program runs and those with anomalies. In addition, these specialized programs were given additional training to determine which one was the best at diagnosing these problems. One technique that was highlighted is called Random Forest. It was adept at analyzing vast quantities of monitoring data, identifying which metrics are important, and then determining if the supercomputer was being affected by anomaly. Ultimately, the analysis process was further streamlined by incorporating calculations of various statistics for each metric including values like average, fifth percentile, and 95th percentile, along with more complex indications like noisiness as well as trends over time and symmetry that help suggest abnormal behavior. The end result was a trained machine learning program that could use less than one percent of the system’s processing power to analyze data and find these complexities. Future work on this prototype would entail more work with artificial anomalies while also finding ways to validate these diagnostics to gauge their performance in finding real anomalies during normal runs on these supercomputers. The research team’s paper was published in the journal High Performance Computing. View the discussion thread. © Copyright 2018 Advantage Business Media"
ebd0b913cd,AI spots Alzheimer’s brain changes years before symptoms emerge | New Scientist,"Daily news
14 September 2017
baranozdemir/Getty By Anil Ananthaswamy Artificial intelligence can identify changes in the brains of people likely to get Alzheimer’s disease almost a decade before doctors can diagnose the disease from symptoms alone. The technique uses non-invasive MRI scans to identify alterations in how regions of the brain are connected. Alzheimer’s is a neurodegenerative disease that is the leading cause of dementia for the elderly, eventually leading to loss of memory and cognitive functions.
The race is on to diagnose the disease as early as possible. Although there is no cure, drugs in development are likely to work better the earlier they are given. An early diagnosis can also allow people to start making lifestyle changes to help slow the progression of the disease. In an effort to enable earlier diagnosis, Nicola Amoroso and Marianna La Rocca at the University of Bari in Italy and their colleagues developed a machine-learning algorithm to discern structural changes in the brain caused by Alzheimer’s disease. First, they trained the algorithm using 67 MRI scans, 38 of which were from people who had Alzheimer’s and 29 from healthy controls. The scans came from the Alzheimer’s Disease Neuroimaging Initiative database at the University of Southern California in Los Angeles. The idea was to teach the algorithm to correctly classify and discriminate between diseased and healthy brains. The researchers divided each brain scan into small regions and analysed the neuronal connectivity between them, without making any assumptions about the ideal size of these regions for diagnosis. They found that the algorithm made the most accurate classification of Alzheimer’s when the brain regions being compared were about 2250 to 3200 cubic millimetres. This was intriguing, says La Rocca, since this is similar to the size of the anatomical structures connected with the disease, such as the amygdala and hippocampus. The team then tested the algorithm on a second set of scans from 148 subjects. Of these, 52 were healthy, 48 had Alzheimer’s disease and 48 had mild cognitive impairment (MCI) but were known to have developed Alzheimer’s disease 2.5 to nine years later. The AI distinguished between a healthy brain and one with Alzheimer’s with an accuracy of 86 per cent. Crucially, it could also tell the difference between healthy brains and those with MCI with an accuracy of 84 per cent. This shows that the algorithm could identify changes in the brain that lead to Alzheimer’s almost a decade before clinical symptoms appear. The researchers were limited by the scans available from the database, so they weren’t able to test whether the algorithm could predict the onset of disease even earlier. Alzheimer’s disease has been linked to the formation of sticky beta-amyloid plaques and neurofibrillary tau tangles in the brain. “Nowadays, cerebrospinal fluid analyses and brain imaging using radioactive tracers can tell us to what extent the brain is covered with plaques and tangles, and are able to predict relatively accurately who is at high risk of developing Alzheimer’s 10 years later,” says La Rocca. “However, these methods are very invasive, expensive and only available at highly specialised centres.” In contrast, the new technique can distinguish with similar accuracy between brains that are normal and the brains of people with MCI who will go on to develop Alzheimer’s disease in about a decade – but using a simpler, cheaper and non-invasive technique. More work will be needed to distinguish between people with MCI whose brains go on to age normally, or who might develop other kinds of dementia. Blood tests that look for biomarkers of Alzheimer’s could be even cheaper and simpler than the new technique, but none are on the market yet. “There are no blood tests for Alzheimer’s disease,” says Goran Šimić at the University of Zagreb in Croatia. “There have been some attempts, but without much success yet.” Patrick Hof at the Icahn School of Medicine at Mount Sinai in New York is intrigued by the new test. He says that a method that might predict the disease a decade before it is fully expressed would be “incredibly valuable” should preventative therapeutics emerge. La Rocca says her team now intends to extend the technique to help with the early diagnosis of other neurodegenerative conditions such as Parkinson’s disease. “It’s a method that is very versatile,” she says. Reference: arXiv, 1709.02369
More on these topics:
A shorter version of this article was published in New Scientist magazine on 23 September 2017"
9b7adbace2,Computers can now paint like Van Gogh and Picasso — Quartz,"Computers are learning how to write sonnets, compose classical music, and now they’re mastering another high art form: painting. Researchers from the University of Tubingen in Germany recently published a paper on a new system that can interpret the styles of famous painters and turn a photograph into a digital painting in those styles. The researchers’ system uses a deep artificial neural network—a form of machine learning—that intends to mimic the way a brain finds patterns in objects. It was a deep neural network that was behind Google’s crazy Deep Dream system, where images were turned into digital psychedelic fever dreams where everything was made up of dogs and eyeballs. The team at Tubingen’s system is a similar idea, with the craziness toned down a little bit. The researchers taught the system to see how different artists used color, shape, lines, and brushstrokes so that it could reinterpret regular images in the style of those artists. The main test was a picture of a row of houses in Tubingen overlooking the Neckar River. The researchers showed the image to their system, along with a picture of a painting representative of a famous artist’s style. The system then attempted to convert the houses picture into something that matched the style of the artist. The researchers tried out a range of artists, which the computer nailed to varying degrees of success, including Van Gogh, Picasso, Turner, Edvard Munch and Wassily Kandinsky. While in some cases, the end product does very much look like how that artist may have interpreted the row houses, based solely on the input painting, not every Van Gogh looks like “Starry Night.” The system has a sliding scale for how much of the original image is kept versus the styling of the artists. For example, they let the system run various levels of intensity of a style that’s supposed to match Kandinsky’s work. Eventually, the image just becomes a series of unrecognizable color blotches—though Kandinsky may not have been entirely opposed to this style. This system, like many neural networks attempting to mimic human creativity, explores what it actually means to create. The team’s paper is under consideration for publication in Nature Communications, according to Motherboard. Right now, it’s essentially a really, really complicated Instagram filter—adding a painting’s style to a photo—but it could pave the way to a computer vision system that really understands how humans create. As the paper says: “Our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.” And there are other systems that are already learning how to create in much the same way humans are taught. The best musicians learn theory and styles, and then break the rules to create original works of art. MIT lecturer Donya Quick’s system is doing just that, going a step further than mimicking artistic styles and actually generating entirely new pieces of music (albeit still within conventional classical music forms). Does this mean human creativity will one day be replaced by computer programs that can spit out art the way pharmacies spit out one-hour photos? Perhaps, but creating a computer as powerful and complex as the human brain is a daunting task that we’re quite far from completing. In the meantime, Instagram could definitely use a few new filters."
7d1698460c,How recommendation algorithms know what you'll like | TechRadar,"By
PC Plus
2012-05-09T10:00:00.129Z
Internet
One of the biggest innovations in online shopping - first introduced by Amazon - was the automatically generated recommendation. You logged onto the site and, right there on the home page, the site would make suggestions for products you could purchase. For example, if you were a JavaScript developer like me, you'd see recommendations for programming books using that language, whereas if you were a mother with young kids, you'd see mentions of toys and children's books.
This personalisation of the home page has a big benefit for the online store compared to just displaying top 10 lists or banner ads: the click-through and conversion rates are far higher. Customers are more likely to view and buy the suggested products. The prediction algorithms then are of huge importance to online stores - the more accurate they are, the more the online store will sell. Consider though the problems that must be solved by such a recommendation algorithm. A large online store like Amazon may have millions of customers and millions of items in stock. New customers will have limited information about their preferences, while more established customers may have too much. The data on which these algorithms work is constantly updated and changed. Customers are browsing the site and the prediction algorithm should take the recently browsed items into consideration, for example - it doesn't help if I'm looking for a toy for my youngest niece and all I get are recommendations for jQuery. The biggest and most important criterion for these systems (apart from accuracy) is speed. The recommendation algorithm must produce suggestions within a second or so. After all, the user is in the process of displaying the store's home page where the recommendations will appear. Traditionally, these recommendation algorithms have worked by finding similar customers in the database. In other words, they work by finding a set of customers who have bought or rated the same items that you have. Throw out the items you've already purchased or commented on, and then recommend the rest. For example, if I've already bought A and B, and the set of such customers also includes purchases for C, then C is recommended to me. One of the earliest such algorithms is known as collaborative filtering. In essence, the algorithm represents each customer as a vector of all items on sale. Each entry in the vector is positive if the customer bought or rated the item, negative if the customer disliked the item, or empty if the customer has not made his or her opinion known. Most of the entries are empty for most of the customers. Some variants factor in the popularity of the items to bump up the significance of items that are less popular or familiar. The algorithm then creates its recommendations by calculating a similarity value between the current customer and everyone else. The most acceptable way to do this is to calculate the angle between the vectors - the simplest method being to calculate the cosine using the dot product divided by the product of the vector lengths. The larger the cosine, the smaller the angle, and therefore the more similar the customers. As you may have surmised, this process is computationally expensive. There are usually a lot of customers and a lot of calculations have to take place very quickly. There are techniques to reduce the computation (by sampling the customer base or ignoring unpopular items, for example), but in general it's always going to be expensive to calculate recommendations this way. Another traditional prediction algorithm involves the use of cluster models. Here the goal is to prepare the customer base by dividing it into clusters and then to assign the current customer to one of the clusters, in theory choosing the cluster with the most similarity. Once the cluster has been identified, the recommendations come from the purchases and ratings from other customers in that particular cluster. Although the choice of cluster works in roughly the same way as the classification algorithm (we assume that we can calculate a characteristic vector that describes the cluster in much the same way that there is a vector per customer), the real meat of the algorithm is in the creation of the clusters. In general, clustering of customer data is done through a heuristic: start off with some number of empty clusters, assign a randomly selected customer to each, and then assign the other customers to the clusters according to similarity. Since the initial clusters are essentially randomly created, sub-algorithms must be used to merge or split clusters as they are being built up. Using cluster models is less computationally intensive at the point where you need to make recommendations quickly for a customer. After all, there's less work to be done to find a similar cluster rather than a similar customer. If you like, most of the work is done up front in the creation of the clusters themselves. Unfortunately, this particular method tends to result in low quality recommendations since the purchases/ratings are averaged out within a cluster. No longer is a particular customer matched to the most similar customer, but instead to the average of a large group of customers. Certainly the number of clusters can be increased to refine the matching, but then you run into the possibility of increasing computation time. The next traditional algorithm is a fairly simple search algorithm. For example, if I buy Our Kind of Traitor by John Le Carré, the search algorithm would query the items database for other books by John Le Carré, spy books by other authors, DVDs of movies made from Le Carré books, and so on so forth. You can see targeted recommendations like these in banner ads as you surf the internet. I recently searched for a messenger bag (and bought one from Ona), and for a good two weeks afterwards, it was as if every website I visited wanted to recommend Timbuktu bags to me. I'm sure this kind of stalker effect has happened to you and, as you'll know, the recommendations offered tend to be low quality. What Amazon did to improve its recommendations was to turn collaborative filtering on its head. Instead of trying to find similar customers, it matched up items. Its version is called item-to-item collaborative filtering. This algorithm matches each of the current customer's purchased and rated items to similar items and then builds a list from those matched items. First of all, then, the web site must build a 'similar items table' by analysing the items customers tend to purchase together. Here's how this works: for each item X in the catalog, find all customers C who purchased X. For each of those customers, find all items Y purchased by C and record that a customer bought X and Y. Then, for all pairs X and Y, calculate the similarity between X and Y in the same manner as for the collaborative filtering algorithm. Although this calculation is fairly computationally expensive, it can be done beforehand. Once the similarity between every pair of items has been determined, the recommendation list follows easily.
The example above shows a simple example where we just have three customers purchasing four products: Alan, Brian, Cory, and Book, DVD, Toy, Jeans. Taking Book first, we see that all the customers purchased it. (We'll assume that 'purchased it' equals 1, and 'didn't purchase it' equals 0.) The similarity between Book and DVD is 0.58, between Book and Toy or Jeans is 0.82. For DVD, only Alan bought it, and he also bought Book and Jeans. The similarity between DVD and Book or Jeans is therefore 1.0. Notice that similarity is not necessarily associative, but in general, over many products and customers, it will be close. From this, if David lands on the page for Book, we can recommend the product Toy or Jeans to him equally, then DVD. If he lands on DVD, we'll suggest Book and Jeans equally, and so on.
The above example shows the situation if, instead of simply buy/didn't buy, we use the customers' ratings instead. (If a customer bought but didn't rate the product, we could assign a neutral rating like 3 to the purchase.) I calculate that the similarity between Book and DVD is 0.56, between Book and Toy 0.82, between Book and Jeans 0.70. Therefore David, on visiting Book, would be recommended Toy, Jeans and DVD in that order. Netflix uses a home-brewed recommendation algorithm called CineMatch. Back in 2006, the film rental company announced a competition with a million dollar prize to see if anyone could improve on the recommendations provided by CineMatch. The goal was a 10 per cent improvement on CineMatch's score on a test subset of the vast Netflix database. The winner, after nearly three years, was a group that called itself BellKor. The competitors who accepted the challenge were given a huge dataset of 100 million ratings, with each rating (between one and five stars) containing the date of the rating, the title and release year of the movie, and an anonymous ID representing the user. They were also provided a qualifying dataset of a selection of 2.8 million ratings with the same information, but without the actual rating. The object was to devise an algorithm from the large dataset, apply it to the qualifying dataset to guess the ratings, and then Netflix would see how close the guessed ratings were to the actual ratings. It's fascinating to see the strategies BellKor used in order to tune its algorithm. It must be stressed that BellKor's solution is not a single algorithm per se, but can be viewed instead as a series of algorithmic knobs that can be twiddled to produce the best answer. The first strategy was to create a set of baseline predictors. These describe a user's ratings on the average. Suppose that the average rating over all movies is 3.5. As an example of a specific film, Star Wars might have an overall rating of 4, which would be 0.5 better than the average movie. Our hypothetical user though tends to rate movies lower than the mean: say his average over all movies was 3.2. Since he averages 0.3 lower than the mean, our initial guess for how he might rate Star Wars would be 4 – 0.3, or 3.7. The second strategy used was the realisation that time plays a big part in people's ratings. Firstly, a movie's popularity will change over time. For example, a movie may start big and then be forgotten, whereas others may start small and then become cult films. Popularity is also affected by the star or director when they release a better (or worse) movie subsequently, and by their appearance, good or bad, in the media. A user's overall ratings tend to drift over time. This could be because the 'user' is actually a whole household, and so the person making the ratings may change, or it could be because of the psychological effect that after the user has made a run of good ratings, their next rating may be better than would otherwise be warranted (or vice versa: after a run of bad movies, the next good film may be rated lower than expected). The next strategy could also be described as partially temporal: the user may 'batch' up and enter ratings for a set of recently-viewed movies on one day. The user would tend to let the ratings influence each other unconsciously (if most movies were good, the bad movies would tend to get better than expected ratings, or vice versa), rather than considering them all independently. This strategy is known as frequency. The BellKor development team then described these strategies mathematically and statistically to provide them parameters to the model that could be tweaked. Taking a large subset of the training data, it repeatedly ran the model, changing the parameters bit by bit, until it predicted the remaining smaller subset's ratings. At that point it was able to submit its guesses for the qualifying subset. From all this, I'm sure that you can see that prediction algorithms are certainly not exact. But then again, providing they are fast and generally accurate enough, it doesn't matter. For Amazon, the recommendation engine is a differentiating factor, and for Netflix it's a primary reason for keeping customers in their subscriptions - after all, once a user has watched Star Wars and its collection of sequels/prequels, they're going to want suggestions for other things or they'll cancel. Get the best tech deals, reviews, product advice, competitions, unmissable tech news and more! TechRadar is part of Future plc, an international media group and leading digital publisher. Visit our corporate site. ©
Future Publishing Limited
Quay House, The Ambury,
Bath
BA1 1UA. All rights reserved. England and Wales company registration number 2008885."
7e7f7a2176,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
8fbba644a0,Scientists have used groundbreaking technology to figure out how the Earth looked a billion years ago — Quartz,"By the time Dietmar Mueller arrived at the University of Texas as a graduate student in the mid-1980s, scientists had already long embraced a once-astonishing idea: that the continents on which all human history has unfolded, rather than fixtures of constancy, were orphans of a former grand supercontinent called Pangaea. Showered with awards, the pioneers of this theory—plate tectonics—had by and large dispersed in search of the next big challenge. But Mueller and his classmates sensed far more ground to cover. Three decades later, Mueller, now at the University of Sydney, is part of a new upheaval in tectonics, this time ignited by advances in computing power. The same leaps in big-data analysis, supercomputing, and intelligent algorithms that have shaken up finance, genetics, and espionage are transforming our view of the elusive ancient world. These tools are allowing Mueller and a bold group of scientists, known as paleogeologists, to chart not only Pangaea, but the supercontinents that came before it, with the aim of producing the most precise map of the Earth and its natural resources ever known.
“It’s like detective work. You have pieces of evidence. You have to find who committed the crime.”  Already, Mueller’s team, working with collaborators at the University of Oslo and Caltech, has completed a stunningly detailed digital reconstruction of 410 million years of earthly history, going back to the collision of then-existing continents that created Pangaea. In January, they plan to release version 2.0 of GPlates, an open-source software model that anyone can use to go back in time to the early days of Earth. Next, they are preparing for peer review of an extension of the model to 1 billion years ago—the era of Pangaea’s ancestor, a supercontinent called Rodinia. Then they will lunge again—to 2 billion years, yet another supercontinent, and the evolution of the first multicellular life. To some degree, paleogeology is merely an academic enthusiasm. But it’s also serious business. It is essential, for instance, to virtually all deep-water oil prospecting: Oil companies note where petroleum has already been found, and decipher where that reservoir was situated during the time of Pangaea; in what’s called “analog exploration,” they then take a hard look at the rock with which it was conjoined 200 million years ago, and often drill there, too. When it comes to jewels and minerals, the time scale goes past 1 billion years. Plate tectonics influences present-day politics, too: It created the geography and geology that set the boundaries of countries, parcel out their natural wealth, and determine which ones will be hit by earthquakes, tsunamis and volcanoes. Moreover, some geologists who study the universe think that plate tectonics, which were crucial to the evolution of complex life on Earth, may be a prerequisite (pdf) for its emergence on other planets too. If so, that would narrow down the search for life elsewhere in the galaxy; scientists are already devising ways to detect signs of tectonics on exoplanets, such as traces of volcanic activity that might be spotted with near-future telescopes. “It’s like detective work,” Mueller said. “You have pieces of evidence. You have to find who committed the crime.” The story of plate tectonics goes back to 1912, when a German atmospheric physicist, Alfred Wegener, looked at a map and noticed that the opposing coastlines of Africa and South America seemed oddly congruent, like matching pieces of a jigsaw puzzle. Other geologists had remarked similarly, and in fact written that the two continents were once conjoined. But somehow these declarations had escaped both serious scrutiny and general notice. Wegener—unburdened by any prior geological expertise—launched into frenetic research. Among the facts he turned up were that the highlands of Scotland seemed to continue in the Appalachian Mountains of the United States, separated only by the Atlantic Ocean; the fossils of similar animals and plants had been found on both continents. He concluded with a sweeping observation: The lands as we knew them had been arranged by continental drift, a process by which all the continents had begun their existence fused into one gigantic expanse that he called Pangaea, or All Earth. He convinced few contemporaries. Around the world, especially in the United States, hostile geologists denounced Wegener as a dilettante, a superficial interloper, and just dead wrong, according to Mott Greene, the author of a new biography of him. The coastal likenesses were a coincidence, they said. In 1930, the discredited Wegener died on Greenland in yet another attempt to find physical proof of the theory.  Until recently, much of the research in plate tectonics was being conducted with primitive tools. It took four decades before a chance discovery revived his theories. In World War II, the US Navy had recorded sonar and other readings to track and evade German submarines, an exercise that resulted in spectacular maps of the seafloor. The Navy was so beguiled that, once the fighting was over, it poured funding into continued ocean research. In the 1950s, American and British scientists studying the Earth’s crust at the sea bottom discovered that the Earth’s magnetic field wasn’t stationary as was supposed, but had regularly flipped throughout history. The “magnetic signature” it left behind in rocks as they cooled and formed pieces of crust showed that at times, magnetic north was near the geographic north pole, and other times the south pole. But the signatures were confusing—they made it seem as if the magnetic poles hadn’t just flipped, but also moved all around the Earth’s surface. In a flurry of papers through the 1960s, however, the scientists cleared everything up—it was the continents that had moved. Generally speaking, Wegener had been right. Only generally speaking, though. The continents themselves did not actually drift; they instead were found to rest like light Styrofoam atop slabs of dense rock—tectonic plates millions of square miles in size, made of silicon, magnesium and aluminum. It was the plates that jostled for position, pushing up mountains, shifting and causing earthquakes, interacting with the mantle below in a churning system that produced volcanoes—in effect a living machine, taking the land masses along with them for the ride. Plate tectonics became a sensation. Newspapers around the world described it as a revolutionary conceptual breakthrough, comparable to Darwin’s theory of evolution. Yet until recently, much of the research was being conducted with primitive tools that provided only the vaguest picture of what was really going on. When the Earth formed 4.6 billion years ago, it had no plate tectonics, and no one knows when or how the process got started. Yet life as we know it simply wouldn’t exist without it. Essentially, the Earth is a heat-driven engine, 6,000°C (nearly 11,000°F) at its core and a maximum of 40°C or so at the surface. The temperature difference drives currents in the magma, the molten rock under the Earth’s crust. The tectonic plates move atop these currents. Their movements power the essential system of carbon recycling that—until our industrial civilization came along—kept the level of carbon dioxide (CO2) in the atmosphere optimally balanced; they prevented Earth from turning into an overheated wasteland like Venus.  Life as we know it simply wouldn’t exist without plate tectonics. A similar process also enriches the atmosphere with nitrogen, which is essential to the bacteria and plants that convert CO2 into oxygen, making more complex forms of life possible. (Before life got started, the atmosphere had only a trace of oxygen.) Finally, plate tectonics are in part responsible for the earth’s magnetic field. This field shields the planet from the solar wind, the torrent of charged particles and radiation from the sun that, undeflected, would deplete the atmosphere and threaten life itself. To Mueller, considering all this in the early 1990s, now with his PhD and a position as a lecturer at the University of Sydney, what was missing was a reliable, mathematically grounded model that would make sense of the entire geological show from start to finish. As a field, paleogeology had stayed on the boil. After Pangaea was accepted, scientists had excited a new stir by assembling geological evidence of prior supercontinents: first Kenorland, which would have broken up some 2.4 billion years ago, then Nuna, and after that, about 1 billion years ago, Rodinia. By this stage, multicellular organisms had appeared in the oceans, but the land would still have been bare of anything but bacteria and some algae. Rodinia begat Pangaea, which has hosted all of the complex plant and animal history with which we are familiar. But what did these supercontinents look like, and how did they visually interact with the earthly core and the atmosphere? Mueller was intimately familiar with widely available short animations of the Pangaean breakup (such as the one below), because many of them went back to his supervisor at Austin, a pioneer of the field named Christopher Scotese. There were also hand-drawn illustrations of lush landscapes teeming with strange flying and terrestrial reptiles, savannah, coral and jungles. But neither Scotese, Mueller nor anyone else could be certain these were reliable. This was the defining tension underlying the half-century-long study of the supercontinents: That, unlike in other fields that deal in the very old, the scientists had no time machine. Astronomers, by looking through telescopes at galaxies billions of light years away, are transported back to the early universe. Paleontologists, by stumbling on ancient fossils, can look directly at remnants of prehistoric life. But no instrument or evidence had ever similarly teleported their paleogeologist comrades back to the age of supercontinents. Instead, paleogeologists painstakingly pieced together their theories using disparate fragments of clues, mainly from the magnetic signatures in old rocks. At first, to give their field a face, they translated these clues into cut-out shapes on paper or in Adobe Illustrator, and strung them together into mosaic-like animations still found today on Google.  The world of Pangaea and its breakup as presented in the animations was more or less informed fiction. But while pretty good as far as they went, most such depictions were faulty in important ways. Among the unavoidable imperfections was their typical reliance on a “flat Earth,” two-dimensional illustrations that distorted the appearance and movement of the continents. In addition, they provided plate movements, but ignored the inextricable system that penetrates thousands of miles into the bowels of the Earth, linked all the way to the core. Then there was the problem with longitude. The paleomagnetic signature gave scientists evidence of the latitude of a long-vanished land—how far north or south it was—but not its longitude, its position east or west. And even on latitude, errors of up to 10 or 20 degrees, meaning as many as 1,400 miles, could creep in if you went back far enough, and the age of rocks could be off by as many as 60 million years. Neither could you tell from the magnetic signature whether a continent was north or south of the equator, right side up, or upside down. Finally, classic depictions of the supercontinents embraced Wegener’s flawed original thesis of drift. They allowed the continents to wander across the globe without respect to the anchoring plates. In reality, the continents have a sort of underlying keel that connects them to their plate, which prevents them from independent movement. Thus, the world of Pangaea and its breakup as presented in the animations was more or less informed fiction—close approximations, but with considerable flaws. Mueller and his collaborators set out to change that—to produce a model anchored in tectonic geometry, showing the boundaries and seamless motion of the dozen or so major plates, without gaps between them, fully integrated with the convecting mantle, and including a more rigorous appraisal of the correct longitude. It would be what he called a virtual time machine that would allow anyone to pick almost any point in the long-ago geological eras on Earth, and return there.  “We want to travel into the geological past, and understand what the planet looked like, not only at the surface, but…all the way to the core.” As it was, no widely available software could accomplish all of this. Oil companies and some other small teams of geologists had systems they had created, but didn’t share them with others. Nor was it clear, in any case, that any of them achieved Mueller’s aims. So Mueller, along with geophysicist Mike Gurnis at Cal Tech and geologist Trond Torsvik of the University of Oslo, banded together to create it themselves from scratch. Mueller named his team EarthByte, and the system he created with Gurnis and Torsvik is now called GPlates. “We don’t just want to make inferences about what happened in the past,” Mueller said. “We actually want to travel into the geological past, and understand what the planet looked like, not only at the surface, but also at depth, in fact all the way to the core.” Mueller’s initial aim wasn’t superlatively ambitious. He just sought to create an open-source platform on which geologists could reconstruct the post-Pangaea Earth digitally, so that state-of-the-art “deep time” paleogeology would no longer be the province solely of oil companies and well-heeled research groups. Nor, even when he and his collaborators went further and began to reconstruct the supercontinents, did they presume to research the whole of earthly history from scratch. Rather, like Wegener, they would synthesize the decades of research that was already out there. Using fast supercomputers, they would try out different possible scenarios of continental evolution, and then settle on the depiction that best fit the existing data and how tectonics were known to work.  Continents cannot jump north, then south, then north again. Nor can they flounce, twist, lurch or vault. The team’s first goal was to create a program that would model a plate’s movement not on a flat piece of paper, but on a sphere. The continents were given multiple axes, or rotations, allowing the land and plates to move independently, which they appear to do if you do not know they are connected. (The old models generally got the continents right, but took no account of the boundaries and movement of the plates.) One of their biggest advances was in strictly modeling the globe according to a set of laws—the “rules of plate tectonics”—that scientists have gleaned from decades of meticulous study of the sea floor. One version of these rules goes back to Scotese, but the Sydney group and others have updated them. For instance, no continent can exceed a speed limit of 20 centimeters a year, but ocean floors can move faster than continents. Generally speaking, continents cannot make abrupt, jerky movements (although, as Scotese says, “every once in a while … wham!”). Large continents have never rotated completely around, or even by 180 degrees, over just a few tens of millions of years. And, aside from the occasional wham, the affairs of the Earth have generally unfolded slowly and smoothly. Continents cannot jump north, then south, then north again. Nor can they flounce, twist, lurch or vault. Here is a depiction by the Sydney group showing the speed of the Pangaea breakup. A key thing that made this possible was leaps in computing power. The kind of brute force required, Mueller said, was generally not available for use by academic geologists. When time could be arranged on a fast computer, it could take 10 days or even two weeks to run dozens of possible configurations of the solid Earth over a 250-million-year period, to figure out which one was the likeliest to be correct. The latest supercomputers, though, can do the same work in three or four days. One of Mueller’s researchers, Kara Matthews, recently finished knitting together two models of the pre- and post-Pangaean eras—one that went to 230 million years ago (230 Ma, in the standard notation), and the second from there to 410 Ma. The former is Mueller’s and the latter was created by Oslo’s Torsvik and a postdoc named Mathew Domeier. Both pieces are regarded as standard works, and the challenge for Matthews was mainly how to make them seamlessly work together. The result is an exhaustive model in 1-million year increments, much more granular than the 10-million-year intervals used in prior depictions. Another big advance is that the new model seeks to correctly place the continents longitudinally. Torsvik had used an experimental method that he argues correctly finds longitude. (In April he will be awarded the prestigious Arthur Holmes Medal in part for his pioneering work on the method.) It relies on what he believes are lodestars—enormous chunks of superlatively old, deeply buried rock with the inelegant names Large Igneous Province (LIP) and Large Low Shear Wave Velocity Province (LLSVP). In a 2014 paper, Torvsik asserted that his longitudinal method works at least as far back as the Cambrian explosion of 540 Ma, when the diversity of life experienced a momentous increase. One key reason is the stability of two LLSVPs (one below Africa and a second underneath the Pacific), which, unlike much of the Earth’s land mass, appear to have stayed almost immobile since the Cambrian explosion. While everything else on the planet was moving around through time, the LLSVPs were not, and thus are fixed longitudinal references. Another key advance in Matthews’ combined model is in unifying the surface with the deep Earth all the way to the core. Distinct from competing animations, which feature only the land masses on the surface of the globe, the GPlates model reveals the moving, roiling Earth underneath, in addition to the tell-tale appearance of the reversing magnetic lines on a spreading Atlantic Ocean. But while thrilling, that would be almost an anti-climax if Mueller’s team stopped there—if they did not now set out to extend the GPlates model to Rodinia and beyond. Which is what came next. When the existence of Rodinia was first proposed in the 1970s, it suffered some of the same ridicule as Wegener’s original theory. Zheng-Xiang Li, a professor at Curtin University and one of the world’s foremost experts on Rodinia, said that all through the 1980s, he risked professional scorn to suggest that there had been a cycle of supercontinents—“cycles was a bad word,” he said. The most daring acceptable hypothesis was that Pangaea was an accident, or at most an “episode,” words that did not necessarily suggest that it had predecessors. As for Rodinia, its name had not even been coined yet. The orthodoxy was that Pangaea was unique and discreet.  It would be the first model that presented the period from 1 billion to 410 million years ago as a single piece, all anchored in the tectonic rules. All that changed in the 1990s with the publication of landmark papers that began to document Rodinia’s existence, starting with the presence of extremely old slabs of rock in the interior of the existing continents—billions of years old, it would be later discovered, based on potassium argon dating. The cratons, as they were called, were usually encased in much younger rock, implying their separate origin long before. They were often hundreds of thousands of square miles in area, and penetrated hundreds of miles into the Earth. Rocks of similar age were found strewn across the planet—in Sweden, Mexico, Mozambique, Australia, Antarctica—suggesting that all were once part of the same, much larger expanse. The new canon now embraced Rodinia. Last year, with Pangaea out of the way, Mueller assigned a PhD student—a wild-haired opals expert named Andrew Merdith—to start work on extending the GPlates model to Rodinia. But how? The further back in time you went, the fewer and fewer rocks you encountered—the evidence required to make any accurate picture. The plates themselves were different—some had been swallowed up into the Earth’s mantle; many had broken into pieces; most were different shapes. The oldest verified crusts—in Australia and Canada—are about 3.8 or 4 billion years old, but they are extremely lucky to have escaped destruction. However, Merdith was not starting from scratch. There were the known cratons into which Rodinia was thought to have broken up: four pieces in Africa, for instance—Congo, the Kalahari and the Sahara, not to mention West Africa, which had been conjoined with Amazonia (of today’s South America) in the Rodinia days. There also was a quarter century of research papers, including the work behind those hand-drawn animations on Google (below). Most important, he had Li, the Rodinia expert at Curtin. Li had already co-led a five-year United Nations-funded project that, finishing work in 2004, had produced a model of the supercontinent, although it was not loaded into GPlates. He agreed to collaborate with the younger man. The resulting model would aim to be the likeliest facsimile. It would be the first that knitted together the post-Rodinian plates, and presented the period from 1 billion to 410 million years ago not as fragmented snapshots, but as a single piece, all anchored in the tectonic rules. Merdith said, “We’re not experts in any one particular area—on the Australia and India collision or the South America collision with Africa—but we’ll get the model out and give it to people and say, ‘Great, you know more about how South America works than we do. So you fix it or tell us what is wrong and we can fix it.’” It will be “a benchmark foundation for people to use to improve their models.” That was his and Mueller’s hope, anyway. When Li’s work arrived in Sydney, it was clear to Merdith and his supervisor, a Briton named Simon Williams, that a few things were off. Li’s model violated at least a couple of the more serious tectonic rules. “We went through and there were some periods of time when the Chinese and Siberian plates were doing crazy stuff, moving really fast and rotating,” Merdith said. “If it was north at 900 million years, by 800 it was south-facing, and then at 700 it was east or west, which if you judge by the tectonic rules, are erratic and far too fast.” But that would be relatively easy to fix. The tougher task was that, while the Li model was a respected depiction of the breakup of Rodinia and the movement of the subsequent continents, it did not attempt to find the boundaries of the tectonic plates on which the continents lay. The existing models of Rodinia were more like guesswork, and redolent of Wegener’s theory of drift.  When Kara Matthews had done her Pangaea section of the GPlates model, the plate boundaries had already been configured by her Oslo collaborators. But between the eras of Rodinia and Pangaea, some plates would have broken up or changed, and the geological evidence for what had existed so long ago was much more scant. In this regard, the existing models of Rodinia were more like guesswork, and redolent of Wegener’s theory of drift. Merdith himself, therefore, had to decide the size, shape and boundaries of the plates. To get there, he took the clues at hand, such as the known age and thrust of a mountain range, the cratons, and the paleomagnetic evidence. He knew that if he could get the model right from 410 Ma—where Matthews’ work ended—through about 750 Ma, he would be pretty much home, because Rodinia would not have shifted much if at all from then to 1 billion years ago, which was about when the supercontinent formed. So what he had to document was a roughly 350-million-year gap. Getting to work, Merdith made an adjustment for Li’s erratic China and Siberia, finding an interpretation of the data that moved them from the center of action to the periphery of the main blocks of land, and allowed for a smoother smashing together of continents. This was artistic license; it was possible to look at the same data and reach different conclusions as to the appearance of geological scenes. Now Merdith set out to advance methodically through the whole period. He established small daily goals—working on a 50- to 100-million-year time interval on one part of the globe, then another, and another. First, he tackled the continent of Gondwana, from 520 Ma to 650 Ma, the period about which he had the most information. He scanned academic papers written over the years, with an eye on figuring out the all-important plate boundaries. He saw that the timing seemed off for the collisions of Australia, Antarctica, Africa and India—they did not mesh with new data he had collected. So he began to adjust the model accordingly. He fit together the pieces of Africa, but now found that that forced Australia out of the picture. He had to return to the data on Australia. He made a new tweak that worked it back in. Now he moved on to South America, and the then-existing continents of Lorentia, Baltica and Rio de la Plata, which is more or less present-day Uruguay. But all of that came in stretches. He found he could complete a small block of land in South Africa in a few days; but he was spending several weeks on larger blocks like India and Australia. The model was moving in increments of 20 or 30 million years, much less fine than the 1-million-year granularity of Matthews’ Pangaea-era model, but it was what the data made possible. In some spots, there were no data at all, which forced him to hand-draw what made the most sense given the tectonic rules. By October, Merdith had a rough cut covering the period 500 Ma to 650 Ma (see below). Now he jumped all the way back to 750 Ma—when Rodinia started to break up. He was still tweaking the plate boundaries, hoping that when he was done, the result would align with those he created for 650 Ma “and really flow in together.” Merdith had a personal deadline—he was getting married half-way through December, and by then had to reach 750 Ma with a model in which “everything [was] moving as one big block on the Earth’s surface.” “It’s baby steps,” he said. “You do a little bit each day and it doesn’t feel like you do much. Then you look back after a few months and you have a model that is coming together nicely.” The GPlates system that Mueller’s team developed isn’t good only for tracking continental evolution. It’s general-purpose mapping software that lets scientists visualize all kinds of data about the Earth. It’s been used for making a map of how gravity varies around the planet, and how flows of liquid rock deep in the mantle gradually distort the Earth’s surface. And in 2014, Adriana Dutkiewicz, a Polish-born paleogeologist and Mueller’s wife, used it to tackle a problem very relevant to the present day: how the seafloor interacts with the atmosphere and influences climate change. She would lead a project that, for the first time since the 1970s, would comprehensively map the sea floor, this time digitally. The idea had been around for a long time—to cull a database of some 200,000 seafloor crust samples collected since the 1950s on exploratory ocean cruises, and held by the US National Oceanic and Atmospheric Administration (NOAA). But before Dutkiewicz, no one had been prepared to wade through the mountain of NOAA reports. With a team including her husband and a machine-learning specialist, Dutkiewicz began to examine the samples. First, there was bad news; the database was faulty. What was written in individual summaries of the initial rock samples she looked at did not always match the samples themselves. So she had to read the research paper attached to every single sample. Only then could she know which ones met GPlates standards. After three months, Dutkiewicz emerged with 14,500 solid samples. The team fed those into a machine-learning algorithm designed to produce a visual depiction of her conclusions. It spat out an interactive, three-dimensional geological map of the seafloor—a virtual, rotating globe (video below) that she announced in an August 2015 article in the journal Geology. (You can also play with the interactive model online.) It was widely picked up by mainstream digital media. “People really like playing with things like that on their computers,” Dutkiewicz said. “For an ordinary person, you could see that it’s not just this deep, dark pit full of mud.” The map means that for the first time, there is an anchored digital model connecting data from the surface of the sea, which is most exposed to the atmosphere and the immediate impact of the burning of fossil fuels, to the seabed itself. Phytoplankton—microscropic marine plants—absorb carbon dioxide from the atmosphere at the surface. When they die, they sink down and coat the seabed in sediments, trapping the carbon there. Collectively, they absorb much more carbon from the atmosphere than plants do on land, Dutkiewicz said. Her map shows where these carbon sinks are concentrated. But Dutkiewicz said the model shows that scientists understand the sinks much less than they thought. Specifically, there is less of a direct connection between the surface and the seafloor than was previously believed, and the composition of the sediments is much more complex. More research is needed. “We urgently need to understand how the ocean responds to climate change,” she said. The super-computer and the algorithm had made all the difference to the map’s creation. It had allowed her to connect different data sets together—“really big data sets,” she said. “It wouldn’t be possible to do without a computer. The human mind cannot process all that information.” How far back into the history of Earth can you go? Mike Tetley, a high-school-dropout, former rock bassist, and one of Mueller’s top PhD students, said that paleogeologically speaking, you can go back in time 3 billion years. Before that, there aren’t enough rocks or correlating data to reach any conclusions. At least, not right now. In 2014, Tetley got talking to Li, the Rodinia expert, who wondered aloud whether there might not be a new way to map the long-ago world, including longitude. The paleomagnetic tools used to date rocks rely on the behavior of igneous rocks (those that have formed from cooling magma) at the so-called “Curie temperature.” When igneous rocks cool to their Curie point, they are left with an indelible impression of the magnetic field at that point in time. From that, researchers decipher the latitude at which the rock formed, but it could not reveal longitude, because the Earth’s magnetic field points north-south, not east-west. What Li suggested was that Tetley try another experimental method—that he take a look at basalt, one of the most common rocks on the Earth. Basalt had been thought to be impervious to detection of its physical origin, without a directional signature of any type, but Li had a hunch that an algorithm could be written that would crack open its secrets. Only he had no idea where to start.  “I say, ‘This is basalt from a subduction zone. Learn it. These are the basalts from mid-ocean ridges—learn them.’ It learns the patterns of things.” Tetley offered to have a go. One piece of basalt seemed indistinguishable from another to the human eye, but Tetley went after the problem using machine learning. He loaded up 500 million years of basalt data, gleaned from rock samples catalogued in a global archive. “I said to it, ‘This is what basalt looks like.’ It’s like the ‘50s robots – ‘This is what a woman looks like.’ But I say, ‘This is basalt from a subduction zone. Learn it. These are the basalts from mid-ocean ridges—learn them.’ It learns the patterns of all the things.” If the method were successful, Tetley said, it could solve the longitudinal problem—“knowing where on Earth it all happened”—because the signature for longitude might be locked into the basalt, in a way previously imperceptible by humans, but that the algorithm could pick up. “You throw all the data in, and the algorithm will deal with it,” he said. “It effectively allows you to identify a rock from any time period if you have some measurements.” Tetley got the code written, and he and Li ran a test. They took a model of the Earth between 5 Ma and 30 Ma, based on paleomagnetic data. Then they asked Tetley’s algorithm to predict how the continents would have moved over the same period of time, using only the basalt data he had loaded up. There was a 75% match, “which in algorithm land is very good,” Tetley said. “Because we didn’t tell it anything. It used what it knew from data and made predictions on stuff outside its data set.” At least right now, Tetley’s algorithm can’t be used in isolation—for a reliable result, it needs a second source of anchoring data, he said. But it suggests that machine learning can be used to tighten up paleogeological maps throughout ancient time. Mueller called Tetley “one of those one-in-a-million types,” who is “actually on the path of revolutionizing, I would say, how these deep-time plate reconstructions are done.” To Mueller, this is the long-sought teletransporter. “If the vision is a time machine,” he said, “then that’s not just to travel to a particular place back in time, but to really analyze it. You know, what happened at that time period in many different ways.” The next step is to combine Andrew Merdith’s data on Rodinia–once it goes through peer evaluation and is published—with Tetley’s algorithm, which should harden the longitudinal placement of the plates and continents. They will add in a database on minerals. The result, among other things, Mueller hopes, will be a predictive tool for mining companies. “It will tell you in which places copper and gold deposits likely formed,” he said. And this could help to answer the next big challenge—how to go back before Rodinia to the almost data-absent time of its predecessor, the supercontinent of Nuna, which would have formed nearly 2 billion years ago. Li already has a head start on mapping that one, too. “It’ll put us in the position, I think for the first time, to actually study the evolution of the Earth through several supercontinent cycles—supercontinent amalgamation and dispersal, and study, you know, how this affected the deep Earth,” Mueller said. He paused. “This will take time. Obviously, history has taught us that things always take longer than we think. But we have a lot of critical mass and momentum here, and I think over the next few years, we are going to see a lot of exciting things happen.”"
2b90ff97a4,System uses ‘deep learning’ to detect cracks in nuclear reactors - Purdue University,"Find Info For Quick Links November 6, 2017 WEST LAFAYETTE, Ind. – A system under development at Purdue University uses artificial intelligence to detect cracks captured in videos of nuclear reactors and represents a future inspection technology to help reduce accidents and maintenance costs.  “Regular inspection of nuclear power plant components is important to guarantee safe operations,” said Mohammad R. Jahanshahi, an assistant professor in Purdue’s Lyles School of Civil Engineering. “However, current practice is time-consuming, tedious, and subjective and involves human technicians reviewing inspection videos to identify cracks on reactors.” Complicating the inspection process is that nuclear reactors are submerged in water to maintain cooling. Consequently, direct manual inspection of a reactor’s components is not feasible due to high temperatures and radiation hazards. Technicians review remotely recorded videos of the underwater reactor surface, a procedure that is vulnerable to human error. Researchers are proposing a “deep learning” framework called a naïve Bayes-convolutional neural network to analyze individual video frames for crack detection. An innovative “data fusion scheme” aggregates the information extracted from each video frame to enhance the overall performance and robustness of the system. Findings are detailed in a paper appearing in October in the journal IEEE Transactions on Industrial Electronics. The paper was authored by Purdue electrical and computer engineering doctoral student Fu-Chen Chen and Jahanshahi, who is the director of Purdue’s Smart Informatix Laboratory. The paper is available online at http://ieeexplore.ieee.org/document/8074762/.
Convolutional neural networks are a form of artificial intelligence called deep learning that have been instrumental in the development of various commercial products capable of facial and speech recognition. The new system detects cracks in overlapping “patches” in each video frame while the data fusion algorithm scheme is capable of tracking the crack from one frame to the next. The approach achieves a 98.3 percent success rate, which is significantly higher than other state-of-the-art approaches, Jahanshahi said. (A YouTube video is available at https://youtu.be/8O8FFey4GJo) The United States is the world’s largest supplier of commercial nuclear power, which provides around 20 percent of the nation’s total electric energy. Between 1952 to 2010, there have been 99 major nuclear power incidents worldwide that cost more than $20 billion, with 56 incidents occurring in the United States, according to the findings of a 2010 study published in the Journal of Contemporary Asia. Incidents are defined as either resulting in the loss of human life or more than $50,000 of property damage, the amount the federal government uses to define major energy accidents that must be reported. “One important factor behind these incidents has been cracking that can lead to leaking,” Jahanshahi said. “Nineteen of the above incidents were related to cracking or leaking, costing $2 billion. Aging degradation is the main cause that leads to function losses and safety impairments caused by cracking, fatigue, embrittlement, wear, erosion, corrosion and oxidation.” Although several vision-based crack detection approaches have been developed for concrete, rock, or pavement surfaces, only a few approaches consider crack detection on metallic surfaces. The majority of existing approaches focus on detecting cracks in a single image. “If a crack is not detected in these single images or a noisy pattern is falsely detected as a crack in the image, no other information is available to correct the detection results,” he said. To develop and evaluate the proposed system, videos of 20 underwater specimens representing internal nuclear power plant components were collected. Samples were scanned at 30 frames per second, and the convolutional neural network examined each frame for cracks. The data were provided by the Electric Power Research Institute (EPRI), a nonprofit organization funded by the electric utility industry.
As the data-fusion algorithm observes a crack from one frame to the next it is able to account for changing configurations due to the moving camera, pinpointing the location of the crack. The algorithm mimics the ability of human vision to scrutinize cracks from different angles, which is important because some cracks are obscured by the play of light and shadow, Jahanshahi said. “Data fusion comes up with more robust decision making than would otherwise be possible,” he said. The approach also uses powerful graphical processing units to train the neural network how to detect cracks with a dataset that contains around 300,000 crack and non-crack patches, he said. A patent application on the crack-detection technology has been filed through the Purdue Research Foundation’s Office of Technology Commercialization. The Purdue research team also is using deep learning to detect corrosion in photographs of metal surfaces, a technology that might be used to inspect structures such as light poles and bridges. That research has been detailed in a paper accepted for publication in the Journal of Structural Health Monitoring. The paper was authored by Purdue undergraduate student Deegan Atha, who also recently presented findings during the International Workshop on Structural Health Monitoring at Stanford University. Future research will include work to further improve the technologies. Writer: Emil Venere, 765-494-4709, venere@purdue.edu   Source: Mohammad R. Jahanshahi, 765-494-2217, jahansha@purdue.edu Note to Journalists: The research paper is available at http://ieeexplore.ieee.org/document/8074762/. A YouTube video is available at https://youtu.be/8O8FFey4GJo and other video is accessible on Google Drive at http://goo.gl/vezwH1. The video was produced by Erin Easterling, Purdue College of Engineering digital producer, 765-496-3388, Easterling@purdue.edu. ABSTRACT NB-CNN: Deep Learning-based Crack Detection Using Convolutional Neural Network and Naïve Bayes Data Fusion      Fu-Chen1 Chen and Mohammad R. Jahanshahi2 1School of Electrical and Computer Engineering, Purdue University 2Lyles School of Civil Engineering, Purdue University Regular inspection of nuclear power plant components is important to guarantee safe operations. However, current practice is time-consuming, tedious, and subjective which involves human technicians review the inspection videos and identify cracks on reactors. A few vision-based crack detection approaches have been developed for metallic surfaces, and they typically perform poorly when used for analyzing nuclear inspection videos. Detecting these cracks is a challenging task since they are tiny, and noisy patterns exist on the components’ surfaces. This study proposes a deep learning framework called NB-CNN to analyze individual video frames for crack detection while a novel data fusion scheme is proposed to aggregate the information extracted from each video frame to enhance the overall performance and robustness of the system. To this end, a Convolutional Neural Network (CNN) is proposed to detect crack patches in each video frame while the proposed data fusion scheme maintains the spatiotemporal coherence of cracks in videos, and the Naïve Bayes decision making discards false positives effectively. The proposed framework achieves 98.3% hit rate against 0.1 false positives per frame that is significantly higher than state-of-the-art approaches as presented in this paper. Purdue University, 610 Purdue Mall, West Lafayette, IN 47907, (765) 494-4600
© 2015-17 Purdue University | An equal access/equal opportunity university | Copyright Complaints | Maintained by Office of Strategic Communications Trouble with this page? Disability-related accessibility issue? Please contact News Service at purduenews@purdue.edu."
9a87923177,Vending machines are getting smarter with machine learning and facial recognition | VentureBeat,"We’re living in an era of extraordinary technological change. From smartphones to drones, the evidence is in our pockets, in the sky, at the office, and in the streets. The pace of technological evolution, though, can be even harder to keep up with than the Kardashians. When asked to think about cutting-edge tech, I’ll be the first to admit that vending machines aren’t the first innovation to come to mind. However, the technology behind today’s most advanced vending machines is some of the most disruptive in the book. Like today’s smartphones, which barely resemble the clunky, tethered machines of yesteryear, vending machines have undergone massive upgrades since dispensing holy water in the first century and their official debut in the 1880s. Trends like robotics, machine learning, self-service, and cashless pay are inspiring some truly amazing innovations and otherwise transforming the face of retail. Modern vending machines let users make custom requests and get quality products on-demand, whether it’s frozen yogurt, skin care products, soda, or even weed, legality permitting. But what I really want to talk about is the intelligence aspect of the vending revolution, because that’s the aspect most relevant to industries of all kinds. If robotics is the force driving vending machines forward, artificial intelligence is the next frontier — and we’re right on track. What is an intelligent vending machine? As with anything, there are levels of intelligence both basic and advanced. If a machine knows how to give you the right change, that’s pretty basic, but it’s intelligence nonetheless. Today’s vending machines are far smarter than their ancestors, and those with smart software and hardware prove it. For new-age machines, intelligence means interacting with your smartphone and logging your preferences or carrying out unique orders tailored to your needs. These machines incorporate machine learning and algorithms to better serve their users. Some can even make recommendations based on your preferences, just like Netflix does with movies. Vending machine technology is advancing across the world, and innovation in this field is everywhere if you know where to look. With machine learning keeping pace with robotics, already-impressive machinery is setting the bar high. Coca Cola’s smart digital vending machine, for instance, is sure to make waves when it’s launched in New Zealand and the U.S. Using AI software, this intelligent machine will feature a chatbot tool for users and will connect to the cloud so purchases can be made remotely for convenient pickup. Then there’s the smart vending machine VICKI, short for Viatouch Intelishelf Cognitive Kinetic Interactions. This machine goes a step further by allowing users to log in through social media, fingerprints, or iris scans. It can provide information about products and display ads and make recommendations based on a person’s purchasing history, As it turns out, tech giant Intel is on top of this trend, too. Intel’s machines offer “high-definition displays running rich graphics, the ability to interact with the customer’s smartphone, and more.” They promise an engaging shopping experience that uses touch-screen controls, video, audio, scent, gesture-based interaction, and cashless payment. Even if you haven’t (yet) seen a smart vending machine, you will — and soon. According to a March 2017 report from U.S.-based Grand View Research, the market is expected to reach $11.84 billion by 2025. With growing intelligence, vending machines can provide brick-and-mortar quality goods while taking up far less space, making them both convenient and profitable. According to Berg Insight, the world’s leading M2M/IoT market research provider, the number of intelligent vending machines will reach 3.6 million by 2020. America represents the largest market for this technology worldwide, but we can look to Tokyo for what’s left to accomplish. Japan already has about one vending machine per 23 people, and these machines sell everything from fish soup to puppies. Of course, not everyone is pleased by this trend. As with any form of automation, job loss is a concern. Some people have privacy concerns. To top it off, the technology is not always being applied well: Bodega, for instance, needlessly attempted to appropriate the classic New York corner store with a glorified vending machine. It may not have seemed like a bad idea, but while vending machine offer convenience, some things just can’t be replicated, and it’s safe to say bodegas are on that list. Overall, the case for intelligent vending machines is still a good one, and there’s ample evidence to support this assertion. In a rapidly urbanizing world, they are a valuable way to sell products of all types in high-traffic communities. Machines known for high-calorie snacks can now even be programmed to serve healthier options, a boon to societies plagued with health issues. They can also reduce waste and cost by using analytics and AI engines to help achieve the optimum mix of product, space, and price. To those who are still unsure, I say let the chips fall where they may (get it?). Some shoppers may love the evolving vending experience more than others, but there’s no denying that the tech isn’t just as cool as a drone or a phone, it’s a trend with staying power. Nick Yates is the chairman of GenerationNext Brands, the parent company to the revolutionary robotic vending concept Reis & Irvy’s."
747d95e28a,[1610.02273] Near-Data Processing for Differentiable Machine Learning Models,"Link back to: arXiv, form interface, contact."
f890f98580,Recognizing correct code | MIT News,"Login
or
Subscribe Newsletter
“One of the most intriguing aspects of this research is that we’ve found that there are indeed universal properties of correct code that you can learn from one set of applications and apply to another set of applications,” Martin Rinard says. Image: MIT News Automatic bug-repair system fixes 10 times as many errors as its predecessors.
Larry Hardesty | MIT News Office
January 29, 2016
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
MIT researchers have developed a machine-learning system that can comb through repairs to open-source computer programs and learn their general properties, in order to produce new repairs for a different set of programs. The researchers tested their system on a set of programming errors, culled from real open-source applications, that had been compiled to evaluate automatic bug-repair systems. Where those earlier systems were able to repair one or two of the bugs, the MIT system repaired between 15 and 18, depending on whether it settled on the first solution it found or was allowed to run longer. While an automatic bug-repair tool would be useful in its own right, professor of electrical engineering and computer science Martin Rinard, whose group developed the new system, believes that the work could have broader ramifications. “One of the most intriguing aspects of this research is that we’ve found that there are indeed universal properties of correct code that you can learn from one set of applications and apply to another set of applications,” Rinard says. “If you can recognize correct code, that has enormous implications across all software engineering. This is just the first application of what we hope will be a brand-new, fabulous technique.” Fan Long, a graduate student in electrical engineering and computer science at MIT, presented a paper describing the new system at the Symposium on Principles of Programming Languages last week. He and Rinard, his advisor, are co-authors. Users of open-source programs catalogue bugs they encounter on project websites, and contributors to the projects post code corrections, or “patches,” to the same sites. So Long was able to write a computer script that automatically extracted both the uncorrected code and patches for 777 errors in eight common open-source applications stored in the online repository GitHub. Feature performance As with all machine-learning systems, the crucial aspect of Long and Rinard’s design was the selection of a “feature set” that the system would analyze. The researchers concentrated on values stored in memory — either variables, which can be modified during a program’s execution, or constants, which can’t. They identified 30 prime characteristics of a given value: It might be involved in an operation, such as addition or multiplication, or a comparison, such as greater than or equal to; it might be local, meaning it occurs only within a single block of code, or global, meaning that it’s accessible to the program as a whole; it might be the variable that represents the final result of a calculation; and so on. Long and Rinard wrote a computer program that evaluated all the possible relationships between these characteristics in successive lines of code. More than 3,500 such relationships constitute their feature set. Their machine-learning algorithm then tried to determine what combination of features most consistently predicted the success of a patch. “All the features we’re trying to look at are relationships between the patch you insert and the code you are trying to patch,” Long says. “Typically, there will be good connections in the correct patches, corresponding to useful or productive program logic. And there will be bad patterns that mean disconnections in program logic or redundant program logic that are less likely to be successful.” Ranking candidates In earlier work, Long had developed an algorithm that attempts to repair program bugs by systematically modifying program code. The modified code is then subjected to a suite of tests designed to elicit the buggy behavior. This approach may find a modification that passes the tests, but it could take a prohibitively long time. Moreover, the modified code may still contain errors that the tests don’t trigger. Long and Rinard’s machine-learning system works in conjunction with this earlier algorithm, ranking proposed modifications according to the probability that they are correct before subjecting them to time-consuming tests. The researchers tested their system, which they call Prophet, on a set of 69 program errors that had cropped up in eight popular open-source programs. Of those, 19 are amenable to the type of modifications that Long’s algorithm uses; the other 50 have more complicated problems that involve logical inconsistencies across larger swaths of code. When Long and Rinard configured their system to settle for the first solution that passed the bug-eliciting tests, it was able to correctly repair 15 of the 19 errors; when they allowed it to run for 12 hours per problem, it repaired 18. Of course, that still leaves the other 50 errors in the test set untouched. In ongoing work, Long is working on a machine-learning system that will look at more coarse-grained manipulation of program values across larger stretches of code, in the hope of producing a bug-repair system that can handle more complex errors. “A revolutionary aspect of Prophet is how it leverages past successful patches to learn new ones,” says Eran Yahav, an associate professor of computer science at the Technion in Israel. “It relies on the insight that despite differences between software projects, fixes — patches — applied to projects often have commonalities that can be learned from. Using machine learning to learn from ‘big code’ holds the promise to revolutionize many programming tasks — code completion, reverse-engineering, et cetera.” Topics: Research, School of Engineering, Computer science and technology, Machine learning, Algorithms, Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical engineering and computer science (EECS) This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
f646e176cd,Google's DeepMind could be used to cure blindness | WIRED UK,"Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
Machine learning will be applied to eye scans to search for sight loss symptoms
By
Emily Reynolds
Google's DeepMind is teaming up with NHS-funded Moorfields Eye Hospital to research whether machine learning can help the fight against blindness. The NHS said AI could play a ""big role in tackling avoidable sight loss"" and the partnership is intended to explore ""how cutting edge technologies can help medical research into eye diseases"". This includes macular degeneration, which generally affects the elderly, as well as diabetes-related sight loss.
Machine learning processes will be applied to around a million eye scans to help search for early symptoms of sight loss.
By
Michael Rundle
The number of people with sight loss is set to double by 2050, with around two million people in the UK currently living with sight loss – around one in 30. While 360,000 people are registered blind or partially sighted.
By detecting early signs, especially in diabetes, around 98 per cent of sight loss can be prevented.
""Our research with DeepMind has the potential to revolutionise the way professionals carry out eye tests and could lead to earlier detection and treatment of common eye diseases such as age-related macular degeneration,"" said Peng Tee Khaw, director of the National Institute for Health Research Biomedical Research Centre in Ophthalmology at Moorfields Eye Hospital. ""With sight loss predicted to double by the year 2050 it is vital we explore the use of cutting-edge technology to prevent eye disease."" Mustafa Suleyman, co-founder of DeepMind, said the project was part of DeepMind's mission to solve ""some of society's biggest challenges"". ""Diabetic retinopathy is the fastest growing cause of blindness worldwide,"" he said. ""There are more than 350m sufferers across the planet, so I'm really excited to announce this collaboration with leading researchers at Moorfields."" ""Detecting eye diseases as early as possible gives patients the best possible chance of getting the right treatments. I really believe that one day this work will be a great benefit to patients across the NHS. We are proud of our NHS, and this is one of the ways I think we can help nurses and doctors continue to provide world-class care.""
By
Emily Reynolds
By
Matt Burgess
By
Michael Rundle
By
WIRED"
f3d3b6dd26,Artificial Intelligence in Video Games: Towards a Unified Framework,"Indexed in Web of Science To receive news and publication updates for International Journal of Computer Games Technology, enter your email address in the box below. Université de Liège, Grande Traverse 10, Sart Tilman, 4000 Liège,
Belgium Received 27 August 2014; Revised 26 December 2014; Accepted 8 February 2015 Academic Editor: Alexander Pasko
Copyright © 2015 Firas Safadi
et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. With modern video games frequently featuring sophisticated
and realistic environments, the need
for smart and comprehensive agents that understand
the various aspects of complex environments
is pressing. Since video game AI is often
specifically designed for each game, video game AI
tools currently focus on allowing video game developers
to quickly and efficiently create specific
AI. One issue with this approach is that it does not
efficiently exploit the numerous similarities that
exist between video games not only of the same
genre, but of different genres too, resulting in a
difficulty to handle the many aspects of a complex
environment independently for each video game.
Inspired by the human ability to detect analogies
between games and apply similar behavior on a
conceptual level, this paper suggests an approach
based on the use of a unified conceptual framework
to enable the development of conceptual AI
which relies on conceptual views and actions to
define basic yet reasonable and robust behavior.
The approach is illustrated using two video games,
Raven and StarCraft: Brood War. Because artificial intelligence (AI) is a broad notion in video games, it is important to start by defining the scope of this work. A video game can be considered to have two main aspects, the context and the game. The game includes the elements that define the actual challenges players face and the problems they have to solve, such as rules and objectives. On the other hand, the context encompasses all the elements that make up the setting in which these problems appear, such as characters and plot. This work focuses on game AI, that is, AI which is concerned with solving the problems in the game such as defeating an opponent in combat or navigating in a maze. Conversely, context AI would deal with context-specific tasks such as making a character perform a series of actions to advance the plot or reacting to player choices. Thus, the scope of discussion is limited to the game aspect in this work. Since video games are designed for human beings, it is only natural that they focus on their cognitive skills and physical abilities. The richer and more complex a game is, the more skills and abilities it requires. Thus, creating a truly smart and fully autonomous agent for a complex video game can be as challenging as replicating a large part of the complete human intelligence. On the other hand, AI is usually independently designed for each game. This makes it difficult to create thoroughly robust AI because its development is constrained to the scope of an individual game project. Although each video game is unique, they can share a number of concepts depending on their genre. Genres are used to categorize video games according to the way players interact with them as well as their rules. On a conceptual level, video games of the same genre typically feature similar challenges based on the same concepts. These similar challenges then involve common problems for which basic behavior can be defined and applied regardless of the problem instance. For example, in a first-person shooter one-on-one match, players face problems such as weapon selection, opponent position prediction and navigation. Each moment, a player needs to evaluate the situation and switch to the most appropriate weapon, predict where the opponent likely is or is heading and find the best route to get there. All of these problems can be reasoned about on a conceptual level using data such as the rate of fire of a weapon, the current health of the opponent and the location of health packs. These concepts are common to many first-person shooter games and are enough to define effective behavior regardless of the details of their interpretation. Such solutions already exist for certain navigation problems for instance and are used across many video games. Moreover, human players can often effortlessly use the experience acquired from one video game in another of the same genre. A player with experience in first-person shooter games will in most cases perform better in a new first-person shooter game than one without any experience and can even perform better than a player with some experience in the new game, indicating that it is possible to apply the behavior learned for one game in another game featuring similar concepts to perform well without knowing the details of the latter. Obviously, when the details are discovered, they can be used to further improve the basic conceptual behavior or even override it. It may therefore be possible to create cross-game AI by identifying and targeting conceptual problems rather than their game-specific instances. Detaching AI or a part of it from the development of video games would remove the project constraints that push developers to limit it and allow it to have a continuous and more thorough design process. This paper includes seven sections in addition to Introduction and Conclusion. Section 2 presents some related work and explains how this work positions itself beside it. Sections 3–6 present a development model for video game AI based on the use of a unified conceptual framework. Section 3 suggests conceptualization as a means to achieve unification. Section 4 discusses the design of conceptual AI while Section 5 discusses conceptual problems. Section 6 then focuses on the integration of conceptual AI in video games. Sections 7-8 include some applications of the development model presented in the previous sections. Section 7 describes a design experiment conducted on an open-source video game in order to concretize the idea of introducing a conceptual layer between the game and the AI. Section 8 then describes a second experiment which makes use of the resulting code base to integrate a simple conceptual AI in two different games. The Conclusion section ends the paper by discussing some of the merits of the proposed approach and noting a few perspectives for the extension of this research. Conceptualizing video games is a process which involves abstraction and is similar to many other approaches that share the same goal, namely, that of factoring AI in video games. More generally, abstraction makes it possible to create solutions for entire families of problems that are essentially the same when a certain level of detail is omitted. For example, the problem of sorting an array can take different forms depending on the type of elements in the array, but considering an abstract data type and comparison function allows a programmer to write a solution that can sort any type of array. This prevents unnecessary code duplication and helps programmers make use of existing solutions as much as possible so as to minimize development efforts. Another example of widely used abstraction application is hardware abstraction. Physical components in a computer can be seen as abstract devices in order to simplify software development. Different physical components that serve the same purpose, storage for example, can be abstracted into a single abstract storage device type, allowing software developers to write storage applications that work with any kind of storage component. Such a mechanism is used in operating systems such as NetBSD [1] and the Windows NT operating system family [2]. The idea of creating a unified video game AI middleware is not new. The International Game Developers Association (IGDA) launched an Artificial Intelligence Interface Standards Committee (AIISC) in 2002 whose goal was to create a standard AI interface to make it possible to recycle and even outsource AI code [3]. The committee was composed of several groups, each group focusing on a specific issue. There was a group working on world interfacing, one on steering, one on pathfinding, one on finite state machines, one on rule-based systems and one on goal-oriented action planning, though the group working on rule-based systems ended up being dissolved [3–5]. Thus, the committee was concerned not only with the creation of a standard communication interface between video games and AI, but with the creation of standard AI as well [6]. It was suggested that establishing AI standards could lead to the creation of specialized AI hardware. The idea of creating an AI middleware for video games is also discussed in Karlsson [7], where technical issues and approaches for creating such middleware are explored. Among other things, it is argued that when state systems are considered, video game developers require a solution in between simple finite state machines and complex cognitive models. Another interesting argument is that functionality libraries would be more appropriate than comprehensive agent solutions because they provide more flexibility while still allowing agent-based solutions to be created. Here too, the possibility of creating specialized AI hardware was mentioned and a parallel with the impact mainstream graphics acceleration cards had on the evolution of computer graphics was drawn. An Open AI Standard Interface Specification (OASIS) is proposed in Berndt et al. [8], aiming at making it easier to integrate AI in video games. The OASIS framework is designed to support knowledge representation as well as reasoning and learning and comprises five layers each dealing with different levels of abstraction, such as the object level or the domain level, or providing different services such as access, translation or goal arbitration services. The lower layers are concerned with interacting with the game while the upper layers deal with representing knowledge and reasoning.
Evidently, video game AI middleware can be found in video game engines too. Video game engines such as Unity [9], Unreal Engine [10], CryEngine [11], and Havok [12], though it may not be their primary focus, increasingly aim at not only providing building blocks to create realistic virtual environments but realistic agents as well. Another approach that, albeit not concerned with AI in particular, also shares a similar goal, which is to factor development efforts in the video game industry, is game patterns. Game design patterns allow game developers to document recurring design problems and solutions in such a way that they can be used for different games while helping them understand the design choices involved in developing a game of specific genre. Kreimeier [13] proposes a pattern formalism to help expanding knowledge about game design. The formalism describes game patterns using four elements. These are the name, the problem, the solution and the consequence. The problem describes the objective and the obstacles that can be encountered as well as the context in which it appears. The solution describes the abstract mechanisms and entities used to solve the problem. As for the consequence, it describes the effect of the design choice on other parts of the development and its costs and benefits.
Björk et al. [14] differentiates between a structural framework which describes game components and game design patterns which describe player interaction while playing. The structural framework includes three categories of components. These are the bounding category, which includes components that are used to describe what activities are allowed or not in the game such as rules and game modes, the temporal category which includes components that are involved in the temporal execution of the game such as actions and events, and the objective category which includes concrete game elements such as players or characters. More details about this framework can be found in Björk and Holopainen [15]. As for game design patterns, they do not include problem and solution elements as they do in Kreimeier [13]. They are described using five elements which are name, description, consequences, using the pattern and relations. The consequences element here focuses more on the characteristics of the pattern rather than its impact on development and other design choices to consider, which is the role of the using the pattern element. The relations element is used to describe relations between patterns, such as subpatterns in patterns and conflicting patterns. In Olsson et al. [16], design patterns are integrated within a conceptual relationship model which is used to clarify the separation of concerns between game patterns and game mechanics. In that model, game mechanics are derived from game patterns through a contextualization layer whose role is to concretize those patterns. Conversely, new patterns can be extracted from the specific implementation of these game mechanics, which in the model is represented as code. Also comparable are approaches which focus on solving specific AI issues. It is easy to see why, since these approaches typically aim at providing standard solutions for common AI problems in video games, thereby factoring AI development. For instance, creating models for intelligent video game characters is a widely researched problem for which many approaches have been suggested. Behavior languages aim to provide an agent design model which makes it possible to define behavior intuitively and factor common processes. Loyall and Bates [17] presents a goal-driven reactive agent architecture which allows events that alter the appropriateness of current behavior to be recognized and reacted to. ABL, a reactive planning language designed for the creation of believable agents which supports multicharacter coordination, is described in Mateas and Stern [18] and Mateas and Stern [19]. Situation calculus was suggested as a means of enabling high-level reasoning and control in Funge [20]. It allows the character to see the world as a sequence of situations and understand how it can change from one situation to another under the effect of different actions in order to be able to make decisions and achieve goals. A cognitive modeling language (CML) used to specify behavior outlines for autonomous characters and which employs situation calculus and exploits interval methods to enable characters to generate action plans in highly complex worlds is also proposed in Funge et al. [21], Funge [22]. It was argued in Orkin [23, 24] that real-time planning is a better suited approach than scripting or finite state machines for defining agent behavior as it allows unexpected situations to be handled more naturally. A modular goal-oriented action planning architecture for game agents similar to the one used in Mateas and Stern [18, 19] is presented. The main difference with the ABL language is that a separation is made between implementation and data. With ABL, designers implement the behavior directly. Here, the implementation is done by programmers and designers define behavior using data. Anderson [25] suggests another language for the design of intelligent characters. The avatar definition language (AvDL) enables the definition of both deterministic and goal directed behavior for virtual entities in general. It was extended by the Simple Entity Annotation Language (SEAL) which allows behavior definitions to be directly embedded in the objects in a virtual world by annotating and enabling characters to exchange information with them [26, 27]. Finally, learning constitutes a different approach which, again, leads to the same goal. By creating agents capable of learning from and adapting to their environment, the issue of designing intelligent video game characters is solved in a more general and reusable way. Video games have drawn extensive interest from the machine learning community in the last decade and several attempts at integrating learning in video games have been made with varying degrees of success. Some of the methods used are similar to the previously mentioned approaches in that they use abstraction or concepts to deal with the large diversity found in video games. Case-based reasoning techniques generalize game state information to make AI behave more consistently across distinct yet similar configurations. The possibility of using case-based plan recognition to reduce the predictability of real-time strategy computer players is discussed in Cheng and Thawonmas [28]. Aha et al. [29] presents a case learning and plan selection approach used in an agent that learns to win against a number of different AI opponents in Wargus. In Ontañón et al. [30], a case based planning framework for real-time strategy games which allows agents to automatically extract behavioral knowledge from annotated expert replays is developed and successfully tested in Wargus as well. More work using Wargus as a test platform includes Weber and Mateas [31] and Weber and Mateas [32] which demonstrate how conceptual neighborhoods can be used for retrieval in case-based reasoning approaches. Transfer learning approaches attempt to use the experience learned from some task to improve behavior in other tasks. In Sharma et al. [33], transfer learning is achieved by combining case-based reasoning and reinforcement learning and used to improve performance over successive games against the AI in MadRTS. Lee-Urban et al. [34] also uses MadRTS to apply transfer learning using a modular architecture which integrates hierarchical task network (HTN) planning and concept learning. Transfer of structure skills and concepts between disparate tasks using a cognitive architecture is achieved in Shapiro et al. [35]. Although machine learning technology may lead to the creation of a unified AI that can be used across multiple games, it currently suffers from a lack of maturity. Even if some techniques have been successfully applied to a few commercial games, it may take a long time before they are reliable enough to become mainstream. On the other hand, video game engines are commonly used and constitute a more practical approach at factoring game development processes to improve the quality of video games. They are however comprehensive tools which developers need to adopt for the entire game design rather than just their AI. Furthermore, they allow no freedom in the fundamental architecture of the agents they drive. The approach presented in this paper bears the most resemblance to that of creating a unified AI middleware. It is however not an AI middleware, strictly speaking. It makes use of a conceptual framework as the primary component which enables communication between video games and AI, allowing video game developers to use conceptual, game-independent AI in their games at the cost of handling the necessary synchronization between game data and conceptual data. A key difference with previous work is that it makes no assumptions whatsoever on the way AI should be designed, such as imposing an agent model or specific modules. Solutions can be designed for any kind of AI problem and in any way. A clear separation is made between the development of the conceptual framework, that of AI and that of video games. Because AI development is completely separated from the conceptual framework, its adoption should be easier as it leaves complete freedom for AI developers to design and implement AI in whichever way they are accustomed to. Furthermore, the simplicity of the approach made it possible to provide a complete deployment example detailing how an entire video game was rewritten following the proposed design. In addition, the resulting limited conceptual framework prototype was successfully employed to reuse some of the game AI modules in a completely different game. Since video games, despite their apparent diversity, share concepts extensively, creating AI that operates solely on concepts should allow developers to use it for multiple games. This raises an important question however, namely, that of the availability of a conceptual interpretation of video games. In reality, for AI to handle conceptual objects, it must have access to a conceptual view of game data during runtime. When humans play a video game, they use their faculty of abstraction to detect analogies between the game and others they have played in the past. Abstraction in this context can be seen as a process of discarding details and extracting features from raw data. By recalling previous instances of the same conceptual case, the experience acquired from the other games is generalized and transformed into a conceptual policy (i.e., conceptualized). For example, a player could have learned in a role-playing game (RPG) to use ranged attacks on an enemy while staying out of its reach. This behavior is known as kiting. Later, in a real-time strategy (RTS) game, that player may be faced with the same conceptual situation with a ranged unit and an enemy. If, at that time, the concept of kiting is not clearly established in the player’s mind, they may remember the experience acquired in the RPG and realize that they are facing a similar situation: control over an entity with a ranged attack and the ability to move and the presence of an enemy. The player will thereby conceptualize the technique learned in the RPG and attempt to apply it in the RTS game. On the other hand, if the player is familiar with the concept of kiting, a simple abstraction of the situation will lead to the retrieval of the conceptual policy associated with it, without requiring the recall of previous instances and associated experiences and their conceptualization. Note that kiting can be defined using only concepts, such as distance, attack range and movement. Distance can have several distinct interpretations, for example yards, tiles or hops. Attack range can be a spell range, a firearm range or a gravity range. Walking, driving and teleporting are all different forms of movement. Kiting itself being a concept, it is clear that concepts can be used to define other concepts. In fact, in order to define conceptual policies, different types of concepts are necessary, such as objects, relationships, conditions and actions. Weapon, enmity, mobility (The condition of being mobile.) and hiding are all examples of concepts. According to the process shown in Figure 1, conceptual AI, that is AI which operates entirely on concepts, could be used in video games under the premise that three requirements are met. These would be:(1)the ability to translate game states into conceptual states,(2)the ability to translate conceptual actions into game actions,(3)and the ability to define conceptual policies. (A conceptual policy maps conceptual states to conceptual actions.) Though the third requirement raises no immediate questions, the other two appear more problematic, as translating states and actions needs to be done in real-time and there currently exists no reliable replacement for the human faculty of abstraction. It follows from the latter assertion that this translation must be manually programmed at the time of development. This means that the game developer must have access to a library of concepts during development and write code to provide access at runtime to both conceptual views and conceptual controls of the game for the AI to work with. Using such a process, both the real-time availability and the quality conditions of the translation are satisfied. As is hinted in Figure 2, rather than translating game states into conceptual states discretely, it is easier to simply maintain a conceptual state in the conceptual data space (CDS). In other words, the conceptual state is synchronized with the game state. Every change in the game state, such as object creation, modification or destruction, is directly propagated to the conceptual state. Note that there is no dynamic whatsoever in the CDS. A change in the CDS can only be caused by a change on the game side, wherein the game engine lies. Obviously, this design calls for a unified conceptual framework (CF). That is, different developers would use the same conceptual libraries. This would allow each of them to use any AI written using this unique framework. For example, a single AI could drive agents in different games featuring conceptually similar environments, such as a first-person shooter (FPS) arena. This is illustrated in Figure 3. From a responsibility standpoint, the design clearly distinguishes three actors:(1)the game developers,(2)the AI developers,(3)and the CF developers. The responsibilities of game developers include deciding which AI they need and adding code to their game to maintain in the CDS the conceptual views required by the AI as well as implementing the conceptual control interfaces it uses to command game agents. Thus, game developers neither need to worry about designing AI nor conceptualizing games. Instead, they only need to establish the links between their particular interpretation of a concept and the concept itself. On the opposite side, AI developers can write conceptual AI without worrying about any particular game. Using only conceptual elements, they define the behavior of all sorts of agents. They also need to specify the requirements for each AI in terms of conceptual views and controls. Finally, the role of CF developers is to extract concepts from games (i.e., conceptualize) and write libraries to create and interact with these concepts. This includes writing the interfaces used by game developers to create and maintain conceptual views and by AI developers to access these views and control agents. Because the CF should be unique and is the central component with which both game developers and AI developers interact, it should be developed using an open-source and extensible model. This would allow experienced developers from different organizations and backgrounds to collaborate and quickly produce a rich and accessible framework. Incidentally, it would allow game developers to write their own AI while extending the framework with any missing concepts. From a technical perspective, writing conceptual AI is similar to writing regular AI. That is, developers are free to design their AI any way they see fit. Conceptual AI does not require a specific form. The only difference between conceptual AI and regular AI is that the former is restricted to the use of conceptual data. Rather than operating on concrete objects, such as knights, lightning guns or fireball spells, it deals with concepts such as melee (Opposite of ranged, can only attack within grappling distance.) tanking units (A tanking unit, or tank, is a unit who can withstand large amounts of damage and whose primary role is to draw enemy attacks in order to ensure the survival of weaker allied units.), long-range hitscan weapons (A hitscan weapon is a weapon that instantly hits the target when fired (i.e., no traveling projectile).) and typed area-of-effect damage projectile abilities. (Area-of-effect abilities target an entire area rather than a single unit.) Likewise, actions involve conceptual objects and properties instead of concrete game elements and can consist in producing an anti-air unit or equipping a damage reduction accessory. This difference is illustrated in Algorithms 1 and 2.
Algorithm 1 shows an excerpt from the combat code of a Fortress Defender, a melee non-player character (NPC) in a RPG. A Fortress Defender can immobilize enemies, a useful ability against ranged opponents who might attempt to kite it. Before commanding the NPC to attack an encountered enemy, the code checks whether the type of opponent is one of those who use a ranged weapon and starts by using its immobilization ability if it is the case. Algorithm 2 shows a possible conceptualization of the same code. Note how the design remains identical and the only change consists in replacing game elements with conceptual ones. As a result, the new code mimics a more conceptual reasoning. In order to prevent a ranged enemy from kiting the melee NPC, the latter checks whether a movement-impairing ability is available and uses it on the target before moving towards it. Whether the actual ability turns out to slow, immobilize or completely stun the opponent holds little significance as long as the conceptual objective of preventing it from kiting the NPC is accomplished. Although this requires developers to think in a more abstract way, they do retain the freedom of designing their AI however they are accustomed to. Despite this technical similarity, the idea of conceptualizing video games suggests looking at AI in a more problem-driven way. There are two obvious reasons. First, conceptual AI does not target any game in particular, meaning that it should not be defined as a complete solution for an entire game. Second, with the various interpretation details omitted, AI developers can more easily identify the conceptual problems that are common to games of different genres and target the base problems first rather than their combinations in order to leverage the full factoring potential of conceptualization. The idea of solving the base conceptual problems and combining conceptual solutions is illustrated in Figure 4. Besides combining them, it can be necessary to establish dependencies between solutions. An AI module may rely on data computed by another module and require it to be running to function properly. For example, an ability planner module could require a target selection module by planning abilities for a unit or character according to its current target. This can be transparent to game developers when the solutions with dependencies are combined together into a larger solution. When they are not however, game developers need to know whether an AI module they plan on using has any dependencies in order to take into account the conceptual interfaces required by those dependencies. This means that AI developers have to specify not only the conceptual interface an AI solution uses, but also those required by its dependencies. Dependencies in combined and individual AI solutions are illustrated in Figure 5. It can be argued that problems are actual video game elements. The difference between them and other elements such as objects is that they are rarely defined explicitly. They might be in games where the rules are simple enough to be listed exhaustively in a complete description of the problem the player is facing, but often in video games the rules are complex and numerous and a complete definition of the problems players must face would be difficult to not only write, but also read an understand. Instead, a description of the game based on features such as genres, environments or missions convey the problems awaiting players in a more intuitive way. With such implicit definitions, there can be many ways of breaking down video games into conceptual problems. Different AI developers might consider different problems and compositions. There are no right or wrong configurations of conceptual problems, though some may allow developers to produce AI more efficiently than others, just like the concepts making up the CF. It was suggested that the CF should be developed using an open-source model to quickly cover the numerous existing concepts through collaboration and ease the addition of new ones. The same suggestion could be made for conceptual problems. If conceptual problems are listed and organized in the CF, AI developers can focus on solving conceptual problems instead of identifying them. As with concepts, as conceptual problems are identified and included in the CF, they become part of the AI developers’ toolkit and allow them to better design their solutions. This task can be added to the responsibilities of CF developers, though since AI developers are the ones facing these conceptual problems and dealing with their hidden intricacies, they are likely to detect similarities between solutions to seemingly distinct problems, and in extension similarities between problems, and could collaborate with CF developers to restructure problems or contribute to the CF directly. Similarly, game developers deal with the details of the explicit elements and may have valuable contributions to make to the CF. In a way, CF developers can include both game and AI developers who could be assuming a double role either as direct contributors or as external collaborators. Such an organization together with the idea of breaking down video games into conceptual problems and using these as targets for conceptual AI is shown in Figure 6. The AI used in a video game could thus be described as solutions to elementary or composite conceptual problems. Conceptual problems are the heart of this video game AI development model. Indeed, it would serve little purpose to conceptualize video games if the resulting concepts could not be used to identify problems that are common to multiple games. Problem recurrence in video games is the raison d’être of such a model and why factoring video game AI is worth pursuing. The amount of factoring that can be achieved depends on how well recurring problems are isolated in video games not only of the same genre, but of any genre. This could be used as a measure of the efficiency of the model, as could be the amount of redundancy in AI solutions to disjoint problems. Clearly identifying and organizing conceptual problems is therefore a crucial dimension of this development model. Problems and their solutions can either be elementary or composite. Elementary problems are problems whose decomposition into lesser problems would not result in any AI being factored. They are the building blocks of composite problems. The latter combine several problems, elementary or composite, into a single package to be handled by a complete AI solution. For example, an agent for a FPS arena deathmatch can be seen as a solution to the problem of control of a character in that setting. This problem could be decomposed into smaller problems which can be found in different settings such as real-time combat and navigation. Navigation is a popular and well-studied problem found in many video games. Navigation in a virtual world often involves pathfinding. Common definitions as well as optimal solutions already exist for pathfinding problems. Examples include the
search algorithm, which solves the single-pair shortest path problem (Find the least-cost path between a source node and a destination node in a graph.), and Dijkstra’s algorithm, which solves the single-source shortest path problem (Find the least-cost path between a root node and all other nodes in a graph.). Although standard implementations can be found in developer frameworks and toolboxes, it is not unusual for developers to commit to their own implementation for environment-based customization. A problem decomposition is often reflected in the AI design of a video game. For example, the AI in a RTS game may be divided into two main components. One component would deal with the problem of unit behavior and define behavior for units in different states such as being idle or following specific orders. This AI component could in turn include subcomponents for subproblems such as pathfinding. Defining autonomous unit behavior involves elements such as the design of unit response to a threat, an attack or the presence of an ally and is a problem that can be found in other games such as RPGs and FPSs. The other main component would deal with the problem of playing the RTS game to make it possible for a human player to face opponents without requiring other human players. This component could be organized in a number of modules to deal with the various tasks a player has to handle in a RTS game. A strategy manager can handle decisions such as when to attack and which units to produce. A production manager can handle construction tasks and assign workers to mining squads. A combat manager can designate assault locations and assign military units to combat squads. Squad managers can handle tasks such as unit formations and coordination and target selection. These AI components can provide insight on the different conceptual problems they attempt to solve and their organization. Coordination between a group of units to select a common target or distribute targets among units and maneuver units can be included in the larger problem of real-time combat which is not exclusive to the RTS genre. On the other hand, production-related decisions could be taken based on generic data such as total air firepower or total ground armor, making it possible for the same conceptual policy to be used for any RTS game providing a conceptual view through which such data can be computed. More conceptual problems could be derived from these AI components. The real-time combat problem is a complex recurring problem found in many different games and may incorporate problems such as role management, equipment tuning, positioning, target selection and ability planning. Many such problems have already been studied and the video game AI literature is rich in books which explore common problems in depth. Examples include Programming Game AI by Example by Buckland [36], AI Game Engine Programming by Schwab [37], Artificial Intelligence for Games by Millington and Funge [38], Artificial Intelligence: A Modern Approach by Russel and Norvig [39] and the AI Game Programming Wisdom books by Rabin [40–43]. More specific publications that focus on positioning for example also exist, such as work from Straatman and Beij [44] and Pottinger [45]. The remainder of this section briefly presents some of these problems and attempts to reason about them conceptually. Role management in combat is a recurring problem in video games. Role management deals with the distribution of responsibilities, such as damaging, tanking, healing and disabling, among a group of units or characters fighting together. This problem is often encountered in popular genres such as RPG (e.g., World of Warcraft, Blizzard Entertainment 2004), RTS (e.g., Command & Conquer: Red Alert 3, Electronic Arts 2008) and FPS (e.g., Left 4 Dead, Valve Corporation 2008). Roles can be determined based on several factors, including unit type or character class, attributes and abilities, equipment and items, unit or character state and even player skill or preference. Without targeting any specific game, it is possible to define effective policies for role management using conceptual data only. The data can be static like a sorted list of role proficiencies indicating in order which roles a unit or character is inherently suited for. Such information can be used by the AI to assign roles in a group of units of different type in combat. Dynamic data can also be used to control roles in battle, like current hit points (The amount of damage a unit can withstand.), passive damage reduction against a typed attack and available abilities of a unit. For instance, these can be used together to estimate the current tanking capacity for units of the same type. Naturally, the interpretation of these concepts varies from one game to another. Yet a conceptual policy remains effective in any case. In a RPG, if a party is composed of a gladiator, an assassin and two clerics, the gladiator may assume the role of tank while a cleric assumes the role of healer and both the assassin and the other cleric assume the role of damage dealers. This distribution can vary significantly however. For example, the gladiator may be very well equipped and manage to assume the double role of tank and damage dealer, or conversely, the assassin may be dealing too much damage and become the target. If the tank dies, the healer may become the target (Healing often increases the aggression level of a monster towards the healer, sometimes more than damaging the monster would.) and assume both the role of tank and healer. In this case, the other cleric may switch to a healer role because the tanking cleric could get disabled by the enemy or simply because the lack of defense compared to a gladiator could cause the damage received to increase drastically, making two healers necessary to sustain enemy attacks. Roles can thus be attributed during combat depending on character affinities and on current state data too. A similar reasoning process can be used for units in a RTS game. In a squad composed of knights, sorcerers and priests, knights will be assuming the role of tanks and fighting at the frontlines, while priests would be positioned behind them and followed by the sorcerers. Sorcerers would thus be launching spells from afar while knights prevent enemy units from getting to them and priests heal both injured knights and sorcerers. Even among knights, some might be more suited for tanking than others depending on their state. Heavily injured knights should not be tanking lest they not survive a powerful attack. They should instead move back and wait for priests to heal them while using any long range abilities they might have. Unit state includes not only attributes such as current hit points but also status effects (A status effect is a temporary alteration of a unit’s attributes such as speed or defense.) and available abilities. Abilities can significantly impact the tanking capacity of a unit. Abilities could create a powerful shield around a unit, drastically increase the health regeneration of a unit or even render a unit completely invulnerable for a short amount of time. Likewise, healing and damage dealing capacities can vary depending on available abilities. The healing or damage dealing capacity of a unit may be severely reduced for some time if the unit possesses powerful but high-cooldown (The cooldown of an ability is the minimum amount of time required between two consecutive activations. It is used to regulate the impact of an ability in a battle.) abilities which have been used recently. If the knights fall, either priests stay at the front and become the tanks or they move to the back and let the sorcerers tank depending on who of the two has the higher tanking capacity. Again, conceptual data can be used to generate operating rules to dynamically assign roles among units. Algorithm 3 shows a conceptual AI function which can be used to determine the primary tank in a group. The primary tank is usually the unit or character that engages the enemy and is more likely to initiate a battle. Algorithm 4 details a possible implementation of the scoring function. It estimates the total amount of damage a unit could withstand based on its hit points and the overall damage reduction factor it could benefit from that can be expected during the battle given the abilities of both sides. A damage reduction factor is just one way of conceptualizing defensive attributes such as armor or evasion. The  dmgred_abilities function could create a list of available damage reduction abilities and average their effects. For each ability, the amount of reduction it contributes to the average can be estimated using the reduction factor it adds, the duration of the effect, the cooldown of the ability as well as its cast time. In the case of conflicting abilities (i.e., abilities whose effects override each other), the average reduction bonus could be estimated by spreading the abilities over the cooldown period of the one with the strongest effect. The  dmgamp_abilities function could work with damage amplification abilities in a similar way. It could also take into account the unit’s resistance to status effects.
Any form of distribution of responsibilities between units or characters fighting together can be considered role management. Role management does not assume any objective in particular. Depending on the goal of the group, different distribution strategies can be devised. The problem of role management in combat can therefore be described as follows. Given an objective, two or more units or characters and a set of roles, define a policy which dynamically assigns a number of roles to each unit or character during combat in a way which makes the completion of the objective more likely than it would be if units or characters each assumed all responsibilities individually. An example of objective is defeating an enemy unit. Roles do not have to include multiple responsibilities. They can be simple and represent specific responsibilities such as acting as a decoy or baiting the enemy. Another common problem in video games is ability planning. It can be found in genres such as multiplayer online battle arena (MOBA) (e.g., League of Legends, Riot Games 2009 and Dota 2, Valve Corporation 2013) and RPG (e.g., Aion: The Tower of Eternity, NCsoft 2008 and Diablo III, Blizzard Entertainment 2012). Units or characters may possess several abilities which can be used during combat. For instance, a wizard can have an ice needle spell which inflicts water damage on an enemy and slows it for a short duration, a mana shield spell which temporarily causes damage received to reduce mana points (Also called magic points or ability points. Using abilities usually consumes mana points.) instead of health points and a dodge skill which can be used to perform a quick sidestep to evade an attack. Each of these abilities is likely to have a cost such as consuming a certain amount of mana points or ability points and a cooldown to limit its use. Units or characters thus need to plan abilities according to their objective to know when and in what order they should be used. As with role management, both static and dynamic data can serve in planning abilities. For example, if the enemy’s class specializes in damage dealing, disabling abilities or protective abilities could take precedence over damaging abilities because its damage potential may be dangerously high. However, if the enemy’s currently equipped weapon is known to be weak or its powerful abilities are known to be on cooldown, the use of protective abilities may be unnecessary. Although abilities can be numerous, the number of ability types is often limited. These may include movement abilities, damaging abilities, protective abilities, curative abilities, enhancing abilities, weakening abilities and disabling abilities. Evidently, it is possible for an ability to belong to multiple categories. Abilities can be described in a generic way using various conceptual properties such as damage dealt, travel distance, conceptual attribute modification such as increasing hit points, effect duration, conceptual attribute cost such as action point cost, and cooldown duration. Abilities could also be linked together for chaining, such as using an ability to temporarily unlock another. Ability planning can then be achieved without considering the materialization of the abilities in a particular world. Even special abilities used under certain conditions, such as a boss attack that is executed when the hit points of the boss fall under a specific threshold, can be handled by conceptual policies. For instance, a powerful special ability of a boss monster can be unavailable until a condition is met. At that point, a policy that scans abilities and selects the most powerful one available would automatically result in the use of the special ability. If the ability must be used only once, a long cooldown can stop subsequent uses assuming cooldowns are reset if the boss exits combat. (This is to ensure that the boss can use the special ability again in a new battle in case its opponents are defeated or run away.) Abilities can be planned according to some goal. For example, the goal could be to maximize the amount of damage dealt over a long period of time, also called damage per second (DPS). Maximizing DPS involves determining a rotation of the most powerful abilities with minimum downtime. (A state where all useful abilities are on cooldown.) Conversely, the goal could be maximizing the amount of damage dealt over a short period of time, or dealing as much damage as possible in the shortest amount of time, also called burst damage. A burst plan is compared to a DPS plan in Figure 7. While the burst strategy (Plan A) obviously deals more damage at the beginning, it is clear that the DPS strategy (Plan B) results in more damage over the entire period. The DPS plan orders long-cooldown abilities in a way that avoids simultaneous cooldown resets because these powerful abilities need to be used as soon as they are ready to make the most out of them, which is not possible if multiple ones become ready at the same time. It also avoids the downtime between the two consecutive uses of the purple ability in Plan A by better interleaving its casts throughout the time period. This leads to a higher output overall. Note that the burst strategy eventually converges towards the the DPS strategy. When combat is largely based on abilities, predicting and taking into account enemy abilities becomes crucial for effective ability planning. If a lethal enemy attack is predicted, a unit or character can use a protective ability such as casting a shield just before the attack is launched to nullify its effect. Alternatively, it can use a disabling ability to prevent the enemy from using the ability or interrupt it. Known enemy abilities could be evaluated in order to predict the enemy’s likely course of action and plan abilities accordingly. Just like role management, ability planning can be dealt with by defining interesting conceptual policies for various frequently encountered objectives. In Algorithm 5, the DPS of an ability chain is estimated by adding up the damage and duration of each ability in the chain. Ability chains can be useful to represent linked abilities, for example when an ability can only be activated after another. They can also be used to generate different versions of the same ability in cases where using an ability after a specific one alters the attributes of the ability. If activating ability
after
increases the damage of
by 100% or reduces its use time by 50%,
and
may be interesting from a DPS standpoint in cases where they otherwise are not when considered individually. The attribute values of
can then be different from their default ones depending on the chain in which they appear. Of course, this function only estimates a theoretical damage and is more useful to generate all-purpose ability rotations than to plan abilities against a specific enemy. DPS can be more accurately estimated by factoring in the attributes and status effects of both the user and the target. If the target is very resistant against a particular type of damage, powerful abilities of this type may be outranked by less powerful ones dealing a different type of damage. The attributes or the status effects of the user can also affect the effectiveness of different abilities in different ways. One ability may have a high base damage value but gain nothing from the strength of the user, while another ability may have a low base damage but greatly benefit from the strength attribute and end up out-damaging the former. Use time can also vary depending on the user’s attributes. Note that the use time corresponds to the total time during which the user is busy using an ability and cannot use another. Some abilities may involve both a cast time (i.e., a phase where the user channels energy without the ability being activated) and an activation duration (i.e., the time between the activation of the ability and the time the user goes back to an idle state). This function does not calculate other costs either. If abilities cost ability points or mana points to use in combat, these additional costs can be estimated for the chain together with the time cost since they usually cannot be ignored during a battle.
The concept of abilities is used in several genres. They usually correspond to actions that can be taken in addition to base actions, such as moving, at a cost. Given an objective and a set of abilities, the problem of ability planning is to produce a sequence of abilities which leads to the completion of the objective. Note that the set of abilities does not have to belong to a single entity. Like in role management, the objective can be fairly abstract and common, such as running away, disabling an enemy or protecting an ally. A frequently encountered problem in video games is positioning in the context of combat. Many genres include it, such as action-adventure (AA) (e.g., The Legend of Zelda: Ocarina of Time, Nintendo 1998), RTS (e.g., StarCraft II: Wings of Liberty, Blizzard Entertainment 2010) and RPG (e.g., TERA: Rising, Bluehole Studio 2011). Maneuverable units or characters have to continuously adjust their position according to their role or plan. A character whose role is to defend other characters will move to a position from which it can cover most of its allies from enemy attacks. An archer will attempt to stay outside the range of its enemies but close enough to reach them. A warrior with strong melee area-of-effect (AoE) attacks must move to the center of a group of enemies so as to hit as many of them as possible with each attack. An assassin may need to stick to the back of an enemy in order to maximize its damage. A specialized unit with poor defense could remain behind its allies in order to easily retreat in case it becomes targeted. This kind of behavior results from conceptual reasoning and needs not be specific to any one game. While navigation deals with the problem of traveling from one position to another, positioning is more concerned with finding new positions to move to. New positions can be explicitly designated for a unit or character or they could be implicitly selected by adjusting movement forces. For example, a unit may need to step outside the range of an enemy tower by moving to a specific position, or it could avoid bumping into a wall while chasing another unit by adding a force that is normal to the direction of the wall to its steering forces instead of selecting a position to move to. When positions are explicitly calculated, navigation may be involved to reach target positions. This can lead to a dependency between solutions to positioning problems and solutions to navigation problems. Algorithm 6 shows a function which moves a unit out of the attack range of a group of enemies. For each enemy, it creates a circular area based on the enemy’s attack range and centered on its predicted position. The latter is simply calculated by adding the enemy’s current velocity to its position. This function ignores enemies that are faster than the unit because even if the unit is currently outside their range, it would eventually fall and remain within their reach. This could be delayed however. A list of immediate threats is thus created and used to compute a force to direct the unit away from the center of threats as quickly as possible. Note that this code does not differentiate between threats. It can be improved by weighting each position in the calculation of the center according to an estimation of the danger the threat represents. The more dangerous the threat, the larger the weight can be. This would cause the unit to avoid pressing threats with higher priority. This function could be used for kiting.
The code in Algorithm 7 shows how a straight line projectile can be dodged by a unit. A ray is created from the current position of the projectile and used to determine whether a collision with the unit is imminent. If this is the case, the unit is instructed to move sideways to avoid collision. The bounding radius of the projectile as well as that of the unit are used to determine the distance which must be traveled. The side on which the unit moves depends on its relative position vis-à-vis the projectile course. Of course, this function does not take into account the speed of the projectile and could therefore be better. If the projectile is slow compared to the unit, the movement could be delayed. On the other hand, if it is too fast, dodging may be impossible and the unit would not need to waste any time trying to do that.
Clearly, both code examples presented above follow a purely conceptual reasoning and could apply to a multitude of video games. They operate solely on conceptual objects and properties such as units, positions, velocities, steering forces and distances. Creating a comprehensive collection of general policies to deal with positioning problems can be time-consuming, making it unlikely to be profitable for a video game developer. When the solutions are conceptual and target all video games however, they may become profitable, providing incentive for AI developers to undertake the challenge. Like role management and ability planning, positioning exists within the context of an objective. It is possible to design conceptual yet effective positioning policies for generic objectives such as maximizing damage dealt or minimizing damage received. Given an objective, the problem of positioning is to control the movement of a maneuverable entity in a way which serves the completion of the objective. Note that objectives could automatically be derived from roles. Depending on the space and the type of movement, different positioning problems could be considered. For example, it may be more interesting to consider 2D positioning and 3D positioning separately than to consider a single multidimensional positioning problem. Since conceptual AI is designed independently from games, an integration mechanism is necessary for it to be used by game developers. Game developers must be able to choose and connect AI solutions to a game. This is achieved by registering AI controllers with conceptual objects. To assign control, partial or complete, of an entity in the game to a particular AI, the corresponding controller must be instantiated and registered with the projection of the entity in CDS. The AI then controls the conceptual entity, effectively controlling the entity in the game. For example, a game developer could use two AI solutions for a racing game, one for controlling computer opponents on the tracks and another for dynamically adjusting the difficulty of a race to the player’s performance. Each time a computer opponent is added to the race, a new instance of the driving AI is created and registered with its conceptual projection. As for the difficulty AI, it can be created at the beginning of the race and registered with a real-time player performance evaluation object. For each controllable conceptual object defined by the CF developers, a controller interface is defined together with it. This interface describes functions the AI must implement in order to be able to properly assume control over the conceptual object. These are not to be confused with the conceptual controls, also defined by the CF developers, which the AI can use to control the conceptual object and which are implemented by the game developers. Figure 8 illustrates the distinction. It is possible for multiple controllers to share control of the same object. For example, a NPC could be controlled by different AI solutions depending on its state. It may have a sophisticated combat AI which kicks in only when the NPC enters a combat state and otherwise remains on standby, while a different AI is used when the NPC is in an idle state to make it roam, wander or rest. Multiple controllers however may lead to conflict in cases with overlapping control. One way to resolve conflicts is for AI controllers to have a table indicating a priority level for each conceptual control. Conceptual control calls would then be issued by controllers with their respective priorities and queued for arbitration. Of course, when multiple AI controllers are integrated into a complete solution, this issue can be handled by the author of the solution in whatever way they may choose and only the complete controller can be required to provide a priority table for conceptual controls. Figure 9 shows how multiple controllers can be registered with a conceptual object. First, an object in the game, an Undead Peon, is created. Following this, its projection in CDS, a NPC, is created and linked to the Undead Peon. Finally, several independent AI controllers, one for generating idle behavior when the Undead Peon is idle, another for generating social behavior when the Undead Peon is around other Undead Peons and other types of NPCs and another for generating combat behavior when the Undead Peon is facing enemies, are created and registered with the NPC in CDS. In this case, there is no overlap in the control of the NPC by the different AI solutions. Using this registration mechanism, an AI controller can also verify that its dependencies are running and access them via the conceptual object. Examples of functions found in controller interfaces are an update function and event handlers. An update function is used to update the internal state of the AI and can be called every game cycle or at an arbitrarily set update rate. This function is illustrated in Figure 10. Note how the NPC in CDS has no internal state update cycle. This is because there is no dynamic in the CDS. Objects in CDS are projections of game objects and are only modified as a result of a change in game objects. Event handlers are used to notify AI controllers of game events, such as a unit being killed by another. When an event occurs in the game, a conceptual projection is fired at the projection of the involved game object. The events that can involve a conceptual object are determined by the CF developers and used to create the controller interface. An AI controller does not necessarily need to handle all events. This is obvious for partial controllers. Therefore, it is possible for AI controllers to ignore some events. Event handlers are illustrated in Figure 11. Other examples are functions for suspending and resuming the controller. When game developers link AI solutions to their games, they can either link them statically at build time or load them dynamically at runtime. Loading AI at runtime makes it easier to test different AI solutions and can also allow players to hook their own AI to the game. Typically, the AI would be running within the video game process, though it can be interesting to consider separating their execution. Deploying the AI in a separate process means it can run on a different machine. The latter could be optimized for AI processing or it could even be on the Internet, making it possible for AI developers to offer AI as a service. A multiprocess design can easily be imagined, as shown in Figure 12. The Graven experiment consists in rebuilding an open-source video game called Raven according to the design presented in the previous sections. (See Figure 2.) Namely, the AI is separated from the game and a conceptual layer is added in between. The AI is adapted to interact with the conceptual layer rather than directly with the game and the latter is modified to maintain a conceptual view in memory and use the conceptual AI. Albeit basic, Raven involves enough concepts to use as a decent specimen for conducting experiments relating to the deployment and use of a CF. The goal of the experiment is twofold.(1)Concretize the design architecture as well as key processes in a working example.(2)Obtain a code base to use as a limited prototype for testing conceptual AI in multiple games. Note that the Graven experiment does not directly aim at demonstrating the efficiency of conceptual AI. Raven is an open-source game written by Mat Buckland. A detailed presentation of the game as well as the code can be found in Programming Game AI by Example Buckland [36], where it is used to demonstrate a number of AI techniques such as path planning, goal-driven behavior, and fuzzy logic. It is a single-player, top-down 2D shooter featuring a deathmatch mode. Maps are made of walls and doors and define spawn locations for players as well as items. When players die, they randomly respawn at one of the fixed spawn locations. Items also respawn at fixed time intervals after they are picked up. There are two types of items in Raven, weapons and health packs. Three weapons can be picked up. These are the Shotgun, the Rocket Launcher and the Railgun. A fourth weapon, the Blaster, is automatically included in every player’s inventory at spawn time. Each weapon is characterized by a unique set of features such as a firing rate and the maximum quantity of ammunition that can be carried for it. The Blaster is a basic projectile weapon with unlimited ammo. The Shotgun is a hitscan weapon which fires several pellets that spread out. The Rocket Launcher is a projectile weapon which fires rockets that deal AoE damage when they explode either on impact or after traveling a certain distance. The Railgun is a hitscan weapon which fires penetrating slugs that are only stopped by walls. Players can pick up weapons they already have. In that case, only the additional ammo is added to their inventory. Initially, a default number of bots are spawned depending on the map. Bots can then be added to and removed from the game. The player can possess one of the existing bots to participate in a match. The left and right mouse buttons can be used to fire and move respectively, while numbers on the keyboard can be used to switch weapons. Despite their adorable look, these bots will compute the shortest paths to navigate the map, avoid walls, pick up ammo and health when needed, estimate their opponent’s position to aim projectiles properly, use the most appropriate weapon depending on the situation, remember where they last saw or heard an opponent, chase or run away from an opponent, perform evasive maneuvers and, of course, kill. A preview of the game is shown in Figures 13 and 14. The world in a Raven game is essentially composed of a map, bots and projectiles. The map is composed of walls and includes a navigation graph used for pathfinding as well as triggers. Triggers are used to define item pick up locations as well as temporary sound sources. This composition is illustrated in Figure 15. The bot AI is primarily made of 6 interdependent modules, as shown in Figure 16. The brain module handles abstract goals and takes decisions such as attacking the current target or retrieving an item. The steering module manages steering forces resulting from multiple simultaneous behaviors such as avoiding a wall while seeking an enemy. The path planner module handles navigation requests by computing paths between nodes in the navigation graph. The sensory memory module keeps track of all the information the bot senses and remembers, such as visible enemies, hidden enemies and gunshot sound locations. The target selection module is used to select a target among a group of enemies. Finally, the weapon system module handles aiming and shooting and also includes per-weapon specific modules to evaluate the desirability of each weapon given the current situation. The code structure in Graven comprises five categories of components:(1)the Raven classes,(2)the conceptual view classes,(3)the conceptual AI classes,(4)the conceptual controls,(5)and the Raven control classes. The Raven classes are the game classes and an adaptation of the original code where all the AI components are removed and code to synchronize the conceptual view with the game state is added. The second category is a library of objects representing concepts corresponding to the Raven objects. The conceptual AI classes are a modification of the original AI code in which the AI is made to interact with the conceptual layer rather than the game. The fourth category includes a set of conceptual controls used by the conceptual AI to control bots. Finally, the Raven control classes implement these conceptual controls. Note that from a design perspective, the conceptual controls belong in the conceptual layer classes and their implementation in the game classes. They are separated in the code structure for the purpose of clarity. Raven is primarily composed of generic elements, as can be seen in Figure 15. A 2-dimensional world, projectiles, or walls are concepts commonly found in many video games. The added conceptual layer thus largely consists of clones of the objects in Raven. Unlike their Raven counterpart, however, conceptual objects are entirely static and do not update their own state. Instead, their state is only modified as a result of a modification on the game side. This is illustrated in Algorithm 8.
In Algorithm 8, the  Raven_Weapon class declares a  ShootAt function which is used to fire the weapon and which is implemented by each of the four Raven weapon classes. It also defines an  IncrementRounds function which is used to update the number of rounds left for the weapon when a bot picks up additional ammo. In the corresponding  CptWeapon class, the  ShootAt function has been removed, and the  IncrementRounds function has been replaced with a  SetNumRoundsLeft function which can be used by the game to update the number of rounds left for the weapon in CDS. The synchronization process is detailed in a subsequent section. Four conceptual controls have been defined. These are used by the conceptual AI to control the bots in Graven and are shown in Algorithm 9. The  ApplyForce function can be used to apply a steering force to a bot and control its movement. The  RotateToward function can be used to rotate a bot and control the direction of its field of view. The  EquipWeapon function can be used to switch a bot’s weapon to any of those it holds in its inventory. Lastly, The  ShootAt function can be used to fire a bot’s equipped weapon. These conceptual controls can be applied to a  CptMvAgent2D object, the conceptual projection of a Raven bot in CDS. They are implemented game-side.
On the AI side, a  CptMvAgent2D represents a controllable object and therefore the class comes with a controller interface. For an AI to be recognized as a valid controller by the game, it has to implement this interface. The interface is shown in Algorithm 10. It includes six functions. The  KilledBy_Handler function is called whenever a bot is killed by an opponent and allows the controller to retrieve information about the killer. The  HeardSound_Handler function is called when a bot hears a gunshot and can be used by the AI to find the origin of the sound. The  BotRemoved_Handler function is called when a player removes a bot from the game via the main menu and can be used to notify other bots that the removed bot no longer exists. The  Suspend and  Resume functions serve to temporarily disable the controller when a bot is possessed by the player. The last  Update function is used to allow the AI to update its state every game cycle.
Functionally, the AI in Graven is the same as the original Raven AI. It slightly differs in its structure, however. In Raven, the  Raven_WeaponSystem class serves as a weapon inventory and handles weapon switching and also aiming and shooting, whereas weapon selection and aiming and shooting are separated in Graven. The central AI module through which other AI modules interact is the  CptBot class. It resembles the original  Raven_Bot class, though there are two significant differences. One, it interacts solely with the conceptual layer instead of the game. Two, it does not host any game state data such as current position and velocity, which is found in the  CptMvAgent2D it controls. The AI state is thus clearly separated from the game state. The following process is used to synchronize the conceptual view with the Raven game state. For each class representing an object in the Raven game which has some projection in the CDS, a pointer to an object of the corresponding conceptual class is added to its data members. Then, following each statement that directly modifies a member of the class (without calling a function), a second operation is added to update the conceptual object accordingly. The conceptual object is created at the beginning of the class constructor and destroyed at the end of its destructor. By confining the synchronization code of an object to its class, its synchronization is done only once and never mixed with that of other objects. This idea is illustrated in Algorithm 11.
One problem with this technique is that it cannot be used directly with virtual classes because, even if they have corresponding conceptual classes, they do not represent actual objects with an independent projection in the CDS. The projection of a virtual class only exists as a part of the projection of a concrete class (i.e., a conceptual concrete class) and can only be accessed through this conceptual concrete class. A remedy for this problem is using a pure virtual getter implemented by its concrete subclasses, as shown in Algorithm 12. This involves another problem however, since virtual functions cannot be called in the constructor. (In C++, the virtual table of an object is only initialized after its construction is complete.) This is solved by moving the synchronization code in the constructor into an additional  sync function for each class. This applies even to concrete classes. The  sync function in a subclass always starts by calling the  sync function of its superclass, ensuring that the synchronization code of an object remains confined within its class definition. A call to the  sync function is added immediately after the creation of a conceptual object in the constructor of a concrete class, effectively executing the synchronization code of all its superclass constructors.
In order to properly synchronize certain template classes in Raven, it is necessary to use additional data type parameters to accommodate conceptual data types associated with the base parameters. For example, the  Trigger_Respawning template class in Raven takes an entity type parameter which determines the type of game object that can activate the trigger. The class  Trigger_WeaponGiver which extends  Trigger_Respawning uses a  Raven_Bot as parameter. However, its conceptual projection, a  CptTrigger  WeaponGiver, requires a  CptMvAgent2D parameter. For this reason, the  Trigger_Respawning class takes two parameters in Graven, one for the game data type and one for the corresponding conceptual data type. The  CptBot class implements the  CptMvAgent2D_Controller interface and provides the AI functionality of the original Raven. The  CptMvAgent2D class defines an  AddController function which can be used by the game to register  CptMvAgent2D_Controller objects with its instances. All registered controllers are updated and notified through the  CptMvAgent2D instance. This is shown in Algorithm 13.
A  DMController module can be used to instantiate and register  CptBot objects without exposing the class to the game. Algorithm 14 shows how a controller is registered in the constructor of the  Raven_Bot class. After creating and synchronizing a corresponding  CptMvAgent2D, the  RegisterDMController function is used to relegate the control of the bot to the conceptual AI.
Following the Graven experiment which produced a limited CF prototype as well as a number of conceptual AI solutions, a second experiment was conducted to assess the work involved in using a simple conceptual AI solution in different games. Two games were used in this experiment, Raven and StarCraft: Brood War (BW), a real-time strategy game developed by Blizzard Entertainment. Albeit very different, these two games share a common conceptual problem, namely, target selection. Target selection in combat deals with deciding which enemy should be targeted in the presence of multiple ones. In Raven, a bot may face multiple opponents at the same time. Likewise in BW, a unit may face multiple enemy units on the battlefield. This experiment consists in using the same solution to this targeting problem in both Raven and BW, resulting in having the exact same code drive the targeting behavior of both bots in Raven and military units in BW. Although BW is not open-source, hackers have long been able to tamper with the game process by breaking into its memory space. Eventually, a development framework was built on top of this hacking. The Brood War Application Programming Interface (BWAPI) [46] is an open source C++ framework which allows AI developers to create custom agents by providing them with means to access the game state and issue commands. More information regarding the features and the design of the API can be found on the project’s web page. The targeting system module in Graven,   CptTargetingSystem, is used by the main AI module  CptBot. To function, it requires another module, the sensory memory module  CptSensoryMemory, which determines which enemies the bot currently senses. The targeting system works by setting an internal target variable which the bot module can read to find out which enemy it should aim at. The original AI selects targets based on their distance to the bot and prioritizes closer enemies. It was modified to instead select targets based on their health and prioritize weaker enemies, a more interesting strategy for this experiment because the default unit AI in BW also uses distance as the primary factor in target selection. The main module function is shown in Algorithm 15. The vision update function in the sensory module is shown in Algorithm 16.
In terms of conceptual view, the requirements of the targeting module include those of its dependencies (i.e., the sensory memory module). The solution requirements can quickly be determined by looking at Algorithms 15 and 16. It requires a 2D world with the list of targetable entities that exist in it as well as a list of vision-blocking obstacles such as walls typically defined in a map. The entities must have their position, facing direction, field of view and health attributes synchronized. All of these concepts are already defined in the conceptual layer used in Graven. In addition to those, BW involves three more concepts which are not present in Raven and which need to be defined. First, the concept of entity ownership is required to specify the player a unit belongs to. In Raven, a player is associated with a single bot. In BW, a player is associated with multiple units. Therefore, an owner property is required for units to differentiate between allies and enemies. The second concept is that of sight range. In Raven, a bot has a 180 degree field of view but its vision range is only limited by obstacles. In BW, a unit has a 360 degree field of view but can only see up to a certain radius. A sight range property is thus required. The third concept is the plane. The world in BW is two-dimensional but there are ground and air units. Ground units are not always able to attack air units and vice versa. A property to indicate the plane in which a unit exists and which planes it can target is thus needed. As a result, five new members are added to the  CptMvAgent2D class, a player ID, a sight range, a plane flag, and two plane reach flags. Note that the sensory memory module is slightly modified to take into account this information, though this has no impact on its functionality in Raven. As far as conceptual controls are concerned, the aiming and shooting controls in Graven are not necessary for BW. When a unit in BW is given an order to attack another unit, the target only needs to be within firing range to be automatically attacked continuously. Only one conceptual control, an attack command, is required for this experiment and added to the conceptual framework. In order to use the targeting AI from Graven in BW, there are a few tasks that need to be completed. These are(1)adding code to the game to maintain in memory a conceptual view including the elements mentioned above,(2)implementing the attack conceptual control,(3)and creating an AI solution which makes use of the targeting AI to control units. The conceptual view is maintained using 3 callback functions provided by the BWAPI, the  onStart function which is called at the beginning of a BW game, the  onEnd function which is called at the end of the game and the  onFrame function which is called every game frame. The code added in each of these functions is shown in Algorithms 17, 18, and 19, respectively. The  syncUnit function is shown in Algorithms 20.
Because the source code of BW is not available, the synchronization process is different from the one used in the Graven experiment. Every game cycle, the game state is scanned and new and destroyed units are added to and removed from the conceptual view and the states in CDS are synchronized with unit states in the game. The  Attack conceptual control is easily implemented using the basic  attack command players can give to units in BW. The implementation is shown in Algorithms 21.
For units capable of attacking, an attack AI is added to the list of controllers of their projection using the  RegisterDMController function. This function instantiates the  CptBot class, which is similar to the one in Graven but which has been modified to only use the sensory memory and targeting system modules. The update function of the  CptBot module is shown in Algorithms 22. Note that the sensory memory module only registers reachable enemy units. Allied units are ignored.
The same targeting AI was successfully used in both Raven and BW, as shown in Figures 17 and 18. Unsurprisingly, the CF prototype (more specifically the  CptMvAgent2D class) built from Raven, a very simple 2D shooter, had to be slightly extended for this experiment. Even so, the effort required to integrate the Graven targeting AI in BW was minimal. Of course, the AI was minimal too. This shows however that the work involved in creating conceptual AI that can be used in different games does not have to grow significantly with the number of games it can be applied to and that when a conceptual problem is clearly identified, it can be solved independently of the game it appears in. Obviously, though it may not have been the goal of the experiment, the modified unit AI performs better in combat than the original one for ranged units, since it uses a better a strategy. In the presence of enemies, the original unit AI acquires a target by randomly selecting one within firing range. The modified unit AI on the other hand selects among targets within its sight radius the one with the lowest health. Because the sight range of a ranged unit is often close to its firing range, this behavior is similar to the original one in the sense that the unit does not move to reach a target when another target that is already in firing range exists. The behavior is therefore close but the unit does target weak enemies first in order to reduce their firepower as fast as possible. Moreover, setting a short memory span in the  CptSensoryMemory class prevents units from remembering runaway targets for too long and starting to look for them. This helps maintain similarity between the original and modified unit AI. That way, the original unit behavior is maintained, making it harder for players to notice any difference other than the improved targeting strategy. Needless to say, the targeting AI remains completely unchanged. Note that modifying the sensory module to pick up targets that are within firing range rather than sight range makes the strategy work for melee units as well. The modified unit AI was tested using 10 battles of 5 Terran Ghosts versus 5 Terran Ghosts, one group being controlled by the modified unit AI and the other by the original unit AI. Ghosts are ranged ground units. The group with the modified unit AI won every battle. The number of Ghosts lost during each battle is reported in Table 1. The main contribution of this research is an approach for the development of AI for video games based on the use of a unified conceptual framework to create a conceptual layer between the game and the AI. (The AI referred to here is game related and does not include context related AI as specified at the beginning of this work.) This approach is inspired by an interpretation of human behavior. Human players have the ability to detect analogies between games and generalize, or conceptualize, the knowledge acquired in one game and apply it in another. By conceptualizing video games and asking game developers to create conceptual views of their games using a unified framework, it becomes possible to create solutions for common conceptual problems and use them across multiple video games. Developing solutions for conceptual problems rather than specific video games means that AI design is no longer confined to the scope of individual game projects and can be more efficiently refined over time. Such conceptual AI can then serve as a core engine for driving agents in a variety of video games which can be complemented by game developers specifically for each game. This would both reduce AI redundancy and facilitate the development of robust AI. Such an approach can result in a number of advantages for game developers. First, it means that they no longer need to spend a lot of resources to design robust game AI unless they want to and can simply use existing AI solutions. Even though they have to add code for the creation of conceptual views, not having to worry about game AI can result in significant cuts in development time. For example, they would not even need to plan for coordination mechanisms between multiple agents in the game. Moreover, they do not need to use conceptual AI for all tasks. They can select the problems they want to handle using conceptual AI and use regular AI for other tasks. Story and environment related AI, which this approach does not apply to, can be designed using existing tools and techniques, such as scripting engines and behavior trees, which make it easy to implement specific behavior. In addition, the continuous development of conceptual AI is likely to yield better quality solutions over time than what can be achieved through independent game projects. It may also be that clearly identifying and organizing the conceptual problems that make up the challenges offered by video games could allow game developers to compose new challenges more easily. Since this approach allows AI development to progress independently of video games, it could lead to the birth of a new game AI business. AI developers could compete to create the best AI solutions and commercialize them or they could collaborate to design a solid open-source AI core which would be perfected over time. Additionally, machine learning techniques would be more straightforward to apply with a unified conceptual representation of game elements. These techniques can be used to learn specialized behavior for each game which can enhance the basic generic behavior. This is similar to the way humans tune their generic experience as they learn specific data about a video game they are playing to improve their performance in that particular game. With an open-source unified conceptual framework, incentive for both game developers and AI developers to contribute to the development of the framework and the conceptualization of video games would exist. AI developers would benefit from a better conceptual framework because it would help factor AI better and allow more efficient AI development, resulting in better quality AI which benefits game developers directly when they integrate it in their games to create smarter, more challenging and more realistic agents. Because the conceptual layer constitutes a sort of middleware, a new version of the conceptual framework may not be compatible with AI developed prior to its update. Even if it is, legacy AI may require an update in order to benefit from the improved conceptual framework. Another disadvantage of the approach is that it requires more computational resources in order to maintain a conceptual view in memory during runtime, though this may not represent a major obstacle with mainstream hardware featuring increasingly more processing cores and system memory. Other issues may also arise from the separation of AI from video games. Indeed, game developers could lose some control over the components of their games and subsequently over the ability to balance them. For instance, it may be necessary to design new mechanisms to allow game developers to retain control over the difficulty of the game and adjust the skill level of their agents. Furthermore, although machine learning techniques such as imitation learning could benefit from a larger learning set as a unified conceptual representation would give them access to data from many games, they would require a translation process to project human actions into conceptual data space since, unlike AI actions, those are not conceptual. In other words, without a translation process, conceptual game states could only be linked to concrete game actions. Though an implementation of the approach was presented to illustrate some applications, alternative implementations can easily be imagined. For example, even if the AI code was compiled alongside the game code in Graven, it was designed to be independent. AI modules can be compiled independently from game code and either linked to the game statically or dynamically loaded at runtime. An implementation using the latter option would benefit from easier testing of different AI solutions. When deployed, it would allow players to switch between different solutions too. This may not be desirable however, as untested solutions may result in unexpected behavior. A security mechanism could be added to prevent the game from loading unverified AI modules. Perhaps the most exciting extension to this research would be a study of the world of conceptual problems found in video games. Both the video game industry and the scientific community would benefit from tools for describing and organizing problems using a set of convenient standards, perhaps a bit like game design patterns. This would help better categorize and hierarchically structure problems and result in a clearer view and understanding of the complexity of video games. The authors declare that there is no conflict of interests regarding the publication of this paper."
e0f4e76878,"CAPTCHAs may be a thing of the past, thanks to new machine learning research","CAPTCHA is an acronym for Completely Automated Public Turing test to tell Computers and Humans Apart. The term was coined in 2003, when the use of automated bots was becoming commonplace, and it refers to those annoying squiggly distorted letters that you have to type in when creating an online account. Although some companies have found ways around them, CAPTCHAs are still ubiquitous online. Researchers at the AI company Vicarious may have just made CAPTCHAs obsolete, however, by creating a machine-learning algorithm that mimics the human brain. To simulate the human capacity for what is often described as “common sense,” the scientists built a computer vision model dubbed the Recursive Cortical Network. “For common sense to be effective it needs to be amenable to answer a variety of hypotheticals — a faculty that we call imagination,” they noted in a post at their blog. The ability to decipher CAPTCHAs has become something of a benchmark for artificial intelligence research. The new Vicarious model, published in the journal Science, cracks the fundamentals of the CATCHPA code by parsing the text using techniques that are derived from human reasoning. We can easily recognize the letter A for example, even if it’s partly obscured or turned upside down. As Dileep George, the co-founder of the company explained to NPR, the RCN takes far less training and repetition to learn to recognize characters by building its own version of a neural network. “So if you expose it to As and Bs and different characters, it will build its own internal model of what those characters are supposed to look like,” he said. “So it would say, these are the contours of the letter, this is the interior of the letter, this is the background, etc.” These various features get put into groups, creating a hierarchal “tree” of related features. After several passes, the data is given a score for evaluation. CAPTCHAs can be identified with a high degree of accuracy. The RCN was able to crack the BotDetect system with 57 percent accuracy with far less training than conventional “deep learning” algorithms, which rely more on brute force and require tens of thousands of images before they can understand CAPTCHAs with any degree of accuracy. Solving CATCHPAs is not the goal of the research, but it provides insight into how our brains work and how computers can replicate it, NYU’s Brenden Lake told Axios. “It’s an application that not everybody needs,” he said. “Whereas object recognition is something that our minds do every second of every day.” “Biology has put a scaffolding in our brain that is suitable for working with this world. It makes the brain learn quickly,” George said. “So we copy those insights from nature and put it in our model. Similar things can be done in neural networks.”"
a7a54ca015,Australian ‘budget bot’ wins Amazon robot challenge,"An inexpensive robot has triumphed over its more sophisticated competition to win Amazon's annual Robotics Challenge. 'Cartman', a robot designed by the Australian Center for Robotic Vision (ACRV), was built from scratch for a fraction of the cost of other competing robots, and was even held together in places by cable ties. This year's winner -- which netted its creators the AU$80,000 ($63,770) cash prize -- stood out from the competition because of its gantry-like design. Instead of a robotic arm, Cartman functions via a sliding mechanism that picks products from above, moving on three axes. At its end is a rotating gripper, which uses a suction cup or two-finger grip to grab the item. The parts for the robot were cheap by the standards of typical industrial robots, costing under AU$30,000 ($23,913). The competition is designed to showcase automated solutions to the challenges Amazon faces in its goods warehouses. Robots are already used to move products around fulfilment centers, but the company -- like all online retailers -- still relies on humans to pick and box items for delivery. Given the sophisticated robotics technology already on the market it seems that automated picking and packing is something that should already exist. But carrying out such a task properly involves a complex mix of object and pose recognition, grasp and motion planning, and task execution, not to mention error detection and recovery, should things go awry. The Amazon competition tests a robots competency with three timed tasks: picking and packing specific products, retrieving and storing specific products, and a combination of the two. With Amazon already making ambitious strides in automation through its delivery drones, how long until humans are removed from its equation entirely? Update: The image in this article has been updated to illustrate the competition's winning entry. The erroneously-issued original photo depicted a different entrant. Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
7e6dbe7e3c,Eye-tracking system uses ordinary cellphone camera  | MIT News,"Login
or
Subscribe Newsletter
Researchers developed a simple application for devices that use Apple’s iOS operating system. The application flashes a small dot somewhere on the device’s screen, attracting the user’s attention, then briefly replaces it with either an “R” or an “L,” instructing the user to swipe either the right or left side of the screen. Correctly executing the swipe ensures that the user has actually shifted his or her gaze to the intended location. During this process, the device camera continuously captures images of the user’s face. Illustration: Christine Daniloff/MIT Crowd-sourced data yields system that determines where mobile-device users are looking.
Larry Hardesty | MIT News Office
June 15, 2016
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
For the past 40 years, eye-tracking technology — which can determine where in a visual scene people are directing their gaze — has been widely used in psychological experiments and marketing research, but it’s required pricey hardware that has kept it from finding consumer applications. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory and the University of Georgia hope to change that, with software that can turn any smartphone into an eye-tracking device. They describe their new system in a paper they’re presenting on June 28 at the Computer Vision and Pattern Recognition conference. In addition to making existing applications of eye-tracking technology more accessible, the system could enable new computer interfaces or help detect signs of incipient neurological disease or mental illness. “The field is kind of stuck in this chicken-and-egg loop,” says Aditya Khosla, an MIT graduate student in electrical engineering and computer science and co-first author on the paper. “Since few people have the external devices, there’s no big incentive to develop applications for them. Since there are no applications, there’s no incentive for people to buy the devices. We thought we should break this circle and try to make an eye tracker that works on a single mobile device, using just your front-facing camera.” Khosla and his colleagues — co-first author Kyle Krafka of the University of Georgia, MIT professors of electrical engineering and computer science Wojciech Matusik and Antonio Torralba, and three others — built their eye tracker using machine learning, a technique in which computers learn to perform tasks by looking for patterns in large sets of training examples. Strength in numbers Khosla and his colleagues’ advantage over previous research was the amount of data they had to work with. Currently, Khosla says, their training set includes examples of gaze patterns from 1,500 mobile-device users. Previously, the largest data sets used to train experimental eye-tracking systems had topped out at about 50 users. To assemble data sets, “most other groups tend to call people into the lab,” Khosla says. “It’s really hard to scale that up. Calling 50 people in itself is already a fairly tedious process. But we realized we could do this through crowdsourcing.” In the paper, the researchers report an initial round of experiments, using training data drawn from 800 mobile-device users. On that basis, they were able to get the system’s margin of error down to 1.5 centimeters, a twofold improvement over previous experimental systems. Since the paper was submitted, however, they’ve acquired data on another 700 people, and the additional training data has reduced the margin of error to about a centimeter. To get a sense of how larger training sets might improve performance, the researchers trained and retrained their system using different-sized subsets of their data. Those experiments suggest that about 10,000 training examples should be enough to lower the margin of error to a half-centimeter, which Khosla estimates will be good enough to make the system commercially viable. To collect their training examples, the researchers developed a simple application for devices that use Apple’s iOS operating system. The application flashes a small dot somewhere on the device’s screen, attracting the user’s attention, then briefly replaces it with either an “R” or an “L,” instructing the user to tap either the right or left side of the screen. Correctly executing the tap ensures that the user has actually shifted his or her gaze to the intended location. During this process, the device camera continuously captures images of the user’s face. The researchers recruited application users through Amazon’s Mechanical Turk crowdsourcing site and paid them a small fee for each successfully executed tap. The data set contains, on average, 1,600 images for each user. Tightening the net The researchers’ machine-learning system was a neural network, which is a software abstraction but can be thought of as a huge network of very simple information processors arranged into discrete layers. Training modifies the settings of the individual processors so that a data item — in this case, a still image of a mobile-device user — fed to the bottom layer will be processed by the subsequent layers. The output of the top layer will be the solution to a computational problem — in this case, an estimate of the direction of the user’s gaze. Neural networks are large, however, so the MIT and Georgia researchers used a technique called “dark knowledge” to shrink theirs. Dark knowledge involves taking the outputs of a fully trained network, which are generally approximate solutions, and using those as well as the real solutions to train a much smaller network. The technique reduced the size of the researchers’ network by roughly 80 percent, enabling it to run much more efficiently on a smartphone. With the reduced network, the eye tracker can operate at about 15 frames per second, which is fast enough to record even brief glances. “In lots of cases — if you want to do a user study, in computer vision, in marketing, in developing new user interfaces — eye tracking is something people have been very interested in, but it hasn’t really been accessible,” says Noah Snavely, an associate professor of computer science at Cornell University. “You need expensive equipment, or it has to be calibrated very well in order to work. So something that will work on a device everyone has, that seems very compelling. And from what I’ve seen, the accuracy they get seems like it’s in the ballpark that you can do something interesting.” “Part of the excitement is that they’ve also created this way of collecting data, and also the data set itself,” Snavely adds. “They did all the legwork that will make other people interested in this problem. And the fact that the community will start working on this will lead to fast improvements.” Topics: Research, School of Engineering, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
c10211b5f8,Artificial intelligence produces realistic sounds that fool humans  | MIT News,"Login
or
Subscribe Newsletter
Image courtesy of the researchers. Image courtesy of the researchers. Video-trained system from MIT’s Computer Science and Artificial Intelligence Lab could help robots understand how objects interact with the world.
Watch Video
Adam Conner-Simons | CSAIL
June 13, 2016
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab For robots to navigate the world, they need to be able to make reasonable assumptions about their surroundings and what might happen during a sequence of events. One way that humans come to learn these things is through sound. For infants, poking and prodding objects is not just fun; some studies suggest that it’s actually how they develop an intuitive theory of physics. Could it be that we can get machines to learn the same way? Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have demonstrated an algorithm that has effectively learned how to predict sound: When shown a silent video clip of an object being hit, the algorithm can produce a sound for the hit that is realistic enough to fool human viewers. This “Turing Test for sound” represents much more than just a clever computer trick: Researchers envision future versions of similar algorithms being used to automatically produce sound effects for movies and TV shows, as well as to help robots better understand objects’ properties. Researchers at MIT have demonstrated an algorithm that has effectively learned how to predict sound. The advance could lead to better sound effects for film and television and robots with improved understanding of objects in their surroundings. Video: MIT CSAIL “When you run your finger across a wine glass, the sound it makes reflects how much liquid is in it,” says CSAIL PhD student Andrew Owens, who was lead author on an upcoming paper describing the work. “An algorithm that simulates such sounds can reveal key information about objects’ shapes and material types, as well as the force and motion of their interactions with the world.” The team used techniques from the field of “deep learning,” which involves teaching computers to sift through huge amounts of data to find patterns on their own. Deep learning approaches are especially useful because they free computer scientists from having to hand-design algorithms and supervise their progress. The paper’s co-authors include recent PhD graduate Phillip Isola and MIT professors Edward Adelson, Bill Freeman, Josh McDermott, and Antonio Torralba. The paper will be presented later this month at the annual conference on Computer Vision and Pattern Recognition (CVPR) in Las Vegas. How it works The first step to training a sound-producing algorithm is to give it sounds to study. Over several months, the researchers recorded roughly 1,000 videos of an estimated 46,000 sounds that represent various objects being hit, scraped, and prodded with a drumstick. (They used a drumstick because it provided a consistent way to produce a sound.) Next, the team fed those videos to a deep-learning algorithm that deconstructed the sounds and analyzed their pitch, loudness and other features. “To then predict the sound of a new video, the algorithm looks at the sound properties of each frame of that video, and matches them to the most similar sounds in the database,” says Owens. “Once the system has those bits of audio, it stitches them together to create one coherent sound.” The result is that the algorithm can accurately simulate the subtleties of different hits, from the staccato taps of a rock to the longer waveforms of rustling ivy. Pitch is no problem either, as it can synthesize hit-sounds ranging from the low-pitched “thuds” of a  soft couch to the high-pitched “clicks” of a hard wood railing. “Current approaches in AI only focus on one of the five sense modalities, with vision researchers using images, speech researchers using audio, and so on,” says Abhinav Gupta, an assistant professor of robotics at Carnegie Mellon University who was not involved in the study. “This paper is a step in the right direction to mimic learning the way humans do, by integrating sound and sight.” An additional benefit of the work is that the team’s library of 46,000 sounds is free and available for other researchers to use. The name of the dataset: “Greatest Hits.” Fooling humans To test how realistic the fake sounds were, the team conducted an online study in which subjects saw two videos of collisions — one with the actual recorded sound, and one with the algorithm’s — and were asked which one was real. The result: Subjects picked the fake sound over the real one twice as often as a baseline algorithm. They were particularly fooled by materials like leaves and dirt that tend to have less “clean” sounds than, say, wood or metal.   On top of that, the team found that the materials’ sounds revealed key aspects of their physical properties: An algorithm they developed could tell the difference between hard and soft materials 67 percent of the time. The team’s work aligns with recent CSAIL research on audio and video amplification. Freeman has helped develop algorithms that amplify movements captured by video that are invisible to the naked eye, which has allowed his groups to do things like make the human pulse visible and even recover speech using nothing more than video of a potato chip bag. Looking ahead Researchers say that there’s still room to improve the system. For example, if the drumstick moves especially erratically in a video, the algorithm is more likely to miss or hallucinate a false hit. It is also limited by the fact that it applies only to “visually indicated sounds” — sounds that are directly caused by the physical interaction that is being depicted in the video. “From the gentle blowing of the wind to the buzzing of laptops, at any given moment there are so many ambient sounds that aren’t related to what we’re actually looking at,” says Owens. “What would be really exciting is to somehow simulate sound that is less directly associated to the visuals.” The team believe that future work in this area could improve robots’ abilities to interact with their surroundings. “A robot could look at a sidewalk and instinctively know that the cement is hard and the grass is soft, and therefore know what would happen if they stepped on either of them,” says Owens. “Being able to predict sound is an important first step toward being able to predict the consequences of physical interactions with the world.” The work was funded, in part, by the National Science Foundation and Shell. Owens was also supported by a Microsoft Research Fellowship. Topics: Computer Science and Artificial Intelligence Laboratory (CSAIL), Machine learning, Computer vision, Networks, Research, Algorithms, Big data, Artificial intelligence, Computer science and technology, School of Engineering, Brain and cognitive sciences, Electrical Engineering & Computer Science (eecs), School of Science CSAIL researchers recently presented an algorithm that teaches computers to predict sounds, writes Kevin Hartnett for The Boston Globe. The ability to predict sounds will help robots successfully navigate the world and “make sense of what’s in front of them and figure out how to proceed,” writes Hartnett. In an article for Wired, Tim Moynihan writes that a team of CSAIL researchers has created a machine-learning system that can produce sound effects for silent videos. The researchers hope that the system could be used to “help robots identify the materials and physical properties of an object by analyzing the sounds it makes.” Writing for the Financial Times, Clive Cookson reports that MIT researchers have developed an artificial intelligence system capable of producing realistic sounds for silent movies. Cookson explains that another application for the system could be “to help robots understand objects’ physical properties and interact better with their surroundings.""  Washington Post reporter Matt McFarland writes that MIT researchers have created an algorithm that can produce realistic sounds. “The findings are an example of the power of deep learning,” explains McFarland. “With deep learning, a computer system learns to recognize patterns in huge piles of data and applies what it learns in useful ways.” Popular Science reporter Mary Beth Griggs writes that MIT researchers have developed an algorithm that can learn how to predict sound. The algorithm “can watch a silent movie and create sounds that go along with the motions on screen. It's so good, it even fooled people into thinking they were actual, recorded sounds from the environment.” This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
a6ccc07f76,Google and MIT’s new machine learning algorithms retouch your photos before you take them - The Verge,"It’s getting harder and harder to squeeze more performance out of your phone’s camera hardware. That’s why companies like Google are turning to computational photography: using algorithms and machine learning to improve your snaps. The latest research from the search giant, conducted with scientists from MIT, takes this work to a new level, producing algorithms that are capable of retouching your photos like a professional photographer in real time, before you take them.
The researchers used machine learning to create their software, training neural networks on a dataset of 5,000 images created by Adobe and MIT. Each image in this collection has been retouched by five different photographers, and Google and MIT’s algorithms used this data to learn what sort of improvements to make to different photos. This might mean increasing the brightness here, reducing the saturation there, and so on.
Using machine learning to improve photos has been done before, but the real advance with this research is slimming down the algorithms so that they are small and efficient enough to run on a user’s device without any lag. The software itself is no bigger than a single digital image, and, according to a blog post from MIT, could be equipped “to process images in a range of styles.” This means the neural networks could be trained on new sets of images, and could even learn to reproduce an individual photographer’s particular look, in the same way companies like Facebook and Prisma have created artistic filters that mimic famous painters. Of course, it’s worth pointing out that smartphones and cameras already process imaging data in real time, but these new techniques are more subtle and reactive, responding to the needs of individual images, rather than applying general rules.
In order to slim down the algorithms, the researchers used a few different techniques. These included turning the changes made to each photo into formulae and using grid-like coordinates to map out the pictures. All this means that the information about how to retouch the photos can be expressed mathematically, rather than as full-scale photos.
“This technology has the potential to be very useful for real-time image enhancement on mobile platforms,” Google research Jon Barron told MIT. “Using machine learning for computational photography is an exciting prospect but is limited by the severe computational and power constraints of mobile phones. This paper may provide us with a way to sidestep these issues and produce new, compelling, real-time photographic experiences without draining your battery or giving you a laggy viewfinder experience.”
Will we be seeing these algorithms pop up in one of Google’s future Pixel phones? It’s not unlikely. The company has previously used its HDR+ algorithms to bring out more detail in light and shadow on mobile devices since the Nexus 6. And speaking to The Verge last year, Google’s computational photography lead, Marc Levoy, said that we’re “only beginning to scratch the surface” with this work.
Command Line delivers daily updates from the near-future."
40808cfadf,This AI Learns Your Fashion Sense and Invents Your Next Outfit - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Artificial intelligence might just spawn a whole new style trend: call it “predictive fashion.” In a paper published on the ArXiv, researchers from the University of California, San Diego, and Adobe have outlined a way for AI to not only learn a person’s style but create computer-generated images of items that match that style. The system could let retailers create personalized pieces of clothing, or could even be used to help predict broader fashion trends. The paper details two different algorithms. First, the researchers trained a convolutional neural network (CNN) to learn and classify a user’s preferences for certain items, using purchase data scraped from Amazon in six categories: shoes, tops, and pants for both women and men. This type of recommender model is common in the online retail world, usually showing up in an “Other items you might like” area at the bottom of a page. The team then used that information to train a generative adversarial network (GAN), a type of artificial intelligence that is especially proficient when it comes to generating realistic images. A GAN works by having two networks train on the same data. One of the networks generates fake images based on that data set, while the other network uses the same data to determine whether an image is real. This method lets the network improve its results. For this research, the GAN created multiple images of items for each user. GANs, which were created by Ian Goodfellow, one of MIT Technology Review’s 35 Innovators Under 35 for 2017, have been in the news recently: after a different research team trained them on real images of Hollywood stars, the networks were able to create eerily believable fake celebrity faces. The faces weren’t all perfect, though—some had blurred areas or were missing features like eyebrows. There were fewer such problems with the fashion project, largely because the images used to train the networks were all shot from the same angle on white backgrounds, which makes generating convincing images much easier—something that would be essential if they were ever to be used to sell clothing. Adding GANs to recommender systems could help online retailers figure out what customers want beyond the items that already exist. Still, researchers would need to figure out quite a few things before that could happen, including how to turn the two-dimensional computer-generated images into 3-D renderings that could be used to produce a piece of clothing. “It’s not like we are generating a sewing pattern,” says Julian McAuley, a computer scientist at UC San Diego and one of the paper’s authors. The team’s GAN also has a way to go before it can replace a stylist or even suggest a new outfit. At the moment, for a shopper who liked blue shirts, the GAN creates more blue shirts—hardly a revelation. Preference for black pants did feed into the GAN to create khakis, but the system can’t create a set of shoes that would go well with a certain pair of pants yet. “You’d have to read the tea leaves a little bit if you want to call that style or not,” McAuley says. Despite the current limitations, fashion seems ripe for an AI invasion; it’s an arena that has great data sets on customers’ interests, and there is a lot of money at stake. Amazon, for one, is already working on AI systems to provide a leg up in spotting fashion trends, and it has also done some work with GANs (see “Amazon Has Developed an AI Fashion Designer”). Alibaba, meanwhile, just debuted FashionAI, a technology that can recommend items to shoppers on the basis of what they brought into the dressing room. Costa Colbert, the chief scientist at Vue.ai, a fashion AI startup that recently revealed a method for creating fake fashion models using GANs, says that as promising as the UCSD and Adobe research appears to be, it requires so much data that it might be helpful only for the biggest names in online retail. “If all the person does is come in and click one thing, you aren’t going to be able to do much,” Colbert says. GANs will continue to make waves in online fashion, however. Colbert points out that some companies already let shoppers send in their personal measurements to get customized pieces. GANs could be a cheap, fast way to show users all the different options available—and, of course, sell more items. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on artificial intelligence.
Amazon,
artificial intelligence,
AI,
generative adversarial network,
GAN,
Ian Goodfellow,
fashion,
clothing,
predictive algorithms,
predictive fashion
Jackie Snow Associate Editor I am MIT Technology Review’s associate editor for artificial intelligence. I cover stories about where AI is currently, where it’s headed, and what’s wrong with the hype around the technology. My stories don’t have the word “Terminator”… More anywhere near them. Previously I worked for Fast Company and have been published by the New York Times, National Geographic, Wall Street Journal, and others.
Please read our commenting guidelines.
More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
{! insider.display.menuOptionsLabel !}
Unlimited online access including articles and video, plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
d6b282634d,How Tesla's autopilot learns | Fortune,"While Tesla’s new hands-free driving is drawing a lot of interest this week, it’s the technology behind-the-scenes of the company’s newly-enabled autopilot service that should be getting more attention. At an event on Wednesday Tesla’s CEO Elon Musk explained that the company’s new autopilot service is constantly learning and improving thanks to machine learning algorithms, the car’s wireless connection, and detailed mapping and sensor data that Tesla collects. Tesla’s cars in general have long been using data, and over-the-air software updates, to improve the way they operate. Machine learning algorithms are the latest in computer science where computers can take a large data set, analyze it and use it to make increasingly accurate predictions. In short, they are learning. Companies like Google (GOOG), Facebook (FB) and now Tesla (TSLA) are using machine learning as a way to train software to help customers or sell them new services. Machine learning is the way that computers can become artificially intelligent, and the technology is a form of AI. While Musk has taken a sort of alarmist stance against the dangers of AI, he clarified during the event on Wednesday that he’s only concerned with artificial intelligence that is meant for nefarious purposes. When a reporter asked Musk during the media Q&A what made his company’s autopilot service different than other computer-based driving assistance features that competing big auto makers are working on, Musk emphasized learning. “The whole Tesla fleet operates as a network. When one car learns something, they all learn it. That is beyond what other car companies are doing,” said Musk. When it comes to the autopilot software, Musk explained that each driver using the autopilot system essentially becomes an “expert trainer for how the autopilot should work.” While most car companies might not be building learning systems, Google’s self-driving cars operate in a similar manner. In that way, Tesla’s cars are more similar to smart connected gadgets like Nest’s learning thermostat (now owned by Google’s Alphabet), than they are to traditional cars. Nest’s thermostat, using sensors and algorithms, learns its owner’s behavior over time, and through software updates offers increasingly useful services, or even informs Nest’s decisions about its next-generation of hardware. So, how does Tesla’s autopilot system, and its cars in general, learn? It all starts with data. Companies building these types of driver-assistance services, as well as full-blown self-driving cars like Google’s, need to teach a computer how to take over key parts (or all) of driving using digital sensor systems instead of a human’s senses. To do that companies generally start out by training algorithms using a large amount of data. You can think of it how a child learns through constant experiences and replication, explained Nvidia’s Senior Director of Automotive, Danny Shapiro in an interview with Fortune. Nvidia (NVDA) sells high performance chips that enable computers to process large amounts of data, and more recently started selling a computing system, called Drive PX, for self-driving cars and driver-assist applications. To create a self-driving car, companies feed hundreds of thousands, or even millions, of miles of driving videos and data into a computer’s data model to basically create a massive vocabulary around driving. The algorithms use visual techniques to break down the videos and to understand them. The goal is that when something unexpected happens — a ball rolls into the street — the car can recognize the pattern and react accordingly (slow down because a child could be running into the street after it). For Nvidia, the company loads this “driving dictionary,” as Shapiro calls it, onto powerful but compact computing hardware that can be used on the car. After that, companies like Google and Tesla add various types of data from different sources to continue to inform the model over time. Companies try to gather as much data as possible to help a car’s computer make smarter and better decisions on the roads. This includes data from customers driving, data from GPS and maps, and data from company employees driving research cars. The data from Tesla drivers was enabled by the hardware choices that Tesla has made. All Tesla cars built in the past year have 12 sensors on the bottom of the vehicle, a front-facing camera next to the rear-view mirror, and a radar system under the nose. These sensing systems are constantly collecting data to help the autopilot work on the road today, but also to amass data that can make Tesla’s operate better in the future. Because all of Tesla’s cars have an always-on wireless connection, data from driving and using autopilot is collected, sent to the cloud, and analyzed with software. For autopilot, Tesla takes the data from cars using the new automated steering or lane change system, and uses it to train its algorithms. Tesla then takes these algorithms, tests them out and incorporates them into upcoming software. Companies will rely on different types of data depending on what they’re trying to do with the cars. For example, Google has used large and expensive LIDAR (light-based radar) sensors on its self-driving cars. But Tesla’s Musk said that LIDAR was basically overkill for what Tesla’s autopilot cars need. But Musk said that Tesla wanted much more detailed high-precision mapping data for its automated steering and lane change applications than was available through the standard navigation tech. To meet its needs, Tesla has started to build high-precision maps —that have 100 times the level of granularity compared to standard navigation systems — using mostly data from Tesla cars driving on roads, but also some data from Tesla employees driving research cars. These new services could provide unexpected business models for companies. Musk said that Tesla might be interested in selling the mapping data to other car companies down the road. Tesla isn’t the only car maker working on driver-assist and self-driving car tech. Google is blazing ahead on its futuristic tech, while Audi has traffic jam assist software. Nvidia’s Shapiro says that most automakers are investigating these technologies. Nvidia started shipping Drive PX this summer, and Shapiro says that it’s engaged with over 50 companies and researchers. Tesla uses Nvidia chips in the 17-inch screen and the instrument cluster for its Model S and there has been speculation around whether Tesla might use the Drive PX system in future versions of the Model X SUV. Shapiro wouldn’t discuss the specifics of its relationships with Tesla or Audi, which uses Nvidia’s tech in its traffic jam system. Shapiro cautioned that despite some companies already deploying these technologies, it’s still early days for self-driving car tech. “A huge amount of work will be done on this over the next decade,” he said. To learn more about Tesla’s autopilot tech watch this Fortune video:"
dbf06520da,10 Companies Using Machine Learning in Cool Ways | WordStream,"Search If science-fiction movies have taught us anything, it’s that the future is a bleak and terrifying dystopia ruled by murderous sentient robots. Fortunately, only one of these things is true – but that could soon change, as the doomsayers are so fond of telling us.   Image via Abdul Rahid Artificial intelligence and machine learning are among the most significant technological developments in recent history. Few fields promise to “disrupt” (to borrow a favored term) life as we know it quite like machine learning, but many of the applications of machine learning technology go unseen. Want to see some real examples of machine learning in action? Here are 10 companies that are using the power of machine learning in new and exciting ways (plus a glimpse into the future of machine learning). Few things compare to trying out a new restaurant then going online to complain about it afterwards. This is among the many reasons why Yelp is so popular (and useful). While Yelp might not seem to be a tech company at first glance, Yelp is leveraging machine learning to improve users’ experience.
Classifying images into simple exterior/interior categories is easy for humans, but surprisingly difficult for computers Since images are almost as vital to Yelp as user reviews themselves, it should come as little surprise that Yelp is always trying to improve how it handles image processing. This is why Yelp turned to machine learning a couple of years ago when it first implemented its picture classification technology. Yelp’s machine learning algorithms help the company’s human staff to compile, categorize, and label images more efficiently – no small feat when you’re dealing with tens of millions of photos. Whether you’re a hardcore pinner or have never used the site before, Pinterest occupies a curious place in the social media ecosystem. Since Pinterest’s primary function is to curate existing content, it makes sense that investing in technologies that can make this process more effective would be a priority – and that’s definitely the case at Pinterest.
In 2015, Pinterest acquired Kosei, a machine learning company that specialized in the commercial applications of machine learning tech (specifically, content discovery and recommendation algorithms). Today, machine learning touches virtually every aspect of Pinterest’s business operations, from spam moderation and content discovery to advertising monetization and reducing churn of email newsletter subscribers. Pretty cool. Although Facebook’s Messenger service is still a little…contentious (people have very strong feelings about messaging apps, it seems), it’s one of the most exciting aspects of the world’s largest social media platform. That’s because Messenger has become something of an experimental testing laboratory for chatbots.
Some chatbots are virtually indistinguishable from humans when conversing via text Any developer can create and submit a chatbot for inclusion in Facebook Messenger. This means that companies with a strong emphasis on customer service and retention can leverage chatbots, even if they’re a tiny startup with limited engineering resources. Of course, that’s not the only application of machine learning that Facebook is interested in. AI applications are being used at Facebook to filter out spam and poor-quality content, and the company is also researching computer vision algorithms that can “read” images to visually impaired people. Twitter has been at the center of numerous controversies of late (not least of which were the much-derided decisions to round out everyone’s avatars and changes to the way people are tagged in @ replies), but one of the more contentious changes we’ve seen on Twitter was the move toward an algorithmic feed.
Rob Lowe was particularly upset by the introduction of algorithmically curated Twitter timelines Whether you prefer to have Twitter show you “the best tweets first” (whatever that means) or as a reasonably chronological timeline, these changes are being driven by Twitter’s machine learning technology. Twitter’s AI evaluates each tweet in real time and “scores” them according to various metrics. Ultimately, Twitter’s algorithms then display tweets that are likely to drive the most engagement. This is determined on an individual basis; Twitter’s machine learning tech makes those decisions based on your individual preferences, resulting in the algorithmically curated feeds, which kinda suck if we’re being completely honest. (Does anybody actually prefer the algorithmic feed? Tell me why in the comments, you lovely weirdos.) These days, it’s probably easier to list areas of scientific R&D that Google – or, rather, parent company Alphabet – isn’t working on, rather than trying to summarize Google’s technological ambition. Needless to say, Google has been very busy in recent years, having diversified into such fields as anti-aging technology, medical devices, and – perhaps most exciting for tech nerds – neural networks.   A selection of images created by Google’s neural network The most visible developments in Google’s neural network research has been the DeepMind network, the “machine that dreams.” It’s the same network that produced those psychedelic images everybody was talking about a while back. According to Google, the company is researching “virtually all aspects of machine learning,” which will lead to exciting developments in what Google calls “classical algorithms” as well as other applications including natural language processing, speech translation, and search ranking and prediction systems.  For years, retailers have struggled to overcome the mighty disconnect between shopping in stores and shopping online. For all the talk of how online retail will be the death-knell of traditional shopping, many ecommerce sites still suck. Edgecase, formerly known as Compare Metrics, hopes to change that.   Edgecase hopes its machine learning technology can help ecommerce retailers improve the experience for users. In addition to streamlining the ecommerce experience in order to improve conversion rates, Edgecase plans to leverage its tech to provide a better experience for shoppers who may only have a vague idea of what they’re looking for, by analyzing certain behaviors and actions that signify commercial intent – an attempt to make casual browsing online more rewarding and closer to the traditional retail experience. Google isn’t the only search giant that’s branching out into machine learning. Chinese search engine Baidu is also investing heavily in the applications of AI.   A simplified five-step diagram illustrating the key stages of a natural language processing system One of the most interesting (and disconcerting) developments at Baidu’s R&D lab is what the company calls Deep Voice, a deep neural network that can generate entirely synthetic human voices that are very difficult to distinguish from genuine human speech. The network can “learn” the unique subtleties in the cadence, accent, pronunciation and pitch to create eerily accurate recreations of speakers’ voices. Far from an idle experiment, Deep Voice 2 – the latest iteration of the Deep Voice technology – promises to have a lasting impact on natural language processing, the underlying technology behind voice search and voice pattern recognition systems. This could have major implications for voice search applications, as well as dozens of other potential uses, such as real-time translation and biometric security. Anyone who is familiar with HubSpot probably already knows that the company has long been an early adopter of emerging technologies, and the company proved this again earlier this month when it announced the acquisition of machine learning firm Kemvi.   Predictive lead scoring is just one of the many potential applications of AI and machine learning HubSpot plans to use Kemvi’s technology in a range of applications – most notably, integrating Kemvi’s DeepGraph machine learning and natural language processing tech in its internal content management system. This, according to HubSpot’s Chief Strategy Officer Bradford Coffey, will allow HubSpot to better identify “trigger events” – changes to a company’s structure, management, or anything else that affects day-to-day operations – to allow HubSpot to more effectively pitch prospective clients and serve existing customers. The inclusion of IBM might seem a little strange, given that IBM is one of the largest and oldest of the legacy technology companies, but IBM has managed to transition from older business models to newer revenue streams remarkably well. None of IBM’s products demonstrate this better than its renowned AI, Watson.   An example of how IBM’s Watson can be used to test and validate self-learning behavioral models Watson may be a Jeopardy! champion, but it boasts a considerably more impressive track record than besting human contestants in televised game shows. Watson has been deployed in several hospitals and medical centers in recent years, where it demonstrated its aptitude for making highly accurate recommendations in the treatment of certain types of cancers. Watson also shows significant potential in the retail sector, where it could be used as an assistant to help shoppers, as well as the hospitality industry. As such, IBM is now offering its Watson machine learning technology on a license basis – one of the first examples of an AI application being packaged in such a manner. Salesforce is a titan of the tech world, with strong market share in the customer relationship management (CRM) space and the resources to match. Lead prediction and scoring are among the greatest challenges for even the savviest digital marketer, which is why Salesforce is betting big on its proprietary Einstein machine learning technology.   Salesforce Einstein allows businesses that use Salesforce’s CRM software to analyze every aspect of a customer’s relationship – from initial contact to ongoing engagement touch points – to build much more detailed profiles of customers and identify crucial moments in the sales process. This means much more comprehensive lead scoring, more effective customer service (and happier customers), and more opportunities. One of the main problems with rapid technological advancement is that, for whatever reason, we end up taking these leaps for granted. Some of the applications of machine learning listed above would have been almost unthinkable as recently as a decade ago, and yet the pace at which scientists and researchers are advancing is nothing short of amazing. So, what’s next in machine learning trends? Before long, we’ll see artificial intelligences that can learn much more effectively. This will lead to developments in how algorithms are treated, such as AI deployments that can recognize, alter, and improve upon their own internal architecture with minimal human supervision. The rise of cybercrime and ransomware has forced companies of all sizes to reevaluate how they respond to systemic online attacks. We’ll soon see AI take a much greater role in monitoring, preventing, and responding to cyberattacks like database breaches, DDoS attacks, and other threats. Generative models, such as the ones used by Baidu in our example above, are already incredibly convincing. Soon, we won’t be able to tell the difference at all. Improvements to generative modeling will result in increasingly sophisticated images, voices, and even entire identities generated entirely by algorithms. Even the most sophisticated AI can only learn as effectively as the training it receives; oftentimes, machine learning systems require enormous volumes of data to be trained. In the future, machine learning systems will require less and less data to “learn,” resulting in systems that can learn much faster with significantly smaller data sets. Find out how you're REALLY doing in AdWords!
Watch the video below on our Free AdWords Grader:
Visit the AdWords Grader.
Thanks for the post and image.
It's going to be interesting to watch how AI and ML emerge and continue to mature.
From a professional standpoint, I see vast applications within Marketing, Sales & Operations.
I anticipate too the formidable impact ML can have on the medical field as you mention in your post. Great article!
Thank you for valuable information. Machine learning is fast growing and the remark of the year, and with good reason; Google is using it to block spam, Hospitals are even using it to predict Sepsis. Role of machine leaning in the Telecoms industry has no exception.
FREE AdWords
Performance Grader Find out if you're making mistakes with AdWords. GET GRADED TODAY
FREE AdWords
Performance Grader Find out if you're making mistakes with AdWords.
GET GRADED TODAY Get educated & increase your PPC knowledge. VISIT PPC UNIVERSITY Get educated & increase your PPC knowledge. VISIT PPC UNIVERSITY Sign up to get our top weekly tips & tricks FREE! Find out if you're making mistakes with AdWords.
GET GRADED TODAY WordStream is proud to be
a Premier Google Partner. 101 Huntington Ave, Floor 7 Boston, MA 02199 © WordStream. All Rights Reserved."
372f9fd8c5,Vodacom tests machine learning capabilities | ITWeb,"Vodacom is experimenting with big data analytics and machine learning across various areas of its business, after finding successful use cases for the technology. This is according to Vodacom group CEO Shameel Joosub, who says machine learning is already ""deeply ingrained"" in the telco's 'Just 4 You' platform. ""Out of your behaviours we then adapt and personalise the offers on the platform. When you use your app or the USSD platform, the bundles you see are personalised for you, which makes your propensity to buy much higher. As you buy it keeps teaching itself and there are thousands of different packages and combinations. The machine is then learning, based on you and your segment, and teaching itself,"" Joosub told ITWeb in an interview. ""That can stretch further, with your permission of course. For example, when you go overseas it can pick up that you are travelling and offer you a roaming bundle. ""Or in time, with your permission, it could pick up that you are searching for other deals [on other networks] which can give us a churn prediction model. This would feed back into the sales desk which can see the customer is a risk of changing networks and can act to try retain the customer,"" he says. The platform seems to be working, as Vodacom's total bundles sold in SA increased 64.8% to 1.1 billion in the six months ended 30 September. Of these, 800 million were voice bundles, which resulted in the effective price per minute reduction of 10.9%. In SA, data bundle sales also grew by 55.5% to 347 million and Vodacom says better in-bundle usage resulted in a 24.2% reduction in the effective price per MB. The investment in predictive analytics is part of the telco's strategy to become a leading digital company. It believes the investments will provide better promotions for customers and drive revenue and customer growth across all its markets. Joosub says the 'Just 4 You' offering was Vodacom's first big step in using machine learning and big data but the telco believes there are many other use cases where it can work in ""various different areas"". ""For instance, in the call centres, it gives you the ability to predict with 65% to 70% accuracy who is going to call you next month, based on past behaviour and trends using predictive analytics. ""If a customer is historically calling for a balance inquiry then you can come up with a service where you can push the balance to the customer more frequently and that then stops the call to the call centre and relieves pressure there. So you use the data predictively,"" he explains. He says Vodacom's data scientists are working on profile matching, where you can match the problem to the best agent to resolve the issue. ""Also in time, you will be able to do personality matching, so based on your personality you can get an agent that will better reflect the type of person you like to deal with."" He says voice recognition has great potential to enhance call centre optimisation and Vodacom is developing the technology in-house. ""We have had software from other people but it doesn't work, because when you take all of the vernacular languages in SA, it works for English and then it gets lost. So we have come up with more sophisticated ways to do the voice recognition and we have employed special data scientists who are doing that, which then gives us the ability to use voice recognition much more successfully,"" he says. ""We have created a dedicated big data team in Johannesburg and we are basically providing support to all of the Vodacom and Vodafone operations in terms of writing algorithms and doing deep modelling, etc, for the different countries."" Joosub says the team of about 30 data scientists is still busy with more research and development but some innovations are well defined already. ""There are clear business cases and use cases that we will deliver next year like the call centres, insurance and financial services, so some will launch this financial year and some will go into the next year."" Other use cases include predictive analytics for insurance or financial services. ""For example, this year we will do R3.2 billion in advances, which is basically people borrowing their airtime. Now we are using predictive modelling to look at your ability to pay based on your previous behaviour. So can I give you R20 instead of R10? Do I give you multiple times to pay it off over the next two recharges or three recharges? Do I lend you voice, do I lend you data? So all of this predictive modelling starts to come in."" He says the digitisation of insurance will help with lead generation. The group is also using Web scraping of deals to improve competitiveness. ""If someone already has insurance, the chance of them buying a life policy or funeral policy from us is much less. So it's more targeted, which then gives rise to a better conversion ratio. But also digitalising that entire journey is key so that you don't have to have a call centre and you can conclude the entire journey through the app,"" he adds. The Vodacom app now has 2.4 million users and 23 million customers still use USSD daily. According to Vodacom's interim results released yesterday, it has 40 million customers in SA and just over 71 million total customers across its five operations in Africa. Paula Gilbert is ITWeb telecoms editor."
1600265107,Mapping molecular neighborhoods | MIT News,"Login
or
Subscribe Newsletter
Ernest Fraenkel Photo: David Sella Associate Professor Ernest Fraenkel uses biological network modeling to identify new targets for disease.
Watch Video
Alice McCarthy | MIT Industrial Liaison Program
July 5, 2016
Eric MarkowskyEmail: markowsky@ilp.mit.eduPhone: 617-335-0886MIT Industrial Liaison Program At first glance, it seems counterintuitive. To learn more about how cells go awry causing disease, it seems logical to focus on the minutiae of a cell’s molecular components: the specific genes, proteins, and small molecules that change over time leading to a disease state. Instead, Ernest Fraenkel, MIT associate professor in the Department of Biologic Engineering, first takes a macro view for finding new ways to understand and cure diseases. “When you are faced with making any sense of the 10,000 or 20,000 molecules that are present within a cell and evolve during disease, you need an entirely new approach to figure out what is really important among all the changes you see,” Fraenkel explains. He develops computational and laboratory experimental methods to uncover the molecular pathways that go awry in disease and search for new strategies and intervention targets. MIT Biological Engineering Associate Professor Ernest Fraenkel leverages machine learning technology to better understand human disease and potential treatments. Video: MIT Industrial Liaison Program The Fraenkel Lab draws heavily on technologies from machine learning algorithms and high-throughput experimental approaches to probe thousands of genes and proteins and tens of thousands of small molecules simultaneously. “Now we are zooming out farther afield to get a sense of the entire complex of pathways that are altered in the disease state,” he says. “This is a complete change in biology, which previously focused on single proteins or single genes to understand what that single gene does in some disease states.” Scoping out the molecular neighborhood These computer science-based technologies, which were first developed in the context of marketing and telecommunications applications, allow Fraenkel to gather massive amounts of data and create maps of molecular activity. Researchers then probe hot-spots in those maps more closely to better understand disease processes and targets to develop therapies. “We can find regions of molecules, pathways, and molecular processes that seem to be altered in the disease ... more than just one molecule but a whole set of molecules together,” says Fraenkel. His approach relies on maps of interactions that are pieced together partly by computer-driven machine learning and partly by high-throughput experiments that measure the interactions among all of the molecules, allowing his research to push past what is known from annotated pathways published in scientific literature. Machine learning Fraenkel believes in taking an engineering approach in addressing the complexity of biology since “engineering is about making predictable models based on the complexity of the system.” This is where Fraenkel applies machine-learning techniques, a fairly new field within computer science involving pattern recognition algorithms. “Biology is unusual in that you are dealing with probabilities about what certain molecules are going to do, but we can have the computer make educated guesses about what is likely to happen and then give us probabilistic answers.” He uses the results from those predictions — whether they are right or wrong — to create continually improving models of the biological system he studies. Applied to cell biology, Fraenkel uses machine learning algorithms to find signal among the data. “We use patterns in data as they relate to disease,” he says. “We see lots of changes in the proteins and genes found in a patient’s cell. The magnitude of the change is huge. And buried in there is some signal that is specific to the disease. That’s what we are looking for.” Taking aim at ALS Fraenkel is using this research approach to find the molecular causes of amyotrophic lateral sclerosis (ALS), a neurodegenerative disease commonly known as “Lou Gehrig’s disease.” Working with colleagues across the country, he is engaged in a 1,000-patient study collecting cells and patient history from each patient. The cells are analyzed at the molecular level to find changes in proteins, genes, and the epigenome — that part of the genome that instructs what molecules are turned on or off. The team’s plan includes mapping all of the activity within a patient’s ALS cell and hopefully the proteins, genes, and small molecules that are directly causing the disease changes. “From there we can go back into the lab and see what happens when we apply drugs that target those molecules,” he adds. “Through that iterative process we can hopefully build detailed models of exactly what is wrong in these different groups of patients and hopefully move forward to a cure.” “It seems likely that ALS is not one disease, but there may be multiple causes,” Fraenkel adds. “By gathering all this data we hope to find groups of patients that show varying types of symptoms compared with each other. Having the richness from the data both from the clinic and from the cells allows us to hopefully identify the molecular basis for these different subtypes.” Along with neurodegenerative diseases, Fraenkel’s lab is applying these core technologies to other disease areas including cancer and metabolic syndrome. Partnering with industry Fraenkel has already partnered with several biotechnology and pharmaceutical companies who are pioneering this type of research approach, and he sees opportunities for others. Because his group is interested in the preclinical aspects of disease, this represents an essential capability that is already part of the drug discovery process that stretches back to understand the basic disease biology. “This revolution in technology that brings together both high-throughput experimentation and techniques from computer science will allow the pharma and biotech industries to focus in on the molecules that need to be targeted in a disease in a way that was never possible before and to have a much broader set of candidates to bring forward for therapeutic discovery,” Fraenkel explains. He also hopes his research can give new life to thousands of drugs that do not reach the market even though proven safe. “There is a huge library of bioactive, safe compounds that are on the shelf that could be repurposed,” he explains. “The technologies we have developed can provide new insights into how to repurpose those drugs which could really cut short the time and cost for developing new drugs.” As a group, Fraenkel’s lab is committed to making a positive impact on human health, including moving these technologies into the pharma discovery pipeline. His goal: to leverage the tremendous strengths of the pharma and biotech industries in taking the ideas one can get from preclinical discoveries and moving them forward toward molecules that get shipped to the clinic. Topics: Faculty, Profile, Biological engineering, Biology, Algorithms, Big data, Data, Bioengineering and biotechnology, Medicine, Disease, Health sciences and technology, Genetics, Engineering Systems, Amyotrophic lateral sclerosis (ALS), Industry, School of Engineering This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
65cf81db9d,4 Mind-Blowing Ways Facebook Uses Artificial Intelligence,"Continued from page 1 1. Textual analysis  A large proportion of the data shared on Facebook is still text. Video may involve larger data volumes in terms of megabytes, but in terms of insights, text can still be just as rich. A picture may paint 1,000 words, but if you just want to answer a simple question, you often don’t need 1,000 words. Every bit of data which isn’t essential to answering your question is just noise, and more importantly, a waste of resources to store and analyze. Facebook uses a tool it developed itself called DeepText to extract meaning from words we post by learning to analyze them contextually. Neural networks analyze the relationship between words to understand how their meaning changes depending on other words around them. Because this is semi-unsupervised learning, the algorithms do not necessarily have reference data – for example a dictionary – explaining the meaning of every word. Instead, it learns for itself based on how words are used. This means that it won’t be tripped up by variations in spelling, slang or idiosyncrasies of language use. In fact, Facebook say the technology is “language agnostic” – due to the way it assigns labels to words, it can easily switch between working across different human languages and apply what it has learned from one to another.
At present the tool is used to direct people towards products they may want to purchase based on conversations they are having – this video gives an example of how it decides whether providing a user with a shopping link is appropriate or not, depending on the context. 2. Facial recognition  Facebook uses a DL application called DeepFace to teach it to recognize people in photos. It says that its most advanced image recognition tool is more successful than humans in recognizing whether two different images are of the same person or not – with DeepFace scoring a 97% success rate compared to humans with 96%. It’s fair to say that use of this technology has proven controversial. Privacy campaigners said it went too far as it would allow Facebook – based on a high resolution photograph of a crowd -   to put names to many of the faces which is clearly an obstacle to our freedom to move in public anonymously. EU legislators agreed and persuaded Facebook to remove the functionality from European citizens’ accounts in 2013. Back then the social media giant was using an earlier version of the facial recognition tool which did not use Deep Learning. Facebook has been somewhat quiet about the development of this technology since it first hit headlines, and can be assumed to be waiting on the outcome of pending privacy cases before saying more about their plans to roll it out."
24083db261,Google’s DeepMind achieves machine learning breakthroughs at a terrifying pace,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
It’s time to add “AI research” to the list of things that machines can do better than humans. Google’s Alpha Go, the computer that beat the world’s greatest human go player, just lost to a version of itself that’s never had a single human lesson. Google is making progress in the field of machine learning at a startling rate. The company’s AutoML recently dropped jaws with its ability to self-replicate, and DeepMind is now able to teach itself better than the humans who created it can. DeepMind is the machine behind both versions of Alpha Go, with the latest evolution dubbed Alpha Go Zero — which sounds like the prequel to a manga. The original Alpha Go is a monster of technology with 48 AI processors, and the data from thousands of go matches built into it. From the ground up, it was “born” with a pretty decent understanding of the game. Over time, and under the direction of humans, it began to learn the game and its nuanced strategies. Eventually Alpha Go became so advanced, it was able to defeat the world’s top human player and establish AI’s supremacy in a game so difficult it makes chess look like checkers. In short, Alpha Go is pretty legit. The brilliant minds at Google decided being the best wasn’t good enough; they “evolved” Alpha Go into “Alpha Go Zero.” It was able to defeat Alpha Go at its own game only 40 days later. Credit: Google Let that sink in. Now here’s the shocking part: Alpha Go Zero has only four AI processors and the only data it was given was the rules of the game. Nobody taught it how to play or fed it thousands of matches to study. According to Google’s blog: This technique is more powerful than previous versions of AlphaGo because it is no longer constrained by the limits of human knowledge. Instead, it is able to learn tabula rasa from the strongest player in the world: AlphaGo itself. The AI plays Go against itself, improving with every match. After millions of matches its strategy is, as far as humans are concerned, infallible. Both versions of the machine play the game at a level that’s considered superhuman.
The speed with which Google’s AutoML and DeepMind have taken “self learning” to the next level is wonderful and terrifying at the same time. In order for AI to fullfill its promise to humanity it has to ease our burdens and free our minds to solve uniquely human problems. A version of DeepMind that – in a little over a month – can teach itself to outperform a previous iteration is the realization of that ideal. It’s time we took Sundar Pichai’s assertion that Google is an AI company seriously.
Read next:
Emails demise raises security concerns about messaging
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
94686bf228,How PayPal beats the bad guys with machine learning | InfoWorld,"By Eric Knorr,
Editor in Chief,
InfoWorld
|
Apr 13, 2015
Use commas to separate multiple email addresses Your message has been sent. There was an error emailing this page. When Amazon Web Services announced a new machine learning service for its cloud last week, it was a sort of mini-milestone. Now all four of the top clouds -- Amazon, Microsoft, Google, and IBM -- will offer developers the means to build machine learning into their cloud applications. What sort of applications? As InfoWorld’s Andrew Oliver has observed, both machine learning and big data will eventually disappear as separate technology categories and insinuate themselves into many, many different aspects of computing. Nonetheless, right now certain uses of machine learning stand out for their immediate payback. Fraud detection is first among them, because it addresses an urgent problem that would be impractical to solve if machine learning didn't exist. To get a sense of how machine learning is combating fraud, I interviewed Dr. Hui Wang, senior director of risk sciences for PayPal. Wang holds a Ph.D. in statistics from UC Berkeley, and prior to her 11 years at PayPal conducted credit scoring research at Fair Isaac. You can easily imagine why PayPal would be concerned about fraud, given the innumerable scams that have targeted PayPal users. As it turns out, however, PayPal has already ventured beyond fraud detection to address other areas of risk management, including “modern machine learning in the credit decision world,” which Wang says is a lot more complex -- in part due to regulatory requirements. According to Wang, PayPal is a pioneer in risk management, although some advanced efforts are just now emerging from the lab. PayPal uses three types of machine learning algorithms for risk management: linear, neural network, and deep learning. Experience has shown PayPal that in many cases, the most effective approach is to use all three at once. Running linear algorithms to detect fraud is an established practice, Wang says, where “we can separate the good from the bad with one straight line.” When this either/or categorization fails, however, things get more interesting: Neural net algorithms were developed decades ago, but today’s modern computing infrastructure -- along with the enormous quantity of data we can now throw at those algorithms -- has increased neural net effectiveness by a magnitude. Wang says these advances have been essential for risk management: Quickly determining trustworthy customers and putting them in the express lane to a transaction is a key objective, Wang explains, using caching mechanisms to run relational queries and linear algorithms, among other techniques. The more sophisticated algorithms apply to customers who may be problematic, which slows down the system a bit as it acquires more data to perform in-depth screening. This downstream process extends all the way to deep learning, which today also powers computer vision, speech recognition, and other applications. When I asked Wang for a layman’s explanation of the difference between neural nets and deep learning, she offered this explanation: Wang emphasizes that you need large quantities of data to support these complex neural network structures. PayPal itself collects gargantuan amounts of data about buyers and sellers, including their network information, machine information, and financial data. The deep learning beast is well fed. But again, PayPal does not use deep learning in isolation. It applies all three together: linear, neural network, and deep learning algorithms. Wang explains why: Wang says she is proud to be managing the data science team at PayPal, which is on the forefront of developing machine learning and data mining technology. Because her team is so advanced, particularly in the practical application of deep learning, I couldn’t resist asking her what she thought about widely publicized warnings from Stephen Hawking, Elon Musk, and Bill Gates regarding the potential dangers of artificial intelligence in the future: There’s a practical lesson in that statement: Now and in the future, machine learning depends not only on big data, but on the right data. Cloud infrastructure and integration presents abundant compute power for deep learning -- as well as access to unthinkably large data sets across a potentially unlimited number of domains. What we refer to as big data and machine learning today will tomorrow simply be integrated into the fabric of computing. As the major cloud providers open up those capabilities to all developers, the stage is set for a new wave of applications that will be much more intelligent than before. Eric Knorr is the editor in chief of IDG Enterprise. Eric has received the Neal and Computer Press Awards for journalistic excellence. Copyright © 2018 IDG Communications, Inc."
4eeb43f088,Data for good: Protecting consumers from unfair practices | SAS,"Sign In
Welcome
Edit Profile
Log Out
Worldwide Sites
Worldwide Contacts If you don't find your country in the list, see our worldwide contacts list.
Solutions Products Why SAS Built for analytics innovation.
Get details Quickly prepare data for analytics in a self-service environment. Get product details Industries
Get more details Watch the video Support Knowledge Base Support by Product SAS Services Downloads & Hot Fixes SAS Administrators Manage Your Tracks Product Resources Get the help you need to resolve problems.
Get help Explore documentation for SAS software products.
Read documentation Get Started Learning SAS Training Certification Books For Students and Educators Documentation Focus Areas Resource Center Find a Partner View all featured platinum and gold partners, or search our complete A-Z listing.
About Our Program Read about partner program levels and channel opportunities.
Sign in to PartnerNet Get training, marketing and membership resources for current partners. Platinum Partners Connect Blogs (blogs.sas.com) Communities (communities.sas.com) Get the recognition you deserve. Read about program Meet local SAS users, network and exchange ideas.
View users groups About SAS Careers News Room Customer Stories Office Information World Headquarters
SAS Institute Inc.
100 SAS Campus Drive
Cary, NC 27513-2414, USA
Phone: +1-919-677-8000
Fax: +1-919-677-4444
Events View Now Download now Anne-Lindsay Beall, SAS Insights Editor When Joan Morgan got a delinquent tax notice for her Wake County property taxes, she was surprised. She had an escrow account with her mortgage company, and they were supposed to pay the tax bill from the escrow.
“After many calls to my mortgage company, a lot of time on hold and being transferred from one person to the next, I finally got someone who told me that ‘the funds were disbursed.’ I was relieved. Until I got my second delinquent tax notice and had to start the process all over again.” Unfortunately, the mortgage company continued to put Morgan off. Her final recourse was to submit a complaint with the Consumer Financial Protection Bureau (CFPB), a government agency set up to protect consumers from unfair, deceptive, or abusive practices and take action against companies that break the law. The CFPB reached out to Morgan’s mortgage company on her behalf -- and got the issue resolved.
The CFPB sends thousands of consumers’ complaints about financial products and services to companies for response. Since the agency was established in 2011, they’ve handled more than 1.2 million complaints and are responsible from $11.9 billion in recompense to individuals who’ve submitted complaints.
Download free SAS Global Forum paper to learn more   For an in-depth description of how applying text analytics and machine learning can help assess consumer financial complaints, download this paper from SAS Global Forum.
Download paper   Keeping up – and doing more with all that data But given the exponential increase in complaints year over year, how can the CFPB not only keep up, but help more people with the wealth of data they have? For instance, is there a way for them to quantitatively assess the data for various trends? Is there a better way to discover the areas of greatest concern for consumers and help address those problems on a macro-level, before they become unmanageable? “Adding more readers for a manual analysis of the text is not the answer,” says SAS’ Tom Sabo, who has explored the problem at length. “First, unless very specific standards are adopted, the method that one reader uses to address and tag a complaint can be quite different from the method a second reader uses. Scale this difference up to many readers, and you have many different, qualitative interpretations of the textual data.” Reader fatigue is also a problem, points out Sabo. If you’re responsible for reviewing over 100 complaints a day, the way you assess the first 10 is not going to necessarily receive the same detail as the last ten. “And suppose a trend is uncovered, and readers are told to go back and retag all the data from the past year with this new trend,” says Sabo. “This is a case where manual analysis doesn’t scale, and a simple search operation for a trend pattern won’t be sufficient.”
Applying text analytics and machine learning Sabo had a solution: Apply SAS technology to assess consumer complaints. He applied text analytics to publically available CFPB data to explore sentiment in consumer complaints, and he used machine learning to model the natural language available in each free-form complaint. The benefits are many. Each record has already been tagged with a disposition code, denoting the action taken by the organization against which the complaint was filed. “Applying machine learning against the free-form comments associated to these disposition codes can semi-automatically generate a taxonomy highlighting key issues that the CFPB deals with,” explains Sabo. “Interactive reporting allows the analyst to explore the pre-existing data for complaints enhanced with the sentiment and rules that are generated from text analytics,” says Sabo. That allows the analyst to sub-divide and prioritize avenues for exploration, guided by the relative levels of sentiment toward each of the categories. This drilldown report also includes a time series line chart so that analysts can observe trends over time. “With this information, we can uncover trends surrounding the actions taken. For example, what were the defining characteristics of complaints where the organization in question paid monetary compensation to the individuals filing the complaints vs. complaints that were simply closed with an explanation?” says Sabo. How this analysis could have saved us all a lot of time and money To demonstrate, Sabo pulled a segment of complaints from March 2015 to Oct. 2015 and ended up with 37K complaints for his analysis. Using text analytics, he specified his category as ‘company response to consumer.’ “I wanted to see if there were specific phrases and terms that, when put together, differentiated those complaints that received monetary relief,” says Sabo. “As opposed to a method where I go in and write down the rules after some kind of manual review, this is where machine learning, in a very short amount of time,does its magic and identifies patterns without the input of business rules,” says Sabo. It leverages the subject matter expertise inherent in the way each complaint was encoded via the disposition code. Sabo’s high-level results for complaints that received monetary relief uncovered frequent use of the term “good faith estimate” (GFE). A GFE is a document that breaks down the approximate payments due upon the closing of a mortgage loan. From this, an analyst could infer that that lending organizations might be taking advantage of the complexity of good faith estimates and using them to misrepresent or hide fees. “In cases like this, text analytics can quantitatively depict that there’s a practice by lending institutions that’s likely being abused or misused, and provide the opportunity for an overseeing organization like the CFPB to take action,” says Sabo. When Sabo drilled down into the complaint data over time, he saw that in September 2015, the GFE complaints started to flatten out. “In October of 2015, Congress directed the CFPB to revise the GFE so that costs were more transparent to the consumer, and therefore more difficult for financial organizations to misuse,” says Sabo. “The question remains, if analytics had been part of the assessment, could this ultimate decision to protect the consumer have been reached sooner? Regardless, a quantitative analysis like this one backs up the decision of Congress and the CFPB to do away with the GFE.” Broader applications? While text analytics and machine learning would certainly help an overseeing agency like the CFPB account for growing volumes of data and better protect consumers, there are interesting broader applications. “I’ve applied this methodology to tip line data, to surveys, to medical encounters -- for instance, after a hurricane, there are a number of stand-up clinics (in the absence of functioning hospitals) that provide assistance and record what people are experiencing post natural disaster. There’s a lot that we can do with that data to determine the type and quantity of materials needed at the clinics to ensure medical needs are met for survivors,” says Sabo. “Similar analysis could also improve the ability of epidemiologists to catch and fight infectious disease outbreaks early on, and of public health researchers to identify prescription drug users at-risk of overdose,” says Sabo. “The possibilities are endless.”
Ready to subscribe to Insights now? The SAS® Platform Diversity. Scale. Trust. Achieve excellence in analytics with the SAS Platform. Thank you for subscribing to Insights!
Subscribe to Insights newsletter Getting Started with SAS Customer Support Insights & Trends Quick Links Privacy Statement | Terms of Use | © SAS Institute Inc. All Rights Reserved
Feedback
Back to Top"
bd33002c4a,Deep learning proves effective in spotting liver masses in CT | Radiology Business,"The consternation of radiologists about the impact of artificial intelligence is real—but so are the benefits of machine learning. Recent research showed that deep learning with a convolutional neural network (CNN) was successful in differentiating liver masses in CT. The retrospective study, published online Oct. 23 in Radiology, examined the diagnostic abilities of a deep learning method with a CNN. Researchers tested the CNN with 100 liver mass image sets from 2016, including 74 men and 26 women with the average age of 66 years old. “This preliminary study, which used 55, 536 image sets (1068 image sets augmented by a factor of 52) to obtain models, indicated that classifying liver masses into five categories can be accomplished with a high degree of accuracy by using a deep learning method with a CNN on dynamic contrast-enhanced CT images,” wrote Koichiro Yasaka, MD, PhD, with the department of radiology at the University of Tokyo Hospital in Japan, and colleagues. The study featured two stages. The first was a training stage, where researchers built the deep learning models by using CT image sets. An abdominal radiologist searched image archives for CT studies that examined liver lesions. The test stage then examined the accuracy of the model. “After building the models, we examined the accuracy of the trained models in distinguishing among the five liver mass categories by using test CT image sets,” wrote Koichiro et al. “These image sets were provided for the CNN.” The deep learning method, according to the researchers, was designed to allow radiologists to more narrowly focus on tumor detection. “Our model required the radiologists only to focus on the tumors, capture the images and provide the image to the CNN,” wrote Koichiro et al. “Our model also differs from previous studies that used conventional machine-learning methods for the differentiation of liver masses in that it does not require complex-shaped regions of interest tracing boundaries of tumors, or circular or elliptical regions of interest within tumors.” The CNN model showed its ability to detect liver masses, but the researchers also mentioned that further improvements were necessary before such a method could be used to diagnose rare malignant masses.
News
Magazine
Topics
Partner Voice
Editorial
Team /
Calendar /
Submit Press Release /
Contact
Advertising
Team /
Media Kit /
Contact
TriMed Media Group
Cardiovascular Business /
Clinical Innovation + Technology /
Health Exec /
Health Imaging /
Healthcare Technology /
imagingBiz /
Radiology Business
Subscribe
© TriMed Media Inc. All rights reserved.
Visit us at TriMedMedia.com"
5e33a1937b,An algorithm for your blind spot  | MIT News,"Login
or
Subscribe Newsletter
A new system from MIT's Computer Science and Artificial Intelligence Laboratory works by analyzing light at the edge of walls, which is impacted by the reflections of objects around the corner from the camera. Image courtesy of the researchers. Using smartphone cameras, system for seeing around corners could help with self-driving cars and search-and-rescue.
Watch Video
Adam Conner-Simons | CSAIL
October 9, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Light lets us see the things that surround us, but what if we could also use it to see things hidden around corners? It sounds like science fiction, but that’s the idea behind a new algorithm out of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) — and its discovery has implications for everything from emergency response to self-driving cars. The CSAIL team’s imaging system, which can work with smartphone cameras, uses information about light reflections to detect objects or people in a hidden scene and measure their speed and trajectory — all in real-time. To explain, imagine that you’re walking down an L-shaped hallway and have a wall between you and some objects around the corner. Those objects reflect a small amount of light on the ground in your line of sight, creating a fuzzy shadow that is referred to as the “penumbra.” Using video of the penumbra, the system — which the team has dubbed “CornerCameras” — can stitch together a series of one-dimensional images that reveal information about the objects around the corner. “Even though those objects aren’t actually visible to the camera, we can look at how their movements affect the penumbra to determine where they are and where they’re going,” says Katherine Bouman SM '13, PhD '17, who is lead author on a new paper about the system. “In this way, we show that walls and other obstructions with edges can be exploited as naturally-occurring ‘cameras’ that reveal the hidden scenes beyond them.” Bouman says that the ability to see around obstructions would be useful for many tasks, from firefighters finding people in burning buildings to drivers detecting pedestrians in their blind spots. She co-wrote the paper with MIT professors Bill Freeman, Antonio Torralba, Greg Wornell, and Fredo Durand; master’s student Vickie Ye; and PhD student Adam Yedidia. She will present the work later this month at the International Conference on Computer Vision in Venice, Italy. How it works Most approaches for seeing around obstacles involve special lasers. Specifically, researchers shine cameras on specific points that are visible to both the observable and hidden scene, and then measure how long it takes for the light to return. However, these so-called “time-of-flight cameras” are expensive and can easily get thrown off by ambient light, especially outdoors. In contrast, the CSAIL team’s technique doesn’t require actively projecting light into the space, and works in a wider range of indoor and outdoor environments and with off-the-shelf consumer cameras. From viewing video of the penumbra, CornerCameras generates one-dimensional images of the hidden scene. A single image isn’t particularly useful, since it contains a fair amount of “noisy” data. But by observing the scene over several seconds and stitching together dozens of distinct images, the system can distinguish distinct objects in motion and determine their speed and trajectory. “The notion to even try to achieve this is innovative in and of itself, but getting it to work in practice shows both creativity and adeptness,” says Professor Marc Christensen, who serves as dean of the Lyle School of Engineering at Southern Methodist University and was not involved in the research. “This work is a significant step in the broader attempt to develop revolutionary imaging capabilities that are not limited to line-of-sight observation.” The team was surprised to find that CornerCameras worked in a range of challenging situations, including weather conditions like rain. “Given that the rain was literally changing the color of the ground, I figured that there was no way we’d be able to see subtle differences in light on the order of a tenth of a percent,” says Bouman. “But because the system integrates so much information across dozens of images, the effect of the raindrops averages out, and so you can see the movement of the objects even in the middle of all that activity.” The system still has some limitations. For obvious reasons, it doesn’t work if there’s no light in the scene, and can have issues if there’s low light in the hidden scene itself. It also can get tripped up if light conditions change, like if the scene is outdoors and clouds are constantly moving across the sun. With smartphone-quality cameras the signal also gets weaker as you get farther away from the corner. The researchers plan to address some of these challenges in future papers, and will also try to get it to work while in motion. The team will soon be testing it on a wheelchair, with the goal of eventually adapting it for cars and other vehicles. “If a little kid darts into the street, a driver might not be able to react in time,” says Bouman. “While we’re not there yet, a technology like this could one day be used to give drivers a few seconds of warning time and help in a lot of life-or-death situations."" This work was supported, in part, by the DARPA REVEAL Program, the National Science Foundation, Shell Research, and a National Defense Science and Engineering Graduate Fellowship. Topics: Research, Algorithms, Machine learning, Computer vision, Computer Science and Artificial Intelligence Laboratory (CSAIL), Networks, Data, School of Engineering, Research Laboratory of Electronics, Imaging, Artificial intelligence, National Science Foundation (NSF), Defense Advanced Research Projects Agency (DARPA) Using video to processes shadows, MIT researchers have developed an algorithm that can see around corners, writes Alyssa Meyers for The Boston Globe. “When you first think about this, you might think it’s crazy or impossible, but we’ve shown that it’s not if you can understand the physics of how light propagates,” says lead author and MIT graduate Katie Bouman. CSAIL researchers have developed a system that detects objects and people hidden around blind corners, writes Anthony Cuthbertson for Newsweek. “We show that walls and other obstructions with edges can be exploited as naturally occurring ‘cameras’ that reveal the hidden scenes beyond them,” says lead author and MIT graduate Katherine Bouman. MIT researchers have developed a new system that can spot moving objects hidden from view by corners, reports Douglas Heaven for New Scientist. “A lot of our work involves finding hidden signals you wouldn’t think would be there,” explains lead author and MIT graduate Katie Bouman.  Wired reporter Matt Simon writes that MIT researchers have developed a new system that analyzes the light at the edges of walls to see around corners. Simon notes that the technology could be used to improve self-driving cars, autonomous wheelchairs, health care robots and more.   This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
e7e7ae9f15,Cyber Security Powered by AI and Machine Learning | IBM Big Data & Analytics Hub,"Jump to navigation
The latest executive report published by IBM Institute for Business Value puts the estimated cost of cyber crime to the global economy in a range of USD 375–575 billion per year. Reputational damage, which is hard to calculate, comes on top of all this. No industry and geography has remained untouched with recent spurt of cyber attacks.  Most companies have cyber security systems in place with some Fortune 500 and government agencies having highly advanced sophisticated systems in place. The difficulty is, not many have constructed living, dynamic and vibrant cyber security strategy that can match the advances of hackers and cyber criminal activities. Staying ahead is the most critical component of corporate cyber security.  With ever-evolving cyber threat landscape, it's no longer sufficient to leverage only traditional security solutions like SIEM, network and endpoint security tools, intrusion prevention systems etc. The current security tools are near perfect in identifying and preventing known attack vectors. These legacy solutions are built on known and identified rule sets with quantified responses and actions. Signature driven security monitoring capabilities cannot scale to fully meet the demand of advanced cyber security objectives.  These solutions don't offer protection from new unknown emerging threat vectors, zero-day attacks, low and slow attacks, and, to top it all, compromised credential attacks. These attacks will eventually show a pattern but are not yet programmed to be detected.
A more flexible mechanism is needed to explore data sets in a holistic manner and uncover otherwise unknown threats. Modern big data analytics — powered by machine learning, data science and AI capabilities — is emerging as powerful solution. Building machine learning, powered with adaptive baseline behavior models, will be super effective in detecting new unknown attacks. Coupling past and current analytics (the knowns) with predictive analytics and machine intelligence — for security and intelligence — will boost the cyber security landscape tremendously.  The first step to leverage power of machine learning and data science is creating a “Cyber Security Data Lake” which will augment existing security analytics and anomaly detection solutions. Incorporate this with additional data sets that are valuable for security intelligence, yet difficult to address with SIEM and other traditional security tools.  Building machine learning and AI models requires lot of data which currently no system would be capturing in organization. The cyber security data lake will facilitate tasks ranging from profile persistence, log ingestion and IoT data captures. The next advance layer enabled by Apache Spark will support building machine learning models, develop algorithm for Forensics and Pattern Detection, provide discovery analytics, and automate alerting.  With these advance cyber security data lakes, enterprises can move away from a break/fix reactive mode to proactive models built from larger data sets for countering unknown attacks.  The majority of security data falls into the category of time-series data or log data. Common examples come from firewalls, intrusion detection systems, antivirus software, operating systems, proxies, and web servers. The other important data type to ingest in these data lakes is contextual security data. Contextual data can be data from vulnerability scans, asset databases, configuration management systems, directories, or special-purpose applications. Contextual data in the form of threat intelligence is becoming more common. Contextual data is handled separately from log records because it requires a different storage model. Mostly the data is stored in a key value store like Hbase in Hadoop ecosystem to allow for quick lookups.  In addition to the problem of scalability, openness is an issue of traditional tools like SIEMs. They were not built to let other products or sophisticated machine learning models reuse the data they collect. Enterprises can leverage an open source ecosystem to break down traditional, expensive cyber security analytics stack and data constraints in order to detect a new breed of sophisticated attacks. Apache Spark provides strong framework that can perform batch processing to build a machine learning model from scratch or leveraging existing models from Github. It then uses Spark streaming functionality to apply the intelligence in real-time.   Another important aspect of Apache Spark is powerful libraries like GraphX.  GraphX supports constructing a dynamic relationship graph of entities that allows for the building of baseline patterns of normalcy, flags anomalies dynamically on the fly, does in-depth analyses of the context of an event, and eventually identifies and protects against new unknown cyber threats.  IBM Data Science experience powered by Apache Spark enables detection of patterns and outliers to detect and eliminate emerging cyber threats. It supports building accurate and faster fraud models on the leading open source Hadoop platform, Hortonworks. Enterprises security teams can efficiently build and deploy machine learning models to unstructured and structured data to focus on the discovery of unknown attack vectors. Coupled with seamless access to data by using the industry leading SQL engine, IBM BigSQL, it enables security analysts to deliver insights and data points needed to build the signatures of abnormal behavior beyond traditional security tools.
Follow @IBMAnalytics View the discussion thread."
f4a2e3df0c,Apple details how it performs on-device facial detection in latest machine learning journal entry | 9to5Mac,"November 16, 2017
Zac Hall
- Nov. 16th 2017 8:50 am PT
@apollozac Apple has published its latest machine learning journal entry with a new article detailing the challenges of implementing facial detection features while maintaining a high level of privacy.
Apple started using deep learning for face detection in iOS 10. With the release of the Vision framework, developers can now use this technology and many other computer vision algorithms in their apps. We faced significant challenges in developing the framework so that we could preserve user privacy and run efficiently on-device. The new journal entry explains how the facial detection algorithm works locally on your iPhone without relying on information leaving your device: Apple’s iCloud Photo Library is a cloud-based solution for photo and video storage. However, due to Apple’s strong commitment to user privacy, we couldn’t use iCloud servers for computer vision computations. Every photo and video sent to iCloud Photo Library is encrypted on the device before it is sent to cloud storage, and can only be decrypted by devices that are registered with the iCloud account. Therefore, to bring deep learning based computer vision solutions to our customers, we had to address directly the challenges of getting deep learning algorithms running on iPhone. That requires the right hardware resources to pull off while maintaining efficiency: We faced several challenges. The deep-learning models need to be shipped as part of the operating system, taking up valuable NAND storage space. They also need to be loaded into RAM and require significant computational time on the GPU and/or CPU. Unlike cloud-based services, whose resources can be dedicated solely to a vision problem, on-device computation must take place while sharing these system resources with other running applications. Finally, the computation must be efficient enough to process a large Photos library in a reasonably short amount of time, but without significant power usage or thermal increase. Apple launched its machine learning journal over the summer and has published multiple entries including one on how “Hey Siri” works on iPhone and Apple Watch. You can read the complete latest entry here. Subscribe to 9to5Mac on YouTube for more Apple news:
@apollozac
Zac covers Apple news and product reviews for 9to5Mac and hosts the weekly 9to5Mac Happy Hour podcast. Apple selling mesh Wi-Fi system, AirPorts unchanged Video: First Alert Safe & Sound AirPlay 2 smoke alarm Siri learns tennis and golf tournament results, stats Video: DJI's Osmo Mobile 2 iPhone stabilizer"
0add4af287,Artificial intelligence could help farmers diagnose crop diseases | Penn State University,"Kelsey Pryze, undergraduate researcher, captures photographs of potato leaves at Penn State's Russell E. Larson Agricultural Research Center at Rock Springs. UNIVERSITY PARK, Pa. -- A network of computers fed a large image dataset can learn to recognize specific plant diseases with a high degree of accuracy, potentially paving the way for field-based crop-disease identification using smartphones, according to a team of researchers at Penn State and the Swiss Federal Institute of Technology (EPFL), in Lausanne, Switzerland. The technology could have particular benefits for producers in developing countries, such as in sub-Saharan Africa, who often do not have the research infrastructure or agricultural extension systems to support smallholder farmers, the researchers said. ""Global food security is threatened by a number of factors, not the least of which is plant diseases that can reduce yields or even wipe out a crop,"" said study co-author David Hughes, assistant professor of entomology and biology, College of Agricultural Sciences and Eberly College of Science, Penn State. In addition, Hughes said, plant diseases can have disastrous consequences for smallholder farmers whose livelihoods depend on healthy crops. In the developing world, more than 80 percent of agricultural production is generated by smallholder farmers, and as many as half of hungry people live in smallholder farming households. ""Identifying a disease correctly when it first appears is a crucial step for effective disease management,"" he said. ""With the proliferation of smart phones and recent advances in computer vision and machine learning, disease diagnosis based on automated image recognition, if technically feasible, could be made available on an unprecedented scale."" President Barron explores how Penn State researches are using crowdsourcing and mobile technology to protect the world’s food supply. To begin to test this hypothesis, the researchers built a neural network, which is a large cluster of computers with graphical processing units. Using a deep-learning approach -- an emerging area of machine learning that uses algorithms to model high-level abstractions in data across multiple processing layers -- they fed more than 53,000 images of diseased and healthy plants into the network and trained it to recognize patterns in the data. The research builds upon tremendous improvements in the past few years in computer vision, and object recognition in particular, said co-author Marcel Salathé, associate professor and head of the Laboratory of Digital Epidemiology, EPFL. ""Neural networks provide a mapping between an input, such as an image of a diseased plant, to an output, such as a crop-disease pair,"" he explained. ""Deep neural networks recently have been applied successfully in many diverse domains. These networks are trained by tuning the network parameters in such a way that the mapping improves during the training process."" As an example of how this emerging technology works, Hughes cited the way Facebook can identify a user by analyzing an uploaded photo. Pete McCloskey,  member of the PlantVillage team, uses  a GoPro camera in a Tanzanian cassava field to shoot video from which still images are pulled. ""These algorithms can classify complex phenotypes, such as recognizing a face,"" he said. ""Our goal is to use them to identify plant diseases."" The images used in the study were part of a public-access archive of photographs contained in PlantVillage, a free, online plant-disease library and database Hughes and Salathé developed in 2012. The data set depicted 14 crop species -- both healthy and with disease symptoms -- and 26 diseases. Each image was assigned to one of 38 classes, each representing a crop-disease pair, and the researchers measured the performance of their model in placing images into the correct class. ""Our goal was to classify crop species and the presence and type of disease on images that the model had not seen before,"" said lead author Sharada Mohanty, doctoral researcher in biotechnology and bioengineering, EPFL. ""Within the PlantVillage data set, the model achieved an accuracy rate as high as 99.35 percent, meaning it correctly classified crop and disease from 38 possible classes in 993 out of 1,000 images."" Mohanty noted that building the algorithms and training the model require significant computing power and time, but once the algorithms are built, the classification task itself is very fast, and the resulting code is small enough to easily be installed on a smartphone. ""This presents a clear path towards smartphone-assisted crop-disease diagnosis on a massive global scale,"" he said. Hughes noted that in addition to assisting growers in developing countries, the technology has great potential in a developed-world setting. ""This could be a tool for land-grant extension personnel at public institutions as they assist their grower clients, as well as for the legions of backyard gardeners who want to identify what is harming their produce,"" he said. The researchers pointed out online in Frontiers in Plant Science that this approach is not intended to replace existing solutions for disease diagnosis, but rather to supplement them. ""Laboratory tests ultimately always are more reliable than diagnoses based on visual symptoms alone, and early-stage diagnosis only by visual inspection often is challenging,"" Hughes explained. ""Nevertheless, given the expectation that more than 5 billion smartphones will be in use around the world by 2020 -- almost a billion of them in Africa -- we do believe that the approach represents a viable additional method to help prevent yield loss. With the ever improving number and quality of sensors on mobile devices, we consider it likely that highly accurate diagnoses via the smartphone are only a question of time."" Penn State and EPFL supported this research. A background to the motivation for creating PlantVillage. PlantVillage is a creation of David Hughes (Penn State) and Marcel Salathe (EPFL, Switzerland) 312 Old Main, University Park, Pennsylvania 16802 814-865-7517 The Pennsylvania State University © 2015"
5a4edd27c6,Alibaba's FashionAI shows how machine learning might save the mall,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Alibaba’s sales from Saturday’s Singles Day event exceeded 25 billion dollars, more than quadruple what Americans spent last year during Black Friday. While the majority of those sales undoubtedly came via online purchases, the company also quietly experimented with an AI-powered project designed to woo offline shoppers. FashionAI was developed by Alibaba researchers in order to provide a recognizable interface for customers to use while trying on clothes. It’s a basic screen interface that uses machine-learning to make clothing and accessory suggestions to customers based on the items they are trying on. There’s no camera; it uses information embedded in the item’s tag to make the recommendations. Using the system, customers can try clothes on, receive fashion tips and suggestions from the AI, then make choices on-screen. If a user wants to try something different, or add other items, a store attendant can be summoned with the press of a button. Deep learning allows the AI to make connections in real-time by accessing massive quantities of data and making ‘smart’ decisions. When you’re on Amazon, for example, looking at a pair of cowboy boots might prompt the algorithm to recommend hiking or biker boots. But, search for a cowboy hat too, and you’re likely to be recommended other cowboy themed items, like belts, as opposed to just more hats and boots. The idea is that eventually the AI will get better at determining what you’re going to look at, what you’re going to compare it with, what you’ll want to purchase with it, and which items you’ll actually end up buying. Where no staff of humans could possibly be expected to remember the personal shopping preferences of every single customer, AI can. Alibaba’s FashionAI may have been nothing more than an experiment at this point, but it’s an exciting one. It’s taken the weirdness out of ideas like Amazon’s AI-powered shops and turned it into something more palatable. Most attempts at changing the way brick-and-mortar stores operate have revolved around trying to cram the online experience into kiosks, lobbies, and the checkout area. The trade-off is typically one where customers invest their own time and effort into a system that doesn’t quite work as well as logging on to Amazon or Alibaba with a smartphone does. By placing a screen against a wall in a changing booth, Alibaba integrated its already successful shopping system into the real-world seamlessly. If brick-and-mortar stores are to remain viable, they’ll have to leverage consumer attention by giving us all a reason to leave our phones in our pockets when we shop. People are probably getting sick of being told that Amazon and Alibaba can afford to sell items cheaper because they don’t have overhead. That doesn’t make most of us feel better about being overcharged. It feels like we’re being charged for the ‘privilege’ of standing in line and carrying our own bags through stores and parking lots, at most brick-and-mortar stores. If AI can be harnessed to bring the brick-and-mortar experience more closely in line with shopping on Alibaba.com, we all stand to benefit. Putting up AI-powered screens in dressing rooms is an important first step, and one that may ultimately save a lot of human jobs. Instead of replacing people with robots, Alibaba’s FashionAI integrates humans and machines in a way that provides the customer with everything they need. And the customer is always right.
Alibaba’s AI Fashion Consultant Helps It Set a New Singles’ Day Record
on MIT Technology Review
Read next:
5 tools for improving the efficiency (and conversion power) of your call center
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
23cc722448,Miniaturizing the brain of a drone | MIT News,"Login
or
Subscribe Newsletter
Engineers at MIT have taken a first step in designing a computer chip that uses a fraction of the power of larger drone computers and is tailored for a drone as small as a bottlecap.
Image: Christine Daniloff/MIT Method for designing efficient computer chips may get miniature smart drones off the ground.
Jennifer Chu | MIT News Office
July 11, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
In recent years, engineers have worked to shrink drone technology, building flying prototypes that are the size of a bumblebee and loaded with even tinier sensors and cameras. Thus far, they have managed to miniaturize almost every part of a drone, except for the brains of the entire operation — the computer chip. Standard computer chips for quadcoptors and other similarly sized drones process an enormous amount of streaming data from cameras and sensors, and interpret that data on the fly to autonomously direct a drone’s pitch, speed, and trajectory. To do so, these computers use between 10 and 30 watts of power, supplied by batteries that would weigh down a much smaller, bee-sized drone. Now, engineers at MIT have taken a first step in designing a computer chip that uses a fraction of the power of larger drone computers and is tailored for a drone as small as a bottlecap. They will present a new methodology and design, which they call “Navion,” at the Robotics: Science and Systems conference, held this week at MIT. The team, led by Sertac Karaman, the Class of 1948 Career Development Associate Professor of Aeronautics and Astronautics at MIT, and Vivienne Sze, an associate professor in MIT's Department of Electrical Engineering and Computer Science, developed a low-power algorithm, in tandem with pared-down hardware, to create a specialized computer chip. The key contribution of their work is a new approach for designing the chip hardware and the algorithms that run on the chip. “Traditionally, an algorithm is designed, and you throw it over to a hardware person to figure out how to map the algorithm to hardware,” Sze says. “But we found by designing the hardware and algorithms together, we can achieve more substantial power savings.” “We are finding that this new approach to programming robots, which involves thinking about hardware and algorithms jointly, is key to scaling them down,” Karaman says. The new chip processes streaming images at 20 frames per second and automatically carries out commands to adjust a drone’s orientation in space. The streamlined chip performs all these computations while using just below 2 watts of power — making it an order of magnitude more efficient than current drone-embedded chips. Karaman, says the team’s design is the first step toward engineering “the smallest intelligent drone that can fly on its own.” He ultimately envisions disaster-response and search-and-rescue missions in which insect-sized drones flit in and out of tight spaces to examine a collapsed structure or look for trapped individuals. Karaman also foresees novel uses in consumer electronics. “Imagine buying a bottlecap-sized drone that can integrate with your phone, and you can take it out and fit it in your palm,” he says. “If you lift your hand up a little, it would sense that, and start to fly around and film you. Then you open your hand again and it would land on your palm, and you could upload that video to your phone and share it with others.” Karaman and Sze’s co-authors are graduate students Zhengdong Zhang and Amr Suleiman, and research scientist Luca Carlone. From the ground up Current minidrone prototypes are small enough to fit on a person’s fingertip and are extremely light, requiring only 1 watt of power to lift off from the ground. Their accompanying cameras and sensors use up an additional half a watt to operate. “The missing piece is the computers — we can’t fit them in terms of size and power,” Karaman says. “We need to miniaturize the computers and make them low power.” The group quickly realized that conventional chip design techniques would likely not produce a chip that was small enough and provided the required processing power to intelligently fly a small autonomous drone. “As transistors have gotten smaller, there have been improvements in efficiency and speed, but that’s slowing down, and now we have to come up with specialized hardware to get improvements in efficiency,” Sze says. The researchers decided to build a specialized chip from the ground up, developing algorithms to process data, and hardware to carry out that data-processing, in tandem. Tweaking a formula Specifically, the researchers made slight changes to an existing algorithm commonly used to determine a drone’s “ego-motion,” or awareness of its position in space. They then implemented various versions of the algorithm on a field-programmable gate array (FPGA), a very simple programmable chip. To formalize this process, they developed a method called iterative splitting co-design that could strike the right balance of achieving accuracy while reducing the power consumption and the number of gates. A typical FPGA consists of hundreds of thousands of disconnected gates, which researchers can connect in desired patterns to create specialized computing elements. Reducing the number gates with co-design allowed the team to chose an FPGA chip with fewer gates, leading to substantial power savings. “If we don’t need a certain logic or memory process, we don’t use them, and that saves a lot of power,” Karaman explains. Each time the researchers tweaked the ego-motion algorithm, they mapped the version onto the FPGA’s gates and connected the chip to a circuit board. They then fed the chip data from a standard drone dataset — an accumulation of streaming images and accelerometer measurements from previous drone-flying experiments that had been carried out by others and made available to the robotics community. “These experiments are also done in a motion-capture room, so you know exactly where the drone is, and we use all this information after the fact,” Karaman says. Memory savings For each version of the algorithm that was implemented on the FPGA chip, the researchers observed the amount of power that the chip consumed as it processed the incoming data and estimated its resulting position in space. The team’s most efficient design processed images at 20 frames per second and accurately estimated the drone’s orientation in space, while consuming less than 2 watts of power. The power savings came partly from modifications to the amount of memory stored in the chip. Sze and her colleagues found that they were able to shrink the amount of data that the algorithm needed to process, while still achieving the same outcome. As a result, the chip itself was able to store less data and consume less power. “Memory is really expensive in terms of power,” Sze says. “Since we do on-the-fly computing, as soon as we receive any data on the chip, we try to do as much processing as possible so we can throw it out right away, which enables us to keep a very small amount of memory on the chip without accessing off-chip memory, which is much more expensive.” In this way, the team was able to reduce the chip’s memory storage to 2 megabytes without using off-chip memory, compared to a typical embedded computer chip for drones, which uses off-chip memory on the order of a few gigabytes. “Any which way you can reduce the power so you can reduce battery size or extend battery life, the better,” Sze says. This summer, the team will mount the FPGA chip onto a drone to test its performance in flight. Ultimately, the team plans to implement the optimized algorithm on an application-specific integrated circuit, or ASIC, a more specialized hardware platform that allows engineers to design specific types of gates, directly onto the chip. “We think we can get this down to just a few hundred milliwatts,” Karaman says. “With this platform, we can do all kinds of optimizations, which allows tremendous power savings.” This research was supported, in part, by Air Force Office of Scientific Research and the National Science Foundation. Topics: Aeronautical and astronautical engineering, Algorithms, Computer science and technology, Drones, Autonomous vehicles, Electrical Engineering & Computer Science (eecs), Research, Robots, Robotics, School of Engineering, National Science Foundation (NSF), Artificial intelligence, Machine learning This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
28029d9084,Machine learning used to predict earthquakes in a lab setting | EurekAlert! Science News,"Experimental Biology 2017April 22 - 26, 2017Chicago, IL 2018 AAAS Annual Meeting
February 15 - 19, 2018Austin, TX EurekAlert! provides embargoed and breaking science news you can't afford to miss. EurekAlert! offers a one-stop science news distribution service you can trust. EurekAlert! is a service of the American Association for the Advancement of Science.
University of Cambridge A group of researchers from the UK and the US have used machine learning techniques to successfully predict earthquakes. Although their work was performed in a laboratory setting, the experiment closely mimics real-life conditions, and the results could be used to predict the timing of a real earthquake.
The team, from the University of Cambridge, Los Alamos National Laboratory and Boston University, identified a hidden signal leading up to earthquakes, and used this 'fingerprint' to train a machine learning algorithm to predict future earthquakes. Their results, which could also be applied to avalanches, landslides and more, are reported in the journal Geophysical Review Letters.
For geoscientists, predicting the timing and magnitude of an earthquake is a fundamental goal. Generally speaking, pinpointing where an earthquake will occur is fairly straightforward: if an earthquake has struck a particular place before, the chances are it will strike there again. The questions that have challenged scientists for decades are how to pinpoint when an earthquake will occur, and how severe it will be. Over the past 15 years, advances in instrument precision have been made, but a reliable earthquake prediction technique has not yet been developed.
As part of a project searching for ways to use machine learning techniques to make gallium nitride (GaN) LEDs more efficient, the study's first author, Bertrand Rouet-Leduc, who was then a PhD student at Cambridge, moved to Los Alamos National Laboratory in New Mexico to start a collaboration on machine learning in materials science between Cambridge University and Los Alamos. From there the team started helping the Los Alamos Geophysics group on machine learning questions. The team at Los Alamos, led by Paul Johnson, studies the interactions among earthquakes, precursor quakes (often very small earth movements) and faults, with the hope of developing a method to predict earthquakes. Using a lab-based system that mimics real earthquakes, the researchers used machine learning techniques to analyse the acoustic signals coming from the 'fault' as it moved and search for patterns.
The laboratory apparatus uses steel blocks to closely mimic the physical forces at work in a real earthquake, and also records the seismic signals and sounds that are emitted. Machine learning is then used to find the relationship between the acoustic signal coming from the fault and how close it is to failing.
The machine learning algorithm was able to identify a particular pattern in the sound, previously thought to be nothing more than noise, which occurs long before an earthquake. The characteristics of this sound pattern can be used to give a precise estimate (within a few percent) of the stress on the fault (that is, how much force is it under) and to estimate the time remaining before failure, which gets more and more precise as failure approaches. The team now thinks that this sound pattern is a direct measure of the elastic energy that is in the system at a given time.
""This is the first time that machine learning has been used to analyse acoustic data to predict when an earthquake will occur, long before it does, so that plenty of warning time can be given - it's incredible what machine learning can do,"" said co-author Professor Sir Colin Humphreys of Cambridge's Department of Materials Science & Metallurgy, whose main area of research is energy-efficient and cost-effective LEDs. Humphreys was Rouet-Leduc's supervisor when he was a PhD student at Cambridge.
""Machine learning enables the analysis of datasets too large to handle manually and looks at data in an unbiased way that enables discoveries to be made,"" said Rouet-Leduc. Although the researchers caution that there are multiple differences between a lab-based experiment and a real earthquake, they hope to progressively scale up their approach by applying it to real systems which most resemble their lab system. One such site is in California along the San Andreas Fault, where characteristic small repeating earthquakes are similar to those in the lab-based earthquake simulator. Progress is also being made on the Cascadia fault in the Pacific Northwest of the United States and British Columbia, Canada, where repeating slow earthquakes that occur over weeks or months are also very similar to laboratory earthquakes.
""We're at a point where huge advances in instrumentation, machine learning, faster computers and our ability to handle massive data sets could bring about huge advances in earthquake science,"" said Rouet-Leduc.
### Disclaimer: AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system. Media Contact Sarah Collins
sarah.collins@admin.cam.ac.uk
44-012-237-65542
@Cambridge_Uni
http://www.cam.ac.uk  University of Cambridge
Copyright © 2018 by the American Association for the Advancement of Science (AAAS) Copyright © 2018 by the American Association for the Advancement of Science (AAAS)"
1d02ef05e3,GitHub - pakoito/MarI-O: Github clone of MarI/O by SethBling,"GitHub is home to over 20 million developers working together to host and review code, manage projects, and build software together.
Sign up
Use Git or checkout with SVN using the web URL.
Github clone of MarI/O by SethBling Taken from https://www.youtube.com/watch?v=qv6UVOQ0F44 MarI/O is a program made of neural networks and genetic algorithms that kicks butt at Super Mario World. Source Code: http://pastebin.com/ZZmSNaHX ""NEAT"" Paper: http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf Some relevant Wikipedia links: https://en.wikipedia.org/wiki/Neuroevolution https://en.wikipedia.org/wiki/Evolutionary_algorithm https://en.wikipedia.org/wiki/Artificial_neural_network BizHawk Emulator: http://tasvideos.org/BizHawk.html SethBling Twitter: http://twitter.com/sethbling SethBling Twitch: http://twitch.tv/sethbling SethBling Facebook: http://facebook.com/sethbling SethBling Website: http://sethbling.com SethBling Shirts: http://sethbling.spreadshirt.com Suggest Ideas: http://reddit.com/r/SethBlingSuggestions"
c3c8fa28d9,Learning Cross-modal Embeddings for Cooking Recipes and Food Images - MIT,"♰ Universitat Politecnica de Catalunya
✦ Massachusetts Institute of Technology
✥ Qatar Computing Research Institute
CVPR 2017
* contributed equally
In this paper, we introduce Recipe1M, a new large-scale,
structured corpus of over 1m cooking recipes and 800k food
images. As the largest publicly available collection of recipe
data, Recipe1M affords the ability to train high-capacity
models on aligned, multi-modal data. Using these data, we
train a neural network to find a joint embedding of recipes
and images that yields impressive results on an image-recipe
retrieval task. Additionally, we demonstrate that regularization
via the addition of a high-level classification objective
both improves retrieval performance to rival that of humans
and enables semantic vector arithmetic. We postulate that
these embeddings will provide a basis for further exploration
of the Recipe1M dataset and food and cooking in general.
We present the large-scale Recipe1M dataset which contains one
million structured cooking recipes with associated images. Follow this link to download the dataset. Below are the dataset statistics: We train a joint embedding composed of an encoder for each modality (ingredients, instructions and images). We evaluate all the recipe representations for im2recipe
retrieval. Given a food image, the task is to retrieve its recipe
from a collection of test recipes. In order to better assess the quality of our embeddings we
also evaluate the performance of humans on the im2recipe
task. Check out the paper for full details and more analysis.
[Coming soon !] Check out our online demo, in which you can upload your food images to retrieve a recipe from our dataset.
We explore whether any semantic concepts emerge in the neuron
activations and whether the embedding space has certain
arithmetic properties.
We show the localized unit activations in both image and recipe embeddings. We find that
certain units show localized semantic alignment between
the embeddings of the two modalities. We demonstrate the capabilities of our learned embeddings with simple arithmetic
operations. In the context of food recipes, one would expect that:
where v represents the map into the embedding
space. We investigate whether our learned embeddings have
such properties by applying the previous equation template
to the averaged vectors of recipes that contain the queried
words in their title. The figures below show some results with same and cross-modality embedding arithmetics.
This work has been supported by CSAIL-QCRI collaboration projects and the framework of projects TEC2013-43935-R and TEC2016-75976-R, financed by the Spanish Ministerio de Economia y Competitividad and the European Regional Development Fund (ERDF)."
37afb15339,"Nvidia Invests In Deep Instinct, A Deep Learning-Based Cybersecurity Startup","Nvidia announced that it has invested in Deep Instinct, an Insrael-based startup that uses deep learning technology to detect and prevent the “most advanced cyberattacks.” Deep learning is a promising machine learning technology  used across industries. Many companies are now using it to train neural network models that achieve high accuracy without too much help from programmers in specifying what the rules should be for a given task. Academics researching AI that can stop cyberattacks have previously shown an 85% success rate in detecting cyberattacks. Although that number may look impressive, it means that one out of seven attempts to attack your systems will be successful.
It’s also not much better than what you’d obtain from an antivirus program that doesn’t use machine learning to stop attacks. In fact, most of the top antivirus programs seem to be doing much better than that.
The latest report from AV-Comparatives on how the antivirus programs fare against real threats, they all score from 92% to 100%. However, these are automated tests, and they may not account for the full range of attacks that a system may see, especially regarding advanced cyberattacks from nation-state actors. Deep Instinct, which claims to be the ""first company to apply deep learning to cybersecurity,"" has also stated that its deep-learning solution can defeat 99% of “cyberattacks."" That includes both known malware as well as “first-seen” attacks against a system. Nvidia vice president of business development Jeff Herbst said: Deep Instinct is an emerging leader in applying GPU-powered AI through deep learning to address cybersecurity, a field ripe for disruption as enterprise customers migrate away from traditional solutions. We’re excited to work together with Deep Instinct to advance this important field. Due to its deep learning based technology, Deep Instinct’s solution can run locally on any type of device, and it doesn’t require a connection for cloud analysis. This should keep the data on the devices private, as the file analysis is not sent over to the company’s servers.
The solution is also lightweight, because it doesn’t have to check against hundreds of thousands of malware signatures in real-time. All it needs is the neural network--which was trained against hundreds of millions of malicious and benign files--and a powerful enough GPU to run the model. Deep Instinct was recently named a “Technology Pioneer” by The World Economic Forum, and the “Most Disruptive Startup” at NVIDIA’s 2017 Inception Awards. Lucian Armasu is a Contributing Writer for Tom's Hardware US. He covers software news and the issues surrounding privacy and security."
d5d0e9b17a,"Google’s new Go-playing AI learns fast, and even thrashed its former self","Professor of Neuroscience and Mathematics, The University of Queensland
Geoff Goodhill does not work for, consult, own shares in or receive funding from any company or organisation that would benefit from this article, and has disclosed no relevant affiliations beyond their academic appointment.
University of Queensland provides funding as a member of The Conversation AU. The Conversation UK receives funding from Hefce, Hefcw, SAGE, SFC, RCUK, The Nuffield Foundation, The Ogden Trust, The Royal Society, The Wellcome Trust, Esmée Fairbairn Foundation and The Alliance for Useful Evidence, as well as sixty five university members. View the full list
Republish our articles for free, online or in print, under Creative Commons licence.
Just last year Google DeepMind’s AlphaGo took the world of Artificial Intelligence (AI) by storm, showing that a computer program could beat the world’s best human Go players.
But in a demonstration of the feverish rate of progress in modern AI, details of a new milestone reached by an improved version called AlphaGo Zero were published this week in Nature. Using less computing power and only three days of training time, AlphaGo Zero beat the original AlphaGo in a 100-game match by 100 to 0. It wasn’t even worth humans showing up. Read more: Why Google wants to think more like you and less like a machine Go is a game of strategy between two players who take it in turns to place “stones” on a 19x19 board. The goal is to surround a larger area of the board than your opponent. Go has proved much more challenging than chess for computers to master. There are many more possible moves in each position in Go than chess, and many more possible games. The original AlphaGo first learned from studying 30 million moves of expert human play. It then improved beyond human expertise by playing many games against itself, taking several months of computer time. By contrast, AlphaGo Zero never saw humans play. Instead, it began by knowing only the rules of the game. From a relatively modest five million games of self-play, taking only three days on a smaller computer than the original AlphaGo, it then learned super-AlphaGo performance. Fascinatingly, its learning roughly mimicked some of the stages through which humans progress as they master Go. AlphaGo Zero rapidly learned to reject naively short-term goals and developed more strategic thinking, generating many of the patterns of moves often used by top-level human experts.
But remarkably it then started rejecting some of these patterns in favour of new strategies never seen before in human play. AlphaGo Zero achieved this feat by approaching the problem differently from the original AlphaGo. Both versions use a combination of two of the most powerful algorithms currently fuelling AI: deep learning and reinforcement learning. To play a game like Go, there are two basic things the program needs to learn. The first is a policy: the probability of making each of the possible moves in a given position. The second is a value: the probability of winning from any given position. In the pure reinforcement learning approach of AlphaGo Zero, the only information available to learn policies and values was for it to predict who might ultimately win. To make this prediction it used its current policy and values, but at the start these were random. This is clearly a more challenging approach than the original AlphaGo, which used expert human moves to get a head-start on learning. But the earlier version learned policies and values with separate neural networks.
The algorithmic breakthrough in AlphaGo Zero was to figure out how these could be combined in just one network. This allowed the process of training by self-play to be greatly simplified, and made it feasible to start from a clean slate rather than first learning what expert humans would do. An Elo rating is a widely used measure of the performance of players in games such as Go and chess. The best human player so far, Ke Jie, currently has an Elo rating of about 3,700. AlphaGo Zero trained for three days and achieved an Elo rating of more than 4,000, while an expanded version of the same algorithm trained for 40 days and achieved almost 5,200. This is an astonishingly large step up from the best human – far bigger than the current gap between the best human chess player Magnus Carlsen (about 2,800) and chess program (about 3,400). AlphaGo Zero is an important step forward for AI because it demonstrates the feasibility of pure reinforcement learning, uncorrupted by any human guidance. This removes the need for lots of expert human knowledge to get started, which in some domains can be hard to obtain.
It also means the algorithm is free to develop completely new approaches that might have been much harder to find had it been been initially constrained to “think inside the human box”. Remarkably, this strategy also turns out to be more computationally efficient. But Go is a tightly constrained game of perfect information, without the messiness of most real-world problems. Training AlphaGo Zero required the accurate simulation of millions of games, following the rules of Go.
For many practical problems such simulations are computationally unfeasible, or the rules themselves are less clear.
Read more: No more playing games: AlphaGo AI to tackle some real world challenges There are still many further problems to be solved to create a general-purpose AI, one that can tackle a wide range of practical problems without domain-specific human intervention.
But even though humans have now comprehensively lost the battle with Go algorithms, luckily AI (unlike Go) is not a zero-sum game. Many of AlphaGo Zero’s games have now been published, providing a lifetime of inspirational study for human Go players. More importantly, AlphaGo Zero represents a step towards a world where humans can harness powerful AIs to help find unimaginably (to humans) creative solutions to difficult problems. In the world of AI, there has never been a better time to Go for it.
Introduction to Artificial Intelligence: Back to the future
—
York, York
Seeing past and present in the glazed cloister of Park Abbey (1635-1644)
—
York, York
Poetry: The true language of religion
—
Egham, Surrey
Personhood and Selfhood: Social Personhood
—
Manchester, Manchester
Nostalgic for war: Reflections on the stories we tell about World War II and contemporary British militarism
—
York, York
Copyright © 2010–2018, The Conversation Trust (UK) Limited"
d82a33e1d7,Alibaba's FashionAI shows how machine learning might save the mall,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Alibaba’s sales from Saturday’s Singles Day event exceeded 25 billion dollars, more than quadruple what Americans spent last year during Black Friday. While the majority of those sales undoubtedly came via online purchases, the company also quietly experimented with an AI-powered project designed to woo offline shoppers. FashionAI was developed by Alibaba researchers in order to provide a recognizable interface for customers to use while trying on clothes. It’s a basic screen interface that uses machine-learning to make clothing and accessory suggestions to customers based on the items they are trying on. There’s no camera; it uses information embedded in the item’s tag to make the recommendations. Using the system, customers can try clothes on, receive fashion tips and suggestions from the AI, then make choices on-screen. If a user wants to try something different, or add other items, a store attendant can be summoned with the press of a button. Deep learning allows the AI to make connections in real-time by accessing massive quantities of data and making ‘smart’ decisions. When you’re on Amazon, for example, looking at a pair of cowboy boots might prompt the algorithm to recommend hiking or biker boots. But, search for a cowboy hat too, and you’re likely to be recommended other cowboy themed items, like belts, as opposed to just more hats and boots. The idea is that eventually the AI will get better at determining what you’re going to look at, what you’re going to compare it with, what you’ll want to purchase with it, and which items you’ll actually end up buying. Where no staff of humans could possibly be expected to remember the personal shopping preferences of every single customer, AI can. Alibaba’s FashionAI may have been nothing more than an experiment at this point, but it’s an exciting one. It’s taken the weirdness out of ideas like Amazon’s AI-powered shops and turned it into something more palatable. Most attempts at changing the way brick-and-mortar stores operate have revolved around trying to cram the online experience into kiosks, lobbies, and the checkout area. The trade-off is typically one where customers invest their own time and effort into a system that doesn’t quite work as well as logging on to Amazon or Alibaba with a smartphone does. By placing a screen against a wall in a changing booth, Alibaba integrated its already successful shopping system into the real-world seamlessly. If brick-and-mortar stores are to remain viable, they’ll have to leverage consumer attention by giving us all a reason to leave our phones in our pockets when we shop. People are probably getting sick of being told that Amazon and Alibaba can afford to sell items cheaper because they don’t have overhead. That doesn’t make most of us feel better about being overcharged. It feels like we’re being charged for the ‘privilege’ of standing in line and carrying our own bags through stores and parking lots, at most brick-and-mortar stores. If AI can be harnessed to bring the brick-and-mortar experience more closely in line with shopping on Alibaba.com, we all stand to benefit. Putting up AI-powered screens in dressing rooms is an important first step, and one that may ultimately save a lot of human jobs. Instead of replacing people with robots, Alibaba’s FashionAI integrates humans and machines in a way that provides the customer with everything they need. And the customer is always right.
Alibaba’s AI Fashion Consultant Helps It Set a New Singles’ Day Record
on MIT Technology Review
Read next:
5 tools for improving the efficiency (and conversion power) of your call center
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
06f9da3d1a,How RobotCar works – Oxford Robotics Institute,"We use the mathematics of probability and estimation to allow computers in robots to interpret data from sensors like cameras, radars and lasers, aerial photos and on-the-fly internet queries. We use machine learning techniques to build and calibrate mathematical models which can explain the robot’s view of the world in terms of prior experience (training), prior knowledge (aerial images, road plans and semantics) and automatically generated web queries. We wish to produce technology which allows robots always to know precisely where they are and what is around them.   Already, robots carry goods around factories and manage our ports, but these are constrained, controlled and highly managed workspaces. Here, the navigation task is made simple by installing reflective beacons or guide wires. Our goal is to extend the reach of robot navigation to truly vast scales without the need for such expensive, awkward and inconvenient modification of the environment. It is about enabling machines to operate for, with and beside us in the multitude of spaces we inhabit, live and work. Even when GPS is available, it does not offer the accuracy required for robots to make decisions about how and when to move safely. Even if it did, it would say nothing about what is around the robot, and that has a massive impact on autonomous decision-making. Perhaps the ultimate application is civilian transport systems. We are not condemned to a future of congestion and accidents. We will eventually have cars that can drive themselves, interacting safely with other road users and using roads efficiently, thus freeing up our precious time. But to do this the machines need life-long infrastructure-free navigation, and that is the focus of this work. Although the car itself only moves in 2D, it senses in 3D. It only offers the driver autonomy if the 3D impression it forms as it moves matches that which it has stored in its memory. So before the car can operate, it must learn what its environment looks like. As an example, below is a video of the car learning/discovering what Woodstock town centre looks like. Tucked under the front and rear bumpers of the vehicle are two scanning lasers. These lasers allow us to sense the 3D structure of the cars environment – from this we can figure out the car’s location and orientation on the road. We can use discrete stereo cameras to figure out the trajectory of the vehicle relative routes it has been driven on before. This movie shows the vehicle interpreting live images in the context of its memory of our test site at Begbroke Science Park. We can also use these cameras to detect the presence of obstacles – although vision is not great at night so we also use laser. Static information consists of semantic information like the location and type of road markings and traffic signs, traffic lights, lane information, where curbs are, etc. This kind of information rarely changes and so a fairly accurate model can be built before the vehicle actually goes out. And it will last. Of course, you don’t really want to blindly believe such a prior map for all time – after all, things do change when conducting roadworks, for example – but knowing where you can expect to find certain things in the world is already incredibly helpful. The prior semantic map will get updated over time with information the vehicle actually gathers out there in the real world. Dynamic information relates to potential obstacles which are either moving or stationary: cars, bicycles, pedestrians, etc. Knowing where they are – and where they are likely going to be in the near future – with respect to the planned vehicle trajectory is crucial for safe operation as well as for appropriate trajectory planning. Dynamic obstacles are detected and tracked using an off-the-shelf laser scanner. The system scans an 85 degree field of view ahead of the car 13 times a second to detect obstacles up to 50 metres ahead. Oxford Robotics Institute, The George Building, 23 Banbury Road, Oxford OX2 6NN Email: The ORI Team"
d78fabe890,New Machine Learning Program Recognizes Handguns in Even Low-Quality Video - Motherboard,"Michael Byrne The handgun is a ballistic extension of the hand itself. It's remarkable for confining vast destruction to a small, subtle instrument. It can be wielded casually, and, until the trigger is pulled, nigh invisibly. A semi-automatic pistol, once awoken, will fire as fast as the shooter can twitch their finger. The relation between package and power offered by a handgun feels almost nuclear.  A team of computer scientists based at the University of Granada in Spain thinks that we can help neutralize the threat of handguns through early detection. If we can register the gun before it's actually fired, we can regain some control. To this end, they've developed a machine learning program that can reliably detect handguns based on visual recognition and classification. It's capable of catching guns from even low-quality YouTube footage in just under a quarter second.   ""The crime rates caused by guns are very concerning in many places in the world, especially in countries where the possession of guns is legal or was legal for a period of time,"" Siham Tabik and colleagues write in a paper posted recently to the arXiv preprint server. ""One way to reducing this kind of violence is prevention via early detection so that the security agents or policemen can act. In particular, one innovative solution to this problem is to equip surveillance or control cameras with an accurate automatic handgun detection alert system."" Watch Motherboard's A Smarter Gun in full: Given the success of machine learning facial recognition systems, we might assume this is a simple problem. The algorithm just needs to look at a whole bunch of images of handguns in a variety of settings and it will eventually learn the visual features that make a handgun a handgun, or at least enough of those features to identify a handgun.  It would be this simple if only we had as many pictures of handguns as we do of faces, which we don't. Facial recognition using convolutional neural networks (CNNs) is possible only because we have many millions of images of faces to learn from. Tabik only had about 3,000 images of handguns, which is all but nothing when it comes to doing visual recognition tasks with machine learning.  Over the past few years, researchers have been developing a fudge of sorts that allows, in some cases, for the creation of visual recognition models even when data is scarce. Generally, this is known as transfer learning. The basic idea is that we can use knowledge about one category of thing and apply that to another, related category. We may have very few images of trucks, for example, but if we have enough images of cars then we can get somewhere. Say we already have a good model trained for recognizing cars. It's possible to take that car recognition model and ""fine tune"" it using new images of trucks. The model for cars already contains a lot of semantic information about trucks because, well, trucks are pretty similar to cars. So, through the fine tuning process, we can retrain the existing car model to be a truck model by teaching it the difference between a car and a truck. Its abstract representation—the generic features it uses to recognize something—expands and adapts.  The model Tabik and co. fine tuned is called VGG-16 and is based on a 1.28 million image dataset known as ImageNet. Its utility is in object recognition across many unrelated categories. Given in input image, it can classify it with respect to 1,000 different object classes: can opener, vulture, coral fungus, toilet paper, etc. By fine tuning the VGG-16 model with their 3,000 handgun images, they were able to create essentially new classes of object.  ""The best performing model shows a high potential even in low quality YouTube videos and provides satisfactory results as automatic alarm system,"" Tabik writes. ""Among 30 scenes,it successfully activates the alarm, after five successive true positives, within aninterval of time smaller than 0.2 seconds, in 27 scenes."" When it comes to gun detection technology there isn't a whole lot occupying the gulf between ultra high-tech and crudely obvious. On the one hand, we have the NYPD testing a system that tracks guns based on the radiative signatures of human bodies, while, on the other, a firm called Shooter Detection Systems is pushing a system that automatically detects and reports gunfire: ""a smoke alarm for gun fire detection."" As of 2014, the latter had been installed in about a half-dozen schools. Tabik's system is somewhere in between those two poles.  Training a machine learning model is computationally taxing. It takes a long time—days, weeks. But once you have the model—which is really just a big grid of numbers saved in a file—actually doing object recognition is pretty quick. Across several different combinations of detection classes, the researchers were able to all but eliminate false positives, but still wound up with a significant set of false negatives (as above), resulting in overall accuracies of between 90 and 95 percent, excluding a first attempt that mostly missed the mark.  This relatively new ability of building out new object recognition classes from small datasets has the potential to make this stuff much more real-world in the near future.  Sign up for Motherboard Premium. Find us in the future."
28242e5495,"Tim Cook is a fan of VisualDx, a $100 a year machine learning app for skin diagnosis - Business Insider","Visual Dx
Apple CEO Tim Cook isn't a doctor, but he talked about a piece of medical software, VisualDx, during Apple's most recent earnings call. It was an interesting choice of an app to highlight. Apple has deep ambition to break into the health and medical worlds, but although VisualDx is available to consumers through the Apple App Store, it's not really an app for the public. It's targeted at trained and credentialed doctors who can use it to help diagnose skin conditions and disorders.  This fall, the app has gotten a new trick — it can use an iPhone's camera and machine learning to automatically identify skin conditions, or at least narrow down what they could be. Snap a picture in the app, and it will return a list of conditions the rash could be, based on decades of medical research. ""VisualDx is even pioneering new health diagnostics with Core ML, automating skin image analysis to assist dermatologists with their diagnoses,"" Cook said.  In some ways, VisualDx offers a hint of the future of medicine, where a hot topic of conversation is whether sufficiently advanced computers and artificial intelligence could automate one of the core parts of what doctors do: identifying what the problem is.  In the future, some of this technology will trickle down to consumers, giving them the ability to snap photos of their own bodies, answer some questions, and ultimately figure out whether it's a problem that requires medical attention, or simply a common rash, VisualDx CEO and medical doctor Art Papier tells Business Insider. VisualDx is currently developing a version of this tool, called Aysa, for common skin disorders. ""Consumers don't want to play doctor themselves. But on a Saturday, they want to know, do I need to go to the emergency room with my child or can this wait until Monday when I could see my pediatrician,"" Papier said.  ""It's really education and triage. It's not diagnosis, we don't believe in that,"" he continued, ""At least in the next few years, we're not going to tell patients you're totally OK, you don't need to see a doctor.""  Dr. Art Papier.RochesterThe reason why Apple's CEO mentioned VisualDx is because it's using CoreML, a new set of software that makes it possible to run machine learning algorithms on a phone, instead of uploading the photos online to a server for processing.
""Our clients are hospitals and they really don't like the idea of a doctor in their hospital taking a picture of a patient and then sending the picture to a third party or a private company,"" Papier said.  ""We realized when Apple announced CoreML in June, they announced that you can move your models onto the iPhone,"" he continued. ""Now the image gets analyzed on the phone, and the image never goes up to the cloud to us. We never see the image."" Even still, the software can return an analysis in a second on a newer iPhone. The identification neural network is ""trained"" by researchers at VisualDx, but it can run on a phone, Papier said.  The models are trained using VisualDx's own library of professional medical images, Papier said.  ""We're not like a wiki model where you know anyone can upload images to us and just tell us what they think they are,"" Papier said. ""We're very very selective to work with the universities and individual physicians we know are experts."" Many of VisualDx's images were scanned from old collections of slides and film, from leading departments and doctors. It's built a library of 32,000 images to train its models. ""We developed this reputation as somebody that was going to preserve the legacy of medical photography,"" Papier said.  Still, even with high-quality models and training data, Papier doesn't think completely automated diagnosis will happen anytime soon. ""The hype cycle right now for machine learning is off the charts,"" he said.  ""Machine learning will get you into a category on this, but to get to the final mile, you have to ask the patient, 'did you take a drug a week ago?' 'Did you travel?'"" he said.  Medical professionals can download the app from the App Store. If your institution doesn't have a subscription, various in-app purchases range from $99 for a year's access to DermExpert. A complete purchase with access to other medical information is $499 per year.  You can sign up for the Aysa mailing list here.   Advances in machine learning now mean that...
Retirement is lasting longer than ever — here's how to prepare financially
More ""Finance"" »
Here's the event that has global brands debating disruptive technology
More ""Strategy"" »
Get the best of Business Insider delivered to your inbox every day."
d4ab1b13e4,Machine learning will keep us healthy longer | WIRED UK,"Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
By
Nayanah Siva
This article was taken from The WIRED World in 2016 -- our fourth annual trends report, a standalone magazine in which our network of expert writers and influencers predicts what's coming next. Be the first to read WIRED's articles in print before they're posted online, and get your hands on loads of additional content by subscribing online. When assessing a patient, medics look at snapshots of physiological data that are manually taken by doctors or nurses, and make decisions against patient history, family background and test results, as well as their own knowledge and experience. But what if this data was constantly being taken, every second of every day? And what if a system was clever enough to compare these readings to thousands of patients worldwide with a similar history and disorder, as well as all the current clinical guidelines and studies, and make clinical suggestions to doctors?
In 2016, this kind of data-led decision-making will come ever closer. Sentrian, a California-based early-stage machine learning and biosensor analytics company for remote patient management, has created a system that does just that, and it's currently being trialled on patients. ""We actually don't monitor people very frequently,"" says Jack Kreindler, Sentrian's founder and chief medical officer. ""If I see a patient once a year, I may spend one hour listening to them, and the rest of the year's 8,759 hours not listening to them. We are trying to build a system that will enable us to listen to the lives and bodies of patients all the time, so we can make better, earlier and more personalised decisions."" Currently, wireless biosensors can collect simple data such as body temperature and heart rate as well as more complex information like oxygen saturation of the blood and potassium levels. Remote patient monitoring is typically done with one or two sensors at a time and the data is usually assessed by clinicians. But if a patient could constantly wear several sensors at a time, the amount of data produced would be enormous.
By
Natalie Massenet
Sentrian's approach collects data streams from biosensors and uses machine learning algorithms to detect subtle patterns based on general information within the system on chronic conditions. These can include heart disease, diabetes and chronic obstructive pulmonary disease (COPD). Data such as heart rate, blood pressure and oxygen saturation from wireless biosensors on the patient are pushed to a cloud-based engine that analyses this data and notifies doctors when needed.
Martin Kohn, chief medical scientist at Sentrian, who practised emergency medicine for 30 years, explains the value in this approach. ""It's based on the premise that for many patients with diseases such as congestive heart failure and COPD, the processes that lead to severe illness start days before the patient actually becomes acutely ill,"" he says.
The system is currently being tested in clinical trials in the US and UK in patients with chronic heart failure, COPD, high risk of falls and cancer. Early unpublished evidence has already shown the possibility of being able to spot congestive heart failure exacerbations up to ten days in advance. ""That is quite extraordinary - before you maybe only had hours,"" Kreindler says. ""We are seeing subtle, personalised patterns and data where odd things, which we didn't really expect before, may end up having strong statistical significance in predicting whether someone is going to fall over many days in advance."" Very early research is showing that, in some people, factors such as heart rate variability, sleep duration and body temperature may be indicators of an impending crisis, These differ from the currently accepted warning signs and evidence-based triggers for treatment. But are we ready to hand over all decision making to a black box, particularly when it comes to healthcare? ""At the moment, there is a barrier, even from the profession themselves, to trust the kind of outputs that machine learning can deliver,"" Kreindler says. Sentrian has tried to account for this mistrust by giving some control back to humans - for example, by allowing doctors to specify rules for their patients. So, a scenario may run: ""If Mr Smith's heart rate rises significantly, but his activity is going down and his breathing rate is going up, send a text message to the patient and a family member. If there is no response from the caregiver or patient after a text message, one email, and one phone call, then make a call to their doctor.""
By
Rachel Botsman
These rules, which can be general or personalised, are the kind of complex event-processing and subtle pattern-recognition that is going on in an experienced team of clinician's heads when they are monitoring a patient in intensive care, explains Kriendler. And Sentrian's system will continue to learn exactly which rules and interventions work best for which patients: if a false alarm is indicated, the doctor can report this. The more patients the system ""sees"" and the more feedback it gets, the more the system learns. ""Normally, the human brain remembers the last 30 or so patients it looked at,"" says Kreindler. ""With this we may have more than 300,000 patients in memory.""
Another issue that machine learning is being applied to is the volume and rate at which new medical information is growing. Knowledge is expanding faster than doctors are able to assimilate and apply. It is estimated that in 1950, the time to double the volume of the world's medical knowledge was 50 years; in 2010 it took 3.5 years; and in 2020, it will take 73 days. In another study, researchers projected that it took, on average, 17 years for new evidence-based findings to find their way to the clinic. What if a doctor was able bring up every single case study, clinical study and national guideline worldwide on a particular disorder to the forefront of their mind? The second part of Sentrian's project aims to do just this, and augment the system with the ability to read and learn from all current clinical evidence.
IBM Watson, the supercomputer that won a game of Jeopardy! against humans in 2011, has already demonstrated that this sort of learning is possible. Kohn was chief medical scientist at IBM research, where he led the company's Watson supercomputer initiative in healthcare. Watson has ""read"" 204 oncology textbooks, medical databases of journals (one of which, PubMed has 24 million citations of biomedical literature), thousands of patient records, and had 14,700 hours of ""training"" from clinicians. In a study published in 2014, scientists from Baylor College of medicine in Houston, Texas and IBM used Watson technology to analyse more than 70,000 scientific articles to identify proteins that can modify a tumour-suppressing protein. Over the past 30 years scientists have identified 28 similar target proteins - Watson identified six in a month.
""Watson will assist your clinician by providing timely insights into the specific condition by analysing the patient's detailed medical records including genomic considerations,"" says Robert S Merkel of Watson Health. ""Watson will then suggest potential treatment recommendations from a very large repository of knowledge spanning millions of pages of medical literature, research articles and 180,000 clinical trial protocols."" Merkel explains that Watson does not have to be used as just a tool for suggesting clinical management options to doctors - it could also be a benefit in clinical trials. ""Consider clinical trial matching,"" he says. ""A clinical trial for an experimental breast cancer treatment may require a hundred patients who meet a variety of criteria, like a specific genetic marker, age, current stage of the tumour, history of interventions and a response to treatments and medications. Today, physicians and nurses spend hours manually reviewing patient records and comparing patient data to the criteria for a trial. This process introduces the possibility of errors, delays and missed matches."" Watson's computing power is able to help doctors by accurately matching their patients with clinical trials that could benefit their care. Sentrian's system is currently undergoing several randomised controlled trials, which means that the data of thousands of patients will be added to the platform. While we wait on clinical evidence, it is just a matter of time before this sort of artificial intelligence becomes a regular occurrence in the doctor's office.
By
Libby Plummer
By
Emily Reynolds
By
Marc Goodman
By
Emily Reynolds"
b5975e2a3b,"Tim Cook is a fan of VisualDx, a $100 a year machine learning app for skin diagnosis - Business Insider","Visual Dx
Apple CEO Tim Cook isn't a doctor, but he talked about a piece of medical software, VisualDx, during Apple's most recent earnings call. It was an interesting choice of an app to highlight. Apple has deep ambition to break into the health and medical worlds, but although VisualDx is available to consumers through the Apple App Store, it's not really an app for the public. It's targeted at trained and credentialed doctors who can use it to help diagnose skin conditions and disorders.  This fall, the app has gotten a new trick — it can use an iPhone's camera and machine learning to automatically identify skin conditions, or at least narrow down what they could be. Snap a picture in the app, and it will return a list of conditions the rash could be, based on decades of medical research. ""VisualDx is even pioneering new health diagnostics with Core ML, automating skin image analysis to assist dermatologists with their diagnoses,"" Cook said.  In some ways, VisualDx offers a hint of the future of medicine, where a hot topic of conversation is whether sufficiently advanced computers and artificial intelligence could automate one of the core parts of what doctors do: identifying what the problem is.  In the future, some of this technology will trickle down to consumers, giving them the ability to snap photos of their own bodies, answer some questions, and ultimately figure out whether it's a problem that requires medical attention, or simply a common rash, VisualDx CEO and medical doctor Art Papier tells Business Insider. VisualDx is currently developing a version of this tool, called Aysa, for common skin disorders. ""Consumers don't want to play doctor themselves. But on a Saturday, they want to know, do I need to go to the emergency room with my child or can this wait until Monday when I could see my pediatrician,"" Papier said.  ""It's really education and triage. It's not diagnosis, we don't believe in that,"" he continued, ""At least in the next few years, we're not going to tell patients you're totally OK, you don't need to see a doctor.""  Dr. Art Papier.RochesterThe reason why Apple's CEO mentioned VisualDx is because it's using CoreML, a new set of software that makes it possible to run machine learning algorithms on a phone, instead of uploading the photos online to a server for processing.
""Our clients are hospitals and they really don't like the idea of a doctor in their hospital taking a picture of a patient and then sending the picture to a third party or a private company,"" Papier said.  ""We realized when Apple announced CoreML in June, they announced that you can move your models onto the iPhone,"" he continued. ""Now the image gets analyzed on the phone, and the image never goes up to the cloud to us. We never see the image."" Even still, the software can return an analysis in a second on a newer iPhone. The identification neural network is ""trained"" by researchers at VisualDx, but it can run on a phone, Papier said.  The models are trained using VisualDx's own library of professional medical images, Papier said.  ""We're not like a wiki model where you know anyone can upload images to us and just tell us what they think they are,"" Papier said. ""We're very very selective to work with the universities and individual physicians we know are experts."" Many of VisualDx's images were scanned from old collections of slides and film, from leading departments and doctors. It's built a library of 32,000 images to train its models. ""We developed this reputation as somebody that was going to preserve the legacy of medical photography,"" Papier said.  Still, even with high-quality models and training data, Papier doesn't think completely automated diagnosis will happen anytime soon. ""The hype cycle right now for machine learning is off the charts,"" he said.  ""Machine learning will get you into a category on this, but to get to the final mile, you have to ask the patient, 'did you take a drug a week ago?' 'Did you travel?'"" he said.  Medical professionals can download the app from the App Store. If your institution doesn't have a subscription, various in-app purchases range from $99 for a year's access to DermExpert. A complete purchase with access to other medical information is $499 per year.  You can sign up for the Aysa mailing list here.   Advances in machine learning now mean that...
Retirement is lasting longer than ever — here's how to prepare financially
More ""Finance"" »
Here's the event that has global brands debating disruptive technology
More ""Strategy"" »
Get the best of Business Insider delivered to your inbox every day."
b3a27cbc90,Machine Learning Is Redefining The Enterprise In 2016,"Bottom line: Machine learning is providing the needed algorithms, applications, and frameworks to bring greater predictive accuracy and value to enterprises’ data, leading to diverse company-wide strategies succeeding faster and more profitably than before. Industries Where Machine Learning Is Making An Impact   The good news for businesses is that all the data they have been saving for years can now be turned into a competitive advantage and lead to strategic goals being accomplished. Revenue teams are using machine learning to optimize promotions, compensation and rebates drive the desired behavior across selling channels. Predicting propensity to buy across all channels, making personalized recommendations to customers, forecasting long-term customer loyalty and anticipating potential credit risks of suppliers and buyers are Figure 1 provides an overview of machine learning applications by industry.
Source: Tata Consultancy Services, Using Big Data for Machine Learning Analytics in Manufacturing – TCS
Machine Learning Is Revolutionizing Sales and Marketing   Unlike advanced analytics techniques that seek out causality first, machine learning techniques are designed to seek out opportunities to optimize decisions based on the predictive value of large-scale data sets. And increasingly data sets are comprised of structured and unstructured data, with the global proliferation of social networks fueling the growth of the latter type of data.  Machine learning is proving to be efficient at handling predictive tasks including defining which behaviors have the highest propensity to drive desired sales and marketing outcomes. Businesses eager to compete and win more customers are applying machine learning to sales and marketing challenges first.  In the MIT Sloan Management Review article, Sales Gets a Machine-Learning Makeover the Accenture Institute for High Performance shared the results of a recent survey of enterprises with at least $500M in sales that are targeting higher sales growth with machine learning. Key takeaways from their study results include the following: Why Machine Learning Adoption Is Accelerating
Machine learning's ability to scale across the broad spectrum of contract management, customer service, finance, legal, sales, quote-to-cash, quality, pricing and production challenges enterprises face is attributable to its ability to continually learn and improve. Machine learning algorithms are iterative in nature, constantly learning and seeking to optimize outcomes.  Every time a miscalculation is made, machine learning algorithms correct the error and begin another iteration of the data analysis. These calculations happen in milliseconds which makes machine learning exceptionally efficient at optimizing decisions and predicting outcomes. The economics of cloud computing, cloud storage, the proliferation of sensors driving Internet of Things (IoT) connected devices growth, pervasive use of mobile devices that consume gigabytes of data in minutes are a few of the several factors accelerating machine learning adoption. Add to these the many challenges of creating context in search engines and the complicated problems companies face in optimizing operations while predicting most likely outcomes, and the perfect conditions exist for machine learning to proliferate."
b3fb6cb2c9,"Actuaries at QBE Business Insurance, whose day job is predicting the impact of catastrophes such as earthquakes and floods, have turned their hand to predicting the results of the Autumn rugby internationals. - QBE European Operations","Actuaries at QBE Business Insurance, whose day job is predicting the impact of catastrophes such as earthquakes and floods, have turned their hand to predicting the results of the Autumn rugby internationals. The QBE Predictor is a complex mathematical formula which has been used to simulate each of the 22 Autumn matches 10,000 times. It predicts that England and New Zealand will be the only teams to win all their games. The actuaries, who usually work on predicting the likelihood and impact of events that put businesses at risk, inputted a wide range of variables in their computer model including the number of tries, kicks and conversions scored by the teams, the caps of each captain, latest world ranking and home/away advantage. The fact that England and New Zealand are predicted to emerge as the strongest teams of the Autumn series will underline disappointment that the two teams will not meet this year with their clash being pushed back to autumn 2018. These are the predicted scores for Saturday’s games: Scotland 31 Samoa 22England 38  Argentina 21Wales    20  Australia 24Ireland   20  South Africa 20France  16   New Zealand 35Italy       20   Fiji 27 The highlight of this weekend’s games could be the meeting in Dublin between Ireland and South Africa, which is predicted to go to the wire and would follow the pattern of Ireland’s 2-1 tour defeat in South Africa in 2016 in which every game was within one score. There is not much joy in the predictions for the other home nations with Scotland predicted to win only once against Samoa and Wales destined to win one match, against Georgia. MATTHEW CRANE, Executive Director of Market Management, QBE European Operations, said: “The science behind the Predictor is tried and tested and one of the tools we use to help businesses understand their risks and mitigate against them. It’s not about knowing what will happen, it’s about being prepared for what could happen. While Wales and Scotland are predicted to have a difficult time in the Autumn Internationals they also have a history of pulling the unlikely out of the bag. Anything can happen, that’s why you’ve always got to have a back-up plan, whether you’re a business or a rugby team.” For further information contact:Sandra Villanueva, Corporate Communications, QBE, 020 7105 5284Sandra.villanueva@uk.qbe.comAbout QBEQBE European Operations is part of QBE Insurance Group, one of the world’s leading international insurers and reinsurers and Standard & Poor’s A+ rated. Listed on the Australian Securities Exchange, QBE’s gross written premium for the year ended 31 December 2016, was US$14.3 billion.As a business insurance specialist, QBE European Operations offers a range of insurance products from the standard suite of property, casualty and motor to the specialist financial lines, marine and energy. All are tailored to the individual needs of our small, medium and large client base.We understand the crucial role that effective risk management plays in all organisations and work hard to understand our clients’ businesses so that we offer insurance solutions that meet their needs – from complex programmes to simpler e-trading solutions – and support them in minimising their risk exposures. Our expert risk management and rehabilitation practitioners focus on helping clients improve their risk management so that they may benefit from a reduction in claims frequency and costs. Senior Media Relations Manager Tel: +44
20 7105 5284 sandra.villanueva@uk.qbe.com Senior Media Relations Manager Tel: +44
20 7105 5284 sandra.villanueva@uk.qbe.com Senior Media Relations Manager Tel: +44
20 7105 5284 sandra.villanueva@uk.qbe.com"
2fe65af290,Alibaba's FashionAI shows how machine learning might save the mall,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Alibaba’s sales from Saturday’s Singles Day event exceeded 25 billion dollars, more than quadruple what Americans spent last year during Black Friday. While the majority of those sales undoubtedly came via online purchases, the company also quietly experimented with an AI-powered project designed to woo offline shoppers. FashionAI was developed by Alibaba researchers in order to provide a recognizable interface for customers to use while trying on clothes. It’s a basic screen interface that uses machine-learning to make clothing and accessory suggestions to customers based on the items they are trying on. There’s no camera; it uses information embedded in the item’s tag to make the recommendations. Using the system, customers can try clothes on, receive fashion tips and suggestions from the AI, then make choices on-screen. If a user wants to try something different, or add other items, a store attendant can be summoned with the press of a button. Deep learning allows the AI to make connections in real-time by accessing massive quantities of data and making ‘smart’ decisions. When you’re on Amazon, for example, looking at a pair of cowboy boots might prompt the algorithm to recommend hiking or biker boots. But, search for a cowboy hat too, and you’re likely to be recommended other cowboy themed items, like belts, as opposed to just more hats and boots. The idea is that eventually the AI will get better at determining what you’re going to look at, what you’re going to compare it with, what you’ll want to purchase with it, and which items you’ll actually end up buying. Where no staff of humans could possibly be expected to remember the personal shopping preferences of every single customer, AI can. Alibaba’s FashionAI may have been nothing more than an experiment at this point, but it’s an exciting one. It’s taken the weirdness out of ideas like Amazon’s AI-powered shops and turned it into something more palatable. Most attempts at changing the way brick-and-mortar stores operate have revolved around trying to cram the online experience into kiosks, lobbies, and the checkout area. The trade-off is typically one where customers invest their own time and effort into a system that doesn’t quite work as well as logging on to Amazon or Alibaba with a smartphone does. By placing a screen against a wall in a changing booth, Alibaba integrated its already successful shopping system into the real-world seamlessly. If brick-and-mortar stores are to remain viable, they’ll have to leverage consumer attention by giving us all a reason to leave our phones in our pockets when we shop. People are probably getting sick of being told that Amazon and Alibaba can afford to sell items cheaper because they don’t have overhead. That doesn’t make most of us feel better about being overcharged. It feels like we’re being charged for the ‘privilege’ of standing in line and carrying our own bags through stores and parking lots, at most brick-and-mortar stores. If AI can be harnessed to bring the brick-and-mortar experience more closely in line with shopping on Alibaba.com, we all stand to benefit. Putting up AI-powered screens in dressing rooms is an important first step, and one that may ultimately save a lot of human jobs. Instead of replacing people with robots, Alibaba’s FashionAI integrates humans and machines in a way that provides the customer with everything they need. And the customer is always right.
Alibaba’s AI Fashion Consultant Helps It Set a New Singles’ Day Record
on MIT Technology Review
Read next:
5 tools for improving the efficiency (and conversion power) of your call center
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
451f4c3b13,Microsoft uses machine learning to combat security vulnerabilities - SD Times,"Microsoft is applying machine learning and deep neural networks to its software security approach. The company announced a new research project, neural fuzzing, designed to augment traditional fuzzing techniques, discover vulnerabilities, and learn from past software experiences.
The research is based on Microsoft’s Security Risk Detection tool that incorporates artificial intelligence to find and detect software bugs.
Fuzzing is a software security testing technique used to find vulnerabilities in complex software solutions. “Fuzzing involves presenting a target program with crafted malicious input designed to cause crashes, buffer overflows, memory errors, and exceptions,” Microsoft researchers wrote.
Some fuzz testing categories include: blackbox fuzzers that rely on sample input files generate new inputs, whitebox fuzzers that analyze the target program statically or dynamically to help search for new inputs, and greybox fuzzers that uses a feedback loop to guide their search. Microsoft’s new category, neural fuzzing, uses machine learning models to learn from the feedback loop of a greybox fuzzer, according to the researchers. The team says they have been able to improve the code coverage, code paths, and crashes of four input formats: ELF, PDF, PNG and XML.
“We present a learning technique that uses neural networks to learn patterns in the input files from past fuzzing explorations to guide future fuzzing explorations. In particular, the neural models learn a function to predict good (and bad) locations in input files to perform fuzzing mutations based on the past mutations and corresponding code coverage information,” the researchers wrote in their research.
According to William Blum, from the Microsoft Security Risk Detection engineering team, this is only just the beginning of what can be used when applying deep neural networks to fuzz testing. “We could also use it to learn other fuzzing parameters such as the type of mutation or strategy to apply. We are also considering online versions of our machine learning model, in which the fuzzer constantly learns from ongoing fuzzing iterations,” Blum wrote in a post.
deep neural networks, DNN, fuzz testing, fuzzers, machine learning, Microsoft, Microsoft Security Risk Detection, security vulnerabilities, software security
Christina Cardoza is the News Editor of SD Times. She is responsible for the oversight of the daily news published to the website as well as the company's weekly newsletter, News on Monday. She covers agile, DevOps, AI, machine learning, mixed reality and software security. She is an undeniable nerd who loves Marvel comics and Star Wars. On Follow her on Twitter at @chriscatdoza!
Ready for your SD Times magazine? It's just a click away! Subscribe"
4eed05d6b0,The $1 Million Netflix Challenge - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Earlier this week, Netflix, the online movie rental service, announced it will award $1 million to anyone who can come up with an algorithm that improves the accuracy of its movie recommendation service.
In doing so, the company is putting out a call to researchers who specialize in machine learning–the type of artificial intelligence used to build systems that recommend music, books, and movies. The entrant who can increase the accuracy of the Netflix recommendation system, which is called Cinematch, by 10 percent by 2011 will win the prize.
Recommendation systems such as those used by Netflix, Amazon, and other Web retailers are based on the principle that if two people enjoy the same product, they’re likely to have other favorites in common too. But behind this simple premise is a complex algorithm that incorporates millions of user ratings, tens of thousands of items, and ever-changing relationships between user preferences.
To deal with this complexity, algorithms for recommendation systems are “trained” on huge datasets. One dataset used in Netflix’s system contains the star ratings–one to five–that Netflix customers assign to movies. Using this initial information, good algorithms are able to predict future ratings, and therefore can suggest other films that an individual might like.
Because access to such a dataset is critical to improving the quality of its recommendation systems, the company also released 100 million recommendations–stripped of any personal identifying information–according to Jim Bennett, vice president of recommendations systems at Netflix.
We spoke with Bennett this week about how recommendation systems work–and the challenges of building a better one. Technology Review: Before building a better recommendation system, it would be useful to understand your current approach. How does Cinematch work?
Jim Bennett: First, you collect 100 million user ratings for about 18,000 movies. Take any two movies and find the people who have rated both of them. Then look to see if the people who rate one of the movies highly rate the other one highly, if they liked one and not the other, or if they didn’t like either movie. Based on their ratings, Cinematch sees whether there’s a correlation between those people. Now, do this for all possible pairs of 65,000 movies. TR: So Cinematch would recommend movies to me based on the evaluations of people who rated movies to the way I did. Does that method work for all movies at Netflix? JB: A lot of the really obscure discs, for instance, the “How to Mow a Lawn” DVDs, don’t have very many ratings and this method doesn’t work as well. For movies with a large number of ratings, you do substantially well. But to make it work, there needs to be a lot of data-tuning because people can sometimes have interesting rating patterns.
TR: Like what? JB: For example, there are many people who rate a movie with only one star or five stars. And there are some people who just rate everything with three stars. What you’re looking for is an interesting spread of opinions because you’re trying to capture correlations. That’s the core of the engine.
TR: How do you quantitatively measure the accuracy of your system? JB: We trained Cinematch on 100 million ratings and asked it to predict what the other 3 million would be. We compared ours with the actual answers. We do that every day. We get about 2 million ratings per day and we track the daily fluctuations of the system. We expect to measure submissions to the contest [the same way]. The actual prize dataset is 103 million ratings, but we only released 100 million of them. TR: In order to win the $1 million prize, a new algorithm needs to improve the accuracy of recommendations by 10 percent over Cinematch. You’re also rewarding a $50,000 “progress” prize each year for the algorithm that shows the most improvement over the previous year’s best algorithm, by at least 1 percent. What will these percentage improvements mean to a Netflix customer?
JB: If you go to the website and rate 100 movies for us, the red stars shown under each movie are personalized for you. We use these ratings to adjust the prediction away from the average recommendation, according to your taste. A three-percent difference, for instance, might make a difference of one-quarter star. We have millions of people rating millions of DVDs, and that quarter-star difference helps us sort the list. The individual movie recommendation might not get so much better, but, overall, the set of recommended movies is very different. Move a battleship a little bit, and it makes a huge difference. TR: Why are recommendation systems so hard to improve? JB: One of the reasons is there are no datasets. Many of the machine-learning applications require fairly substantial datasets that easily have millions of data points. There are lots of different approaches to solving the problem, but they all need large datasets. And as with many datasets, once we’ve applied the techniques to those datasets, there’s no place to go. TR: So you’re looking for an algorithm that tackles the problem in a completely different way than Cinematch? JB: Correct. As far as we know, there are many good ideas out in the field. We just can’t test them all. We know that there are people who are really on top of the literature who know the ins and outs of [recommendation systems] and we’d really like to know which ones would be better. TR: What are some approaches, discussed in the literature, which could work, but haven’t been tested with movie recommendations yet? JB: It’s hard to say. There was an article in Science a few months ago [July 28, 2006] that used an interesting combination of two types of neural networks [a computational method that sorts data similar to the human brain]. One neural network supervises the machine learning and the other steers that learning. At Netflix, we look at correlations between ratings, and that’s a linear model. Not all knowledge can be represented by a linear combination of features. This particular model in Science uses a nonlinear approach. I think that technique could be quite good. TR: Are there any other pressing technical challenges at Netflix that might be solved by offering a prize? JB: I wouldn’t want to speculate on more contests. Are there other technical challenges? Absolutely. Beyond the systems challenge of keeping the recommendation engines up and running with an increasing customer base, we also have a huge number of challenges within the company–like trying to ship two millions discs a day to people. And there are interesting challenges ahead as we get ready for the download world [where people can download movies via the Internet]. The company’s filled with tremendous challenges. Want to go ad free?
No ad blockers needed. Kate Greene
I’m a freelance science and technology journalist based in San Francisco. I was the information technology editor at MIT Technology Review from 2005 to 2009, where I wrote more than 350 stories about emerging technologies in areas that include… More computers, mobile devices, displays, communication networks, Internet startups, and more.
I was an integral part of a technology trend-spotting team, highlighting early work in reality mining, plasmonics, adaptable networks, and racetrack memory. I’ve contributed to The Economist,
U.S News & World Report, Gizmodo, New Scientist, Science News, and SELF, among other publications. And I’m currently working on a book with Nathan Eagle called Reality Mining: Using Big Data to Engineer a Better World (MIT Press).
Please read our commenting guidelines.
More videos
Business Impact
Business Impact
Business Impact
Business Impact
How technology advances are changing the economy and providing new opportunities in many industries.
A crowdsourced psychology experiment reveals that when it comes to dishonesty, there are three kinds of people. by
Emerging Technology from the arXiv
Blockchain holds the potential to securely unlock the business and operational value of Internet of Things (IoT) to support common tasks, such as sensing, processing, storing information, and communicating Technology often gets blamed for job losses, but in some areas it’s spurring big-time demand for workers. by
Erin Winick
More from Business Impact
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
{! insider.display.menuOptionsLabel !}
Unlimited online access including articles and video, plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
d4a4890803,Improving the Realism of Synthetic Images - Apple,"Vol. 1, Issue 1 ∙
July 2017
July Two Thousand Seventeen
Most successful examples of neural nets today are trained with supervision. However, to achieve high accuracy, the training sets need to be large, diverse, and accurately annotated, which is costly. An alternative to labelling huge amounts of data is to use synthetic images from a simulator. This is cheap as there is no labeling cost, but the synthetic images may not be realistic enough, resulting in poor generalization on real test images. To help close this performance gap, we’ve developed a method for refining synthetic images to make them look more realistic. We show that training models on these refined images leads to significant improvements in accuracy on various machine learning tasks.
Training machine learning models on standard synthetic images is problematic as the images may not be realistic enough, leading the model to learn details present only in synthetic images and failing to generalize well on real images. One approach to bridge this gap between synthetic and real images would be to improve the simulator which is often expensive and difficult, and even the best rendering algorithm may still fail to model all the details present in the real images.
This lack of realism may cause models to overfit to ‘unrealistic’ details in the synthetic images.
Instead of modeling all the details in the simulator, could we learn them from data?
To this end, we developed a method for refining synthetic images to make them look more realistic (Figure 1). The goal of ‘improving realism’ is to make the images look as realistic as possible to improve the test accuracy. This means we want to preserve annotation information for training of machine learning models. For example, the gaze direction in Figure 1 should be preserved, as well as not generate any artifacts, as machine learning models could learn to overfit to them.
We learn a deep neural network, which we call ‘refiner network’, that processes synthetic images to improve their realism. To learn such a refiner network, we need some real images. One option would be to require pairs of real and synthetic images with pixel-wise correspondence, or real images with annotations—e.g. the gaze information in the case of eyes.
This is arguably an easier problem, but such data is very difficult to collect.
To create pixel-wise correspondence, we need to either render a synthetic image that corresponds to a given real image, or capture a real image that matches a rendered synthetic image.
Could we instead learn this mapping without pixel-wise correspondence, or any labels for the real images? If so, we could just generate a bunch of synthetic images, capture real images of eyes, and without labeling any real images at all, learn this mapping—making the method cheap and easy to apply in practice. To learn our refiner network in an unsupervised manner, we utilize an auxiliary discriminator network that classifies the real and the refined (or fake) images into two classes.
The refiner network tries to fool this discriminator network into thinking the refined images are the real ones.
Both the networks train alternately, and training stops when the discriminator cannot distinguish the real images from the fake ones.
The idea of using an adversarial discriminator network is similar to the GANs (Generative Adversarial Networks [1]) approach that maps a random vector to an image such that the generated image is indistinguishable from the real ones.
Our goal is to train a refiner network—a generator—that maps a synthetic image to a realistic image.
Figure 2 shows an overview of the method. In addition to generating realistic images, the refiner network should preserve the annotation information of the simulator. For example, for gaze estimation the learned transformation should not change the gaze direction.
This restriction is an essential ingredient to enable training a machine learning model that uses the refined images with the simulator’s annotations.
To preserve the annotations of synthetic images, we complement the adversarial loss with a self-regularization L1 loss that penalizes large changes between the synthetic and refined images. Another key requirement for the refiner network is that it should learn to model the real image characteristics without introducing any artifacts.
When we train a single strong discriminator network, the refiner network tends to over-emphasize certain image features to fool the current discriminator network, leading to drifting and producing artifacts. A key observation is that any local patch sampled from the refined image should have similar statistics to a real image patch.
Therefore, rather than defining a global discriminator network, we can define a discriminator network that classifies all local image patches separately (Figure 3).
This division not only limits the receptive field, and hence the capacity of the discriminator network, but also provides many samples per image for learning the discriminator network. The refiner network is also improved by having multiple ‘realism loss’ values per image. The generator can fool the discriminator either with samples from a novel distribution or the target (real data) distribution. Generating from a novel distribution fools the discriminator only until the discriminator learns that novel distribution. The more useful way the generator can fool the discriminator is by generating from the target distribution. Given these two ways of evolving, the easiest is usually to generate a novel output, which is what we observed when training the current generator and discriminator against each other.
A simplified illustration of this unproductive sequence is shown in the left-hand side of Figure 4. The generator and discriminator distributions are shown in yellow and blue respectively. By introducing a history which stores generator samples from previous generations (right-hand of Figure 4), the discriminator is less likely to forget the part of the space about which it has already learned. The more powerful discriminator helps the generator to move toward the target distribution faster. The illustration is a simplification and neglects to show that the distributions are complex and often disconnected regions. In practice however, a simple random replacement buffer captures enough diversity from previous generator distributions to prevent repetition by strengthening the discriminator. Our notion is that any refined image generated by the refiner network at any time during the entire training procedure is really a ‘fake’ image for the discriminator. We found that by constructing a mini-batch for D with half the samples from the history buffer and the other half from the current generator’s output (as shown in Figure 5) we can improve training. We first train the refiner network with only self-regularization loss, and
introduce the adversarial loss after the refiner network starts producing blurry versions of the input synthetic images. Figure 6 shows the output of refiner network at various steps of training.
In the beginning, it outputs a blurry image that becomes more and more realistic as training progresses. Figure 7 visualizes the discriminator and generator losses at different training iterations. Note that the discriminator loss is low in the beginning—meaning it can easily tell the difference between real and refined. Slowly, the discriminator loss increases and the generator loss decreases as training progress, generating more real images. When the synthetic and real images have significant shift in the distribution, a pixel-wise L1 difference may be restrictive. In such cases, we can replace the identity map with an alternative feature transform, putting a self-regularizer on a feature space. These could be hand-tuned features, or learnt features such as a mid layer from VGGnet. For example, for color image refinement, the mean of RGB channels can generate realistic color images such as in Figure 8. To verify that the labels do not change significantly, we manually drew ellipses on the synthetic and refined images, and computed the difference between their centers. In Figure 9, we show a scatter plot of 50 such center differences. The absolute difference between the estimated pupil center of synthetic and corresponding refined image is quite small: 1.1 +/- 0.8px (eye width = 55px). First we initialized G only with the self-regularization loss so that it can start producing blurry version of the synthetic input. Typically it took 500-2,000 steps of G (without training D). We used different number of steps for the generator and discriminator in each training iteration. For hand-pose estimation using depth, we used 2 steps of G for each D step, and for eye gaze estimation experiment we ended up using 50 steps of G for each D step. We found the discriminator to converge more quickly compared to the generator, partly because of the batch-norm in the discriminator. So, we fixed the #D steps to 1, and started varying #G steps starting from a small number, slowly increasing it depending on the discriminator loss values. We found it helpful to keep learning rate very small (~0.0001) and train for a long time. This approach worked probably because it keeps both the generator or the discriminator from making sudden shifts which would leave the other behind. We found it difficult to stop the training by visualizing the training loss. Instead, we saved training images as training progresses, and stopped training when the refined images looked visually similar to real images. To evaluate the visual quality of the refined images, we designed a simple user study where subjects were asked to classify images as real or refined synthetic. The subjects found it very difficult to tell the difference between the real and
refined images. In our aggregate analysis, 10 subjects chose the correct label 517 times out of 1000 trials, meaning they were not able to reliably distinguish real images from refined synthetic ones. In contrast, when testing on original synthetic images vs real images, we showed 10 real and 10 synthetic images per subject, and the subjects chose correctly 162 times out of 200 trials. In Figure 10 we show some example synthetic and corresponding refined images. Figure 11 shows the improvement using refined data, compared to training with original synthetic data. Two things to note from this figure: (1) Training with refined images is better than training with original synthetic images, and (2) using more synthetic data further improves performance. In Figure 12, we compare the gaze estimation error with other state-of-the-art methods, and show that improving the realism significantly helps the model generalize on real test data. Recently there has been a lot of interest in domain adaptation using adversarial training. The image-to-image translation [4] paper by Isola et al. describe a method that learns to change an image from one domain to another, but it needs pixel-wise correspondences. The unpaired image-to-image translation paper [5] discussing relaxing the requirement of pixel-wise correspondence, and follows our strategy of using the generator history to improve the discriminator. The unsupervised image-to-image translation network [6] uses a combination of a GAN and variational auto-encoder to learn the mapping between source and target domains. Costa et al. [7] use ideas from our work to learn to generate images of the eye fundus. Sela et al. [8] use a similar self-regularization approach for facial geometry reconstruction. Lee et al. [9] learn to synthesize an image from key local patches using a discriminator on patches.
For more details on the work we describe in this article, see our CVPR
paper “Learning from Simulated and Unsupervised Images through Adversarial Training” [10]. [1] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative Adversarial Nets. Proceedings Neural Information Processing Systems Conference, 2014. [2] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, Appearance-based Gaze Estimation in the Wild. Proceedings Computer Vision Pattern Recognition Conference, 2015. [3] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
Learning an Appearance-based Gaze Estimator from One Million Synthesised Images. Proceedings ACM Symposium on Eye Tracking Research & Applications, 2016. [4] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, Image-to-Image Translation with Conditional Adversarial Networks. ArXiv, 2016. [5] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ArXiv, 2017. [6] M.-Y. Liu, T. Breuel, and J. Kautz, Unsupervised Image-to-Image Translation Networks. ArXiv, 2017. [7] P. Costa, A. Galdran, M. I. Meyer, M. D. Abràmoff, M. Niemeijer, A. M.Mendonça, and A. Campilho, Towards Adversarial Retinal Image Synthesis. ArXiv, 2017. [8] M. Sela, E. Richardson, and R. Kimmel, Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation. ArXiv, 2017 [9] D. Lee, S.Yun, S. Choi, H. Yoo, M.-H. Yang, and S. Oh, Unsupervised Holistic Image Generation from Key Local Patches. ArXiv, 2017. [10] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, R. Webb, Learning from Simulated and Unsupervised Images through Adversarial Training. CVPR, 2017. Send questions or feedback via email Apply now for a career at Apple Apple Developer Program"
e69421afa2,How newsrooms are using machine learning to make journalists' lives easier | Poynter,"By Gurman Bhatia · August 5, 2015
Tags: Much of what many journalists do every day doesn't involve gathering news.
Just consider the typical process for publishing a story: The reporter reports, writes (or produces) the content, and the editor makes suggestions for revision. Then comes fact-checking and proofreading and other processes focused on polishing the copy. After that are processes geared toward presenting and distributing the story: Selecting a photo, designing art, creating interactives and crafting headlines for social media and the Web that are attuned to search-engine optimization.
But this isn't the only way. Using ""machine learning,"" technologists at news outlets around the world are helping newsrooms eliminate extra time-consuming tasks and giving humans more time to do what they do best: reporting the news. And it's a good thing, too: As significant cuts hit the industry, the need for automation has become more urgent.
The New York Times Research and Development Lab and BBC News Labs are two such places. At both organizations, technologists are using machine learning tools and automation to make journalism less tedious and more valuable. Their aim is to make the information more ""structured"" by using tags within HTML (a.k.a. Web code or markup) or by adding additional metadata to non-textual content such as audio, photo or video. Categorizing content with these tags and metadata pays dividends further down the line, saving journalists time and effort.
How much time? These technologies could potentially eliminate the many browser tabs journalists use during a breaking news event to run searches on Facebook, Twitter, federal databases, Google news or archives.
And that's not all. The New York Times Research and Development Lab announced an experimental project, Editor, last week. The tool sorts through a story in real time looking for people, places, organizations and events and categorizes the text accordingly. In this story, for instance, the tool would have recognized two organizations that have been already been mentioned, namely The New York Times and the BBC.
The first step in this process is recognizing a term that can be categorized, or ""tagged."" The second step is linking that entity to existing databases (internally or externally) or microservices. The third step is making information from those databases accessible to journalists.
Editor currently recognizes locations, people, concepts and organizations as the reporter types in his or her story. It also has a context menu that allows the writer to mark up the title, the byline, pull quotes and key points of the story. Mike Dewar, a data scientist at the lab, calls these semantic tags.
When Dewar uses the term ""semantic,"" he is referring to how machine learning can be used to decipher meaningful connections and relationships within a piece of text.
""I think there could be all different kinds of microservices that service journalism,"" said Alexis Lloyd, creative director of the lab. ""You could imagine microservices that could do things like try to identify quotes from people and ones that could try to find relationships between people and organizations in the text.""
The possibilities are huge. Imagine, if you were quoting from a previous story, the technology helped you verify or link to the source. If you link a microservice with a campaign finance database, you could also access and attach information about a politician's biggest donor as you type in his or her name.
The difficulty comes when Editor is trying to recognize these entities. The computer needs to be able to differentiate between instances that seem similar but are actually different, such as the actor Denzel Washington, the state of Washington or the The Washington Post. Depending on the type of entity, Editor can then apply tags to access different services or tools throughout the system.
Editor, an experiment in publishing from NYT R&D on Vimeo.
Thousands of miles away from The New York Times, the BBC is also developing technology that can be used to make its journalism easier and more valuable.
Until last year, Jacqui Maher was building tools at The New York Times Research and Development Lab. But in January, she moved to London to work at BBC News Labs.
Instead of working on futuristic projects at The Times that were always five years out, she started building things that could be produced and were used on a short-term basis, she said. The maximum time span for her projects is now a year.
Most recently, Maher's team at the BBC has devised something called a structured journalism manifesto. The idea behind it is to use technology and machine learning to scale otherwise cumbersome tasks undertaken by humans.
Structured journalism, though a broad term, could be broken down to two domains: The reporter side, where automation helps improve a journalist's reporting and make it less cumbersome, and the audience side, where the tools help scale things that can improve the reader's experience. While tools such as Editor are built for the journalist, features like The Washington Post's Knowledge Map are outward-facing. When it comes to processes that scale large amounts of work, the latter usually requires the former.
Though Maher's team builds tools for both purposes, many of them come with a hardcore newsroom application. One of them, is called ""Juicer.""
Juicer, a tool that was instituted at the BBC in 2012, is at its core is an aggregator. It recognizes entities similar to the way The New York Times' Editor does, but it doesn't work in real-time and is trained on news sources outside of the BBC as well. The primary aim of of building Juicer was to refine the company's entity extraction abilities, Maher said.
Juicer recognizes tags in stories, something the team refers to as ""semantic entity extraction,"" and uses the company's ""Linked Data"" technology to categorize people, places and organizations.
Here is a demo of the tool in action:
The team has created different prototypes to use Juicer. An experimental news map project uses data from the tool and puts it on a global map. Click on a country and see the latest news that happened there.
The BBC has also integrated some of the Linked Data technology behind Juicer into their in-house content management system, CPS. Vivo, a tool built on top of CPS, lets writers discover recent content on the site under a particular topic.
Imagine a mashup of that with The New York Times' Editor. As you type in a name, say ""Ted Cruz,"" the tool would show you recent articles written about that particular person quickly, providing context in a short amount of time. Lloyd says The New York Times is capable of building such a tool in a future update to Editor.
While most organizations use these tags and structured information within text, the approach can also be used to categorize video or audio. Since the BBC is both a broadcast and digital news organization, staffers there have been using object recognition to classify and tag massive video and audio archives owned by the organization.
Maher says that BBC could make contextual information pop up as readers hover or click over keywords (as with The Washington Post's Knowledge Map,) but its focus is broader than that.
""But where it gets really interesting for the BBC is in our much more massive video and audio output,"" said Maher. ""Imagine as the different topics would deem appropriate – styling and interactions on video. On mobile even. And as for audio, we're just starting to explore what the experience could be.""
By way of example, she cites ""Pop-Up Video,"" a show on VH1 that took music videos and added context and fun facts with quote bubble overlays. Staffers at BBC News Labs have already been running object recognition software on their video archives and tagging instances and elements that are part of a video to build a database of different instances.
""The Editor project does contain key ideas that are now being articulated under the umbrella of 'structured journalism' — namely that having more comprehensive and fine-grained structured data about our reporting will enable us to create all kinds of new tools for journalists and experiences for readers,"" Lloyd said. ""These ideas have been central to our work at the lab for the past couple of years and have been explored in other ways through projects like Lazarus, Madison, and Kepler.""
She sees the Editor project as the future of computational journalism and artificial intelligence. The future of news, she says, is one where newsrooms have collaborative systems between people and machines, ""Where people can do the things that they are uniquely good at and computers can do the things that they are uniquely good at.""
""The industry continues to face significant cuts,"" Maher said. ""We want to do more. We have to do more. We have to reach where people are. It is a complicated task. We want to reach them on radio waves, we want to reach them on their TV sets.""
Although she agrees that the most efficient way is to hire more editors, that's not exactly practical.
""We have to embrace automation where we can,"" she said.
CareerLeadership
Career
I report, write and produce interactives for Poynter.org as the institute's 2015 Google Journalism Fellow. Tweet me @gurmanbhatia or email at gbhatia@poynter.org.
View the discussion thread.
ⓒ All Rights Reserved Poynter Institute 2018"
303dbacc87,Building A.I. That Can Build A.I. - The New York Times,"Google and others, fighting for a small pool of researchers, are looking for automated ways to deal with a shortage of artificial intelligence experts. By CADE METZNOV. 5, 2017
SAN FRANCISCO — They are a dream of researchers but perhaps a nightmare for highly skilled computer programmers: artificially intelligent machines that can build other artificially intelligent machines. With recent speeches in both Silicon Valley and China, Jeff Dean, one of Google’s leading engineers, spotlighted a Google project called AutoML. ML is short for machine learning, referring to computer algorithms that can learn to perform particular tasks on their own by analyzing data. AutoML, in turn, is a machine-learning algorithm that learns to build other machine-learning algorithms. With it, Google may soon find a way to create A.I. technology that can partly take the humans out of building the A.I. systems that many believe are the future of the technology industry. The project is part of a much larger effort to bring the latest and greatest A.I. techniques to a wider collection of companies and software developers. Advertisement The tech industry is promising everything from smartphone apps that can recognize faces to cars that can drive on their own. But by some estimates, only 10,000 people worldwide have the education, experience and talent needed to build the complex and sometimes mysterious mathematical algorithms that will drive this new breed of artificial intelligence. The world’s largest tech businesses, including Google, Facebook and Microsoft, sometimes pay millions of dollars a year to A.I. experts, effectively cornering the market for this hard-to-find talent. The shortage isn’t going away anytime soon, just because mastering these skills takes years of work. The industry is not willing to wait. Companies are developing all sorts of tools that will make it easier for any operation to build its own A.I. software, including things like image and speech recognition services and online chatbots. “We are following the same path that computer science has followed with every new type of technology,” said Joseph Sirosh, a vice president at Microsoft, which recently unveiled a tool to help coders build deep neural networks, a type of computer algorithm that is driving much of the recent progress in the A.I. field. “We are eliminating a lot of the heavy lifting.” This is not altruism. Researchers like Mr. Dean believe that if more people and companies are working on artificial intelligence, it will propel their own research. At the same time, companies like Google, Amazon and Microsoft see serious money in the trend that Mr. Sirosh described. All of them are selling cloud-computing services that can help other businesses and developers build A.I. Advertisement “There is real demand for this,” said Matt Scott, a co-founder and the chief technical officer of Malong, a start-up in China that offers similar services. “And the tools are not yet satisfying all the demand.” This is most likely what Google has in mind for AutoML, as the company continues to hail the project’s progress. Google’s chief executive, Sundar Pichai, boasted about AutoML last month while unveiling a new Android smartphone. Eventually, the Google project will help companies build systems with artificial intelligence even if they don’t have extensive expertise, Mr. Dean said. Today, he estimated, no more than a few thousand companies have the right talent for building A.I., but many more have the necessary data. “We want to go from thousands of organizations solving machine learning problems to millions,” he said.
Please verify you're not a robot by clicking the box. Invalid email address. Please re-enter. You must select a newsletter to subscribe to. View all New York Times newsletters. Google is investing heavily in cloud-computing services — services that help other businesses build and run software — which it expects to be one of its primary economic engines in the years to come. And after snapping up such a large portion of the world’s top A.I researchers, it has a means of jump-starting this engine. Neural networks are rapidly accelerating the development of A.I. Rather than building an image-recognition service or a language translation app by hand, one line of code at a time, engineers can much more quickly build an algorithm that learns tasks on its own. By analyzing the sounds in a vast collection of old technical support calls, for instance, a machine-learning algorithm can learn to recognize spoken words. But building a neural network is not like building a website or some run-of-the-mill smartphone app. It requires significant math skills, extreme trial and error, and a fair amount of intuition. Jean-François Gagné, the chief executive of an independent machine-learning lab called Element AI, refers to the process as “a new kind of computer programming.” In building a neural network, researchers run dozens or even hundreds of experiments across a vast network of machines, testing how well an algorithm can learn a task like recognizing an image or translating from one language to another. Then they adjust particular parts of the algorithm over and over again, until they settle on something that works. Some call it a “dark art,” just because researchers find it difficult to explain why they make particular adjustments. Advertisement But with AutoML, Google is trying to automate this process. It is building algorithms that analyze the development of other algorithms, learning which methods are successful and which are not. Eventually, they learn to build more effective machine learning. Google said AutoML could now build algorithms that, in some cases, identified objects in photos more accurately than services built solely by human experts. Barret Zoph, one of the Google researchers behind the project, believes that the same method will eventually work well for other tasks, like speech recognition or machine translation. This is not always an easy thing to wrap your head around. But it is part of a significant trend in A.I. research. Experts call it “learning to learn” or “meta-learning.” Many believe such methods will significantly accelerate the progress of A.I. in both the online and physical worlds. At the University of California, Berkeley, researchers are building techniques that could allow robots to learn new tasks based on what they have learned in the past. “Computers are going to invent the algorithms for us, essentially,” said a Berkeley professor, Pieter Abbeel. “Algorithms invented by computers can solve many, many problems very quickly — at least that is the hope.” This is also a way of expanding the number of people and businesses that can build artificial intelligence. These methods will not replace A.I. researchers entirely. Experts, like those at Google, must still do much of the important design work. But the belief is that the work of a few experts can help many others build their own software. Renato Negrinho, a researcher at Carnegie Mellon University who is exploring technology similar to AutoML, said this was not a reality today but should be in the years to come. “It is just a matter of when,” he said. Follow Cade Metz on Twitter: @CadeMetz. A version of this article appears in print on November 6, 2017, on Page B1 of the New York edition with the headline: This A.I. Can Build A.I. Itself.
Order Reprints| Today's Paper|Subscribe
We’re interested in your feedback on this page. Tell us what you think. Go to Home Page »"
4f8fccc395,Scientists Use Machine-learning to Analyze Language in Movies  | Technology Networks,"News   Nov 14, 2017
| Original story from University of Washington
In the movie Frozen, only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of agency and power as Cinderella, a movie character that debuted in 1950. Credit: University of Washington
At first glance, the movie ""Frozen"" might seem to have two strong female protagonists -- Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom.But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists.The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed.""'Frozen' is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,"" said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies.""Anna is actually portrayed with the same low levels of power and agency as Cinderella, which is a movie that came out more than 60 years ago. That's a pretty sad finding,"" Sap said.The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like ""Heathers"" to romantic comedies like ""500 Days of Summer"" to war films like ""Apocalypse Now.""In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men. For example, male characters spoke more in imperative sentences (""Bring me my horse"") while female characters tended to hedge their statements (""Maybe I am wrong""). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives.To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on ""connotation frames"" that give insights into how different verbs can empower or weaken different characters through their connotative meanings. The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments.The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3.""For example, if a female character 'implores' her husband, that implies the husband has a stance where he can say no. If she 'instructs' her husband, that implies she has more power,"" said co-author Ari Holtzman, an Allen School doctoral student. ""What we found was that men systematically have more power and agency in the film script universe.""Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose.Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb's subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies.The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters -- exposing subtle differences and biases.In 2010's ""Black Swan,"" a movie centered around a female lead -- a perfectionist ballerina who slowly loses grip on reality -- the movie's dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film.In the 2007 movie ""Juno,"" about an offbeat young woman who unexpectedly gets pregnant, male characters' scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue.The UW team's tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man.The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers.""We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,"" said co-author and Allen School doctoral student Hannah Rashkin.Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn't limited to movies, but could be applied to books, plays or any other texts.""We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,"" said senior author Yejin Choi, an associate professor in the Allen School. ""We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.""This article has been republished from materials provided by University of Washington. Note: material may have been edited for length and content. For further information, please contact the cited source.
SLIMS Users Can Now Connect to the iSpecimen Marketplace
Genohm and iSpecimen have jointly announced that Genohm has established a partnership with iSpecimen to enable users of the SLIMS laboratory information management system to seamlessly connect to the iSpecimen Marketplace.
Ovation and Coriell Life Sciences Partner to Streamline Genetic Reporting Workflows
Ovation.io, Inc. (Ovation), makers of the fastest-growing clinical laboratory information and commercialization platform, and Coriell Life Sciences, Inc., an innovative provider of clinical genetic reporting solutions, announced a partnership and platform integration designed to create a seamless ecosystem for sample and workflow management, genetic data interpretation, and external client communication.
Blue Brain Nexus: An Open-Source Tool for Data-Driven Science
The Blue Brain Nexus will help enable data-driven neuroscience through searching, integrating and tracking large-scale data and models.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 11, 2018 Video News   Jan 11, 2018 News   Jan 11, 2018 Article Article"
c1d36f1622,Microsoft’s machine learning can predict injuries in sports,"Microsoft is bringing its big-data knowledge to sports. Today, the company introduced its new Sports Performance Platform, an analytics system that aims to help teams track, improve and predict their players performance using machine learning and Surface technology. Created by Microsoft Garage, the group responsible for the tech giant's offbeat innovations, the project is designed to make coaches better understand player data and find ways to turn that into actionable insights. Microsoft's Sports Performance Platform can, for example, figure out when a player is at risk of injury, based on his or her most recent performance and recovery time. The company says one of the main benefits to its sports analytics tool is that it's powered by proprietary business tools such as Power BI, a cloud-based intelligence suite also used on products like Excel, as well as Azure and, of course, Surface computers. ""Imagine making clutch decisions that are based on insight, rather than gut,"" said Jeff Hansen, general manager of Microsoft Brand Studio. ""The difference between a win or a loss can be decided by an extra five minutes of wind sprints, levels of hydration or getting to bed 30 minutes earlier the night before."" Professional teams such as the Seattle Reign FC (US, National Women's Soccer League) and Real Sociedad (Spain, La Liga) are already taking advantage of the Sports Performance Platform. But Microsoft says its goal is to expand beyond the pros and bring these tools to other levels, which could benefit school programs and amateur coaches and players. While Microsoft is calling the platform an experiment right now, it is seeking ""sports organizations and partners"" that may be interested in being involved as it's developed. Let's just hope everyone who uses it doesn't feel the same way NFL players and coaches do about the Surface on the sidelines.
Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
74e7344b2e,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
f87990f6ee,"
            This A.I. Chatbot Will Get Revenge on Email Scammers For You | Big Think
        ","$12 billion is lost globally to phishing scams. While you may believe you could easily figure out that the email from your grandma who is desperately asking you for money is not really an email from your grandma, not all phishing scams are that obvious and many people fall for them. In fact, a 2015 survey done by Intel Security covering 19,000 respondents from 144 countries, revealed that a staggering 80% misidentified at least one phishing email. Now, Netsafe, a non-profit organization in New Zealand with a focus on online safety, is fighting back. With their new initiative called Re:scam, Netsafe has deployed a well-educated, artificially intelligent chat-bot that can take on multiple personalities and engage in correspondence with scammers, wasting their time indefinitely or until the scammers themselves realize they are being scammed.
The exchanges can be hilarious. Here is a sample of the bot's replies from a rather long email thread between the bot and a scammer posing as The Bureau of African Affairs and looking to extract personal and bank details.  “Hi, was this letter supposed to go to me? It all seems quite a wee bit official like. I’ve never been beneficiaried before. Just want to make sure I’m who I am before I get too excited.” “How soon can I expect to receive these funds? I owe a pretty significant amount to Readers Digest and need to pay them back before they take legal action.” “Do you mean your business days or our business days? What time zone are you in?” “I understand the urgency. Time is money as they say. Does that make ATM’s time machines? Just a thought I had. Anyway. Keen to move this along.” Engaging with the bot leaves less time for scammers to engage with real people. In addition, the emails that the chat bot receives help to develop its vocabulary and knowledge of scams. These emails also help with the collection of data about scammers’ locations and activities. Netsafe’s CEO Martin Cocker says: “Everyone is susceptible to online phishing schemes and no matter how tech savvy you are, scammers are becoming increasingly sophisticated. Re:scam will adapt as the scammers adapt their techniques, collecting data that will help us to keep up and protect more people across New Zealand.” In just a few days Re:scam has already sent 26,609 emails and wasted more than 3 months worth of scammers’ time. So far, the longest exchange between a scammer and a chatbot was 20 emails long. To help the fight against scammers, instead of deleting your scam emails, you can forward them (old or new) to me@rescam.org and Netsafe will engage in a conversation on your behalf from a proxy email account. They will even send you a summary of the conversation the bot has had with the scammer and, as Netsafe writes on their website, “sometimes they can be quite funny!” Here's a great example of one below:
Big Think Edge helps organizations by catalyzing conversation around the topics most critical to 21st century business success. Led by the world’s foremost experts, our dynamic learning programs are short-form, mobile, and immediately actionable."
017117b55b,"Tim Cook is a fan of VisualDx, a $100 a year machine learning app for skin diagnosis - Business Insider","Visual Dx
Apple CEO Tim Cook isn't a doctor, but he talked about a piece of medical software, VisualDx, during Apple's most recent earnings call. It was an interesting choice of an app to highlight. Apple has deep ambition to break into the health and medical worlds, but although VisualDx is available to consumers through the Apple App Store, it's not really an app for the public. It's targeted at trained and credentialed doctors who can use it to help diagnose skin conditions and disorders.  This fall, the app has gotten a new trick — it can use an iPhone's camera and machine learning to automatically identify skin conditions, or at least narrow down what they could be. Snap a picture in the app, and it will return a list of conditions the rash could be, based on decades of medical research. ""VisualDx is even pioneering new health diagnostics with Core ML, automating skin image analysis to assist dermatologists with their diagnoses,"" Cook said.  In some ways, VisualDx offers a hint of the future of medicine, where a hot topic of conversation is whether sufficiently advanced computers and artificial intelligence could automate one of the core parts of what doctors do: identifying what the problem is.  In the future, some of this technology will trickle down to consumers, giving them the ability to snap photos of their own bodies, answer some questions, and ultimately figure out whether it's a problem that requires medical attention, or simply a common rash, VisualDx CEO and medical doctor Art Papier tells Business Insider. VisualDx is currently developing a version of this tool, called Aysa, for common skin disorders. ""Consumers don't want to play doctor themselves. But on a Saturday, they want to know, do I need to go to the emergency room with my child or can this wait until Monday when I could see my pediatrician,"" Papier said.  ""It's really education and triage. It's not diagnosis, we don't believe in that,"" he continued, ""At least in the next few years, we're not going to tell patients you're totally OK, you don't need to see a doctor.""  Dr. Art Papier.RochesterThe reason why Apple's CEO mentioned VisualDx is because it's using CoreML, a new set of software that makes it possible to run machine learning algorithms on a phone, instead of uploading the photos online to a server for processing.
""Our clients are hospitals and they really don't like the idea of a doctor in their hospital taking a picture of a patient and then sending the picture to a third party or a private company,"" Papier said.  ""We realized when Apple announced CoreML in June, they announced that you can move your models onto the iPhone,"" he continued. ""Now the image gets analyzed on the phone, and the image never goes up to the cloud to us. We never see the image."" Even still, the software can return an analysis in a second on a newer iPhone. The identification neural network is ""trained"" by researchers at VisualDx, but it can run on a phone, Papier said.  The models are trained using VisualDx's own library of professional medical images, Papier said.  ""We're not like a wiki model where you know anyone can upload images to us and just tell us what they think they are,"" Papier said. ""We're very very selective to work with the universities and individual physicians we know are experts."" Many of VisualDx's images were scanned from old collections of slides and film, from leading departments and doctors. It's built a library of 32,000 images to train its models. ""We developed this reputation as somebody that was going to preserve the legacy of medical photography,"" Papier said.  Still, even with high-quality models and training data, Papier doesn't think completely automated diagnosis will happen anytime soon. ""The hype cycle right now for machine learning is off the charts,"" he said.  ""Machine learning will get you into a category on this, but to get to the final mile, you have to ask the patient, 'did you take a drug a week ago?' 'Did you travel?'"" he said.  Medical professionals can download the app from the App Store. If your institution doesn't have a subscription, various in-app purchases range from $99 for a year's access to DermExpert. A complete purchase with access to other medical information is $499 per year.  You can sign up for the Aysa mailing list here.   Advances in machine learning now mean that...
Retirement is lasting longer than ever — here's how to prepare financially
More ""Finance"" »
Here's the event that has global brands debating disruptive technology
More ""Strategy"" »
Get the best of Business Insider delivered to your inbox every day."
ef458edc49,Market Sensei Launches Machine Learning-Powered Stock Market Prediction Platform for Novice and Experienced Investors | Markets Insider,"CAMBRIDGE, Mass., Aug. 31, 2017 /PRNewswire/ -- Machine learning technology provider Expat Inc. (www.expatinc.com) today announced the launch of Market Sensei (www.StockMarketSensei.net), a stock market prediction platform that features patented machine learning algorithms that give powerful prediction abilities previously unavailable to both novice investors and experienced traders. The launch of the release follows a beta period where the service attracted several thousand avid users who provided Expat Inc. with valuable feedback about various capabilities and the user interface, allowing the company to make key refinements. Market Sensei features an accessible UI that makes it usable by novice investors and experienced traders alike. It is powered by patented algorithms that combine various metrics to create a predicted low, high, opening, and closing price for thousands of stocks which are clearly illustrated via the app or website interface. Market Sensei offers reliable predictions for the thousands of stocks listed on the NYSE, NASDAQ, and AMEX markets. Tied into the predictions are Market Sensei's alerts that can be setup to warn the user any time a stock goes past a certain buy or sell price, allowing them to act immediately. ""We're thrilled to bring Market Sensei out of beta and offer a refined and easy-to-use solution to the investing public,"" said Patrick Kwete, CEO of Expat Inc. ""The complexity of the system was all placed in the back end, so users can receive reliable stock predictions at a glance. Our platform allows users to make daily or long-term strategic investing decisions that are based on predictive data."" A core competitive differentiator for Market Sensei is the inherent accountability that is built within the platform. Every stock's prediction data also includes a graphical representation of Market Sensei's prediction accuracy for that stock within the past nine days. This approach contrasts sharply to financial analysts who release reports but do not have reliable data regarding the accuracy of their predictions. Market Sensei provides users with both accountability and accuracy in a user-friendly structure. ""Market Sensei offers investors a reliable and completely transparent prediction tool for thousands of stocks. It's powerful because people can use it not only to initiate trades but also use it as a tool to judge various investing strategies. And we are the first prediction tool to be completely transparent and accountable, as we show users a dynamic accuracy score so they can see our performance over time."" Market Sensei is available for free download and includes a free trial with which users have unlimited access to its daily analysis features and the full computational power of the platform. After the free trial, successful users retain access to the platform for $19.95 per month.
Market Sensei is available for iOS (https://itunes.apple.com/us/app/market-sensei-stock-predictions/id1191375706) and Android (https://play.google.com/store/apps/details?id=com.kwete.marketsensei&hl=en ) devices as well as on the web. at www.stockmarketsensei.net .
About Expat Inc.Expat Inc. is a pioneer in machine learning technology that develops various solutions for internal usage and business partners that add value to everyday life. The company's primary product is Market Sensei, a reliable stock market prediction platform that offers low, high, open, and close price predictions for thousands of U.S. market stocks. The platform runs on Expat's patented machine learning technologies that can turn vast amounts of financial and individual company data into actionable predictions. For more information, visit www.expatinc.com.
CONTACT: Courtney Cohen, 312 750 0871, ccohen@sspr.com   View original content:http://www.prnewswire.com/news-releases/market-sensei-launches-machine-learning-powered-stock-market-prediction-platform-for-novice-and-experienced-investors-300512043.html SOURCE
Expat Inc. Your Personalized Market Center"
78f71ebaef,"Dstl Satellite Imagery Competition, 1st Place Winner’s Interview: Kyle Lee | No Free Hunch","Kaggle Team|04.26.2017 Dstl's Satellite Imagery competition, which ran on Kaggle from December 2016 to March 2017, challenged Kagglers to identify and label significant features like waterways, buildings, and vehicles from multi-spectral overhead imagery. In this interview, first place winner Kyle Lee gives a detailed overview of his approach in this image segmentation competition. Patience and persistence were key as he developed unique processing techniques, sampling strategies, and UNET architectures for the different classes.
During the day, I design high-speed circuits at a semiconductor startup - e.g. clock-data recovery, locked loops, high-speed I/O, etc. - and develop ASIC/silicon/test automation flows.
Kyle on Kaggle. Even though I don’t have direct deep learning research or work experience, the main area of my work that has really helped me in these machine/deep learning competitions is planning and building (coding) lots and lots of design automation flows very quickly. The key competition that introduced me to the tools and techniques needed to win was Kaggle’s “Ultrasound Nerve Segmentation” that ended in August 2016 (and I saw many familiar names from that competition in this one too!).
Knowledge accumulated from vision/deep learning related home projects and other statistical learning competitions has also helped me in this effort.
Patience picked up from running and tweaking long circuit simulations at work over days/weeks were transferable and analogous to neural network training too.
Like many of the competitors, I didn’t have direct experience with multi-spectral satellite imagery. I joined Kaggle after first trying to improve my 3-layer shallow networks on Lasagne for single-board computer (SBCs, e.g. Raspberry Pi) stand-alone inferencing/classification systems for various home/car vision hobbyist projects, and wanted a more state of the art solution.
Then I came across Kaggle’s State Farm Distracted Driver contest, which was a perfect fit.
This was after completing various online machine learning courses - Andrew Ng’s Machine Learning course, Geoffrey Hinton’s course on Neural Networks, to name a few. This was early 2016 - and it’s been quite a journey since then! As I mentioned earlier I participated in one of the earliest segmentation challenges on Kaggle - the “Ultrasonic Nerve Segmentation”.
In that competition, I was ranked 8th on the public leaderboard but ended up as a 12th on private LB - a cursed “top silver” position (not something any hard worker should get!). Immediately after that I was looking forward to the next image segmentation challenge, and this was the perfect opportunity. More importantly, I joined to learn what neural/segmentation networks have to offer apart from medical imaging, and to have fun!
Over the course of the competition, I definitely achieved this goal since this competition was extra fun - viewing pictures of natural scenery is therapeutic and kept me motivated everyday to improve my methodology.
In summary my solution is based on the following: A VISUAL OVERVIEW OF THE COMPLETE SOLUTION (ALL CLASSES) I performed registration of A and M images, and used sliding window at various scales.
In addition, I also oversampled some of the rare classes in some of the ensemble models.
The sliding window steps are shown below: PATCH DISTANCE FOR OVERSAMPLED CLASSES Oversampling standing water and waterway together was a good idea since it helped to reduce the amount of class confusion between the two, with reduced artifacts (particularly for standing water predictions). As far as band usage is concerned, I mostly used panchromatic RGB + M-band and some of the SWIR (A) bands.
For the A-bands I mostly did not use all the bands, but randomly skipped a few bands to save training time and RAM. As mentioned earlier, for vehicles I trained and predicted only on patches/windows with roads and/or buildings - this helped to cut down the amount of images needed for training, and allowed for significant oversampling of vehicle patches.
This scheme was applied also on test images, so results are pipelined as you can see from the flowchart. Finally, preprocessing involved the use of mean/standard deviation normalization using the training set - in other words, each training/validation/test patch was subtracted by the mean and divided by the standard deviation of the training set only. The UNET segmentation network from the “Ultrasonic Nerve Segmentation” competitions and other past segmentation competitions was widely used in my approach, since it is the most easily scalable/sizeable fully convolutional network (FCN) architecture for this purpose.
In fact, if I am not mistaken, most - if not all - of the top competitors used some variant of the UNET. I made tweaks to the original architecture with batch-normalization on the downstream paths + dropout on the post-merge paths, and all activation layers switched to Exponential Linear Unit (ELU).
Various widths (256x256, 288x288, etc.) and depths were used depending on the various classes via cross-validation scores.
For example, in my experiments, the structure class converged best - both in terms of train time and CV - with a UNET that had a wider width (288x288) and a shallow depth (3 groups of 2x conv layers + maxpool). VARIOUS UNET ARCHITECTURES FOR DIFFERENT CLASSES Overall, I generated 40+ models of various scales/widths/depths, training data subsamples, and band selections. FULL MODEL (WIDTH/DEPTH, SAMPLING, BANDS)
LISTING OF ALL CLASSES FULL MODEL (WIDTH/DEPTH, SAMPLING, BANDS)
LISTING OF ALL CLASSES In terms of cross validation, I used a random patch split of 10-20% across images (depending on class, the rarer the larger).
For oversampled classes only 5% random patch were used.
Only one fold per model was used to cut down on runtime in all cases. Training set was train-time augmented (both image+mask) with rotations at 45 degrees, 15-25% zooms/translations, shears, channel shift range (some models only), and vertical+horizontal flips.
No augmentation with ensembling was performed on validation or test data.
Optimization wise I used the Jaccard loss directly with Adam as optimizer (I did not get much improvement from NAdam).
I also had a learning rate policy step which dropped the learning rate at around 0.2 of the initial rate for every 30 epochs. Ensembling involved the use of mask arithmetic averaging (most classes), unions (only on standing water and large vehicles), intersections (only on waterways using NDWI and CCCI). My understanding is that most competitors had either weak public or private scores with standing water and vehicles, which I spent extra effort to deal with in terms of pre- and post-processing.
I believe stabilizing these two (actually three) classes - standing water, large and small vehicles made a large impact on my final score relative to other top competitors. Standing Water Versus Waterways For standing water, one of the main issues with standing water was class confusion with waterways.
As described earlier, oversampling both standing water and waterways helps to dissolve waterway artifacts in standing water UNET predictions, but there was still a lot of waterway-like remnants, as shown below in raw ensembled standing water predictions:
EXAMPLES OF MISCLASSIFIED POLYGONS IN STANDING WATER The key to resolving this was to realize that from a common sense perspective - waterways always touch the boundary of the image, while standing water mostly does not (or has a small overlap area / dimension only).
Moreover, the NDWI mask (generated as part of waterways) could be overlapped with the raw standing water predictions, and very close broken segments could be merged (convexHull) to form a complete contour that may touch the boundary of the image.
In short, boundary contact checking for merged water polygons was part of my post-processing flow which pushed some misclassified standing water images into the waterway class. Vehicles - Large and Small The other important classes which I spent a chunk of time on were the two vehicle classes.
Firstly, I noticed - both on the training data and just simply common sense - is that vehicles are almost always located on or near roads, and near buildings.
EXAMPLES OF SMALL VEHICLES RELATIVE TO ROADS AND BUILDINGS By restricting training and prediction to only patches containing buildings and roads, I was naturally able to allow for oversampling of vehicle patches, and narrow down the scope of scenery for the network to focus on.
Moreover, I chose only RGB images, since in all other bands vehicles were either not visible, or displaced significantly). Secondly, many vehicles were very hard to distinguish between large and small classes both in terms of visibility (blurred) and mask areas.
For reference, their mask areas from training data are shown in the histogram below, and there is a large area overlap between large and small vehicles from around 50-150 pixels^2. CONTOUR/MASK AREA HISTOGRAM OF SMALL VS LARGE VEHICLES To deal with this, I trained additional networks merging both small+large vehicles, and took the union of this network with large vehicle only network ensemble.
The idea is that networks that merge both small+large are able to predict better polygons (since there is no class confusion).
I then performed area filtering of this union (nominally at 200pixel^2) to extract large vehicles only.
For small vehicles, it was basically just to take the average ensemble of small vehicle predictions, and remove whichever contours overlapped with large vehicles and/or over the area threshold.
Additionally, both vehicle masks were cleaned by negating their masks with buildings, trees, and other classes. Post-competition analysis showed that this approach helped large vehicle private LB score - which if I did not, would have dropped by -59%.
On the other hand small vehicles did not have any improvement from the area threshold removal process above. Surprisingly, waterways could well be generated using simple and fast index methods.
I ended up with a intersection of NDWI and CCCI masks (with boundary contact checking to filter out standing water / building artifacts) rather than using deep learning approaches, thus freeing up training resources for other classes.
The public and private LB score for this class seemed competitive relative to other teams who may have used deep learning methods. Finally, here is my CV-public-private split per class.
FINAL LOCAL CV, PUBLIC LB, and PRIVATE LB PER CLASS COMPARISONS The asterisk (*) for private LB score on crops indicate a bug with OpenCV’s findContours, that if I had used the correct WKT generating script for that class I would have had a crop private LB score of 0.8344 instead of 0.7089.
As a result this solution could have achieved an overall private LB score of 0.50434 (over 0.5 - yay!) rather than 0.49272. The bug had to do with masks spanning the entire image not being detected as a contour - I had only found this out after the competition and would have done a WKT mask dump ‘diff’ if I had the time.
All other classes were using the correct shapely versions of the submission script. My guess is that my vehicle and standing water scores (combined) were the ones that made a difference in this competition, since the other top competitors had either weak vehicle scores or weak standing water scores.
Keras with Theano backend + OpenCV / Rasterio / Shapely for polygon manipulation.
No pretrained models were used in the final solution, although I did give fine-tuned (VGG16) classifier-coupling for merged vehicle networks a shot - to no avail. Since this was a neural network segmentation competition, most of time (80%+) was spent on tuning and training the different networks and monitoring the runs.
The remaining (20%) was on developing the post and pre-processing flows.
From a per class effort perspective, I spent over 70% of the overall time on vehicles, standing water, and structures, and I spent the least time on crops. In terms of submissions, I used a majority of the submissions trying to fine tune polygon approximation.
I first tried bounding boxes, then polygon approximation, and then polygon with erosion in OpenCV.
Ultimately, I ended up using rasterio/shapely to perform polygon to WKT conversion.
All classes (except trees) had no approximation, while trees were first resized to 1550x1550 - effectively approximating the polygons - before being converted to WKT format.
I used three desktops for this contest.
The first two were used for all the training/inferencing of all classes, while the last one (#3) was only run on crops. It took about three days to train and predict - assuming all models and all preprocessing scales can be run in parallel.
One day for preprocessing, one day to train and predict, and another day to predict vehicles and generate submission.
Once again, thank you to Dstl and Kaggle for hosting and organizing this terrific image segmentation competition - I believe this is by far the most exciting (and busy, due to the number of classes) competition I have had, and I am sure this is true for many others too.
It’s always interesting to see what neural networks can accomplish with segmentation - first medical imaging, now multi-spectral satellite imagery! I personally hope to see more of these type of competitions in the future.
A lot of experience training neural networks - particularly segmentation networks,
working with multi-spectral images, and improving on traditional computer vision processing skills.
Some of the solution sharing by the top competitors were absolutely fascinating as well - especially clever tricks with multi-scale imagery in a single network. I would have added some ensembling to crops, added heat-map based averaging (and increase the test overlap windows at some expense of runtime), dilated structures training mask (which helped structure scoring for some competitors), and removed most of the expensive rare scale (320x320, for example) ensembling on tracks.
I would also have fixed the contour submission issue on crops had I caught that earlier. Nothing beats learning by practice and competition, so just dive in a Kaggle competition that appeals to you - whether it be numbers, words, images, videos, audio, satellite imagery, etc. (and that you can commit to early on if you want to do well).
Moreover, data science is an ever evolving field.
In fact, this field wasn’t even on the radar a decade ago - so be sure to keep to date on the architectural improvements year-by-year.
Don’t worry, most other competitors are starting on the same ground as you, especially with some of the new developments. Having more systems helps in terms of creating experiments and ensemble permutations, but it’s not absolutely necessary if you have a strong flow or network.
However, for this particular competition, having >= 2 GPU systems will definitely help due to the sheer number of classes and models involved. Most importantly, have fun during the competitions - it won’t even feel like work when you are having fun (!) Having said that, I am still a beginner in many areas in data science - and still learning, of course.
Kyle Lee works as a circuit and ASIC designer during the day. He has been involved in data science and deep learning competitions since early 2016 out of his personal interest for automation and machine learning.
He holds a Bachelor’s degree in Electrical and Computer Engineering from Cornell University.
@kyle lee .. Can you share the code ? Thank you for the great post. Could you please share how did you manage your time schedule?
I had pretty big problems with this. During the day traditional Machine Learning at work. Coming home and spending rest of the evening, working on this problem. Alarm at night to check how does network training go, and reset something if necessary. In the morning analysis of what was generated during the night. Very exhausting.
As a result of the lack of the free time we completely ignored Vehicle classes in our prediction 🙁 How do you balance your work / life / kaggle within a day? Are you allowed to dedicate some part of your work time to the side projects like Kaggle? (similar to the 20% work time Google Policy) (reposting - noticed the spam classifier on Disqus pushed my post as spam 🙁 ) Great question and relevant to all those who have at least somewhat demanding full-time day jobs! 1. I started the competition quite early, about two weeks from the launch date in December - this gave me a bit more time to play around with the pipeline and actually most of my actual work was done in the first two months. 2. VPN. I set up a VPN some time back (at first, mainly to monitor motion video feeds), which gave me access to my home network while I was away - including VNC access on all my systems. I used it extensively during this competition to log back home (e.g. during breaks) and monitor/restart training, or try slightly different architectures or folds. It's pretty slow - I use mobile - so no heavy duty work, but it's much better than being blind to the training process and unable to do anything until the night, and waste a day's worth of training. Unfortunately, I don't get extra time at work to officially work on this. This can be a thorny issue if corporate is strict about doing anything other than work (e.g. browsing online, replying to phone messages, etc.), but in my use case it was closer to ""replying to phone messages"" 😉 since the connection isn't fast enough for lots of lines of code anyway. 3. I lived relatively unhealthily the last few weeks of the competition - something like 3-4 hours on this problem on weekdays (9pm till 1 or 2am), and waking up 6+. I only got around to heavy duty work (viewing images, post-processing flow development) at night. 4. So my schedule was something like this: a. Evenings - 9pm to 1-2am - analysis, flow (pre/post-processing improvements), etc. Mornings ~ 1 hour. Night - my VNC is on the phone, so I woke up to check the results (on the phone) and went back to sleep if there was nothing interesting. b. During the day - monitor, train/retrain (via mobile VPN/VNC) c. Weekends - 6-8 hours per day Private Leaderboard per class. Top 5 teams.
https://docs.google.com/spreadsheets/d/1wi0ydVfaZ9hRFT8-Wgw0FSpLv55QPJclgyZZUXuz8Es/edit?usp=sharing This is really awesome, never thought about how standing water could register different. @Kyle Can you please share the code? Thanks 🙂 ""Optimization wise I used the Jaccard loss directly"" I'm confused about this, because in the 3rd place write up they state ""Jaccard Index is not differentiable"", and they created an approximation to Jaccard: http://blog.kaggle.com/2017/05/09/dstl-satellite-imagery-competition-3rd-place-winners-interview-vladimir-sergey/ Does Keras approximate the gradients if it cannot be computed directly? Sorry for missing this detail, but I too used an approximation of the Jaccard loss - non-differentiable loss functions do not work for Theano/Tensorflow backends (same thing with Dice loss). Thanks for your help - also just wondering how did you calculate your local CV/public/private split per class? Are these percentages? These are based on a per-class Jaccard index.
If you haven't looked at the competition page in more detail...https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection#evaluation Public/private calc - have a look at this: https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/discussion/32197#197106.
Competition evaluation is average of all per-class Jaccards so submitting just one class (the others being empty/null) will give the per class (divided by 10) score.
Local CV is random patch/chip split. @disqus_evtmnmc6fq:disqus Have you written any document/paper of your approach? if yes, can you please share that paper?
Also, if you can, please share your code. Also, what were the main challenges you faced while doing this project that could be addressed as future work?"
5e978e4e91,"Tim Cook is a fan of VisualDx, a $100 a year machine learning app for skin diagnosis - Business Insider","Visual Dx
Apple CEO Tim Cook isn't a doctor, but he talked about a piece of medical software, VisualDx, during Apple's most recent earnings call. It was an interesting choice of an app to highlight. Apple has deep ambition to break into the health and medical worlds, but although VisualDx is available to consumers through the Apple App Store, it's not really an app for the public. It's targeted at trained and credentialed doctors who can use it to help diagnose skin conditions and disorders.  This fall, the app has gotten a new trick — it can use an iPhone's camera and machine learning to automatically identify skin conditions, or at least narrow down what they could be. Snap a picture in the app, and it will return a list of conditions the rash could be, based on decades of medical research. ""VisualDx is even pioneering new health diagnostics with Core ML, automating skin image analysis to assist dermatologists with their diagnoses,"" Cook said.  In some ways, VisualDx offers a hint of the future of medicine, where a hot topic of conversation is whether sufficiently advanced computers and artificial intelligence could automate one of the core parts of what doctors do: identifying what the problem is.  In the future, some of this technology will trickle down to consumers, giving them the ability to snap photos of their own bodies, answer some questions, and ultimately figure out whether it's a problem that requires medical attention, or simply a common rash, VisualDx CEO and medical doctor Art Papier tells Business Insider. VisualDx is currently developing a version of this tool, called Aysa, for common skin disorders. ""Consumers don't want to play doctor themselves. But on a Saturday, they want to know, do I need to go to the emergency room with my child or can this wait until Monday when I could see my pediatrician,"" Papier said.  ""It's really education and triage. It's not diagnosis, we don't believe in that,"" he continued, ""At least in the next few years, we're not going to tell patients you're totally OK, you don't need to see a doctor.""  Dr. Art Papier.RochesterThe reason why Apple's CEO mentioned VisualDx is because it's using CoreML, a new set of software that makes it possible to run machine learning algorithms on a phone, instead of uploading the photos online to a server for processing.
""Our clients are hospitals and they really don't like the idea of a doctor in their hospital taking a picture of a patient and then sending the picture to a third party or a private company,"" Papier said.  ""We realized when Apple announced CoreML in June, they announced that you can move your models onto the iPhone,"" he continued. ""Now the image gets analyzed on the phone, and the image never goes up to the cloud to us. We never see the image."" Even still, the software can return an analysis in a second on a newer iPhone. The identification neural network is ""trained"" by researchers at VisualDx, but it can run on a phone, Papier said.  The models are trained using VisualDx's own library of professional medical images, Papier said.  ""We're not like a wiki model where you know anyone can upload images to us and just tell us what they think they are,"" Papier said. ""We're very very selective to work with the universities and individual physicians we know are experts."" Many of VisualDx's images were scanned from old collections of slides and film, from leading departments and doctors. It's built a library of 32,000 images to train its models. ""We developed this reputation as somebody that was going to preserve the legacy of medical photography,"" Papier said.  Still, even with high-quality models and training data, Papier doesn't think completely automated diagnosis will happen anytime soon. ""The hype cycle right now for machine learning is off the charts,"" he said.  ""Machine learning will get you into a category on this, but to get to the final mile, you have to ask the patient, 'did you take a drug a week ago?' 'Did you travel?'"" he said.  Medical professionals can download the app from the App Store. If your institution doesn't have a subscription, various in-app purchases range from $99 for a year's access to DermExpert. A complete purchase with access to other medical information is $499 per year.  You can sign up for the Aysa mailing list here.   Advances in machine learning now mean that...
This is why you should be buying gold
More ""The Bottom Line"" »
Martha Stewart will never stop reinventing herself — here's how she went from a stockbroker to hosting a cooking show with Snoop Dogg
More ""Stay Hungry"" »
Get the best of Business Insider delivered to your inbox every day."
6d7d12c54b,Edgecase Brings A Smarter Approach To Navigating Online Retailers  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Edgecase is trying to make the online shopping experience friendlier and smarter — closer, perhaps, to what it’s like to shop with a salesperson who understands your needs and is totally knowledgeable about what’s in-stock. The company was previously known as Compare Metrics, but it’s relaunching today with a new name and an upgraded product. To describe the problem that Edgecase is trying to solve, co-founder and CEO Garrett Eastham noted that the conversion rates (i.e., the percentage of visitors who actually buy something) of online stores lag badly behind those of physical stores, something he attributed to the fact that “today’s digital commerce platforms were designed by database engineers not shoppers.” Lisa Roberts, the company’s vice president of product and marketing, added that when you’re navigating most online retailers, your options are limited to browsing by brand, size, price, and not much more. That works if you pretty much know what you’re looking for, but it’s less helpful if you only have a vague idea of what you need, or no idea at wall — in other words, when you’re doing the online equivalent of window shopping. So Edgecase works with online shopping sites to rethink their navigation experience. For one thing, it allows shoppers to get a lot more detail about what they want, in something closer to everyday language — instead of saying they’re looking for a dress from Brand X in Size Y, they can say they want a short, black cocktail dress. Or they can browse based on the occasion that they’re shopping for, like a wedding or a graduation, Edgecase will automatically enter a set of relevant attributes. (You can see these features in action in the demo videEos embedded throughout the post.) As you browse an Edgecase-powered site, there are other touches that improve the experience. For example, as you change your filters, the results will update immediately, so you can tell whether or not you’re moving in the right direction. You can also specify whether one of the attributes is an essential “need to have” or just something that you’d like (so if there’s an item that hits most of your high points but misses on something less important, it won’t be excluded.) And if you see something you like, but you can’t quite articulate why, you can just hit the “more like this” button and you’ll be shown similar items. Eastham said Edgecase can achieve this through a combination of machine learning and human curation. Most of this product information is already available, but Edgecase’s technology can actually structure all the relevant data that’s floating around, and “the human layer” can make sure the data is presented in the language that shoppers actually use. He also noted that Edgecase integrates with existing e-commerce search tools like Endecca and Solar, and that it’s working on mobile capabilities, too. Edgecase customers include Crate & Barrel, Wasserstrom, Urban Decay, Golfsmith, Kate Somerville, and Rebecca Minkoff. The company says that it was able to improve conversion rates for makeup retailer Urban Decay by 16 percent. The company has raised $8M in funding from Allegro Ventures, Austin Ventures, Floodgate, Hurt Family Investments, Mack Capital, and various angel investors. Latest headlines delivered to you daily"
b12b02621b,Just the beginning: 6 applications for machine learning in radiology beyond image interpretation | Radiology Business,"Discussions about machine learning’s impact on radiology might begin with image interpretation, but that’s only the tip of the iceberg. When it comes to realizing the technology’s full potential, it’s like Bachman Turner Overdrive sang many years ago: You ain’t seen nothing yet. The authors of a new analysis published in the Journal of the American College of Radiology wrote at length about the many applications of machine learning. “Machine learning has the potential to solve many challenges that currently exist in radiology beyond image interpretation,” wrote lead author Paras Lakhani, MD, department of radiology at Thomas Jefferson University Hospital in Philadelphia, and colleagues. “One of the reasons there is great excitement in radiology today is the access to digital Big Data. Many institutions have implemented electronic health care databases over the past two decades, including for images in PACS, radiology reports and ordering information in Radiology Information Systems, and electronic health records that encompass information from other sources, including clinical notes, laboratory data and pathology records. Moreover, radiology images themselves are rich in metadata stored in the DICOM format, which may be leveraged as well. As such, there are great opportunities to uncover complex associations within the data using machine learning that would otherwise be difficult for a human to do.” These are some of the many examples Lakhani et al. provided of how machine learning can be used in radiology beyond image interpretation: 1. Creating safety protocols Developing safety protocols is an important part of any radiologist’s job, the authors noted, and machine learning can help speed up the entire process. “This can be a time-consuming but important task,” the authors wrote. “However, recent studies demonstrate that machine learning algorithms utilizing information extracted from the provided study indications can be accurate in determining protocols of studies in both brain and body MRIs.” 2. Decreasing radiation dose in CT Decreasing radiation dose is a huge topic in medical imaging. Lakhani et al. noted that deep learning has the potential to help specialists lower dose without the usual tradeoff of producing “poorer-quality images.” “The idea is to train a classifier to map ‘noisy’ images generated from ultralow-dose CT protocols to high-quality images from regular protocols, using deep learning techniques,” the authors wrote. “This is akin to creating ‘super-resolution’ photorealistic images from down-sampled versions, which has already shown exciting results in every-day color images.” 3. Decreasing scan time in MRI This is similar to what machine learning is capable of with CT images and just as important. MRIs take a long time to acquire, the authors noted, meaning that patients are being scanned for longer periods of time. With deep learning however, anatomic MRIs can be reconstructed using raw data from the scanner, resulting in a drop in scan time of 50 percent or even more. 4. Scheduling patients Scheduling patients is never anyone’s idea of a good time, especially for larger practices. “Many factors are involved, including time of day, day of the week, coverage location (emergency department, inpatient, outpatient), exam complexity, volume of studies, variety of modalities (MR, CT, ultrasound, x-ray), and referring clinician practice patterns,” the authors wrote. “Sometimes radiologists feel overwhelmed with the volume and complexity of studies on a given shift, and other times they are overstaffed.” Machine learning is already being used in other industries, so why not radiology as well? 5. Billing and collections Natural language processing and machine learning can help practices interpret reports more accurately and produce better claims, the authors explained. Insurance denials are said to cost providers 3 to 5 percent, so using these evolving technologies to try to get some of that money back is a plan executives are likely to get behind. 6. Image-based search engines Deep learning has the potential to make text searches a thing of the past. “One could input an image of the lungs containing a ground-glass opacity and see other CT scans containing similar findings, matched with corresponding radiology reports (or even pathology if known),” the authors wrote. “This technology could augment electronic teaching files and result in diagnostic assistance technologies.”
News
Magazine
Topics
Partner Voice
Editorial
Team /
Calendar /
Submit Press Release /
Contact
Advertising
Team /
Media Kit /
Contact
TriMed Media Group
Cardiovascular Business /
Clinical Innovation + Technology /
Health Exec /
Health Imaging /
Healthcare Technology /
imagingBiz /
Radiology Business
Subscribe
© TriMed Media Inc. All rights reserved.
Visit us at TriMedMedia.com"
bbbadf289e,,"Skip to MainContent You have been redirected to this page for one of the following reasons: IEEE Xplore requires cookies to maintain sessions and to
access licensed content. Cookies are used temporarily to maintain
sessions in IEEE Xplore and for no other purpose. The cookies will not persist after a session ends. Please change your browser settings to accept cookies before you access IEEE Xplore. Here are some directions to enable cookies for a few commonly used browsers:
If you are accessing IEEE Xplore from a network that is not configured to accept
cookies, please contact your network administrator. Proxy servers and
other network appliances must be configured to accept cookies from the
*ieee.org domain in order for you to use IEEE Xplore. Once you have enabled cookies, you may
return to the previous page. If you have any questions, please contact onlinesupport@ieee.org.
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions."
81f3da2b30,,"Skip to MainContent You have been redirected to this page for one of the following reasons: IEEE Xplore requires cookies to maintain sessions and to
access licensed content. Cookies are used temporarily to maintain
sessions in IEEE Xplore and for no other purpose. The cookies will not persist after a session ends. Please change your browser settings to accept cookies before you access IEEE Xplore. Here are some directions to enable cookies for a few commonly used browsers:
If you are accessing IEEE Xplore from a network that is not configured to accept
cookies, please contact your network administrator. Proxy servers and
other network appliances must be configured to accept cookies from the
*ieee.org domain in order for you to use IEEE Xplore. Once you have enabled cookies, you may
return to the previous page. If you have any questions, please contact onlinesupport@ieee.org.
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions."
0e2866f7f4,How Tesla's autopilot learns | Fortune,"While Tesla’s new hands-free driving is drawing a lot of interest this week, it’s the technology behind-the-scenes of the company’s newly-enabled autopilot service that should be getting more attention. At an event on Wednesday Tesla’s CEO Elon Musk explained that the company’s new autopilot service is constantly learning and improving thanks to machine learning algorithms, the car’s wireless connection, and detailed mapping and sensor data that Tesla collects. Tesla’s cars in general have long been using data, and over-the-air software updates, to improve the way they operate. Machine learning algorithms are the latest in computer science where computers can take a large data set, analyze it and use it to make increasingly accurate predictions. In short, they are learning. Companies like Google (GOOG), Facebook (FB) and now Tesla (TSLA) are using machine learning as a way to train software to help customers or sell them new services. Machine learning is the way that computers can become artificially intelligent, and the technology is a form of AI. While Musk has taken a sort of alarmist stance against the dangers of AI, he clarified during the event on Wednesday that he’s only concerned with artificial intelligence that is meant for nefarious purposes. When a reporter asked Musk during the media Q&A what made his company’s autopilot service different than other computer-based driving assistance features that competing big auto makers are working on, Musk emphasized learning. “The whole Tesla fleet operates as a network. When one car learns something, they all learn it. That is beyond what other car companies are doing,” said Musk. When it comes to the autopilot software, Musk explained that each driver using the autopilot system essentially becomes an “expert trainer for how the autopilot should work.” While most car companies might not be building learning systems, Google’s self-driving cars operate in a similar manner. In that way, Tesla’s cars are more similar to smart connected gadgets like Nest’s learning thermostat (now owned by Google’s Alphabet), than they are to traditional cars. Nest’s thermostat, using sensors and algorithms, learns its owner’s behavior over time, and through software updates offers increasingly useful services, or even informs Nest’s decisions about its next-generation of hardware. So, how does Tesla’s autopilot system, and its cars in general, learn? It all starts with data. Companies building these types of driver-assistance services, as well as full-blown self-driving cars like Google’s, need to teach a computer how to take over key parts (or all) of driving using digital sensor systems instead of a human’s senses. To do that companies generally start out by training algorithms using a large amount of data. You can think of it how a child learns through constant experiences and replication, explained Nvidia’s Senior Director of Automotive, Danny Shapiro in an interview with Fortune. Nvidia (NVDA) sells high performance chips that enable computers to process large amounts of data, and more recently started selling a computing system, called Drive PX, for self-driving cars and driver-assist applications. To create a self-driving car, companies feed hundreds of thousands, or even millions, of miles of driving videos and data into a computer’s data model to basically create a massive vocabulary around driving. The algorithms use visual techniques to break down the videos and to understand them. The goal is that when something unexpected happens — a ball rolls into the street — the car can recognize the pattern and react accordingly (slow down because a child could be running into the street after it). For Nvidia, the company loads this “driving dictionary,” as Shapiro calls it, onto powerful but compact computing hardware that can be used on the car. After that, companies like Google and Tesla add various types of data from different sources to continue to inform the model over time. Companies try to gather as much data as possible to help a car’s computer make smarter and better decisions on the roads. This includes data from customers driving, data from GPS and maps, and data from company employees driving research cars. The data from Tesla drivers was enabled by the hardware choices that Tesla has made. All Tesla cars built in the past year have 12 sensors on the bottom of the vehicle, a front-facing camera next to the rear-view mirror, and a radar system under the nose. These sensing systems are constantly collecting data to help the autopilot work on the road today, but also to amass data that can make Tesla’s operate better in the future. Because all of Tesla’s cars have an always-on wireless connection, data from driving and using autopilot is collected, sent to the cloud, and analyzed with software. For autopilot, Tesla takes the data from cars using the new automated steering or lane change system, and uses it to train its algorithms. Tesla then takes these algorithms, tests them out and incorporates them into upcoming software. Companies will rely on different types of data depending on what they’re trying to do with the cars. For example, Google has used large and expensive LIDAR (light-based radar) sensors on its self-driving cars. But Tesla’s Musk said that LIDAR was basically overkill for what Tesla’s autopilot cars need. But Musk said that Tesla wanted much more detailed high-precision mapping data for its automated steering and lane change applications than was available through the standard navigation tech. To meet its needs, Tesla has started to build high-precision maps —that have 100 times the level of granularity compared to standard navigation systems — using mostly data from Tesla cars driving on roads, but also some data from Tesla employees driving research cars. These new services could provide unexpected business models for companies. Musk said that Tesla might be interested in selling the mapping data to other car companies down the road. Tesla isn’t the only car maker working on driver-assist and self-driving car tech. Google is blazing ahead on its futuristic tech, while Audi has traffic jam assist software. Nvidia’s Shapiro says that most automakers are investigating these technologies. Nvidia started shipping Drive PX this summer, and Shapiro says that it’s engaged with over 50 companies and researchers. Tesla uses Nvidia chips in the 17-inch screen and the instrument cluster for its Model S and there has been speculation around whether Tesla might use the Drive PX system in future versions of the Model X SUV. Shapiro wouldn’t discuss the specifics of its relationships with Tesla or Audi, which uses Nvidia’s tech in its traffic jam system. Shapiro cautioned that despite some companies already deploying these technologies, it’s still early days for self-driving car tech. “A huge amount of work will be done on this over the next decade,” he said. To learn more about Tesla’s autopilot tech watch this Fortune video:"
404bc29879,Algorithm better at diagnosing pneumonia than radiologists | News Center | Stanford Medicine,"Learn how we are healing patients through science & compassion
Clinical trial finds blood-plasma infusions for Alzheimer’s safe, promising
Learn how we are fueling innovation
Diversity center for med school opens in Lane Library
Learn how we empower tomorrow's leaders
Support teaching, research, and patient care.
Support Lucile Packard Children's Hospital Stanford and child and maternal health Stanford School of Medicine Stanford Health Care Stanford Children's Health Stanford researchers have developed a deep-learning algorithm that evaluates chest X-rays for signs of disease. Radiologist Matthew Lungren, left, meets with graduate students Jeremy Irvin and Pranav Rajpurkar to discuss the results of detections made by the algorithm.
L.A. Cicero/Stanford News Service Stanford researchers have developed an algorithm that offers diagnoses based off chest X-ray images. It can diagnose up to 14 types of medical conditions and is able to diagnose pneumonia better than expert radiologists working alone A paper about the algorithm, called CheXNet, was published Nov. 14 on the open-access, scientific preprint website arXiv. “Interpreting X-ray images to diagnose pathologies like pneumonia is very challenging, and we know that there’s a lot of variability in the diagnoses radiologists arrive at,” said Pranav Rajpurkar, a graduate student in the Machine Learning Group at Stanford and co-lead author of the paper. “We became interested in developing machine learning algorithms that could learn from hundreds of thousands of chest X-ray diagnoses and make accurate diagnoses.” The work uses a public data set initially released by the National Institutes of Health Clinical Center on Sept. 26. That data set contains 112,120 frontal-view chest X-ray images labeled with up to 14 possible pathologies. It was released in tandem with an algorithm that could diagnose many of those 14 pathologies with some success, designed to encourage others to advance that work. As soon as they saw these materials, the Machine Learning Group — a group led by Andrew Ng, PhD, adjunct professor of computer science — knew they had their next project. The researchers, working with Matthew Lungren, MD, MPH, assistant professor of radiology at the School of Medicine, had four Stanford radiologists independently annotate 420 of the images for possible indications of pneumonia. The researchers said they chose to focus on this disease, which brings 1 million Americans to the hospital each year, according to the Centers for Disease Control and Prevention, and is especially difficult to spot on X-rays. In the meantime, the Machine Learning Group team got to work developing an algorithm that could automatically diagnose the pathologies. Within a week, the researchers had an algorithm that diagnosed 10 of the pathologies labeled in the X-rays more accurately than previous state-of-the-art results. In just over a month, their algorithm could beat these standards in all 14 identification tasks. In that short time span, CheXNet also outperformed the four Stanford radiologists in diagnosing pneumonia accurately. Often, treatments for common but devastating diseases that occur in the chest, such as pneumonia, rely heavily on how doctors interpret radiological imaging. But even the best radiologists are prone to misdiagnoses due to challenges in distinguishing between diseases based on X-rays. “The motivation behind this work is to have a deep-learning model to aid in the interpretation task that could overcome the intrinsic limitations of human perception and bias, and reduce errors,” explained Lungren, who is co-author of the paper. “More broadly, we believe that a deep-learning model for this purpose could improve health care delivery across a wide range of settings.” After about a month of continuous iteration, the algorithm outperformed the four individual Stanford radiologists in pneumonia diagnoses. This means that the diagnoses provided by CheXNet agreed with a majority vote of radiologists more often than those of the individual radiologists. The algorithm now has the highest performance of any work that has come out so far related to the NIH chest X-ray data set. Also detailed in their arXiv paper, the researchers have developed a computer-based tool that produces what looks like a heat map of the chest X-rays — but instead of representing temperature, the colors of these maps represent areas that the algorithm determines are most likely to represent pneumonia. This tool could help reduce the amount of missed cases of pneumonia and significantly accelerate radiologist workflow by showing them where to look first, leading to faster diagnoses for the sickest patients. In parallel to other work the group is doing with irregular heartbeat diagnosis and electronic medical record data, the researchers hope CheXNet can help people in areas of the world where people might not have easy access to a radiologist. “We plan to continue building and improving upon medical algorithms that can automatically detect abnormalities and we hope to make high-quality, anonymized medical datasets publicly available for others to work on similar problems,” said Jeremy Irvin, a graduate student and co-lead author of the paper. “There is massive potential for machine learning to improve the current health care system, and we want to continue to be at the forefront of innovation in the field.” Additional Stanford co-authors of this paper are Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding and Aarti Bagul of the Machine Learning Group; Curtis Langlotz, MD, PhD, professor of radiology; and Katie Shpanskaya, MD, a former medical student. Lungren is also a member of Stanford Bio-X, the Stanford Child Health Research Institute and the Stanford Cancer Institute. Stanford Medicine integrates research, medical education and health care at its three institutions - Stanford University School of Medicine, Stanford Health Care (formerly Stanford Hospital & Clinics), and Lucile Packard Children's Hospital Stanford. For more information, please visit the Office of Communication & Public Affairs site at http://mednews.stanford.edu. Automating the analysis of slides of lung cancer tissue samples increases the accuracy of tumor classification and patient prognoses, according to a new study. Stanford Medicine is leading the biomedical revolution in precision health, defining and developing the next generation of care that is proactive, predictive and precise.  Learn more Stanford Medicine's unrivaled atmosphere of breakthrough thinking and interdisciplinary collaboration has fueled a long history of achievements. View timeline Office of Communication & Public Affairs For Journalists For Faculty & Staff School Policies Contacts   About Contact Maps & Directions Jobs Basic Science Departments Clinical Science Departments Academic Programs Vision"
02f6bf3b52,Counterfeiters are using AI and machine learning to make better fakes,"It's terrifyingly easy to just make stuff up online these days, such is life in the post-truth era. But recent advancements in machine learning (ML) and artificial intelligence (AI) have compounded the issue exponentially. It's not just the news that's fake anymore but all sorts of media and consumer goods can now be knocked off thanks to AI. From audio tracks and video clips to financial transactions and counterfeit products -- even your own handwriting can be mimicked with startling levels of accuracy. But what if we could leverage the same computer systems that created these fakes to reveal them just as easily? People have been falling for trickery and hoaxes since forever. Human history is filled with false prophets, demagogues, snake-oil peddlers, grifters and con men. The problem is that these days, any two-bit huckster with a conspiracy theory and a supplement brand can hop on YouTube and instantly reach a global audience. And while the definition of ""facts"" now depends on who you're talking to, one thing that most people agreed to prior to January 20th this year is the veracity of hard evidence. Video and audio recordings have long been considered reliable sources of evidence but that's changing thanks to recent advances in AI. In July 2016, researchers at the University of Washington developed a machine learning system that not only accurately synthesizes a person's voice and vocal mannerisms but lip syncs their words onto a video. Essentially, you can fake anybody's voice and create a video of them saying whatever you want. Take the team's demo video, for example. They trained the ML system using footage of President Obama's weekly address. The recurrent neural network learned to associate various audio features with their respective mouth shapes. From there, the team generated CGI mouth movements, and with the help of 3D pose matching, ported the animated lips onto a separate video of the president. Basically, they're able to generate a photorealistic video using only its associated audio track. While the team took an outsized amount of blowback over the potential misuses of such technology, they had far more mundane uses for it in mind. ""The ability to generate high-quality video from audio could signicantly reduce the amount of bandwidth needed in video coding/transmission (which makes up a large percentage of current internet bandwidth),"" they suggested in their study, Synthesizing Obama: Learning Lip Sync from Audio. ""For hearing-impaired people, video synthesis could enable lip-reading from over-the-phone audio. And digital humans are central to entertainment applications like film special effects and games."" UW isn't the only facility looking into this sort of technology. Last year, a team from Stanford debuted the Face2Face system. Unlike UW's technology, which generates video from audio, Face2Face generates video from other video. It uses a regular webcam to capture the user's facial expressions and mouth shapes, then uses that information to deform the target YouTube video to best match the user's expressions and speech -- all in real time. AI-based audio-video transcription is a two-way street. Just as UW's system managed to generate video from an audio feed, a team from MIT's CSAIL figured out how to create audio from a silent video reel. And do it well enough to fool human audiences. ""When you run your finger across a wine glass, the sound it makes reflects how much liquid is in it,"" Andrew Owens, the paper's lead author told MIT News. ""An algorithm that simulates such sounds can reveal key information about objects' shapes and material types, as well as the force and motion of their interactions with the world."" The MIT's deep learning system was trained over the course of a few months using 1,000 videos containing some 46,000 sounds resulting from different objects being poked, struck or scraped with a drumstick. Like the UW algorithm, MIT's learned to associate different audio properties with specific onscreen actions and synthesize those sounds as the video played. When tested online against a video with authentic sound, people actually chose the fake audio over the real twice as often as the baseline algorithm. The MIT team figures that they can leverage this technology to help give robots better situational awareness. ""A robot could look at a sidewalk and instinctively know that the cement is hard and the grass is soft, and therefore know what would happen if they stepped on either of them,"" Owens said. ""Being able to predict sound is an important first step toward being able to predict the consequences of physical interactions with the world."" Research into audio synthesization isn't limited to universities; a number of major corporations are investigating the technology as well. Google, for example, has developed Wavenet, a ""deep generative model of raw audio waveforms."" Among the first iterations of computer-generated text-to-speech (TTS) systems is ""concatenative"" TTS. That's where a single person records a variety speech fragments, those are fed into a database and then reconstructed by a computer to form words and sentences. The problem is that the output sounds more like the MovieFone guy (ask your parents) than a real person. Waveform, on the other hand, is trained on waveforms of people speaking. The system samples those recordings for data points up to 16,000 times per second. To output sound, Waveform uses a model to predict what the next sound will be based on the sounds that came before it. The process is computationally expensive but does produce superior audio quality compared to the conventional TTS methods. In the future, robots could potentially forge your signature on official documents, if this AI-based handwriting mimic developed at the University College London is ever misused. Dubbed the ""My Text in Your Handwriting"" program, this system can accurately recreate a subject's handwriting with as little as a paragraph's input. The program is based on ""glyphs,"" essentially the unique traits of each person's handwriting. By measuring various aspects like horizontal and vertical spacing, connectors between letters and writing texture, the program can readily copy the style. ""Our software has lots of valuable applications. Stroke victims, for example, may be able to formulate letters without the concern of illegibility, or someone sending flowers as a gift could include a handwritten note without even going into the florist,"" Dr. Tom Haines, UCL Computer Science and lead author of the study, told UCL News. ""It could also be used in comic books where a piece of handwritten text can be translated into different languages without losing the author's original style."" And while this technology could be used to create forgeries, it can just as easily be leveraged to spot them as well. ""Forgery and forensic handwriting analysis are still almost entirely manual processes,"" Dr. Gabriel Brostow, of the UCL computer science department, said. ""But by taking the novel approach of viewing handwriting as texture-synthesis we can use our software to characterise handwriting to quantify the odds that something was forged."" Forgeries and faked products don't stop at the the bounds of the internet. Recent estimates by the Organisation for Economic Co-operation and Development put the global market for counterfeit goods at around $460 billion annually. And that's where the Entrupy authentication system comes in. ""In an ideal world, we shouldn't exist,"" Entrupy CEO Vidyuth Srinivasan lamented. ""The more we can instill trustworthiness in the market, the better it will be for commerce in general."" The company first imaged a wide variety of luxury goods and uses that database to help its customers -- generally those in secondary retail markets like vintage clothing stores or eBay sellers -- authenticate products with around 98.5 percent accuracy. Customers receive a handheld microscope and take various images of the product in question, such as the exterior, logo or interior lining. These photos are then fed into a mobile app and transmitted to the company's servers where a classification algorithm goes to work, differentiating between legitimate goods and counterfeits. If the product is real, the Entrupy will provide a certificate of authenticity. Although the company's product database is varied, there are limits to the system's current capabilities. Because it's optical, reflective or transparent items are no good, nor is anything without surface texture. Some things that it does not work on include porcelain, diamonds and glass, pure plastic and bare metal. Unlike the other AI-based systems discussed here, there's little chance of the Entrupy system being corrupted or gamed. ""We have had [counterfeiters] pose as real customers and legitimate businesses to try and buy [the system] and we're fine with it,"" Srinivasan explained. That's because the system doesn't actually tell the user which of the images they're taking are actually being used to verify the product's authenticity. ""We ask our customers to take images of different parts of the item because it's not just pure material [being used for verification]...,"" he continued. ""It's a holistic view of the different aspects of the item -- from the workmanship to the material used to the wear"" as well as a number of other contextual bits of metadata. What's more, the system is continually updated with new data, not just from the company's internal efforts of posing as secret buyers to acquire counterfeit goods, but also from the users themselves. Images taken during the authentication process -- whether the item turns out to be real or not -- are incorporated into the company's database, further improving the system's accuracy. ""In the near to medium future, I think that AI and ML will, as a counterfeiting solution, will definitely raise the bar,"" he concluded. ""It's a spy versus spy game, cat versus mouse."" Increasing our ability to spot fakes will force counterfeiters to up their game and start using better quality materials and better workmanship. That, however, will increase the production cost of these products, hopefully to a price that is no longer economically viable. ""The MO of any counterfeiter is to make something that they can sell a lot of, that can be easily produced and that does not cost a lot to produce a fake of,"" Srinivasan sid. ""Otherwise there's no profitability."" Similar measures have been adopted by Paypal, one of the the internet's top financial service providers, for cases of account fraud. ""Say my account was accessed today from San Francisco, tomorrow from NYC, and some other IP the day after,"" Hui Wang, Paypal's senior director of global risk sciences, told Engadget. This sort of activity is indicative of some kind of account takeover. ""In order to detect these kinds of fraud,"" she explained, ""we track the IP we track the machine and we track the network."" The company created an algorithm that looks at both the IP and the geolocation of that IP, then compares them to your account history to see if this matches up with previous actions. Paypal developed a proprietary technology that compares this IP location patten with other users, to see if there is a larger effect at work or there's a reasonable explanation for the movement -- i.e., perhaps you're flying through New York on business and buy a souvenir at the airport gift shop before continuing on the trip. The company's AI system also attempts to identify each previous IP, whether it's a hotel's secured ethernet connection or the public WiFi at the airport. ""[The algorithm] is retrieving tons of data from your account history and going beyond your account to look at the traffic on your network, like the other people using the same IP,"" Wang said. From this raw information, the algorithm selects specific data points and uses those to estimate whether the transaction is legitimate. Most of these actions and their subsequent decisions -- such as verifying or denying a payment -- are performed autonomously. However, if the algorithm's confidence value is too low, human investigators from the operations center will investigate the transaction manually. ""We are also in the process of ensuring that human intelligence can be fed back into the automated system,"" she continued, so that the ML system continually learns, improves and increases its accuracy. These sorts of systems, both those designed to generate fakes and those trained to uncover them, are still in their infancy. But in the coming decades, artificial intelligence and machine learning techniques will continue to improve, often in ways that we have yet to envision. There is a very real danger in technologies that can create uncannily convincing lies, hoaxes and fakes -- in front of our very eyes, no less. But, like movable type, radio and internet that came before it, AI systems like these, ones capable of generating photorealistic content, will only be as dangerous as the intentions of the people using it. And that's a terrifying thought. Andrew has lived in San Francisco since 1982 and has been writing clever things about technology since 2011. When not arguing the finer points of portable vaporizers and military defense systems with strangers on the internet, he enjoys tooling around his garden, knitting and binge watching anime. Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
dc405b5aa3,One small backflip for a robot is one giant leaping backflip for humankind - The Verge,"Something that often bothers me about sci-fi is the loner inventor trope. A guy in a garage builds a robot, or AI, or frequently both that are somehow decades beyond the technology of his day, and all the wild implications of his vast technological leap are the fuel for the next two and a half hours of popcorn entertainment. But the latest video from Boston Dynamics is the closest equivalent I've ever witnessed IRL. Sure, it’s the achievement of an entire company, and they’re doing it on YouTube for everyone to see, not in a basement. But a backflip? It’s hard to even appreciate how hard this is for robots to do, because it’s hard to appreciate how difficult walking still is for humanoids. I wrote a whole piece about the problem of building walking robots back in 2011 — it wasn’t pretty back then, and it’s still a challenge for most full-sized humanoids. A backflip though? It's a barely-believable jump forward for the state of the art. It's astonishing. It's a moon landing, basically, except instead of all of the people of Earth gathering around tube televisions to witness it, it just popped up on our social feeds yesterday afternoon without warning. Let's get a bit of historical context. Eleven years ago we were laughing as Honda's Asimo robot fell off a set of stairs. In 2015, here's how far we'd come: Yeah, we were challenging humanoid robots with much more complicated, dynamic, and demanding tasks than a staged ascent of a perfectly level set of shallow steps. But if you asked me, ""How long until these robots are doing backflips?"" in 2015, after a weekend of watching DARPA pratfalls, I would've frowned and said something like, ""Ugh."" Would we have to develop some new form of organic mechanisms, more akin to the human body, to get the power / weight ratio just right? Would we have to rebuild software engineering from scratch to combine realtime responsiveness with machine learning complexity? Would we end up in some economic recession or war that would require the companies and institutions investing in humanoid robotics to stop wasting money and just ship something boring and useful? I guess I could have said, ""Maybe a decade. We have to figure out jumping first, and also running and walking."" But ""a decade"" in the technology world means, ""I literally have no idea."" And I guess I would've been right about one thing: I had no idea. In 2016, not too long after the DARPA challenge, where many of the robots in the competition were based on Boston Dynamics' best-in-class Atlas humanoid, Boston Dynamics hit us with a new YouTube video: ""Atlas, The Next Generation."" The video showcased a much lighter and more agile version of the robot opening a door, walking through snow, picking up boxes, and getting hit with a hockey stick for no reason. It was a big improvement over the previous generation. ""When will it do backflips, Paul?"" ""Um, a decade?"" Earlier this year, Boston Dynamics introduced an all-new robot called Handle with a four foot vertical jump. But it was a wheeled robot, and while impressive, wheels are vastly simpler than biped locomotion. What Handle proved is that Boston Dynamics could blast enough power through its hydraulics to generate the necessary force for lift-off. So all we needed was a few years of software improvements to get the balancing algorithm just right, and we could finally have jumping robots. But yesterday, Atlas jumped on video. It leaped from box to box like a gazelle. It did a 180. And it did a backflip. A humanoid strong enough to jump like that is capable of any ""typical"" human locomotion. Stairs, curbs, uneven ground, accidental jostling, sitting down, standing up, getting in and out of cars, subway lurches... all moves which are frequently performed by humans who can't land a backflip, and who get mad if you shove them with a hockey stick. A backflip is a marvel of mechanical engineering and software control. It's a statement of power and poise. It's bonkers. I'm certain there's still much more to do on the software side. Performing powerful jumps in a controlled, measured environment is easier than doing dynamic, improvisational parkour. And then humanoids still have to be taught how to do something useful with their newfound physical capabilities. Also, other companies will have to catch up with Boston Dynamics — just because this is possible it doesn’t mean it’s easy. We’re still a ways away from having backflipping robots as next door neighbors. But I think we're in a new robotic age now. There was a time before Atlas could do backflips, back when robots were for factories, bomb disposal, vacuuming, and the occasional gimmick, and none of the useful ones were humanoids. Now we're living in an era where humanoid robots are apparently as agile as we are. So what will they be used for? It’s time to get out the popcorn. Command Line delivers daily updates from the near-future."
ffcdaaeae5,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
c1f38b63cf,How a Japanese cucumber farmer is using deep learning and TensorFlow | Google Cloud Big Data and Machine Learning Blog  |  Google Cloud Platform,"Posted by Kaz Sato, Developer Advocate, Google Cloud Platform It’s not hyperbole to say that use cases for machine learning and deep learning are only limited by our imaginations. About one year ago, a former embedded systems designer from the Japanese automobile industry named Makoto Koike started helping out at his parents’ cucumber farm, and was amazed by the amount of work it takes to sort cucumbers by size, shape, color and other attributes. Makoto's father is very proud of his thorny cucumber, for instance, having dedicated his life to delivering fresh and crispy cucumbers, with many prickles still on them. Straight and thick cucumbers with a vivid color and lots of prickles are considered premium grade and command much higher prices on the market. But Makoto learned very quickly that sorting cucumbers is as hard and tricky as actually growing them. ""Each cucumber has different color, shape, quality and freshness,"" Makoto says.
In Japan, each farm has its own classification standard and there's no industry standard. At Makoto's farm, they sort them into nine different classes, and his mother sorts them all herself — spending up to eight hours per day at peak harvesting times. ""The sorting work is not an easy task to learn. You have to look at not only the size and thickness, but also the color, texture, small scratches, whether or not they are crooked and whether they have prickles. It takes months to learn the system and you can't just hire part-time workers during the busiest period. I myself only recently learned to sort cucumbers well,” Makoto said.
There are also some automatic sorters on the market, but they have limitations in terms of performance and cost, and small farms don't tend to use them. Makoto doesn’t think sorting is an essential task for cucumber farmers. ""Farmers want to focus and spend their time on growing delicious vegetables. I'd like to automate the sorting tasks before taking the farm business over from my parents.""
Makoto first got the idea to explore machine learning for sorting cucumbers from a completely different use case: Google AlphaGo competing with the world's top professional Go player. ""When I saw the Google's AlphaGo, I realized something really serious is happening here,” said Makoto. “That was the trigger for me to start developing the cucumber sorter with deep learning technology."" Using deep learning for image recognition allows a computer to learn from a training data set what the important ""features"" of the images are. By using a hierarchy of numerous artificial neurons, deep learning can automatically classify images with a high degree of accuracy. Thus, neural networks can recognize different species of cats, or models of cars or airplanes from images. Sometimes neural networks can exceed the performance of the human eye for certain applications. (For more information, check out my previous blog post Understanding neural networks with TensorFlow Playground.) But can computers really learn mom's art of cucumber sorting? Makoto set out to see whether he could use deep learning technology for sorting using Google's open source machine learning library, TensorFlow. ""Google had just open sourced TensorFlow, so I started trying it out with images of my cucumbers,” Makoto said. “This was the first time I tried out machine learning or deep learning technology, and right away got much higher accuracy than I expected. That gave me the confidence that it could solve my problem."" With TensorFlow, you don't need to be knowledgeable about the advanced math models and optimization algorithms needed to implement deep neural networks. Just download the sample code and read the tutorials and you can get started in no time. The library lowers the barrier to entry for machine learning significantly, and since Google open-sourced TensorFlow last November, many ""non ML"" engineers have started playing with the technology with their own datasets and applications. Here's a systems diagram of the cucumber sorter that Makoto built. The system uses Raspberry Pi 3 as the main controller to take images of the cucumbers with a camera, and in a first phase, runs a small-scale neural network on TensorFlow to detect whether or not the image is of a cucumber. It then forwards the image to a larger TensorFlow neural network running on a Linux server to perform a more detailed classification.
Makoto used the sample TensorFlow code Deep MNIST for Experts with minor modifications to the convolution, pooling and last layers, changing the network design to adapt to the pixel format of cucumber images and the number of cucumber classes. Here's Makoto’s cucumber sorter, which went live in July:
Here's a close-up of the sorting arm, and the camera interface: And here is the cucumber sorter in action: One of the current challenges with deep learning is that you need to have a large number of training datasets. To train the model, Makoto spent about three months taking 7,000 pictures of cucumbers sorted by his mother, but it’s probably not enough. ""When I did a validation with the test images, the recognition accuracy exceeded 95%. But if you apply the system with real use cases, the accuracy drops down to about 70%. I suspect the neural network model has the issue of ""overfitting"" (the phenomenon in neural network where the model is trained to fit only to the small training dataset) because of the insufficient number of training images."" The second challenge of deep learning is that it consumes a lot of computing power. The current sorter uses a typical Windows desktop PC to train the neural network model. Although it converts the cucumber image into 80 x 80 pixel low-resolution images, it still takes two to three days to complete training the model with 7,000 images. ""Even with this low-res image, the system can only classify a cucumber based on its shape, length and level of distortion. It can't recognize color, texture, scratches and prickles,” Makoto explained. Increasing image resolution by zooming into the cucumber would result in much higher accuracy, but would also increase the training time significantly. To improve deep learning, some large enterprises have started doing large-scale distributed training, but those servers come at an enormous cost. Google offers Cloud Machine Learning (Cloud ML), a low-cost cloud platform for training and prediction that dedicates hundreds of cloud servers to training a network with TensorFlow. With Cloud ML, Google handles building a large-scale cluster for distributed training, and you just pay for what you use, making it easier for developers to try out deep learning without making a significant capital investment. Makoto is eagerly awaiting Cloud ML. ""I could use Cloud ML to try training the model with much higher resolution images and more training data. Also, I could try changing the various configurations, parameters and algorithms of the neural network to see how that improves accuracy. I can't wait to try it."""
466b38386e,Faster big-data analysis | MIT News,"Login
or
Subscribe Newsletter
A new MIT computer system speeds computations involving “sparse tensors,” multidimensional data arrays that consist mostly of zeroes.
Image: Christine Daniloff, MIT System for performing “tensor algebra” offers 100-fold speedups over previous software packages.
Larry Hardesty | MIT News Office
October 30, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
We live in the age of big data, but most of that data is “sparse.” Imagine, for instance, a massive table that mapped all of Amazon’s customers against all of its products, with a “1” for each product a given customer bought and a “0” otherwise. The table would be mostly zeroes. With sparse data, analytic algorithms end up doing a lot of addition and multiplication by zero, which is wasted computation. Programmers get around this by writing custom code to avoid zero entries, but that code is complex, and it generally applies only to a narrow range of problems. At the Association for Computing Machinery’s Conference on Systems, Programming, Languages and Applications: Software for Humanity (SPLASH), researchers from MIT, the French Alternative Energies and Atomic Energy Commission, and Adobe Research recently presented a new system that automatically produces code optimized for sparse data. That code offers a 100-fold speedup over existing, non-optimized software packages. And its performance is comparable to that of meticulously hand-optimized code for specific sparse-data operations, while requiring far less work on the programmer’s part. The system is called Taco, for tensor algebra compiler. In computer-science parlance, a data structure like the Amazon table is called a “matrix,” and a tensor is just a higher-dimensional analogue of a matrix. If that Amazon table also mapped customers and products against the customers’ product ratings on the Amazon site and the words used in their product reviews, the result would be a four-dimensional tensor. “Sparse representations have been there for more than 60 years,” says Saman Amarasinghe, an MIT professor of electrical engineering and computer science (EECS) and senior author on the new paper. “But nobody knew how to generate code for them automatically. People figured out a few very specific operations — sparse matrix-vector multiply, sparse matrix-vector multiply plus a vector, sparse matrix-matrix multiply, sparse matrix-matrix-matrix multiply. The biggest contribution we make is the ability to generate code for any tensor-algebra expression when the matrices are sparse.” Joining Amarasinghe on the paper are first author Fredrik Kjolstad, an MIT graduate student in EECS; Stephen Chou, also a graduate student in EECS; David Lugato of the French Alternative Energies and Atomic Energy Commission; and Shoaib Kamil of Adobe Research. Custom kernels In recent years, the mathematical manipulation of tensors — tensor algebra — has become crucial to not only big-data analysis but machine learning, too. And it’s been a staple of scientific research since Einstein’s time. Traditionally, to handle tensor algebra, mathematics software has decomposed tensor operations into their constituent parts. So, for instance, if a computation required two tensors to be multiplied and then added to a third, the software would run its standard tensor multiplication routine on the first two tensors, store the result, and then run its standard tensor addition routine. In the age of big data, however, this approach is too time-consuming. For efficient operation on massive data sets, Kjolstad explains, every sequence of tensor operations requires its own “kernel,” or computational template. “If you do it in one kernel, you can do it all at once, and you can make it go faster, instead of having to put the output in memory and then read it back in so that you can add it to something else,” Kjolstad says. “You can just do it in the same loop.” Computer science researchers have developed kernels for some of the tensor operations most common in machine learning and big-data analytics, such as those enumerated by Amarasinghe. But the number of possible kernels is infinite: The kernel for adding together three tensors, for instance, is different from the kernel for adding together four, and the kernel for adding three three-dimensional tensors is different from the kernel for adding three four-dimensional tensors. Many tensor operations involve multiplying an entry from one tensor with one from another. If either entry is zero, so is their product, and programs for manipulating large, sparse matrices can waste a huge amount of time adding and multiplying zeroes. Hand-optimized code for sparse tensors identifies zero entries and streamlines operations involving them — either carrying forward the nonzero entries in additions or omitting multiplications entirely. This makes tensor manipulations much faster, but it requires the programmer to do a lot more work. The code for multiplying two matrices — a simple type of tensor, with only two dimensions, like a table — might, for instance, take 12 lines if the matrix is full (meaning that none of the entries can be omitted). But if the matrix is sparse, the same operation can require 100 lines of code or more, to track omissions and elisions. Enter Taco Taco adds all that extra code automatically. The programmer simply specifies the size of a tensor, whether it’s full or sparse, and the location of the file from which it should import its values. For any given operation on two tensors, Taco builds a hierarchical map that indicates, first, which paired entries from both tensors are nonzero and, then, which entries from each tensor are paired with zeroes. All pairs of zeroes it simply discards. Taco also uses an efficient indexing scheme to store only the nonzero values of sparse tensors. With zero entries included, a publicly released tensor from Amazon, which maps customer ID numbers against purchases and descriptive terms culled from reviews, takes up 107 exabytes of data, or roughly 10 times the estimated storage capacity of all of Google’s servers. But using the Taco compression scheme, it takes up only 13 gigabytes — small enough to fit on a smartphone. “Many research groups over the last two decades have attempted to solve the compiler-optimization and code-generation problem for sparse-matrix computations but made little progress,” says Saday Sadayappan, a professor of computer science and engineering at Ohio State University, who was not involved in the research. “The recent developments from Fred and Saman represent a fundamental breakthrough on this long-standing open problem.” “Their compiler now enables application developers to specify very complex sparse matrix or tensor computations in a very easy and convenient high-level notation, from which the compiler automatically generates very efficient code,” he continues. “For several sparse computations, the generated code from the compiler has been shown to be comparable or better than painstakingly developed manual implementations. This has the potential to be a real game-changer. It is one of the most exciting advances in recent times in the area of compiler optimization.” Topics: Research, School of
Engineering, Artificial intelligence, Data, Computer modeling, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Software This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
4a26c86396,"Real-Time Recognition of Handwritten Chinese Characters Spanning a Large Inventory of 30,000 Characters - Apple","Vol. 1, Issue 5 ∙
September 2017
September Two Thousand Seventeen
by Handwriting Recognition Team
Handwriting recognition is more important than ever given the prevalence of mobile phones, tablets, and wearable gear like smartwatches. The large symbol inventory required to support Chinese handwriting recognition on such mobile devices poses unique challenges. This article describes how we met those challenges to achieve real-time performance on iPhone, iPad, and Apple Watch (in Scribble mode). Our recognition system, based on deep learning, accurately handles a set of up to 30,000 characters. To achieve acceptable accuracy, we paid particular attention to data collection conditions, representativeness of writing styles, and training regimen. We found that, with proper care, even larger inventories are within reach. Our experiments show that accuracy only degrades slowly as the inventory increases, as long as we use training data of sufficient quality and in sufficient quantity. Handwriting recognition can enhance user experience on mobile devices, particularly for Chinese input given the relative complexity of keyboard methods.
Chinese handwriting recognition is uniquely challenging, due to the large size of the underlying character inventory. Unlike alphabet-based writing, which typically involves on the order of 100 symbols, the set of Hànzì characters in Chinese National Standard GB18030-2005 contains 27,533 entries, and many additional logographic characters are in use throughout Greater China. For computational tractability, it is usual to focus on a restricted number of characters, deemed most representative of usage in daily life. Thus, the standard GB2312-80 set includes only 6,763 entries (3,755 and 3,008 characters in the level-1 and level-2 sets, respectively). The closely aligned character set used in the popular CASIA databases, built by the Institute of Automation of Chinese Academy of Sciences, comprises a total of 7,356 entries [6]. The SCUT-COUCH database has similar coverage [8]. These sets tend to reflect commonly used characters across the entire population of Chinese writers. At the individual user level, however, what is “commonly used” typically varies from one person to the next. Most people need at least a handful of characters deemed “infrequently written,” as they occur in, e.g., proper names relevant to them. Thus ideally Chinese handwriting recognition algorithms need to scale up to at least the level of GB18030-2005. While early recognition algorithms mainly relied on structural methods based on individual stroke analysis, the need to achieve stroke-order independence later sparked interest into statistical methods using holistic shape information [5]. This obviously complicates large-inventory recognition, as correct character classification tends to get harder with the number of categories to disambiguate [3]. On Latin script tasks such as MNIST [4], convolutional neural networks (CNNs) soon emerged as the way to go [11]. Given a sufficient amount of training data, supplemented with synthesized samples as necessary, CNNs decidedly achieved state-of-the-art results [1], [10]. The number of categories in those studies, however, was very small (10). When we started looking into the large-scale recognition of Chinese characters some time ago, CNNs seemed to be the obvious choice. But that approach required scaling up CNNs to a set of approximately 30,000 characters, while simultaneously maintaining real-time performance on embedded devices. This article focuses on the challenges involved in meeting expectations in terms of accuracy, character coverage, and robustness to writing styles. We adopt in this work a general CNN architecture similar to that used in previous handwriting recognition experiments on the MNIST task (see, e.g., [1], [10]). The configuration of the overall system is illustrated in Fig. 1. The input is a medium-resolution image (for performance reasons) of 48x48 pixels representing a Chinese handwritten character. We fed this input to a number of feature extraction layers alternating convolution and subsampling. The last feature extraction layer is connected to the output via a fully connected layer. From one convolution layer to the next, we chose the size of the kernels and the number of feature maps so as to derive features of increasingly coarser granularity. We subsampled through a max-pooling layer [9] using a 2x2 kernel. The last feature layer typically comprises on the order of 1,000 small feature maps. Finally, the output layer has one node per class, e.g., 3,755 for the Hànzì level-1 subset of GB2312-80, and close to 30,000 when scaled up to the full inventory. As baseline, we evaluated the previously described CNN implementation on the CASIA benchmark task [6]. While this task only covers Hànzì level-1 characters, there exist numerous reference results for character accuracy in the literature (e.g., in [7] and [14]). We used the same setup based on CASIA-OLHWDB, DB1.0-1.2, split in training and testing datasets [6], [7], yielding about one million training exemplars. Note that, given our product focus, the goal was not to tune our system for the highest possible accuracy on CASIA. Instead our priorities were model size, evaluation speed, and user experience. We thus opted for a compact system that works real-time, across a wide variety of styles, and with high robustness towards non-standard stroke order. This led to an image-based recognition approach even though we evaluate on online datasets. As in [10], [11], we supplemented actual observations with suitable elastic deformations. Table 1 shows the results using the CNN of Figure 1, where the abbreviation “Hz-1” refers to the Hànzì level-1 inventory (3,755 characters), and “CR(n)” denotes top-n character recognition accuracy. In addition to the commonly reported top-1 and top-10 accuracies, we also mention top-4 accuracy because, as our user interface was designed to show 4 character candidates, the top-4 accuracy is an important predictor of user experience in our system. Table 1. Results on CASIA On-line Database, 3,755 Characters. Standard Training, Associated Model Size = 1MB The figures in Table 1 compare with online results in [7] and [14] averaging roughly 93% for top-1 and 98% for top-10 accuracy. Thus, while our top-10 accuracy is in line with the literature, our top-1 accuracy is slightly lower. This has to be balanced, however, against a satisfactory top-4 accuracy, and perhaps even more importantly, a model size (1 MB) smaller than any comparable system in [7] and [14]. The system in Table 1 is trained only on CASIA data, and does not include any other training data. We were also interested in folding in additional training data collected in-house on iOS devices. This data covers a larger variety of styles (cf. next section) and comprises a lot more training instances per character. Table 2 reports
our results, on the same test set with a 3,755-character inventory. Table 2. Results on CASIA On-line Database, 3,755 Characters. Augmented Training, Associated Model Size = 15MB Though the resulting system has a much greater footprint (15 MB), accuracy only improves slightly (about 2% absolute for top-4 accuracy). This suggests that, by and large, most styles of characters appearing in the test set were already well-covered in the CASIA training set. It also indicates that there is no downside in folding more training data: the presence of additional styles is not deleterious to the underlying model. Since the ideal set of “frequently written” characters varies from one user to the next, a large population of users requires an inventory of characters much larger than 3,755. Exactly which ones to select, however, is not entirely straightforward. Simplified Chinese characters defined with GB2312-80 and traditional Chinese characters defined with Big5, Big5E, and CNS 11643-92 cover a wide range (from 3,755 to 48,027 Hànzì characters). More recently came HKSCS-2008 with 4,568 extra characters, and even more with GB18030-2000. We wanted to make sure that users are able to write their daily correspondence, in both simplified and traditional Chinese, as well as names, poems and other common markings, visual symbols, and emojis. We also wanted to support the Latin script for the occasional product or trade name with no transliteration. We followed Unicode as the prevailing international standard of character encoding, as it encapsulates almost all of the above mentioned standards.
(Note that Unicode 7.0 can specify over 70,000 characters in its extensions B-D, with more being considered for inclusion). Our character recognition system thus focused on the Hànzì part of GB18030-2005, HKSCS-2008, Big5E, a core ASCII set, and a set of visual symbols and emojis, for a total of approximately 30,000 characters, which we felt represented the best compromise for most Chinese users. After selecting the underlying character inventory, it is critical to sample the writing styles that users actually use. While there are formal clues as to what styles to expect (cf. [13]), there exist many regional variations, e.g., (i) the use of the U+2EBF (艹) radical, or (ii) the cursive U+56DB (四) vs. U+306E (の). Rendered fonts can also contribute to confusion as some users expect specific characters to be presented in a specific style. As a speedy input tends to drive toward cursive styles, it tends to increase ambiguity, e.g. between U+738B (王) and U+4E94 (五). Finally, increased internationalization sometimes introduces unexpected collisions: for example, U+4E8C (二), when cursively written, may conflict with the Latin characters “2” and “Z”. Our rationale was to offer users the whole spectrum of possible input from printed to cursive to unconstrained writing [5]. To cover as many variants as possible, we sought data from writers from several regions in Greater China. We were surprised to find that most users have never seen many of the rarer characters. This unfamiliarity results in hesitations, stroke-order errors, and other distortions which we needed to take into account. We collected data from paid participants across various age groups, gender, and with a variety of educational backgrounds. The resulting handwritten data is unique in many ways: comprising several thousands users, written with a finger, not a stylus, on iOS devices, in small batches of data. One advantage is that the sampling of iOS devices results in a very clear handwritten signal. We found a wide variety of writing styles. Figures 2-4 show some examples of the “flower” character U+82B1 (花), in printed, cursive, and unconstrained styles. The fact that, in daily life, users often write quickly and unconstrained can lead to cursive variations that have a very dissimilar appearance. Conversely, sometimes it also leads to confusability between different characters. Figures 5-7 show some of the concrete examples we observe in our data. Note that it is especially important to have enough training material to distinguish cursive variations such as in Figure 7. With the guiding principles discussed previously, we were able to collect tens of millions of character instances for our training data. Compare the 3,755-character system in the previous section with the results shows in Table 3, on the same test set, after increasing the number of recognizable characters from 3,755 to approximately 30,000. Table 3. Results on CASIA On-line Database, 30K Characters Note that the model size remains the same, as the system of
Table 2 was simply restricted to the “Hz-1” set of characters, but was otherwise identical. Accuracy drops marginally, which is to be expected, since the vastly increased coverage creates additional confusability of the kind mentioned earlier, for example “二” vs. “Z”. Compare Tables 1-3 and you’ll see that multiplying coverage by a factor of 10 does not entail 10 times more errors, or 10 times more storage. In fact, as the model size goes up, the number of errors increases much more slowly. Thus, building a high-accuracy Chinese character recognition that covers 30,000 characters, instead of only 3,755, is possible and practical. To get an idea of how the system performs across the entire set of 30,000 characters, we also evaluated it on a number of different test sets comprising all supported characters written in various styles.
Table 4 lists the average results. Table 4.
Average Results on Multiple In-House Test Sets Comprising All Writing Styles, 30,000 Characters Of course, the results in Tables 3-4 are not directly
comparable, since they were obtained on different test sets. Nonetheless, they show that top-1 and top-4 accuracies are in the same ballpark across the entire inventory of characters. This is a result of the largely balanced training regimen. The total set of CJK characters in Unicode (currently around 75,000 [12]) could increase in the future, as the ideographic rapporteur group (IRG) keeps on suggesting new additions from a variety of sources. Admittedly, these character variants will be rare (e.g., used in historical names or poetry). Nevertheless, this is of high interest to every person whose name happens to contain one of those rare characters. So, how well do we expect to handle larger inventories of characters in the future? The experiments discussed in this article support learning curves [2] based on training and test error rates with a varying amount of training data. We can therefore extrapolate asymptotic values, regarding both what our accuracy would look like with more training data, and how it would change with more characters to recognize. For example, given the 10-times larger inventory and corresponding (less than) 2% drop in accuracy between Tables 1 and 3, we can extrapolate that with an inventory of 100,000 characters and a corresponding increase in training data, it would be realistic to achieve top-1 accuracies around 84%, and top-10 accuracies around 97% (with the same type of architecture). In summary, building a high-accuracy handwriting recognition system which covers a large set of 30,000 Chinese characters is practical even on embedded devices. Furthermore, accuracy only degrades slowly as the inventory goes up, as long as training data of sufficient quality is available in sufficient quantity. This bodes well for the recognition of even larger character inventories in the future. [1] D.C. Ciresan, U. Meier, L.M. Gambardella, and J. Schmidhuber, Convolutional
Neural Network Committees For Handwritten Character Classification, in 11th Int. Conf. Document Analysis Recognition (ICDAR 2011), Beijing, China, Sept. 2011. [2] C. Cortes, L.D. Jackel, S.A. Jolla, V. Vapnik, and J.S. Denker, Learning Curves: Asymptotic Values and Rate of Convergence, in Advances in Neural Information Processing Systems (NIPS 1993), Denver, pp. 327–334, Dec. 1993. [3] G.E. Hinton and K.J. Lang, Shape Recognition and Illusory Conjunctions, in Proc. 9th Int. Joint Conf. Artificial Intelligence, Los Angeles, CA, pp. 252–259, 1985. [4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient– based Learning Applied to Document Recognition, Proc. IEEE, Vol. 86, No. 11, pp. 2278–2324, Nov. 1998. [5] C.-L. Liu, S. Jaeger, and M. Nakagawa, Online Recognition of Chinese Characters: The State-of-the-Art, IEEE Trans. Pattern Analysis Machine Intelligence, Vol. 26, No. 2, pp. 198–213, Feb. 2004. [6] C.-L. Liu, F. Yin, D.-H. Wang, and Q.-F. Wang, CASIA Online and Offline Chinese Handwriting Databases, in Proc. 11th Int. Conf. Document Analysis Recognition (ICDAR 2011), Beijing, China, Sept. 2011. [7] C.-L. Liu, F. Yin, Q.-F. Wang, and D.-H.Wang, ICDAR 2011 Chinese Handwriting Recognition Competition,in 11th Int. Conf. Document Analysis Recognition (ICDAR 2011), Beijing, China, Sept. 2011. [8] Y. Li, L. Jin , X. Zhu, T. Long, SCUT-COUCH2008: A Comprehensive Online Unconstrained Chinese Handwriting Dataset (ICFHR 2008), Montreal, pp. 165–170, Aug. 2008. [9] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun, What is the Best Multi-stage Architecture for Object Recognition?, in Proc. IEEE Int. Conf. Computer Vision (ICCV09), Kyoto, Japan, Sept. 2009. [10] U. Meier, D.C. Ciresan, L.M. Gambardella, and J. Schmidhuber, Better Digit Recognition with a Committee of Simple Neural Nets, in 11th Int. Conf. Document Analysis Recognition (ICDAR 2011), Beijing, China, Sept. 2011. [11] P.Y. Simard, D. Steinkraus, and J.C. Platt, Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis, in 7th Int. Conf. Document Analysis Recognition (ICDAR 2003), Edinburgh, Scotland, Aug. 2003. [12] Unicode, Chinese and Japanese,
http://www.unicode.org/faq/han_cjk.html, 2015. [13] F.F. Wang, Chinese Cursive Script: An Introduction to Handwriting in Chinese, Far Eastern Publications Series, New Haven, CT: Yale University Press, 1958. [14] F. Yin, Q.-F. Wang, X.-Y. Xhang, and C.-L. Liu, ICDAR2013 Chinese Handwriting Recognition Competition, in 11th Int. Conf. Document Analysis Recognition (ICDAR 2013), Washington DC, USA, Sept. 2013. Send questions or feedback via email Apply now for a career at Apple Apple Developer Program"
491eebff7b,SpaceX's self-landing rocket is a flying robot that's great at math — Quartz,"What you’re seeing in the video above is a 40 meter tall SpaceX rocket booster, weighing more than 20 tons, fly itself back to Earth from space and land precisely on a target at Cape Canaveral. What you may be wondering is, how did it do that? The first thing to know is that a rocket booster is in fact a large robot, steering itself back to earth without help from anyone else but its internal computers. The challenge SpaceX CEO Elon Musk set for his reusable rocket team, led by MIT-trained aerospace engineer Lars Blackmore, was to teach the rocket how to fly itself back to earth. Landing spacecraft is fundamental to exploring our solar system, as it’s the only effective way to bring heavy scientific equipment or people to a planetary surface. Scientists have been thinking about this since NASA began planning the moon landing in the 1960s. In 2007, Blackmore started working on similar problems at Caltech’s Jet Propulsion Laboratory, thinking about how to improve the landing ability of spacecraft being sent to explore the surface of Mars. In 2009, Blackmore and two colleagues observed in a paper (pdf) that the inability to land precisely on Mars meant that scientific exploration was taking a backseat to the realities of getting a spacecraft to land at a specific location. When planning these trips, engineers visualize landing precision as an imaginary ellipse on the surface of the planet, where the spacecraft has a 99% chance of landing. In 1997, when NASA sent a rover called Mars Pathfinder to the red planet, it was expected to land within an ellipse 150 kilometers across its major axis, which is not exactly what you want to hear if you’re scientist with a specific destination in mind. By the time the Mars Curiosity rover landed in 2012, JPL’s engineers had the landing ellipse shrunk to 20 kilometers across. That’s still a lot of uncertainty—imagine if you were taking a plane somewhere and you were told you’d land within 20 kilometers of your destination. But SpaceX has its rockets landing within ellipses of 60- and 20-meters across, on a Cape Canaveral landing pad and on sea-going landing barges, respectively—an improvement by several orders of magnitude. A big reason for the uncertainty in most Mars rovers is the use of parachutes to land, because engineers are still learning how to use rockets to slow a spacecraft’s descent through a planet’s atmosphere at hypersonic speeds. SpaceX, in developing its reusable rocket, is the only organization to have brought a rocket back from space by actually flying it to Earth at that velocity, and it has shared its groundbreaking work on this aerodynamics problem with NASA to help scientists there plan future Martian missions. But once the physics are mastered of maneuvering a rocket-powered spacecraft in for landing, the rocket still needs to be taught to fly itself down. Using rocket thrust to control descent allows companies like SpaceX and Blue Origin, whose smaller suborbital rocket has successfully landed and been reused several times, to hit their landing targets precisely by eliminating the parachute drift normally associated with spacecraft falling to earth. There aren’t human pilots on most spacecraft these days, and it can be difficult to communicate with spacecraft as they land, either because of the distances involved with planetary exploration, or on Earth, because friction with the atmosphere as a rocket flies back down can create an ionization field (a static shock on a grand scale) capable of blocking radio signals. The computing challenge is simple to describe and hard to execute: Plot the optimal path down to the target without running out of fuel. That’s complicated enough, but also consider the time constraint: The rocket’s computers need to solve this problem before they run out of fuel or crash into earth—in a “fraction of a second,” according to Blackmore. He and his colleagues developed one of the first algorithms to do this in three dimensions in that 2009 paper on Mars landings, receiving a patent on their ideas in 2013. The solution involves solving a “convex optimization problem,” a common challenge in modern machine learning. In wildly reductive layman’s terms, it involves considering all the possible answers to the question of “what’s the best way to get from here to the landing pad without running out of fuel” as a geometric shape, and uses mathematical tools developed first by John von Neumann, the father of game theory, and refined by Indian mathematician Narendra Karmarkar in the 1980s, to quickly choose the best way down from that set. At SpaceX, Blackmore and his team have updated the landing algorithms (PDF, p. 15), using software developed by Stanford computer scientists “to generate customized flight code, which enables very high speed onboard convex optimization.”As the rocket reacts to changes in the environment that alter its course—known as “dispersions”—the on-board computers recalculate its trajectory to ensure that it will still be 99% sure to land within its target. So far, it has: SpaceX has landed eight boosters since its first successful attempt in December 2015, including its last four flights. The three failures to land in that time period were caused by hardware issues, not a failure to navigate successfully to the landing area. SpaceX executives are reluctant to say they now expect landings to succeed, preferring to keep their focus on the primary mission of launching cargo for clients, but it’s clear that reliability is improving. The company plans to fly the first of its “flight-proven” stages in a March mission for the European satellite operator SES. Should they succeed and be able to regularly fly previously-used rockets, CEO Musk has said the cost of launch could drop by 30%."
9d69e0e1ab,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
4f32bddd20,This Company Uses AI To Help Lenders Automate The Mortgage Loan Process,"A recent report by the National Association of Realtors revealed that homeowners under 36 years-old make up the majority of buyers in the United States at 34%, with 66% of these home buyers purchasing for the first time. Additionally, 49% of these buyers are raising children under the age of 18 in the home, with 66% of this demographic representing married couples. More significantly, home buyers between the age of 37-51, considered to be peak earning years, have the highest average income of any buyer type at $106,000. As the purchasing power among millennials continues to increase, there has been a bigger push for young people to purchase property and acquire assets that will not only provide stability but further generate wealth. Seeing these shifting trends in the housing market, mortgage lenders are in a rush to close home loans faster than ever before.
However, low mortgage rates only lead to increased competition amongst lenders. Consequently, for mortgage lenders to remain competitive in the flooded market, they must leverage automation and other tech tools to efficiently expedite the lending process. Noticing this need and growing opportunity, one startup emerged to offer an automated solution. Unisource is a national title and escrow company that uses machine learning and artificial intelligence to provide tailored lending solutions for mortgage and real estate agencies. By using machine learning, Unisource can customize transactional services around the individual needs of each client. Their proprietary technology also allows mortgage and real estate firms to automate the lending process and adapt to regulatory changes while maintaining compliance. Unisource CEO Michael Cohan explains the vision behind his company, the future of the real estate market, and how the rise of automation is transforming the lending industry. What specific void or opportunity did you discover that inspired the idea behind your company? Michael Cohan: I have a very tech heavy background, which submersed me in an environment that had a process for everything. As I researched industries that could use more process and automation, it was apparent that title and escrow had a significant void. Specifically, automated technologies like artificial intelligence and machine learning.
I wanted to provide faster, more efficient services using a turnkey solution that would benefit the real estate sector. What were some of the challenges you faced getting your company off the ground and what lessons did you learn from this process? Michael Cohan: I started Unisource in 2003, and had a few years to build a solid foundation for the company. However, it didn’t feel like long until one of the biggest financial meltdowns this country has seen set in. As a result, the investment we were making into our technology became difficult. There was no stability in the industry. We managed to carry on in a down market, and in surviving that downturn, the most important lesson I learned was to invest in your company. Specifically, in technology and operations, putting all of your focus into your business.
Denying any distractions or naysayers will bring you out stronger on the other side.  We never stopped investing in our company and our processes, even though buildings and high rises were crumbling around us, we stayed focused. Describe how your services work and what is the strategic thinking behind your business model? Michael Cohan: One of the biggest challenges the industry faces is compliance on a national level for our customers. We come in as a strategic partner that specializes our technology to help maintain compliance while streamlining and automating the loan process. depending. Helping our customers maintain compliance is a huge driver of Unisource as well as providing customized technology solutions to help maintain a competitive structure. Mining data in some constituencies is so archaic that in order to mine data, an abstractor must be hired to physically go through the courthouse and source records. We believe that by making data readily available through artificial intelligence, specifically artificial intelligence and machine learning, we can excel in the lending process. What went into the process of developing your proprietary technology and what specific problems were you looking to solve for when building it? Michael Cohan: We worked hand in hand with our customers to see what challenges they were dealing with, particularly challenges that could be automated, such paperwork. It was and still is a ‘help us help you’ scenario. We also sought our internal title and escrow experts and sourced their knowledge on what was lacking in the market both operationally and technology wise. This gave us the prowess we needed to improve the industry from an internal standpoint.
Some of the brightest developers in our industry helped design and execute the vision that our customers and consultants created. How does the use of machine learning give you a competitive advantage in the space and what added benefits does it offer to clients? Michael Cohan: There is both an internal and external competitive advantage to automation in our industry. Internally, the processes are far more efficient. There is less human error, and a larger workload can be completed in less time. This drastically reduces costs for us to produce documents, such as reports, which is then transferred into savings for our customers. By leveraging technology to streamline operations, the real estate sector as a whole will be much more efficient. Externally, we are able to provide higher levels of service to our customers. We can quickly pull all documents, reports, transactions, etc. and query those documents to gather data efficiently. As investment in technology goes on, overhead costs will be drastically lower, therefore providing the consumer a lower price point with faster turnaround times. What are the core principles or central mission driving your business? Michael Cohan: Delivering individualized solutions to every customer has always been a driving force for me. In this industry, there is little pattern to follow. We aim to speed the lending process by staying on top of rules and regulation in all 50 states to make sure our customers can execute loans as quickly as possible. The ability to deliver individual solutions to each client quickly means we can continue to provide a lower cost to the consumer. How do you see the company evolving in 3-5 years and what impact do you hope to have on the industry? Michael Cohan: In the next 3-5 years, there’s going to be a relatively smaller number of providers. Those that remain must adapt to better processes, such as going entirely electronic, in order to stay competitive both operationally and cost offerings. Besides, state and federal regulations are becoming more and more difficult every year. The more digital title and escrow companies get, the more streamlined they will be when it comes to stricter compliance. We will continue to develop streamlined processes for staying on top of national and statewide regulation. Lastly, I can’t wait to see a paperless, mortgage closing. While I don’t dictate the policy on this, I see the future as completely electronic. Imagine the lender working with their customer on an iPad; they can sign and upload docs immediately into the Unisource system and expedite the lending process into a matter of days, not weeks."
e3c7d31d42,Machine learning reveals lack of female screen time in top films | New Scientist,"Daily news
8 March 2017
20th Century Fox By Matt Reynolds Machine learning is taking on Hollywood’s gender bias. Technology that automatically detects how often men and women appear on screen reveals that in recent popular films, men have had almost twice as much screen time as women. The software uses algorithms for face and voice recognition that have been trained on annotated video to identify whether a character is male or female, and can measure how long they are on screen to a fraction of a second. It was developed by Shri Narayanan at the University of Southern California, Los Angeles, in partnership with the Geena Davis Institute on Gender in Media and Google.org, the search engine’s charitable arm. In an analysis of the 100 highest-grossing live-action films from each of the past three years, the software found that women appear on average for just 36 per cent of the total time that characters are on screen. Oscar-winning films are even less representative, with women getting just 32 per cent of screen time and 27 per cent of speaking time in films that received an Academy Award.
Using algorithms for voice and face recognition that run in parallel, Narayanan’s program can analyse a feature film in less than 15 minutes. The software can also distinguish when one person is on screen but someone else is talking, says Narayanan. “Often when women are speaking, it’s actually the men that are shown on screen.” Of the 300 films analysed, the only genre in which women spent more time on screen than men was horror, with female characters receiving 53 per cent of screen time in such films. In dramas, women were only on screen for an average of a third of the film, and for crime films that figure dropped to less than a quarter. The system did not account for non-binary characters. Calculating how often women appear on screen is one way to measure gender bias, says Ginette Vincendeau at King’s College London, but it’s also important  to consider who they are portraying and what their characters are talking about. “It’s not just a question of quantity,” she says. Narayanan is also using machine-learning algorithms to analyse film scripts, which could reveal further insights about the roles of female characters, such as the content of their dialogue. He has previously experimented with using natural language processing – AI that can process speech – to explore how the use of gendered language varies across films and genres. Although the latest study indicates that Hollywood still has a long way to go until women are equally represented in films, it also suggests that audiences have an appetite for movies with female leads. Narayanan found that films with a female lead character performed better at the box office, earning more than $89 million on average, compared with $75 million for those with male leads. The highest-grossing Best Picture nominee at this year’s Oscars to date was Hidden Figures, a biographical film about a group of female African American mathematicians who played a vital role at NASA in the early years of the US space programme. It has grossed over $158 million at the US box office so far. But just because we’ve seen a few films with female leads do well at the box office doesn’t mean that Hollywood is changing its ways, says Vincendeau. The real agents of change in the film world are the production companies, she says, and it’s up to them to make sure that more women are placed in leading roles – both behind the camera and in front of it. A shorter version of this article was published in New Scientist magazine on 18 March 2017"
f21bc6d785,Improving clinical trials with machine learning | Science|Business,"Machine learning could improve our ability to determine whether a new drug works in the brain, potentially enabling researchers to detect drug effects that would be missed entirely by conventional statistical tests, finds a new UCL study published today in Brain. “Current statistical models are too simple. They fail to capture complex biological variations across people, discarding them as mere noise. We suspected this could partly explain why so many drug trials work in simple animals but fail in the complex brains of humans. If so, machine learning capable of modelling the human brain in its full complexity may uncover treatment effects that would otherwise be missed,” said the study’s lead author, Dr Parashkev Nachev (UCL Institute of Neurology). To test the concept, the research team looked at large-scale data from patients with stroke, extracting the complex anatomical pattern of brain damage caused by the stroke in each patient, creating in the process the largest collection of anatomically registered images of stroke ever assembled. As an index of the impact of stroke, they used gaze direction, objectively measured from the eyes as seen on head CT scans upon hospital admission, and from MRI scans typically done 1-3 days later. They then simulated a large-scale meta-analysis of a set of hypothetical drugs, to see if treatment effects of different magnitudes that would have been missed by conventional statistical analysis could be identified with machine learning. For example, given a drug treatment that shrinks a brain lesion by 70%, they tested for a significant effect using conventional (low-dimensional) statistical tests as well as by using high-dimensional machine learning methods. The machine learning technique took into account the presence or absence of damage across the entire brain, treating the stroke as a complex “fingerprint”, described by a multitude of variables.
“Stroke trials tend to use relatively few, crude variables, such as the size of the lesion, ignoring whether the lesion is centred on a critical area or at the edge of it. Our algorithm learned the entire pattern of damage across the brain instead, employing thousands of variables at high anatomical resolution. We used well-established methods of machine learning, teaching the algorithm on subsets of data and then testing its performance on other subsets it had not seen,” explained the study’s first author, Tianbo Xu (UCL Institute of Neurology). The advantage of the machine learning approach was particularly strong when looking at interventions that reduce the volume of the lesion itself. With conventional low-dimensional models, the intervention would need to shrink the lesion by 78.4% of its volume for the effect to be detected in a trial more often than not, while the high-dimensional model would more than likely detect an effect when the lesion was shrunk by only 55%. “Conventional statistical models will miss an effect even if the drug typically reduces the size of the lesion by half, or more, simply because the complexity of the brain’s functional anatomy—when left unaccounted for—introduces so much individual variability in measured clinical outcomes. Yet saving 50% of the affected brain area is meaningful even if it doesn’t have a clear impact on behaviour. There’s no such thing as redundant brain,” said Dr Nachev. The researchers say their findings demonstrate that machine learning could be invaluable to medical science, especially when the system under study—such as the brain—is highly complex. “The real value of machine learning lies not so much in automating things we find easy to do naturally, but formalising very complex decisions. Machine learning can combine the intuitive flexibility of a clinician with the formality of the statistics that drive evidence-based medicine. Models that pull together 1000s of variables can still be rigorous and mathematically sound. We can now capture the complex relationship between anatomy and outcome with high precision,” said Dr Nachev. “We hope that researchers and clinicians begin using our methods the next time they need to run a clinical trial,” said co-author Professor Geraint Rees (Dean, UCL Faculty of Life Sciences). The study was funded by Wellcome and the National Institute for Health Research University College London Hospitals Biomedical Research Centre. For more information or to speak to the researchers involved, please contact Chris Lane, UCL press office: T: +44 (0)20 7679 9222 / +44 (0)7717 728 648, E: chris.lane@ucl.ac.uk Tianbo Xu, Hans Rolf Jäger, Masud Husain, Geraint Rees & Parashkev Nachev, ‘High-dimensional therapeutic inference in the focally damaged human brain,’ will be published in Brain on Wednesday 15 November 2017, 08:00 UK time / 03:00 US EST and is under a strict embargo until this time. The paper will be published at https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awx288.   This release was first published 15 November by University College London.   Meet with universities and research centres, corporate executives, international investors, top European policy makers as well as regional and national organisations dedicated to stimulate innovation. More info Name Email Tweets by scibus
Privacy   |   T&Cs   |   © 2018 Science|Business"
10e498b8e4,Google's neural networks invent their own encryption | New Scientist,"Daily news
26 October 2016
John Lund/Getty By Timothy Revell Computers are keeping secrets. A team from Google Brain, Google’s deep learning project, has shown that machines can learn how to protect their messages from prying eyes. Researchers Martín Abadi and David Andersen demonstrate that neural networks, or “neural nets” – computing systems that are loosely based on artificial neurons – can work out how to use a simple encryption technique. In their experiment, computers were able to make their own form of encryption using machine learning, without being taught specific cryptographic algorithms. The encryption was very basic, especially compared to our current human-designed systems. Even so, it is still an interesting step for neural nets, which the authors state “are generally not meant to be great at cryptography”.
The Google Brain team started with three neural nets called Alice, Bob and Eve. Each system was trained to perfect its own role in the communication. Alice’s job was to send a secret message to Bob, Bob’s job was to decode the message that Alice sent, and Eve’s job was to attempt to eavesdrop. To make sure the message remained secret, Alice had to convert her original plain-text message into complete gobbledygook, so that anyone who intercepted it (like Eve) wouldn’t be able to understand it. The gobbledygook – or “cipher text” – had to be decipherable by Bob, but nobody else. Both Alice and Bob started with a pre-agreed set of numbers called a key, which Eve didn’t have access to, to help encrypt and decrypt the message. Initially, the neural nets were fairly poor at sending secret messages. But as they got more practice, Alice slowly developed her own encryption strategy, and Bob worked out how to decrypt it. After the scenario had been played out 15,000 times, Bob was able to convert Alice’s cipher text message back into plain text, while Eve could guess just 8 of the 16 bits forming the message. As each bit was just a 1 or a 0, that is the same success rate you would expect from pure chance. The research is published on arXiv. We don’t know exactly how the encryption method works, as machine learning provides a solution but not an easy way to understand how it is reached. In practice, this also means that it is hard to give any security guarantees for an encryption method created in this way, so the practical implications for the technology could be limited. “Computing with neural nets on this scale has only become possible in the last few years, so we really are at the beginning of what’s possible,” says Joe Sturonas of encryption company PKWARE in Milwaukee, Wisconsin. Computers have a very long way to go if they’re to get anywhere near the sophistication of human-made encryption methods. They are, however, only just starting to try. Journal reference: arxiv.org/abs/1610.06918 More on these topics:"
b890d54636,How Machine Learning Is Helping Drive Cloud Adoption - DZone AI,"{{node.type}}
·
{{ node.urlSource.name }}
·
by {{node.authors[0].realName }}
Let's be friends:
Comment (0)
Join the DZone community and get the full member experience. Find out how AI-Fueled APIs from Neura can make interesting products more exciting and engaging.  An interesting trend that is helping drive cloud adoption is the rise of machine learning. As organizations seek ways to automate more processes, the use of machine learning is also increasing in order to meet those needs. As machine learning has increased in popularity, it has helped to increase the use of the cloud to help store and process the massive amounts of data that can be required for it.  Machine learning is a way for computers to “learn” things without needing to have as much specific programming done as would a typical application. Machine learning can help create algorithms which can use data and either learn from the data or make various types of predictions based on identified patterns in the data. Such learning can be useful for a number of tasks that would otherwise be much more difficult, such as OCR (optical character recognition), email filtering, medical monitoring, text analysis, photo analysis, video analysis, translation, speech recognition, and many others. For example, email filtering can be done much more simply, since patterns used by spam or scam emails can be identified and allow for a better chance that those types of messages will be moved to a “spam” folder or blocked entirely. This is certainly useful in helping to keep inboxes free of messages most users do not want to sift through on their own every day!
Image depicting machine learning as the focal point of the cloud. Source: Forbes. As noted in the image from Forbes, there are a number of things that are driven by machine learning that also help drive cloud innovation. Things such as business intelligence, personal assistants, IoT (Internet of Things), bots, and cognitive computing are brought about by machine learning, which in turn allows for the cloud to be a desirable place to collect, store, analyze, and retrieve the data needed for these various applications. For instance, IoT is big on connecting machines so that they can communicate with one another and exchange data. Machine learning helps to drive these types of interactions, and using the cloud makes it even easier for machines to exchange data with one another as there will be an easy way to make those connections.   As a result, the cloud has seen much innovation in its ability to handle this type of data exchange as cloud systems have become more flexible and offer the ability to scale much more easily than with a traditional data center. Machine learning and cloud services make an excellent combination, as many cloud services make it easy to provision the resources needed for the collection, storage, and retrieval of large amounts of data. The biggest reasons this works is that such cloud services offer both flexibility and scalability. A cloud service is typically flexible enough to allow you to provision servers with different specifications, depending on the needs of the various pieces required for the machine learning that needs to occur. For example, if you need different operating systems for different servers, it is as simple as selecting them and spinning up the server.  The same applies to provisioning various different databases such as MySQL, MongoDB, and so on — you can simply spin up what you need easily and move on to any setup and programming that needs to be done without worrying about acquiring hardware. Scalability is another big reason that machine learning and the cloud work well together. Since machine learning often needs a progressively larger amount of storage space, using a cloud service makes a lot of sense.  With a standard data center, you can end up spending a great deal on what you determine the maximum amount of storage space is that you will need up front, with the potential to still need more later. This can be quite costly both in the beginning and as time goes on. On the other hand, a typical cloud service allows you to purchase only the storage space you need up front, then scale up to more space as you need it. This definitely saves money in the beginning and allows you to add more space as needed rather than simply jumping to an incredibly large amount of space before you need it. As you can imagine, this would definitely help to drive cloud adoption! To find out how AI-Fueled APIs can increase engagement and retention, download Six Ways to Boost Engagement for Your IoT Device or App with AI today. Free DZone Refcard
Comment (0)
Published at DZone with permission
of
John Pollock, DZone MVB. See the original article here. Opinions expressed by DZone contributors are their own. {{ ::node.title }}"
69bce4311d,How Machine Learning Is Helping Us Predict Heart Disease and Diabetes,"While debate drags on about legislation, regulations, and other measures to improve the U.S. health care system, a new wave of analytics and technology could help dramatically cut costly and unnecessary hospitalizations while improving outcomes for patients. For example, by preventing hospitalizations in cases of just two widespread chronic illnesses — heart disease and diabetes — the United States could save billions of dollars a year. Toward this end, the author and his colleagues at Boston University have been striving to bring the power of machine-learning algorithms to this critical problem. In an ongoing effort with Boston-area hospitals, they found that they could predict hospitalizations due to these two chronic diseases about a year in advance with an accuracy rate of as much as 82%. This will give care providers the chance to intervene much earlier and head off hospitalizations. This accomplishment is only one example of how analytics could transform health care. While debate drags on about legislation, regulations, and other measures to improve the U.S. health care system, a new wave of analytics and technology could help dramatically cut costly and unnecessary hospitalizations while improving outcomes for patients. For example, by preventing hospitalizations in cases of just two widespread chronic illnesses — heart disease and diabetes — the United States could save billions of dollars a year. Toward this end, my colleagues and I at Boston University’s Center for Information and Systems Engineering have been striving to bring the power of machine-learning algorithms to this critical problem. In an ongoing effort with Boston-area hospitals, including the Boston Medical Center and the Brigham and Women’s Hospital, we found that we could predict hospitalizations due to these two chronic diseases about a year in advance with an accuracy rate of as much as 82%. This will give care providers the chance to intervene much earlier and head off hospitalizations. Our team is also working with the Department of Surgery at the Boston Medical Center and can predict readmissions within 30 days of general surgery; the hope is to guide postoperative care in order to prevent them. The hospitals provide patients’ anonymized electronic health records (EHRs) that contain all of the information the hospital has about each patient, including demographics, diagnoses, admissions, procedures, vital signs taken at doctor visits, medications prescribed, and lab results. We then unleash our algorithms to predict who might have to be hospitalized. This gives the hospital opportunities to intervene, treat the disease more aggressively in an outpatient setting, and avoid a costly hospitalization while improving the patient’s condition. The accuracy rates of these predictions surpass what is possible with well-accepted risk scoring systems such as the one that emerged from the famous Framingham Heart Study, the ongoing long-term cardiovascular cohort study that is now in its third generation of participants. Using that system, a doctor assesses the patient’s age, cholesterol, weight, blood pressure, and several other factors to arrive at the individual’s chances of developing cardiovascular disease over the next 10 years. Using the Framingham Study 10-year cardiovascular risk score, one can predict hospitalizations with an accuracy of about 56%, which is substantially lower than the 82% rate we achieved. In fact, we found that feeding the factors used in the Framingham 10-year risk score into more sophisticated machine-learning methods still leads to results inferior to ours (an accuracy rate of about 69%). This suggests that using the entirety of a patient’s EHR (which can contain as much as 200 factors) instead of just a few key factors leads to superior prediction results. What’s more, an algorithmic approach can easily be scaled so it can be applied to a very large number of patients — something that is impossible with human monitors only. The potential benefits from applying machine-learning analytics in health care are enormous. Based on a study of a year’s worth of hospital admissions, the U.S. Agency for Healthcare Research and Quality (AHRQ) estimated that 4.4 million of those admissions in the United States, totaling $30.8 billion in costs, could have been prevented. Of that $30.8 billion, $9 billion was for patients with heart diseases and $5.8 billion for patients with complications from diabetes. That’s half of all unnecessary hospitalizations. Just 5% of Medicaid’s 70 million beneficiaries account for 54% of Medicaid annual expenditures of more than $500 billion, and 1% account for 25% of the total. Of this 1%, 83% have at least three chronic conditions. Approaches like ours could reduce their use of hospital services and save Medicaid a large amount of money. Ongoing U.S. reforms in health care that link payments with outcomes are forcing hospitals to assume more financial risks. In response, hospitals are increasingly making analytics and new technologies an integral part of hospital operations. Business analytics widely used in the transportation industry by airlines and shipping companies are beginning to be employed to schedule operating rooms and staffing. Other algorithms are being developed to assist physicians in making diagnoses. My team has developed methods to automatically titrate medications in intensive care units in response to the patient’s condition. These advances are only the tip of the iceberg. We are on the cusp of major changes in health monitoring and care. Google and other companies with lots of experience in collecting and learning from data appear ready to step into this domain. A myriad of technologies, from implantable medical devices (such as defibrillators and pacemakers) to fit trackers, smart watches, and smartphones already capture our health data and lifestyle choices. Our credit-card and electronic-payment systems know our purchase history and the type of food we consume. The result is the emergence of a rich personal health record we carry in our pockets. If we can now predict future hospitalizations with more than 80% accuracy using medical records alone, imagine what is possible if we can tap into this trove of personal data. Recommender systems could be used to nudge us to adopt healthier eating habits and behaviors. The holy grail of heading off the emergence of conditions by keeping people well could be realized. Yes, analytics and data-driven personalized medicine and health monitoring present risks. Do we want our employers and health insurers to know the status of our health and the risks we face? Privacy, security, and reliability of new systems and methods are also critical concerns. But rather than retreating from this new era, we should be working on how to strengthen our methods, institutions, laws, and regulatory framework to avoid those unintended consequences. Algorithms — the foundation of encryption methods, privacy-preserving data processing, and intrusion- and fraud detection systems — could help. Yannis Paschalidis is a professor of engineering and the director of the Center for Information and Systems Engineering at Boston University."
e509926fbd,Scientists Use Machine-learning to Analyze Language in Movies  | Technology Networks,"News   Nov 14, 2017
| Original story from University of Washington
In the movie Frozen, only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of agency and power as Cinderella, a movie character that debuted in 1950. Credit: University of Washington
At first glance, the movie ""Frozen"" might seem to have two strong female protagonists -- Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom.But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists.The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed.""'Frozen' is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,"" said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies.""Anna is actually portrayed with the same low levels of power and agency as Cinderella, which is a movie that came out more than 60 years ago. That's a pretty sad finding,"" Sap said.The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like ""Heathers"" to romantic comedies like ""500 Days of Summer"" to war films like ""Apocalypse Now.""In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men. For example, male characters spoke more in imperative sentences (""Bring me my horse"") while female characters tended to hedge their statements (""Maybe I am wrong""). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives.To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on ""connotation frames"" that give insights into how different verbs can empower or weaken different characters through their connotative meanings. The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments.The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3.""For example, if a female character 'implores' her husband, that implies the husband has a stance where he can say no. If she 'instructs' her husband, that implies she has more power,"" said co-author Ari Holtzman, an Allen School doctoral student. ""What we found was that men systematically have more power and agency in the film script universe.""Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose.Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb's subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies.The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters -- exposing subtle differences and biases.In 2010's ""Black Swan,"" a movie centered around a female lead -- a perfectionist ballerina who slowly loses grip on reality -- the movie's dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film.In the 2007 movie ""Juno,"" about an offbeat young woman who unexpectedly gets pregnant, male characters' scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue.The UW team's tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man.The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers.""We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,"" said co-author and Allen School doctoral student Hannah Rashkin.Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn't limited to movies, but could be applied to books, plays or any other texts.""We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,"" said senior author Yejin Choi, an associate professor in the Allen School. ""We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.""This article has been republished from materials provided by University of Washington. Note: material may have been edited for length and content. For further information, please contact the cited source.
SLIMS Users Can Now Connect to the iSpecimen Marketplace
Genohm and iSpecimen have jointly announced that Genohm has established a partnership with iSpecimen to enable users of the SLIMS laboratory information management system to seamlessly connect to the iSpecimen Marketplace.
Ovation and Coriell Life Sciences Partner to Streamline Genetic Reporting Workflows
Ovation.io, Inc. (Ovation), makers of the fastest-growing clinical laboratory information and commercialization platform, and Coriell Life Sciences, Inc., an innovative provider of clinical genetic reporting solutions, announced a partnership and platform integration designed to create a seamless ecosystem for sample and workflow management, genetic data interpretation, and external client communication.
Blue Brain Nexus: An Open-Source Tool for Data-Driven Science
The Blue Brain Nexus will help enable data-driven neuroscience through searching, integrating and tracking large-scale data and models.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 11, 2018 Video News   Jan 11, 2018 News   Jan 11, 2018 Article Article"
32416d5af4,AI learns to predict the future by watching 2 million videos | New Scientist,"Technology news
28 November 2016
A deep learning system generates the next few frames of a story based on just one image, helping it to predict the future and understand the present Kenneth Whitten/Plainpicture By Victoria Turk AN ARTIFICIAL intelligence system can predict how a scene will unfold and dream up a vision of the immediate future. Given a still image, the deep learning algorithm generates a mini video showing what could happen next. If it starts with a picture of a train station, it might imagine the train pulling away from the platform, for example. Or an image of a beach could inspire it to animate the motion of lapping waves. Teaching AI to anticipate the future can help it comprehend the present. To understand what someone is doing when they’re preparing a meal, we might imagine that they will next eat it, something which is tricky for an AI to grasp. Such a system could also let an AI assistant recognise when someone is about to fall, or help a self-driving car foresee an accident.
“Any robot that operates in our world needs to have some basic ability to predict the future,” says Carl Vondrick at the Massachusetts Institute of Technology, part of the team that created the new system. “For example, if you’re about to sit down, you don’t want a robot to pull the chair out from underneath you.” Vondrick and his colleagues will present their work at a neural computing conference in Barcelona, Spain, on 5 December. To develop their AI, the team trained it on 2 million videos from image sharing site Flickr, featuring scenes such as beaches, golf courses, train stations and babies in hospital. These videos were unlabelled, meaning they were not tagged with information to help an AI understand them. After this, the researchers gave the model still images and it produced its own micro-movies of what might happen next. “One network generates the videos, and the other judges whether they look real or fake” To teach the AI to make better videos, the team used an approach called adversarial networks. One network generates the videos, and the other judges whether they look real or fake. The two get locked in competition: the video generator tries to make videos that best fool the other network, while the other network hones its ability to distinguish the generated videos from real ones. At the moment, the videos are low-resolution and contain 32 frames, lasting just over 1 second. But they are generally sharp and show the right kind of movement for the scene: trains move forward in a straight trajectory while babies crumple their faces. Other attempts to predict video scenes, such as one by researchers at New York University and Facebook, have required multiple input frames and produced just a few future frames that are often blurry. The videos still seem a bit wonky to a human and the AI has lots left to learn. For instance, it doesn’t realise that a train leaving a station should also eventually leave the frame. This is because it has no prior knowledge about the rules of the world; it lacks what we would call common sense. The 2 million videos – about two years of footage – are all the data it has to go on to understand how the world works. “That’s not that much in comparison to, say, a 10-year-old child, or how much evolution has seen,” says Vondrick. That said, the work illustrates what can be achieved when computer vision is combined with machine learning, says John Daugman at the University of Cambridge Computer Laboratory. He says that a key aspect is an ability to recognise that there is a causal structure to the things that happen over time. “The laws of physics and the nature of objects mean that not just anything can happen,” he says. “The authors have demonstrated that those constraints can be learned.” Vondrick is now scaling up the system to make larger, longer videos. He says that while it may never be able to predict exactly what will happen, it could show us alternative futures. “I think we can develop systems that eventually hallucinate these reasonable, plausible futures of images and videos.” This article appeared in print under the headline “AI predicts the future” More on these topics:
Magazine issue 3102, published 3 December 2016 Previous article Stop buying organic food if you really want to save the planet Next article AI pilot helps US air force with tactics in simulated operations"
612b03564e,Bringing machine learning to last mile health challenges | Devex ,"Connect SAN FRANCISCO — A new microscope will use image recognition software and machine learning technology to identify and count malaria parasites in a blood smear. The EasyScan GO, announced at MEDICA, the medical industry’s leading trade fair, is the result of a partnership between the Global Good Fund, a Seattle-based group funded by philanthropist Bill Gates, and Motic, a China-based company that specializes in manufacturing microscopes. Field tests have demonstrated that the machine learning algorithm is as reliable as an expert microscopist in fighting the spread of drug resistant malaria. EasyScan GO is the latest example of Global Good’s partnering to bring emerging technologies to health systems in low resource settings. Based at the invention company Intellectual Ventures, Global Good is focused on developing and deploying technologies for the poorest parts of the world. It partners with others to bring the benefits of technological innovation to the hardest markets in the world to reach.
The Global Good motto is “invention saving lives.” But invention means nothing if these products are developed without any consideration of the applicability of that technology in the markets they are meant to serve, said Maurizio Vecchione, senior vice president at Global Good. He emphasized that his team cannot fulfill its mission unless that technology is affordable, appropriate, accessible, and those 3 A’s, as he calls them, are at the center of every Global Good partnership.
Proponents see enormous potential for this technology. Today, machine learning can help tell us we are sick; emerging technologies like artificial intelligence will allow us to prevent illness, according to AnthroTronix founder Corinna Lathan, who wrote a World Economic Forum blog post on ways artificial intelligence and robotics will transform health care.
Several recent partnerships point to how Global Good is working to make sure the benefits of AI, machine learning, and deep learning extend to developing countries.
One in four children in the world are not registered at birth. Because they lack formal identification, it is very difficult to track their health records, including vaccinations.
Earlier this month, start-up Element Inc. announced a partnership with Global Good to develop a smartphone based platform to verify the identity of infants and children. Element Inc. focuses on developing mobile software for biometric recognition. Their aim is to deliver identity for everyone, and particularly the 1 billion people who lack proper IDs in Asia and Africa.
In Bangladesh and Cambodia, the BioNIC partnership will extend Element’s adult identity platform to children under age five, testing on fingerprints, ears and feet, irises, and more offline and on any mobile device.
“Accurately identifying patients, and linking them to their electronic medical records — which store past medical conditions, drug allergy information, medications and vaccination history — is critical to health care,” David Bell, director of the global health technologies portfolio at Global Good, said in a press release. “While adult biometrics are widely used in financial services, these have not been translated to the health sector where the need to identify a person from birth to old age presents additional challenges; namely, that infants have delicate, rapidly changing features that are difficult to capture.” This technology has only become possible in recent years, Element co-founder and CEO Adam Perold told Devex via email.
“Mobile devices now proliferate the world, at nearly three billion units; and each year, they get better and less expensive. At the same time, deep learning — through improved algorithms and increasingly powerful computers — was driving breakthroughs in how systems process video and images. We were one of the first to apply deep learning methods on mobile devices, and as a result, we have built a platform that traverses many challenges that still prevent widespread adoption of biometrics across low-resource settings,” he said.
When problems can be tackled by deep learning, the barriers in delivery are often structural, Perold continued, explaining that it is a resource heavy approach requiring lots of data, computing power, and specialized training.
“Deep learning solves key issues of access in biometric recognition. The strengths of the approach, where algorithms train themselves directly from the data, means that anyone can be recognized regardless of their features,” he said, adding that the technology “can be deployed to continuously adapt to any changes as a person develops.” The BioNIC partnership has the ultimate goal of creating a biometric solution capable of following patients from birth to old age, said Perold. Malaria kills almost half a million people every year, and one of the major challenges to eradication is the rapid spread of a multidrug-resistant strain in parts of Southeast Asia.
Currently, only analysis by a World Health Organization certified microscopist can accurately detect severe and drug-resistant cases. EasyScan GO automates that process with an intelligent microscope, simplifying and standardizing malaria detection in under-resourced countries where there is often a lack of trained personnel. “Malaria is one of the hardest diseases to identify on a microscope slide,” Bell of Global Good said in a press release. “By putting machine learning-enabled microscopes in the hands of laboratory technicians, we can overcome two major barriers to combating the mutating parasite — improving diagnosis in case management and standardizing detection across geographies and time.” The EasyScan GO uses AI-enabled software to automatically and accurately identify and count malaria parasites in a blood smear in as little as 20 minutes, Sebastian Nunnendorf, head of research and development at Motic China, said in an email to Devex. “Human capacity issues have long been a major bottleneck in low-resource settings, and the fact that we are able to address these issues through technological innovation is truly exciting,” he said. Any new medical device will need time in the market to validate, Nunnendorf said, and like the BioNIC partnership, EasyScan GO is still early in its journey. But the company hopes to go beyond diagnosing malaria and a few other parasites commonly found on a blood film, including Chagas disease, microfilaria, and sickle cell. Success with the most difficult-to-identify disease, malaria, will pave the way for EasyScan GO to excel at almost any microscopy task, Nunnendorf said. “AI and digital health are increasingly becoming the most exciting trend in healthcare, and with our product leadership and Global Good’s commitment to inventing for humanitarian impact, our innovations will be truly available everywhere,” he said.
Machine learning is the ultimate way to go faster, said Peter Norvig, director of research at Google. But speed can also lead to accidents, he told the Train AI conference in San Francisco.
“In every industry, there’s a place where AI can make things better,” Norvig told Devex. “Look at all of the AI technologies, and all problems, and it’s just a question of fitting them together and figuring out, ‘What’s the right technological match and what’s the right policy match?’”
The most obvious cases for AI use include anything a typical human can do in less than a second of thought, Andrew Ng, one of the leading thinkers on artificial intelligence, told Devex at an event at Stanford University on the role of AI in achieving the SDGs. Image recognition is one common example, but AI also has powerful applications in education and health care in both the developed and developing world, said Ng. He finds it hard to name a major industry he doesn’t think AI can transform in the next several years.
""I actually find it hard to name a major industry that I don't think AI can transform in the next several years."" - @AndrewYNg of @coursera pic.twitter.com/I9wxmr2unm “Deep learning has driven unprecedented breakthroughs across a variety of domains, spanning image processing, to drug discovery and speech recognition – there are dozens of successful use cases, Perold of Element told Devex. “Ultimately, deep learning is a tool that must be applied thoughtfully,”
Technology should bridge gaps, not drive inequalities between those at the top and those at the bottom, said Perold. “We believe that one of the best ways to do this is to meet people where they are – on the lowest common denominator of computing, the mobile device. Solving problems requires not just advanced technology, but a lot of thoughtfulness of context and application. It’s the responsibility of anyone delivering solutions to get to know the people, cultures, and delivery needs directly,” he said. Read more Devex coverage on global health."
46e03e9063,Google's art machine just wrote its first song - The Verge,"Today, Google's newest machine learning project released its first piece of generated art, a 90-second piano melody created through a trained neural network, provided with just four notes up front. The drums and orchestration weren't generated by the algorithm, but added for emphasis after the fact. It's the first tangible product of Google's Magenta program, which is designed to put Google's machine learning systems to work creating art and music systems. The program was first announced at Moogfest last week. ""We believe this area is in its infancy"" Along with the melody, Google published a new blog post delving into Magenta's goals, offering the most detail yet on Google's artistic ambitions. In the long term, Magenta wants to advance the state of machine-generated art and build a community of artists around it — but in the short term, that means building generative systems that plug in to the tools artists are already working with. ""We’ll start with audio and video support, tools for working with formats like MIDI, and platforms that help artists connect to machine learning models,"" the team wrote in an announcement. ""We want to make it super simple to play music along with a Magenta performance model."" Magenta is built on top of Google's TensorFlow system, which is already open-source, and the new project also plans to publish its code as open-source on GitHub. ""We believe this area is in its infancy, and expect to see fast progress here,"" the announcement says. It's not the first time Google has experimented with machine-generated art. The company's DeepDream algorithm — initially developed to visualize the actions of neural networks — has become a popular image tool in its own right and the basis for a gallery show earlier this year. Google also developed the Artists and Machine Intelligence program to sponsor further collaborations along the same lines. Command Line delivers daily updates from the near-future."
b1814168d9,AI-powered lip sync puts old words into Obama's new mouth | WIRED UK,"Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
The technology could eventually be developed to tell if a video is real or fake
By
Libby Plummer
Researchers have developed a machine learning algorithm that can turn audio clips into realistic, lip-synced videos. A video shows former US president Barack Obama apparently speaking on a number of subjects including terrorism, though the clips were artificially generated using existing video addresses.
Researchers from the University of Washington believe the system could eventually be used to improve video calls – or even to ascertain whether a video is real or fake. The system uses a neural network that was trained to watch videos of people talking and convert audio files into realistic mouth shapes. These are then grafted onto the head of that person from another existing video, by combining previous research from the university's image lab with a new mouth synthesis technique. The technology also enables a small time shift so that the neural network can anticipate what the speaker is going to say next. The team chose Obama because the system needs around 14 hours of video to learn from, which isn't a problem for one of the most-filmed faces on the planet.
By
Amit Katwala
""In the future, video chat tools like Skype or Messenger will enable anyone to collect videos that could be used to train computer models,"" said Ira Kemelmacher-Shlizerman, from UW's Paul G. Allen School of Computer Science & Engineering. Because streaming audio over the internet takes up far less bandwidth than video, the new system could spell the end for glitchy, frozen video chats. ""When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,"" said co-author and Allen School professor Steve Seitz. ""So if you could use the audio to produce much higher-quality video, that would be terrific."" Previous audio-to-video conversion technology has focussed on filming multiple people saying the same sentence repeatedly to try and capture how sounds correlate to different mouth shapes. But this process is expensive and time-consuming. By reversing the process - feeding video into the network instead of just audio - the team could potentially develop algorithms to detect whether a video is real or fake. However, the neural network is currently designed to learn on one individual at a time. ""You can't just take anyone's voice and turn it into an Obama video,"" Seitz said. ""We very consciously decided against going down the path of putting other people's words into someone's mouth. We're simply taking real words that someone spoke and turning them into realistic video of that individual."" In future, algorithms may be developed to recognize a person's voice and speech patterns using just an hour of video, rather than 14 hours, the researchers said.
By
Matt Burgess
By
Bonnie Christian
By
David Pierce
By
Matthew Reynolds"
739d340e37,Microsoft applies machine learning to deliver 'neural fuzzing' vulnerability testing - SiliconANGLE,"Home » Emerging Tech
by
Duncan Riley
UPDATED 21:10 EST . 13 NOVEMBER 2017
Microsoft Corp. today announced a new method for discovering software security vulnerabilities that combines machine learning and deep neural networks to use past experience in order to identify overlooked issues better. Dubbed “neural fuzzing,” the method takes traditional fuzz testing, a quality assurance technique used to discover coding errors and security loopholes in software, operating systems or networks, and adds a machine learning model to insert a deep neural network in the feedback loop of a “greybox fuzzer.” Microsoft found that by deploying the neural network to observe past fuzzing interactions on an existing fuzz testing platform and then using that data to discover vulnerabilities, the results outperformed all existing fuzzing methods in terms of code coverage, unique code paths and crashes. “We believe our neural fuzzing approach yields a novel way to perform greybox fuzzing that is simple, efficient and generic,” Development Lead William Blum (pictured) said in a blog post. Blum argued that the new method is simple because it is not based on sophisticated handcrafted heuristics; instead, it simply learns from an existing fuzzer. He also argued that the new method is efficient in that more quickly explorers data than a traditional fuzzer, and that the methodology itself is generic in that it could be applied to any fuzzer, including blackbox and random fuzzers. “We believe our neural fuzzing research project is just scratching the surface of what can be achieved using deep neural networks for fuzzing,” Blum added. “Right now, our model only learns fuzzing locations, but we could also use it to learn other fuzzing parameters such as the type of mutation or strategy to apply.” More details on the fascinating research, which potentially lays the groundwork for the model to learn other fuzzing parameters and thus improving a key technology that makes up security detection tools, can be found on the project’s research site here. Like Free Content?
Subscribe to follow. LG and JBL unveil smart display speakers with Google Assistant integration Responding to criticism, Facebook makes news feed changes to spur more interaction Red Hat says microservices benefits can be realized in just six months Report claims hackers could start a nuclear war through vulnerable weapons systems Microsoft brings end-to-end encryption to Skype through Signal partnership PC market shows small signs of life as Q4 worldwide shipments grow A WhatsApp security flaw let researchers snoop on group chat messages INFRASTRUCTURE - BY ERIC DAVID . 12 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 13 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 17 HOURS AGO Survey finds IT disaster recovery practices still rely on old technologies INFRASTRUCTURE - BY PAUL GILLIN . 19 HOURS AGO Microsoft forced to stop issuing faulty Meltdown and Spectre patches INFRASTRUCTURE - BY DUNCAN RILEY . 2 DAYS AGO Microsoft recruits Qualcomm to help it catch up in the voice assistant market EMERGING TECH - BY MARIA DEUTSCHER . 3 DAYS AGO Cisco consultants provide aspirin and vitamins for network health BIG DATA - BY MARK ALBERTSON . 3 DAYS AGO Verizon acquires machine learning-based threat detection startup Niddel APPS - BY MIKE WHEATLEY . 4 DAYS AGO How those chip flaws could accelerate the shift to cloud computing CLOUD - BY PAUL GILLIN . 6 DAYS AGO A WhatsApp security flaw let researchers snoop on group chat messages INFRASTRUCTURE - BY ERIC DAVID . 12 HOURS AGO Unbabel raises $23M for AI translations backed up by human editors EMERGING TECH - BY ERIC DAVID . 13 HOURS AGO Cisco’s new machine learning tech can spot threats lurking in encrypted traffic INFRASTRUCTURE - BY MARIA DEUTSCHER . 17 HOURS AGO Survey finds IT disaster recovery practices still rely on old technologies INFRASTRUCTURE - BY PAUL GILLIN . 19 HOURS AGO Microsoft forced to stop issuing faulty Meltdown and Spectre patches INFRASTRUCTURE - BY DUNCAN RILEY . 2 DAYS AGO Microsoft recruits Qualcomm to help it catch up in the voice assistant market EMERGING TECH - BY MARIA DEUTSCHER . 3 DAYS AGO Cisco consultants provide aspirin and vitamins for network health BIG DATA - BY MARK ALBERTSON . 3 DAYS AGO Verizon acquires machine learning-based threat detection startup Niddel APPS - BY MIKE WHEATLEY . 4 DAYS AGO How those chip flaws could accelerate the shift to cloud computing CLOUD - BY PAUL GILLIN . 6 DAYS AGO Check out more cube videos and stories Analytics feeds machine maker’s global ambitions Modern infrastructure management: accelerating productivity through machine learning Fleet analytics provider spins vehicle sensor data into productivity gold Big data leads a transformation in PC gaming For personalised content stay with us. Forgot Password? Like Free Content? Subscribe to follow."
8d3635297d,Algorithm uses machine learning to help robots avoid collisions in real time The Engineer,"Our website uses cookies to improve your user experience. If you continue browsing, we assume that you consent to our use of cookies. More information can be found in our Privacy & Cookies Policy Electrical engineers have developed a faster collision detection algorithm that uses machine learning to help robots avoid moving objects and negotiate rapidly changing environments in real time. Developed at the University of California San Diego, the Fastron algorithm is said to run up to eight times faster than existing collision detection algorithms. The engineers, led by Prof Michael Yip envision Fastron being useful for robots that operate in human environments where they work with moving objects and people fluidly. They are also exploring robot-assisted surgeries using the da Vinci Surgical System. In this scenario, a robotic arm would autonomously perform assistive tasks – such as suction, irrigation or pulling tissue back – without obstructing the surgeon-controlled arms or the patient’s organs. The team also envisions that Fastron can be used for robots that work at home for assisted living applications. Existing collision detection algorithms spend time specifying all the points in a given space -the specific 3D geometries of the robot and obstacles – and performing collision checks on every single point to determine whether two bodies are intersecting at any given time. To lighten the computational load, Yip and his team in the Advanced Robotics and Controls Lab (ARClab) developed a minimalistic approach to collision detection. The result was Fastron, an algorithm that uses machine learning strategies – which are traditionally used to classify objects – to classify collisions versus non-collisions in dynamic environments. “We actually don’t need to know all the specific geometries and points. All we need to know is whether the robot’s current position is in collision or not,” said Nikhil Das, an electrical engineering PhD student in Yip’s group and the study’s first author. An important feature of Fastron is that it updates its classification boundaries very quickly to accommodate for moving scenes. Fastron’s active learning strategy works using a feedback loop. It starts out by creating a model of the robot’s configuration space (C-space), which is the space showing all possible positions the robot can attain. Fastron models the C-space with a small number of so-called collision points and collision-free points. The algorithm then defines a classification boundary between the collision and collision-free points – this boundary is essentially a rough outline of where the abstract obstacles are in the C-space. As obstacles move, the classification boundary changes. Rather than performing collision checks on each point in the C-space, as is done with other algorithms, Fastron selects checks near the boundaries. Once it classifies the collisions and non-collisions, the algorithm updates its classifier and then continues the cycle. Because Fastron’s models are more simplistic, the researchers set its collision checks to be more conservative. Since just a few points represent the entire space, Das explained, it’s not always certain what’s happening in the space between two points, so the team developed the algorithm to predict a collision in that space. “We leaned toward making a risk-averse model and essentially padded the workspace obstacles,” Das said. This ensures that the robot can be tuned to be more conservative in sensitive environments like surgery, or for robots that work at home for assisted living. The team has so far demonstrated the algorithm in computer simulations on robots and obstacles in simulation. Prof. Yip presented the new algorithm at the first annual Conference on Robot Learning taking place in California between November 13-15. CLICK HERE FOR MORE ENGINEERING NEWS
Threaded commenting powered by interconnect/it code.
© 2018. All rights reserved. built by interconnect/it Centaur Communications Ltd (a member of the Centaur Media plc group) Wells Point, 79 Wells Street, London W1T 3QN. Registered in England No: 1595235"
b459c0c3c0,Google’s DeepMind achieves machine learning breakthroughs at a terrifying pace,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
It’s time to add “AI research” to the list of things that machines can do better than humans. Google’s Alpha Go, the computer that beat the world’s greatest human go player, just lost to a version of itself that’s never had a single human lesson. Google is making progress in the field of machine learning at a startling rate. The company’s AutoML recently dropped jaws with its ability to self-replicate, and DeepMind is now able to teach itself better than the humans who created it can. DeepMind is the machine behind both versions of Alpha Go, with the latest evolution dubbed Alpha Go Zero — which sounds like the prequel to a manga. The original Alpha Go is a monster of technology with 48 AI processors, and the data from thousands of go matches built into it. From the ground up, it was “born” with a pretty decent understanding of the game. Over time, and under the direction of humans, it began to learn the game and its nuanced strategies. Eventually Alpha Go became so advanced, it was able to defeat the world’s top human player and establish AI’s supremacy in a game so difficult it makes chess look like checkers. In short, Alpha Go is pretty legit. The brilliant minds at Google decided being the best wasn’t good enough; they “evolved” Alpha Go into “Alpha Go Zero.” It was able to defeat Alpha Go at its own game only 40 days later. Credit: Google Let that sink in. Now here’s the shocking part: Alpha Go Zero has only four AI processors and the only data it was given was the rules of the game. Nobody taught it how to play or fed it thousands of matches to study. According to Google’s blog: This technique is more powerful than previous versions of AlphaGo because it is no longer constrained by the limits of human knowledge. Instead, it is able to learn tabula rasa from the strongest player in the world: AlphaGo itself. The AI plays Go against itself, improving with every match. After millions of matches its strategy is, as far as humans are concerned, infallible. Both versions of the machine play the game at a level that’s considered superhuman.
The speed with which Google’s AutoML and DeepMind have taken “self learning” to the next level is wonderful and terrifying at the same time. In order for AI to fullfill its promise to humanity it has to ease our burdens and free our minds to solve uniquely human problems. A version of DeepMind that – in a little over a month – can teach itself to outperform a previous iteration is the realization of that ideal. It’s time we took Sundar Pichai’s assertion that Google is an AI company seriously.
Read next:
Emails demise raises security concerns about messaging
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
ddffba1258,Teaching robots to teach other robots | MIT News,"Login
or
Subscribe Newsletter
MIT doctoral candidate Claudia Pérez-D'Arpino discusses her work teaching the Optimus robot to perform various tasks, including picking up a bottle. Photo: Jason Dorfman/MIT CSAIL “By combining the intuitiveness of learning from demonstration with the precision of motion-planning algorithms, this approach can help robots do new types of tasks that they haven’t been able to learn before, like multistep assembly using both of their arms,” says MIT grad student Claudia Pérez-D'Arpino. Photo: Jason Dorfman/MIT CSAIL CSAIL approach allows robots to learn a wider range of tasks using some basic knowledge and a single demo.
Watch Video
Adam Conner-Simons | CSAIL
May 10, 2017
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Most robots are programmed using one of two methods: learning from demonstration, in which they watch a task being done and then replicate it, or via motion-planning techniques such as optimization or sampling, which require a programmer to explicitly specify a task’s goals and constraints. Both methods have drawbacks. Robots that learn from demonstration can’t easily transfer one skill they’ve learned to another situation and remain accurate. On the other hand, motion planning systems that use sampling or optimization can adapt to these changes but are time-consuming, since they usually have to be hand-coded by expert programmers. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have recently developed a system that aims to bridge the two techniques: C-LEARN, which allows noncoders to teach robots a range of tasks simply by providing some information about how objects are typically manipulated and then showing the robot a single demo of the task. Importantly, this enables users to teach robots skills that can be automatically transferred to other robots that have different ways of moving — a key time- and cost-saving measure for companies that want a range of robots to perform similar actions. “By combining the intuitiveness of learning from demonstration with the precision of motion-planning algorithms, this approach can help robots do new types of tasks that they haven’t been able to learn before, like multistep assembly using both of their arms,” says Claudia Pérez-D’Arpino, a PhD student who wrote a paper on C-LEARN with MIT Professor Julie Shah. The team tested the system on Optimus, a new two-armed robot designed for bomb disposal that they programmed to perform tasks such as opening doors, transporting objects, and extracting objects from containers. In simulations they showed that Optimus’ learned skills could be seamlessly transferred to Atlas, CSAIL’s 6-foot-tall, 400-pound humanoid robot. A paper describing C-LEARN was recently accepted to the IEEE International Conference on Robotics and Automation (ICRA), which takes place May 29 to June 3 in Singapore. How it works With C-LEARN the user first gives the robot a knowledge base of information on how to reach and grasp various objects that have different constraints. (The C in C-LEARN stands for “constraints.”) For example, a tire and a steering wheel have similar shapes, but to attach them to a car, the robot has to configure its arms differently to move them. The knowledge base contains the information needed for the robot to do that. The operator then uses a 3-D interface to show the robot a single demonstration of the specific task, which is represented by a sequence of relevant moments known as “keyframes.” By matching these keyframes to the different situations in the knowledge base, the robot can automatically suggest motion plans for the operator to approve or edit as needed. “This approach is actually very similar to how humans learn in terms of seeing how something’s done and connecting it to what we already know about the world,” says Pérez-D’Arpino. “We can’t magically learn from a single demonstration, so we take new information and match it to previous knowledge about our environment.” One challenge was that existing constraints that could be learned from demonstrations weren’t accurate enough to enable robots to precisely manipulate objects. To overcome that, the researchers developed constraints inspired by computer-aided design (CAD) programs that can tell the robot if its hands should be parallel or perpendicular to the objects it is interacting with. The team also showed that the robot performed even better when it collaborated with humans. While the robot successfully executed tasks 87.5 percent of the time on its own, it did so 100 percent of the time when it had an operator that could correct minor errors related to the robot’s occasional inaccurate sensor measurements. “Having a knowledge base is fairly common, but what’s not common is integrating it with learning from demonstration,” says Dmitry Berenson, an assistant professor at the University of Michigan's Electrical Engineering and Computer Science Department. “That’s very helpful, because if you are dealing with the same objects over and over again, you don't want to then have to start from scratch to teach the robot every new task.” Applications The system is part of a larger wave of research focused on making learning-from-demonstration approaches more adaptive. If you’re a robot that has learned to take an object out of a tube from a demonstration, you might not be able to do it if there’s an obstacle in the way that requires you to move your arm differently. However, a robot trained with C-LEARN can do this, because it does not learn one specific way to perform the action. “It’s good for the field that we're moving away from directly imitating motion, toward actually trying to infer the principles behind the motion,” Berenson says. “By using these learned constraints in a motion planner, we can make systems that are far more flexible than those which just try to mimic what's being demonstrated"" Shah says that advanced LfD methods could prove important in time-sensitive scenarios such as bomb disposal and disaster response, where robots are currently tele-operated at the level of individual joint movements. “Something as simple as picking up a box could take 20-30 minutes, which is significant for an emergency situation,” says Pérez-D’Arpino. C-LEARN can’t yet handle certain advanced tasks, such as avoiding collisions or planning for different step sequences for a given task. But the team is hopeful that incorporating more insights from human learning will give robots an even wider range of physical capabilities. “Traditional programming of robots in real-world scenarios is difficult, tedious, and requires a lot of domain knowledge,” says Shah. “It would be much more effective if we could train them more like how we train people: by giving them some basic knowledge and a single demonstration. This is an exciting step toward teaching robots to perform complex multiarm and multistep tasks necessary for assembly manufacturing and ship or aircraft maintenance.” Topics: Robots, Robotics, Research, Artificial intelligence, Algorithms, Aeronautical and astronautical engineering, Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical Engineering & Computer Science (eecs), School of Engineering, Machine learning, Motion planning CSAIL researchers have developed a system that allows robots to teach one another learned skills, reports Grace Williams for FOX News. Williams explains that the system, “gives non-coders the ability to teach robots various tasks using information about manipulating objects in a single demonstration. These skills can then be passed along to other robots that move in different ways.” Researchers at CSAIL have developed a new system to train robots called C-LEARN, which imbues “a robot with a knowledge base of simple steps that it can intelligently apply when learning a new task,” writes Matthew Hutson for Science.
Anthony Cuthbertson of Newsweek reports that PhD student Claudia Pérez D’Arpino has developed a system that allows robots to learn a skill and teach it to another robot. Armed with knowledge of how to perform a task, a 3-D interface demonstrates the tasks “allowing [the robot] to understand the motions it is being taught in the real world,” explains Cuthbertson. This CNN video highlights a new system developed by CSAIL researchers that allows noncoders to teach robots to perform a task after a single demonstration. The new programming method also enables robots to learn from other robots, which could enable “a variety of robots to perform similar tasks.” Wired reporter Matt Simon writes that CSAIL researchers have developed a new system that allows noncoders to be able to teach robots a wide range of tasks, and enables robots to transfer new skills to other robots. Simon notes that the development is a “glimpse into a future where, more and more, robots communicate without humans at all.” This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
c6a9cb3b77,Bringing machine learning to last mile health challenges | Devex ,"Connect SAN FRANCISCO — A new microscope will use image recognition software and machine learning technology to identify and count malaria parasites in a blood smear. The EasyScan GO, announced at MEDICA, the medical industry’s leading trade fair, is the result of a partnership between the Global Good Fund, a Seattle-based group funded by philanthropist Bill Gates, and Motic, a China-based company that specializes in manufacturing microscopes. Field tests have demonstrated that the machine learning algorithm is as reliable as an expert microscopist in fighting the spread of drug resistant malaria. EasyScan GO is the latest example of Global Good’s partnering to bring emerging technologies to health systems in low resource settings. Based at the invention company Intellectual Ventures, Global Good is focused on developing and deploying technologies for the poorest parts of the world. It partners with others to bring the benefits of technological innovation to the hardest markets in the world to reach.
The Global Good motto is “invention saving lives.” But invention means nothing if these products are developed without any consideration of the applicability of that technology in the markets they are meant to serve, said Maurizio Vecchione, senior vice president at Global Good. He emphasized that his team cannot fulfill its mission unless that technology is affordable, appropriate, accessible, and those 3 A’s, as he calls them, are at the center of every Global Good partnership.
Proponents see enormous potential for this technology. Today, machine learning can help tell us we are sick; emerging technologies like artificial intelligence will allow us to prevent illness, according to AnthroTronix founder Corinna Lathan, who wrote a World Economic Forum blog post on ways artificial intelligence and robotics will transform health care.
Several recent partnerships point to how Global Good is working to make sure the benefits of AI, machine learning, and deep learning extend to developing countries.
One in four children in the world are not registered at birth. Because they lack formal identification, it is very difficult to track their health records, including vaccinations.
Earlier this month, start-up Element Inc. announced a partnership with Global Good to develop a smartphone based platform to verify the identity of infants and children. Element Inc. focuses on developing mobile software for biometric recognition. Their aim is to deliver identity for everyone, and particularly the 1 billion people who lack proper IDs in Asia and Africa.
In Bangladesh and Cambodia, the BioNIC partnership will extend Element’s adult identity platform to children under age five, testing on fingerprints, ears and feet, irises, and more offline and on any mobile device.
“Accurately identifying patients, and linking them to their electronic medical records — which store past medical conditions, drug allergy information, medications and vaccination history — is critical to health care,” David Bell, director of the global health technologies portfolio at Global Good, said in a press release. “While adult biometrics are widely used in financial services, these have not been translated to the health sector where the need to identify a person from birth to old age presents additional challenges; namely, that infants have delicate, rapidly changing features that are difficult to capture.” This technology has only become possible in recent years, Element co-founder and CEO Adam Perold told Devex via email.
“Mobile devices now proliferate the world, at nearly three billion units; and each year, they get better and less expensive. At the same time, deep learning — through improved algorithms and increasingly powerful computers — was driving breakthroughs in how systems process video and images. We were one of the first to apply deep learning methods on mobile devices, and as a result, we have built a platform that traverses many challenges that still prevent widespread adoption of biometrics across low-resource settings,” he said.
When problems can be tackled by deep learning, the barriers in delivery are often structural, Perold continued, explaining that it is a resource heavy approach requiring lots of data, computing power, and specialized training.
“Deep learning solves key issues of access in biometric recognition. The strengths of the approach, where algorithms train themselves directly from the data, means that anyone can be recognized regardless of their features,” he said, adding that the technology “can be deployed to continuously adapt to any changes as a person develops.” The BioNIC partnership has the ultimate goal of creating a biometric solution capable of following patients from birth to old age, said Perold. Malaria kills almost half a million people every year, and one of the major challenges to eradication is the rapid spread of a multidrug-resistant strain in parts of Southeast Asia.
Currently, only analysis by a World Health Organization certified microscopist can accurately detect severe and drug-resistant cases. EasyScan GO automates that process with an intelligent microscope, simplifying and standardizing malaria detection in under-resourced countries where there is often a lack of trained personnel. “Malaria is one of the hardest diseases to identify on a microscope slide,” Bell of Global Good said in a press release. “By putting machine learning-enabled microscopes in the hands of laboratory technicians, we can overcome two major barriers to combating the mutating parasite — improving diagnosis in case management and standardizing detection across geographies and time.” The EasyScan GO uses AI-enabled software to automatically and accurately identify and count malaria parasites in a blood smear in as little as 20 minutes, Sebastian Nunnendorf, head of research and development at Motic China, said in an email to Devex. “Human capacity issues have long been a major bottleneck in low-resource settings, and the fact that we are able to address these issues through technological innovation is truly exciting,” he said. Any new medical device will need time in the market to validate, Nunnendorf said, and like the BioNIC partnership, EasyScan GO is still early in its journey. But the company hopes to go beyond diagnosing malaria and a few other parasites commonly found on a blood film, including Chagas disease, microfilaria, and sickle cell. Success with the most difficult-to-identify disease, malaria, will pave the way for EasyScan GO to excel at almost any microscopy task, Nunnendorf said. “AI and digital health are increasingly becoming the most exciting trend in healthcare, and with our product leadership and Global Good’s commitment to inventing for humanitarian impact, our innovations will be truly available everywhere,” he said.
Machine learning is the ultimate way to go faster, said Peter Norvig, director of research at Google. But speed can also lead to accidents, he told the Train AI conference in San Francisco.
“In every industry, there’s a place where AI can make things better,” Norvig told Devex. “Look at all of the AI technologies, and all problems, and it’s just a question of fitting them together and figuring out, ‘What’s the right technological match and what’s the right policy match?’”
The most obvious cases for AI use include anything a typical human can do in less than a second of thought, Andrew Ng, one of the leading thinkers on artificial intelligence, told Devex at an event at Stanford University on the role of AI in achieving the SDGs. Image recognition is one common example, but AI also has powerful applications in education and health care in both the developed and developing world, said Ng. He finds it hard to name a major industry he doesn’t think AI can transform in the next several years.
""I actually find it hard to name a major industry that I don't think AI can transform in the next several years."" - @AndrewYNg of @coursera pic.twitter.com/I9wxmr2unm “Deep learning has driven unprecedented breakthroughs across a variety of domains, spanning image processing, to drug discovery and speech recognition – there are dozens of successful use cases, Perold of Element told Devex. “Ultimately, deep learning is a tool that must be applied thoughtfully,”
Technology should bridge gaps, not drive inequalities between those at the top and those at the bottom, said Perold. “We believe that one of the best ways to do this is to meet people where they are – on the lowest common denominator of computing, the mobile device. Solving problems requires not just advanced technology, but a lot of thoughtfulness of context and application. It’s the responsibility of anyone delivering solutions to get to know the people, cultures, and delivery needs directly,” he said. Read more Devex coverage on global health."
0052a2e700,IBM detects skin cancer more quickly with visual machine learning | Computerworld,"Use commas to separate multiple email addresses Your message has been sent. There was an error emailing this page.
By Joab Jackson
U.S. Correspondent,
IDG News Service |
Dec 17, 2014 11:27 AM
PT
Skin cancer can be detected more quickly and accurately by using cognitive computing-based visual analytics, researchers at IBM Research have found, in collaboration with New York's Memorial Sloan Kettering Cancer Center.
In a scan of 3,000 images, IBM technology was able to spot melanoma with an accuracy of about 95 percent, much better than the 75 percent to 84 percent average of today's largely manual methods.
""The technology can pull on massive amounts of data to help the doctor make more informed decisions,"" said Noel Codella, a multimedia analytics researcher in the cognitive computing group at the IBM T. J. Watson Research Center in Yorktown Heights, New York.
Once the technology is commercialized, the cognitive computing approach will be able to scan the images in less than a second, much more quickly than humans can.
Such work could go a long way to more effectively treating skin cancer, which afflicts nearly 5 million people a year in the U.S. alone, according to the U.S. surgeon general.
Cognitive computing could bring a new efficiency to recognizing melanoma. Machine learning algorithms could continually improve the ability of the system to identify the disease. Over time, this approach might be able to spot cases that would be too difficult to pinpoint by a doctor.
As with any form of cancer, skin cancer is best diagnosed early. A cognitive computing detection system could flag potential trouble areas before they become noticeable by humans.
IBM's approach involves the use of multiple tests. One set of tests could look for unusual color distributions or texture patterns on the skin. Another series of tests could also identify the rapid progression of lesions, or deviations from normal growth, compared to the rest of the body or to other people with similar genetic or demographic characteristics.
The system would weigh the results of each test. ""We don't take a single approach. So we study a variety of approaches, see how they work, and see if they can be combined in some way so they work better,"" Codella said.
This work builds off research on machine learning technologies that aid computers in recognizing objects within images. It leverages a number of the company's advanced technologies, including a visually oriented machine learning architecture called the IBM Multimedia and Analytics System.
It also uses an IBM system for analyzing medical images, called Medical Sieve, and a visual recognition and search system called Intelligent Video Analytics.
IBM Research will continue to work with Sloan Kettering to develop additional measurements and approaches to further refine diagnosis, as well as refine their approach through larger sets of data.
Joab Jackson covers enterprise software and general technology breaking news for The IDG News Service. Follow Joab on Twitter at @Joab_Jackson. Joab's e-mail address is Joab_Jackson@idg.com
Copyright © 2018 IDG Communications, Inc."
0defd1e34b,World's Most Powerful Particle Collider Taps AI to Expose Hack Attacks - Scientific American,"We use cookies to provide you with a better onsite experience. By continuing to browse the site you are agreeing to our use of cookies in accordance with our Cookie Policy. Machine learning is crucial to staying ahead of hackers trying to break into at CERN’s Large Hadron Collider’s (LHC) massive worldwide computing grid Thousands of scientists worldwide tap into CERN’s computer networks each day in their quest to better understand the fundamental structure of the universe. Unfortunately, they are not the only ones who want a piece of this vast pool of computing power, which serves the world’s largest particle physics laboratory. The hundreds of thousands of computers in CERN’s grid are also a prime target for hackers who want to hijack those resources to make money or attack other computer systems. But rather than engaging in a perpetual game of hide-and-seek with these cyber intruders via conventional security systems, CERN scientists are turning to artificial intelligence to help them outsmart their online opponents. Current detection systems typically spot attacks on networks by scanning incoming data for known viruses and other types of malicious code. But these systems are relatively useless against new and unfamiliar threats. Given how quickly malware changes these days, CERN is developing new systems that use machine learning to recognize and report abnormal network traffic to an administrator. For example, a system might learn to flag traffic that requires an uncharacteristically large amount of bandwidth, uses the incorrect procedure when it tries to enter the network (much like using the wrong secret knock on a door) or seeks network access via an unauthorized port (essentially trying to get in through a door that is off-limits). CERN’s cybersecurity department is training its AI software to learn the difference between normal and dubious behavior on the network, and to then alert staff via phone text, e-mail or computer message of any potential threat. The system could even be automated to shut down suspicious activity on its own, says Andres Gomez, lead author of a paper describing the new cybersecurity framework. CERN—the French acronym for the European Organization for Nuclear Research lab, which sits on the Franco-Swiss border—is opting for this new approach to protect a computer grid used by more than 8,000 physicists to quickly access and analyze large volumes of data produced by the Large Hadron Collider (LHC). The LHC’s main job is to collide atomic particles at high-speed so that scientists can study how particles interact. Particle detectors and other scientific instruments within the LHC gather information about these collisions, and CERN makes it available to laboratories and universities worldwide for use in their own research projects. The LHC is expected to generate a total of about 50 petabytes of data (equal to 15 million high-definition movies) in 2017 alone, and demands more computing power and data storage than CERN itself can provide. In anticipation of that type of growth the laboratory in 2002 created its Worldwide LHC Computing Grid, which connects computers from more than 170 research facilities across more than 40 countries. CERN’s computer network functions somewhat like an electrical grid, which relies on a network of generating stations that create and deliver electricity as needed to a particular community of homes and businesses. In CERN’s case the community consists of research labs that require varying amounts of computing resources, based on the type of work they are doing at any given time. One of the biggest challenges to defending a computer grid is the fact that security cannot interfere with the sharing of processing power and data storage. Scientists from labs in different parts of the world might end up accessing the same computers to do their research if demand on the grid is high or if their projects are similar. CERN also has to worry about whether the computers of the scientists’ connecting into the grid are free of viruses and other malicious software that could enter and spread quickly due to all the sharing. A virus might, for example, allow hackers to take over parts of the grid and use those computers either to generate digital currency known as bitcoins or to launch cyber attacks against other computers. “In normal situations, antivirus programs try to keep intrusions out of a single machine,” Gomez says. “In the grid we have to protect hundreds of thousands of machines that already allow” researchers outside CERN to use a variety of software programs they need for their different experiments. “The magnitude of the data you can collect and the very distributed environment make intrusion detection on [a] grid far more complex,” he says. Jarno Niemel, a senior security researcher at F-Secure, a company that designs antivirus and computer security systems, says CERN’s use of machine learning to train its network defenses will give the lab much-needed flexibility in protecting its grid, especially when searching for new threats. Still, artificially intelligent intrusion detection is not without risks—and one of the biggest is whether Gomez and his team can develop machine-learning algorithms that can tell the difference between normal and harmful activity on the network without raising a lot of false alarms, Niemel says. CERN’s AI cybersecurity upgrades are still in the early stages and will be rolled out over time. The first test will be protecting the portion of the grid used by ALICE (A Large Ion Collider Experiment)—a key LHC project to study the collisions of lead nuclei. If tests on ALICE are successful, CERN’s machine learning–based security could then be used to defend parts of the grid used by the institution’s six other detector experiments. Jesse Emspak
September 30, 2014
—
Elizabeth Gibney and Nature News Blog October 8, 2013
August 20, 2016
—
Elizabeth Gibney and Nature magazine June 13, 2016
—
Michele Redi February 8, 2010
—
Larry Greenemeier Neuroscience. Evolution. Health. Chemistry. Physics. Technology. Follow us © 2018 Scientific American, a Division of Nature America, Inc. All Rights Reserved."
4721bb7a86,Machine learning lets computer create melodies to fit any lyrics | New Scientist,"Daily news
9 December 2016
plainpicture/cultura/Lauren Devon Got words but no melody? A machine learning system turns poetry into song by composing a pop music score to suit the lyrics it’s given. “I was studying singing while I was doing my PhD in computer science,” says Margareta Ackerman at San Jose State University in California, who developed the system with David Loker at technology advisory firm Orbitwerks. “Over time, I started to think of computers as creative partners instead of tools, which could maybe help me write songs.” The system, called ALYSIA, processes short lines of text and associates each syllable with a musical note. It chooses the pairing based on features including the syllable’s position in the word and how it will fit with the previous five notes.
ALYSIA can write whole accompanying scores this way, or provide musicians with a variety of melody options for each segment of lyrics, acting like a co-creator. Ackerman and Loker developed the system to produce pop tunes, but say it could be adapted different genres. The system uses two models, one focused on rhythm and the other on pitch. These were trained on the melody line and lyrics of 24 different pop songs. They then used the system to make melodies to accompany two sets of words written by Ackerman that involved it coming up with tunes for lyrics such as “Now that you’re gone / I just realised that I’m all alone”. They also fed it the lyrics to the vaudeville classic I’m Always Chasing Rainbows to see how it could reimagine the song in the pop genre. The idea of trying to automate musical composition is not new, but David Cope at the University of California, Santa Cruz, says ALYSIA is unusual in taking lyrics as its starting point. He is impressed that the system manages to match the metre of the melody with that of the lyrics, but says the compositions show an “almost annoying” lack of harmony. Rebecca Fiebrink, a researcher in machine learning and music at Goldsmiths, University of London, questions how useful the lyrics-to-melody approach is. “Is this really solving the compositional process for people who want to make music?” she says. “Creating a melody without additional accompaniment, like this system does, is the easiest thing to achieve.” The songs admittedly aren’t about to win any Grammys, but Ackerman says this is just the start. She initially imagined targeting ALYSIA at the electronic music community, but is now working on repurposing it for professional songwriters with the help of classical composers. Ultimately, Ackerman hopes to create a system capable of composing all aspects of a song on its own. “We want to design a program able to generate the music, the lyrics, and ideally even the production and the singing by itself,” she says. Journal reference: arXiv: https://arxiv.org/pdf/1612.01058v1.pdf More on these topics:
A shorter version of this article was published in New Scientist magazine on 17 December 2016"
86967aa91c,,"Our work starts long before a customer types a query. We've been analyzing data, observing past traffic patterns, and indexing the text describing every product in our catalog before the customer has even decided to search. As soon as we see the first keystroke, we're ready with instant suggestions and a comprehensive set of search results. Great search can seem to customers like it is reading their minds. We start the search experience by giving customers suggestions on how to formulate their queries as soon as they start typing. The better we understand the meaning of a query, the better we can help customers find the products they want. So we focus on the words and the intent behind those words. When a customer tells us they are looking for ""Harry Potter in books"", we distinguish in their query the title: ""Harry Potter"" from the category information: ""in books"". One of A9's tenets is that relevance is in the eye of the customer and we strive to get the best results for our users. Once we determine which items are good matches to the customer’s query, our ranking algorithms score them to present the most relevant results to the user. Our ranking algorithms automatically learn to combine multiple relevance features. Our catalog’s structured data provides us with many such relevance features and we learn from past search patterns and adapt to what is important to our customers. We index millions of documents worldwide, and deploy them on highly scalable massive fleets of servers. We reflect millions of price and inventory changes in real time, and we return relevant results in milliseconds. Our core Search Infrastructure team runs a critical service that is powerful, flexible and efficient. Since both query traffic and the size of our product catalog are always growing, we need to continually innovate to prevent exponential growth in the number of servers in our fleet. We ensure lightning fast experiences for customers, regardless of traffic patterns at a given time.
Search and several related services we support are at the core of the Amazon business: they help customers find the items they want to buy. We are always online and ready to respond. Our globally distributed team oversees the smooth running of all search system operations on Amazon sites in North America, Europe and Asia; our Client Services group provides hands on support for those that depend on A9’s search systems. Besides our ultra-high availability frontline operations, we plan and scale with the fast-paced growth of search. We look at the data; we determine what services are needed; we implement solutions and we manage deployments. We are responsible for thousands of servers handling hundreds of millions of customer searches daily. We stay agile so that we can adapt to unexpected change and exponential growth. We are ready when peak traffic surges, and we understand that yesterday’s record is going to be tomorrow’s average, so we always stay ahead with our infrastructure. Within Search we have the Search Operations team which builds and runs the world's largest e-commerce product search. Our ""follow the sun"" operation is based in three locations: Palo Alto, Dublin, and Tokyo. Each of our teams can, during their work hours, address any issue in any locale as soon as it arises, giving it full attention. No matter what the volume of traffic, the conditions on the ground, or the intricacy of the systems, our services perform seamlessly for our customers, 24x7."
60bbe1b22c,Musk's OpenAI reveals how it beat Dota 2 video game world champion,"OpenAI is revealing more information about its bot system after its win streak in the game Dota 2 last Friday, and it’s another step forward in the world of artificial intelligence. The graph below was released this morning by OpenAI, which depicts the rate at which the bot improved in playing Dota 2. (Source: OpenAI) “Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute,” OpenAI wrote in a post. “In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better.” OpenAI’s Dota 2 project started in March of this year, starting the bot off with simple tasks. For background, Dota 2 is a free multiplayer battlefield game on Steam, a gaming streaming site. The game prides itself on not imposing limits on its players. The bot went on an impressive winning streak starting on August 7, beating a notable Dota 2 player named Blitz. That same day, the bot beat two more high-ranking players. The following day, the bot beat Arteezy, another respected player in the game. All four of the players the bot defeated said that fellow player SumaiL could defeat the bot. SumaiL did not fare as well as his comrades thought he would.
Finally, the bot took on Dota 2’s former world champion, Dendi. The bot beat Dendi 2-0. As for how the bot learned to play the came, OpenAI had the following explanation, “The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.” Some of the skills that the bot picked up were the ability to predict where players will move, to improvise in response to new situations, and how to influence the other players. In between battles, OpenAI workers combined some “coaching” with self-play, which helped the bot continuously improve.
OpenAI is a non-profit AI research company, founded by Tesla CEO Elon Musk, that researches safe artificial intelligence. The Musk company has previously trained bots to successfully compete a task after watching it on virtual reality just once, and developed bots that created their own language. Musk’s involvement in AI should come as no surprise as he has reiterated time and time again how important, and potentially dangerous, AI can be. “I have exposure to the most cutting edge AI and I think people should be really concerned about it,” Musk has previously said. “I keep sounding the alarm bell, but until people see robots going down the street killing people, they don’t know how to react because it seems so theorial.” Aside from OpenAI, Musk is also the CEO of Neuralink, a brain-computer interface company. The developments of the bot in these varied complex scenarios is a fairly large step forward in discovering the power of artificial intelligence. With limited input form the OpenAI engineers, the bot went from a decent player to being essentially unstoppable. This holds a promising future for further AI developments, if they can be contained. Interested in solar? Get a solar cost estimate and find out how much a solar system would cost for your home or business. ""I’m setup at Canaveral National Seashore on the top of my vehicle for a better vantage point. There’s a lot of buzz here, large crowd! I’m stoked!! I'm broadcasting live on Instagram Stories, so be sure to stay along for the ride by following and leaving a comment for me!"" News from Palo Alto... Expect additional reservations...
Michael RussoYeah, several advantages associated with the LRB, Johnny.... the right choice if you ca... johnnyboywow 60% of people want the extended battery?!
i didn't think it would be that many pe... Tesla news, rumors and reviews on all things Tesla. Connecting owners and enthusiasts via @TeslaratiApp available on iOS and Android Copyright © 2017 TESLARATI. All rights reserved."
79d8b8c9c3,FaceApp - Free Neural Face Transformation Filters,"Who said augmented reality requires donning a pair of goggles? The gender swapping is perhaps the most interesting feature, and often turns out some quite convincing results. Moving from one gender role to another is a long, time-consuming process. But realistically changing genders in a photo is now a snap. Different from the filters we know through Snapchat, FaceApp instead morphs faces by blending in facial features so that it can change a closed mouth to a toothy smile. Using artificial intelligence, the app morphs faces by merging in facial features. The app uses neural networks for its transformations. Tired of all those duck-face selfies? Replace them with a toothy grin with just one tap. © 2017 FaceApp. All Rights Reserved."
2184b02368,Machine Learning Is Aiding in the Fight Against Mental Illness,"Living in a modern age, one would think that suicide would be a less common occurrence. Sadly, that isn’t the case, and the World Health Organization (WHO) reports that worldwide suicide rates have increased by 60 percent in the last 45 years. Current statistics show that some one million people die from suicide each year, and the WHO anticipates that by 2020 global suicide rate will have increased from one every 40 seconds we see today to one every 20 seconds. That’s incredibly alarming. That’s why a team of researchers from several institutions including Carnegie Mellon University and Harvard University developed a machine learning algorithm trained to understand neural representations of suicidal behavior, and it works with a regular functional magnetic resonance imaging (fMRI).
The researchers tested their technique in 17 patients with suicidal ideation and in 17 more that served as control. They looked for these suicidal brain patterns by watching how the patients’ brains reacted when they were presented with six keywords: death, cruelty, trouble, carefree, good, and praise.The algorithm was able to accurately identify 15 out of the 17 patients with suicidal ideation, and 16 out of the 17 control, using just the MRI scans of their brains, for an overall accuracy rate of 91 percent. The results of their study has been published in the journal Nature Human Behavior, while MedPage Today publishing a video that discusses these findings. At present, the best way to anticipate suicidal behavior is to directly ask a person if he’s ever thought about it. However, that’s not entirely accurate, as studies have shown that almost 80 percent of people who committed suicide denied having had suicidal tendencies during their last appointment with a mental health professional. This new algorithm can help address this issue. It isn’t the first to use artificial intelligence (AI) to identify suicidal persons—for example, there’s Facebook’s AI and one that uses verbal and non-verbal language to spot suicidal behavior. Yet, this new algorithm offers a unique vantage point. It proves that there are differences in the brains of persons with suicidal ideation compared to those without, and these differences can be spotted with this machine learning and MRI combo. It’s not without limitations, however.Click to View Full Infographic
One problem with this technique is it requires the use of an MRI, which’ll be difficult to implement within the confines of a regular therapist’s office “It would be nice to see if we could possibly do this using EEG, if we could assess the thought alterations with EEG. It would be enormously cheaper. More widely used,” lead researcher Dr. Marcel Just from Carnegie Mellon told Yale University’s Francis Perry Wilson in the MedPage Today video. Just also identified an even more crucial limitation. “If somebody didn’t want others to know what they are thinking, they can certainly block that method. They can not cooperate,” he explained. “I don’t think we have a way to get at people’s thoughts against their will.” Still, for a mental health issue that’s as critical as suicide, machine learning might just provide a much needed help that could save the lives of more people.
References:
The Methods Man, Nature Human Behavior
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
41aba200c9,Computer system predicts products of chemical reactions | MIT News,"Login
or
Subscribe Newsletter
A new computer system predicts the products of chemical reactions. “The vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it,” says professor Klavs Jensen.
Image: MIT News Machine learning approach could aid the design of industrial processes for drug manufacturing.
Larry Hardesty | MIT News Office
June 27, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
When organic chemists identify a useful chemical compound — a new drug, for instance — it’s up to chemical engineers to determine how to mass-produce it. There could be 100 different sequences of reactions that yield the same end product. But some of them use cheaper reagents and lower temperatures than others, and perhaps most importantly, some are much easier to run continuously, with technicians occasionally topping up reagents in different reaction chambers. Historically, determining the most efficient and cost-effective way to produce a given molecule has been as much art as science. But MIT researchers are trying to put this process on a more secure empirical footing, with a computer system that’s trained on thousands of examples of experimental reactions and that learns to predict what a reaction’s major products will be. The researchers’ work appears in the American Chemical Society’s journal Central Science. Like all machine-learning systems, theirs presents its results in terms of probabilities. In tests, the system was able to predict a reaction’s major product 72 percent of the time; 87 percent of the time, it ranked the major product among its three most likely results. “There’s clearly a lot understood about reactions today,” says Klavs Jensen, the Warren K. Lewis Professor of Chemical Engineering at MIT and one of four senior authors on the paper, “but it's a highly evolved, acquired skill to look at a molecule and decide how you’re going to synthesize it from starting materials.” With the new work, Jensen says, “the vision is that you’ll be able to walk up to a system and say, ‘I want to make this molecule.’ The software will tell you the route you should make it from, and the machine will make it.” With a 72 percent chance of identifying a reaction’s chief product, the system is not yet ready to anchor the type of completely automated chemical synthesis that Jensen envisions. But it could help chemical engineers more quickly converge on the best sequence of reactions — and possibly suggest sequences that they might not otherwise have investigated. Jensen is joined on the paper by first author Connor Coley, a graduate student in chemical engineering; William Green, the Hoyt C. Hottel Professor of Chemical Engineering, who, with Jensen, co-advises Coley; Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science; and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science. Acting locally A single organic molecule can consist of dozens and even hundreds of atoms. But a reaction between two such molecules might involve only two or three atoms, which break their existing chemical bonds and form new ones. Thousands of reactions between hundreds of different reagents will often boil down to a single, shared reaction between the same pair of “reaction sites.” A large organic molecule, however, might have multiple reaction sites, and when it meets another large organic molecule, only one of the several possible reactions between them will actually take place. This is what makes automatic reaction-prediction so tricky. In the past, chemists have built computer models that characterize reactions in terms of interactions at reaction sites. But they frequently require the enumeration of exceptions, which have to be researched independently and coded by hand. The model might declare, for instance, that if molecule A has reaction site X, and molecule B has reaction site Y, then X and Y will react to form group Z — unless molecule A also has reaction sites P, Q, R, S, T, U, or V. It’s not uncommon for a single model to require more than a dozen enumerated exceptions. And discovering these exceptions in the scientific literature and adding them to the models is a laborious task, which has limited the models’ utility. One of the chief goals of the MIT researchers’ new system is to circumvent this arduous process. Coley and his co-authors began with 15,000 empirically observed reactions reported in U.S. patent filings. However, because the machine-learning system had to learn what reactions wouldn’t occur, as well as those that would, examples of successful reactions weren’t enough. Negative examples So for every pair of molecules in one of the listed reactions, Coley also generated a battery of additional possible products, based on the molecules’ reaction sites. He then fed descriptions of reactions, together with his artificially expanded lists of possible products, to an artificial intelligence system known as a neural network, which was tasked with ranking the possible products in order of likelihood. From this training, the network essentially learned a hierarchy of reactions — which interactions at what reaction sites tend to take precedence over which others — without the laborious human annotation. Other characteristics of a molecule can affect its reactivity. The atoms at a given reaction site may, for instance, have different charge distributions, depending on what other atoms are around them. And the physical shape of a molecule can render a reaction site difficult to access. So the MIT researchers’ model also includes numerical measures of both these features. According to Richard Robinson, a chemical-technologies researcher at the drug company Novartis, the MIT researchers’ system “offers a different approach to machine learning within the field of targeted synthesis, which in the future could transform the practice of experimental design to targeted molecules.” “Currently we rely heavily on our own retrosynthetic training, which is aligned with our own personal experiences and augmented with reaction-database search engines,” Robinson says. “This serves us well but often still results in a significant failure rate. Even highly experienced chemists are often surprised. If you were to add up all the cumulative synthesis failures as an industry, this would likely relate to a significant time and cost investment. What if we could improve our success rate?” The MIT researchers, Robinson says, “have cleverly demonstrated a novel approach to achieve higher predictive reaction performance over conventional approaches. By augmenting the reported literature with negative reaction examples, the data set has more value.” Topics: Research, School of Engineering, Artificial intelligence, Chemical engineering, Chemistry, Computer modeling, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Manufacturing This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
e61c72f244,Creating videos of the future | MIT News,"Login
or
Subscribe Newsletter
Given a still image from a scene, the CSAIL team's deep-learning algorithm can create a brief video that simulates the future of that scene. Image: Carl Vondrick/MIT CSAIL A new algorithm generates videos using a series of neural networks.
Image: Carl Vondrick/MIT CSAIL Given a still image, CSAIL deep-learning system generates videos that predict what will happen next in a scene.
Watch Video
Adam Conner-Simons | Rachel Gordon | CSAIL
November 29, 2016
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab Living in a dynamic physical world, it’s easy to forget how effortlessly we understand our surroundings. With minimal thought, we can figure out how scenes change and objects interact. But what’s second nature for us is still a huge problem for machines. With the limitless number of ways that objects can move, teaching computers to predict future actions can be difficult. Recently, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have moved a step closer, developing a deep-learning algorithm that, given a still image from a scene, can create a brief video that simulates the future of that scene. Trained on 2 million unlabeled videos that include a year’s worth of footage, the algorithm generated videos that human subjects deemed to be realistic 20 percent more often than a baseline model. The team says that future versions could be used for everything from improved security tactics and safer self-driving cars. According to CSAIL PhD student and first author Carl Vondrick, the algorithm can also help machines recognize people’s activities without expensive human annotations. “These videos show us what computers think can happen in a scene,” says Vondrick. “If you can predict the future, you must have understood something about the present.” Vondrick wrote the paper with MIT professor Antonio Torralba and Hamed Pirsiavash, a former CSAIL postdoc who is now a professor at the University of Maryland Baltimore County (UMBC). The work will be presented at next week’s Neural Information Processing Systems (NIPS) conference in Barcelona. How it works Multiple researchers have tackled similar topics in computer vision, including MIT Professor Bill Freeman, whose new work on “visual dynamics” also creates future frames in a scene. But where his model focuses on extrapolating videos into the future, Torralba’s model can also generate completely new videos that haven’t been seen before. Previous systems build up scenes frame by frame, which creates a large margin for error. In contrast, this work focuses on processing the entire scene at once, with the algorithm generating as many as 32 frames from scratch per second. “Building up a scene frame-by-frame is like a big game of ‘Telephone,’ which means that the message falls apart by the time you go around the whole room,” says Vondrick. “By instead trying to predict all frames simultaneously, it’s as if you’re talking to everyone in the room at once.” Of course, there’s a trade-off to generating all frames simultaneously: While it becomes more accurate, the computer model also becomes more complex for longer videos. Nevertheless, this complexity may be worth it for sharper predictions. To create multiple frames, researchers taught the model to generate the foreground separate from the background, and to then place the objects in the scene to let the model learn which objects move and which objects don’t. The team used a deep-learning method called “adversarial learning” that involves training two competing neural networks. One network generates video, and the other discriminates between the real and generated videos. Over time, the generator learns to fool the discriminator. From that, the model can create videos resembling scenes from beaches, train stations, hospitals, and golf courses.  For example, the beach model produces beaches with crashing waves, and the golf model has people walking on grass. Testing the scene The team compared the videos against a baseline of generated videos and asked subjects which they thought were more realistic. From over 13,000 opinions of 150 users, subjects chose the generative model videos 20 percent more often than the baseline.
Vondrick stresses that the model still lacks some fairly simple common-sense principles. For example, it often doesn’t understand that objects are still there when they move, like when a train passes through a scene. The model also tends to make humans and objects look much larger in size than reality. Another limitation is that the generated videos are just one and a half seconds long, which the team hopes to be able to increase in future work. The challenge is that this requires tracking longer dependencies to ensure that the scene still makes sense over longer time periods. One way to do this would be to add human supervision. “It’s difficult to aggregate accurate information across long time periods in videos,” says Vondrick. “If the video has both cooking and eating activities, you have to be able to link those two together to make sense of the scene.” These types of models aren’t limited to predicting the future. Generative videos can be used for adding animation to still images, like the animated newspaper from the Harry Potter books. They could also help detect anomalies in security footage and compress data for storing and sending longer videos. “In the future, this will let us scale up vision systems to recognize objects and scenes without any supervision, simply by training them on video,” says Vondrick. This work was supported by the National Science Foundation, the START program at UMBC, and a Google PhD fellowship. Topics: Research, Algorithms, Machine learning, Computer vision, Computer Science and Artificial Intelligence Laboratory (CSAIL), Networks, Data, Computer science and technology, Electrical Engineering & Computer Science (eecs), Artificial intelligence, School of Engineering, Big data, National Science Foundation (NSF) Steven Melendez of NBC News writes that a new system developed by CSAIL researchers can predict the future by examining a photograph. Grad student Carl Vondrick explains that the system’s ability to forecast normal behavior could allow it to be used for applications like self-driving cars. A new system developed by MIT researchers can predict how a scene will unfold, similar to how humans can visually imagine the future, reports Ed Gent for Scientific American. Graduate student Carl Vondrick explains that the system is “an encouraging development in suggesting that computer scientists can imbue machines with much more advanced situational understanding."" New Scientist reporter Victoria Turk writes that MIT researchers have developed a system that can predict the future based off of a still image. Turk writes that the system could enable “an AI assistant to recognize when someone is about to fall, or help a self-driving car foresee an accident.” This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
03937f7ada,Google’s chatbot analytics platform Chatbase launches to public  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
At Google I/O this year, Google quietly introduced a new chatbot analytics platform called Chatbase, a project developed within the company’s internal R&D incubator, Area 120. Today, that platform is being publicly launched to all, after testing with hundreds of early adopters including Ticketmaster, HBO, Keller Williams, Viber, and others. The idea behind Chatbase’s cloud service is to offer tools to more easily analyze and optimize chatbots. This includes giving bot builders the ability to understand what works to increase customer conversions, improve the bot’s accuracy, and create a better user experience. This data is available through an analytics dashboard, where developers can track specific metrics like active users, sessions, and user retention. These insights give an overall picture of the bot’s health and see general trends. The dashboard also lets bot creators compared the bot’s metrics across platforms, to see if some platforms need additional optimizations.
The system today integrates with any voice or text messaging platform, Google says, including Facebook, Kik, Viber, Slack, WhatsApp, WeChat, Alexa, Cortana, Allo, Line, Skype, Twitter, and more. And though it has had many high-profile testers in its early days, it’s not necessarily meant to be used only by larger companies. As a free service, Chatbase supports bot builders of any size – whether they have one or hundreds of bots in operation. Google notes, for example, that an early customer, BLiP – a bot platform for brands – has been using Chatbase to track over 2 million messages to date across over 50 bots. Ingenious.AI, meanwhile, uses Chatbase with a bot built for a large, Australian healthy insurer to help customers of its eyeglass stores. And Keller Williams uses Chatbase with a bot that lets its 170K associates ask questions, manage appointments, connect with other associates, and track their goals. Other testers on Chatbase’s platform have included bots for external-facing customer support, entertainment, advice and e-commerce, as well as internal-facing bots for productivity and information discovery, Google says. The Chatbase website’s customer list includes: HBO, Keller Williams, Ticketmaster, Poncho, Swelly, Botnation AI, Viber, inGenius.AI, Starbutter AI, Foxsy, Crystal, FitWell, push, mia, and Unicef.   Beyond bot analytics, the tool leverages Google’s machine learning capabilities to figure out what sort of problems could be affecting the bot.
Typically, developers would have to scour through log files to find patterns in user messages, but Chatbase’s system instead clusters user messages that aren’t being handled (this is still available only to Early Access testers) and finds opportunities to answer more requests. It then offers ways to optimize the bot for both problems. “One example would be for finding and fixing ‘misses,’ or alternate phrasing of supported actions that weren’t originally anticipated by the developer,” explains a Google spokesperson. “Like in so many other areas, machine learning and natural language processing are opening up powerful new opportunities in bot analytics. Putting some of Google’s machine learning capabilities to work for our users is a clear differentiator, and our users are really excited about that,” they added. Rakuten-owned Viber, which has over 900 million users in 193 countries, detailed Chatbase’s success with a stickers bot it runs. “We increased query volume by 35% for a popular stickers bot by optimizing queries with high exit rates,” the company said, in a statement shared by Google. “Chatbase has been immensely helpful in improving our bot. Instead of combing through logs, we rely on its machine-learning capability to help prioritize required optimizations — saving precious time that we need to focus on building new features,” Viber added.
Another notable capability in Chatbase is the auto-generated data visualization of conversation flows across sessions. This lets bot developers see what common paths users take, and where they often exit the application. A Funnel report highlights these steps and shows the success rate per step. The company announced the general availability of Chatbase via blog post today, adding that it’s free to use. When asked how the company plans to monetize the platform, Google said that’s something it’s thinking about for the future, but didn’t offer details on those plans. Google also noted users of Dialogflow (formerly API.AI), Google’s end-to-end platform for building cross-platform conversational experiences, will automatically get access to basic Chatbase features within Dialogflow. The public availability of Chatbase comes at a time when chatbots themselves have faced criticism for not being as useful as promised, and often suffering from usability issues. But the market is still in its early days, and chatbots aren’t exiting the scene. Some have even gotten better as developers figure out what works and what doesn’t. Chatbase isn’t the only solution for chatbot analytics, but the machine learning angle could give it an edge. Plus, Google’s ability to offer it for free could help it achieve market share that bot analytics companies can’t necessarily compete with. However, as an Area 120 project, it’s unclear to what extent Google will back the project long-term. To date, most Area 120 projects have been more experimental. Chatbase seems like the kind of thing that should graduate to a Google product in the future. Latest headlines delivered to you daily"
c3433d1aaa,AI demo picks out recipes from food photos - BBC News,"Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp LinkedIn Copy this link These are external links and will open in a new window An algorithm created to identify recipes for food just from a photograph has been demonstrated by researchers at the Massachusetts Institute of Technology (MIT). The neural network was trained on a dataset of one million photos and one million recipes. The trial model worked best on desserts and found smoothies and sushi more challenging, the researchers said. The team has released an online trial although it is not a finished product. When tested by the BBC, recipes that were generated based on a picture of spaghetti bolognese included ""Italian tomato sauce"" and ""gunk on noodles"". A photo of a Black Forest gateau yielded ""chocolate mocha cake"" and ""frozen grasshopper squares"" - both of which looked similar to the image uploaded - and it successfully identified a hot dog. ""In computer vision, food is mostly neglected because we don't have the large-scale datasets needed to make predictions,"" said MIT researcher Yusuf Aytar. ""But seemingly useless photos on social media can actually provide valuable insight into health habits and dietary preferences."" The team will be presenting their paper at a conference in Honolulu later this month. Previous models by other researchers have not used such a large data bank. In the future the system could be developed to include how a food is prepared and could also be adapted to provide nutritional information, MIT said. Artificial intelligence expert Calum Chace, author of The Economic Singularity, said the system was an interesting use of deep learning. ""It's an example of how machines can not only do things that humans cannot, but they perceive the world very differently from us,"" he told the BBC.
""Just as AlphaGo showed the world's best Go players whole new ways of looking at the game they had spent their lives mastering, this system will enable us to see the very food we eat in a different way."" In May 2017 Google's DeepMind AlphaGo artificial intelligence defeated the world's number one Go player Ke Jie, who was reduced to tears. Following its success, AlphaGo was retired, with DeepMind founder Demis Hassabis saying it had achieved its objective. The White House did not deny he used a slur and a UN official condemned the remark as racist."
beffb5d229,Scientists Use Machine-learning to Analyze Language in Movies  | Technology Networks,"News   Nov 14, 2017
| Original story from University of Washington
In the movie Frozen, only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of agency and power as Cinderella, a movie character that debuted in 1950. Credit: University of Washington
At first glance, the movie ""Frozen"" might seem to have two strong female protagonists -- Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom.But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists.The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed.""'Frozen' is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,"" said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies.""Anna is actually portrayed with the same low levels of power and agency as Cinderella, which is a movie that came out more than 60 years ago. That's a pretty sad finding,"" Sap said.The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like ""Heathers"" to romantic comedies like ""500 Days of Summer"" to war films like ""Apocalypse Now.""In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men. For example, male characters spoke more in imperative sentences (""Bring me my horse"") while female characters tended to hedge their statements (""Maybe I am wrong""). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives.To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on ""connotation frames"" that give insights into how different verbs can empower or weaken different characters through their connotative meanings. The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments.The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3.""For example, if a female character 'implores' her husband, that implies the husband has a stance where he can say no. If she 'instructs' her husband, that implies she has more power,"" said co-author Ari Holtzman, an Allen School doctoral student. ""What we found was that men systematically have more power and agency in the film script universe.""Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose.Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb's subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies.The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters -- exposing subtle differences and biases.In 2010's ""Black Swan,"" a movie centered around a female lead -- a perfectionist ballerina who slowly loses grip on reality -- the movie's dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film.In the 2007 movie ""Juno,"" about an offbeat young woman who unexpectedly gets pregnant, male characters' scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue.The UW team's tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man.The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers.""We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,"" said co-author and Allen School doctoral student Hannah Rashkin.Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn't limited to movies, but could be applied to books, plays or any other texts.""We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,"" said senior author Yejin Choi, an associate professor in the Allen School. ""We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.""This article has been republished from materials provided by University of Washington. Note: material may have been edited for length and content. For further information, please contact the cited source.
SLIMS Users Can Now Connect to the iSpecimen Marketplace
Genohm and iSpecimen have jointly announced that Genohm has established a partnership with iSpecimen to enable users of the SLIMS laboratory information management system to seamlessly connect to the iSpecimen Marketplace.
Ovation and Coriell Life Sciences Partner to Streamline Genetic Reporting Workflows
Ovation.io, Inc. (Ovation), makers of the fastest-growing clinical laboratory information and commercialization platform, and Coriell Life Sciences, Inc., an innovative provider of clinical genetic reporting solutions, announced a partnership and platform integration designed to create a seamless ecosystem for sample and workflow management, genetic data interpretation, and external client communication.
Blue Brain Nexus: An Open-Source Tool for Data-Driven Science
The Blue Brain Nexus will help enable data-driven neuroscience through searching, integrating and tracking large-scale data and models.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 11, 2018 Video News   Jan 11, 2018 News   Jan 11, 2018 Article Article"
6f034357f4,Audible’s new romance audiobooks service uses machine learning to jump to the sex scenes  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Let’s admit it: you probably aren’t reading that romance novel for the plot. Or its literary value. Audible knows this, and is today launching a new collection of romance-themed audiobooks that come with a handy feature that lets you skip right to the action. Called “Take Me To The Good Part,” the feature will fast-forward you to the steamy sections of the audiobook, says Audible. The feature was built in response to romance reader and listener feedback, the company notes. And Audible has doubled down on these customers’ desire to do away with the pretense that they’re actually interested in reading by debuting “Take Me To The Good Part” in over 100 Audible Romance package titles. The company plans to bring the technology to more selections over time, it says.
The feature is part of a new package of books under the Audible Romance brand, which is being sold as an add-on to an Audible membership for $6.95 per month, or as a standalone service for $14.95 per month. The package includes access to thousands of romance audiobooks, including best sellers and Audible Originals. There’s no limit on how many you can “read” monthly, either, as with Audible’s main subscription. Participating authors include Nora Roberts, Sylvia Day, Debbie Macomber, Robyn Carr, and others.
There’s technology under the hood that’s powering Audible’s ability to find the “good parts.” Audible is using machine learning to identify key words and phrases, as well as groups of words, in order to determine where things get hot and heavy. The company has even gone so far as to identify 10 type of so-called “good parts,” such as “flirty banter,” “first meeting,” “first kiss,” and one it dubs “hot, hot, hot” – aka the sex scenes. But just like how porn often drives technological innovations that later become mainstream – like adoption of the VCR back in the day, for example – we can only hope that this machine learning technology is later rolled out to all digital books, audio and otherwise, to classify scenes that are also “good parts,” but for non-romantic reasons. Along with the Romance package and its flagship feature, Audible has also rolled out a way to classify books by level of steaminess. (You know, so you can find those with more “good parts.”) Its illustrated “steaminess meter” ranks books on a scale that goes from sweet to simmering to sizzling to hot damn and o-o-omg. Yes, really.  And you can delve into your particular fetishes micro-genres more easily too, as the new Audible Romance service can identify 32 of these romance sub-genres and 122 story and character tropes that will let you find those that are a direct match with your interests. The program is live now and includes a free trial. Featured image credit: Carlos Ciudad Photos/Getty Images Latest headlines delivered to you daily"
34d8b97951,AI computer transforms your sketches into 'works of art'  | Daily Mail Online,"By
Shivali Best For Mailonline
Published:
11:33, 28 September 2017
|
Updated:
11:42, 28 September 2017
2 View
comments If you enjoy art but most of your drawings resemble a child's doodles, then you'll be happy to hear that help is at hand – in the form of an artificial intelligence computer. Scientists have created a new system called 'Vincent' that uses deep learning to transform sketches into 'works of art.' Completed 'works of art' combine a users' sketch with
art since the renaissance, as if Van Gogh, Cézanne and Picasso were inside the machine. Scroll down for video If you enjoy art but most of your drawings resemble a child's doodles, then you'll be happy to hear that help is at hand – in the form of an artificial intelligence computer. Vincent is the first system with the ability to interpret what a human is drawing, and then complete the piece Vincent is the first system with the ability to interpret what a human is drawing, and then complete the piece for them. To design Vincent, the researchers showed the computer thousands of paintings from the Renaissance period to the current day, training the computer on contrast, colour and texture. Now it is ready, Vincent can interpret the edges in paintings, and uses this understanding to produce a complete picture. To use the system, users simply draw directly onto a tablet. Vincent can then interpret different lines being drawn and pick up where the user left off, taking the information provided to build the piece into a completed picture. Vincent has been designed by scientists at Cambridge Consultants, and builds on human input to create its masterpieces. Machine learning has previously been used in the arts to create images and songs based on sounds or pictures it has seen before. But Vincent is the first system with the ability to interpret what a human is drawing, and then complete the piece for them. To design Vincent, the researchers showed the computer thousands of paintings from the Renaissance period to the current day, training the computer on contrast, colour and texture. Now it is ready, Vincent can interpret the edges in paintings, and uses this understanding to produce a complete picture. Mr Monty Barlow, director of machine learning at Cambridge Consultants, said: 'What we've built would have been unthinkable to the original deep learning pioneers. To use the system, users simply draw directly onto a tablet. Vincent can then interpret different lines being drawn and pick up where the user left off, taking the information provided to build the piece into a completed picture Deep-learning software tries to mimic the activity in layers of neurons in the neocortex, which makes up 80 percent of the brain and is where thinking occurs. The software learns to recognize patterns in digital representations of sounds, images, and other data. The idea that software can simulate the neocortex’s neurons in an artificial 'neural network' is decades old, and it has led to disappointments as breakthroughs. But because of improvements in mathematical formulas and increasingly powerful computers, computer scientists can now model many more layers of virtual neurons than ever before. Source: MIT Technology Review  'By successfully combining different machine learning approaches, such as adversarial training, perceptual loss, and end-to-end training of stacked networks, we've created something hugely interactive, taking the germ of a sketched idea and allowing the history of human art to run with it.' To use the system, users simply draw directly onto a tablet. Vincent can then interpret different lines being drawn and pick up where the user left off, taking the information provided to build the piece into a completed picture. Beyond art, the researchers believe that there is a range of potential applications for Vincent-like technology. For example, it could be used in autonomous vehicles if it was trained using driving scenarios and simulations, according to the researchers. Mr Barlow added: 'We're exploring completely uncharted territory – much of what makes Vincent tick was not known to the machine learning community just a year ago. 'We're excited to be at the leading edge of an emerging, transformative industry and to be making the leap from the art of the possible to delivering practical machine learning solutions for our clients.' Scientists have created a new system called 'Vincent' that uses deep learning to transform sketches into 'works of art' Beyond art, the researchers believe that there is a range of potential applications for Vincent-like technology. For example, it could be used in autonomous vehicles if it was trained using driving scenarios and simulations, according to the researchers
Trump CANCELS his visit to London: President pulls out of £750m US embassy opening and blames Obama for selling old site 'for peanuts' as British politicians hit back it's because 'no-one wants you here'
Published by Associated Newspapers Ltd Part of the Daily Mail, The Mail on Sunday & Metro Media Group"
197d5e1b4f,"Google Vizier, Google's newest AI, was tested on 'machine learning cookies' | WIRED UK","Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
The system ""designs excellent cookies"", according to its creators
By
Rowland Manthorpe
Deep Dream cookies Google's computer scientists have created an AI to tweak the company’s other AIs. The advanced machine learning system, which goes by the faintly sinister name of Google Vizier, automatically tunes algorithms right across Google’s parent company Alphabet. But to test it, the researchers used an old-fashioned metric: cookie quality in the canteen. Modern machine learning systems are the algorithmic equivalent of Formula 1 racing cars. The systems have tremendous power, but they are extremely sensitive: to function effectively they need to be finely tuned, usually by hand.
Machine Learning In particular, the systems need carefully set 'hyperparameters': parameters set in advance that are adapted to the problem at hand. This isn’t easy, because machine learning systems are “black boxes”: even when you’ve made them, you can never be entirely sure how they get their results. One common method for tuning is known in the field as “grad student descent”: basically, you get a graduate student to twiddle the parameters until the algorithm works.
By
Matthew Reynolds
Google Vizier cuts short this tedious manual task by automatically optimising hyperparameters of machine learning models. According to Google’s researchers, the system is already in use across the company. ""Our implementation scales to service the entire hyperparameter tuning workload across Alphabet, which is extensive,"" they write in a paper released this week, citing an example where Google researchers “used Vizier to perform hyperparameter tuning studies that collectively contained millions of trials for a research project… That research project would not be possible without effective black–box optimisation.” One process employed in Google Vizier is called 'transfer learning,' essentially, learning from experience. Using data from previous studies as a guide, the Vizier algorithm can suggest optimal hyperparameters for new algorithms. The method is most effective when there have been lots of studies in the area, but it also works when there is relatively little crossover: for instance ""when the observed metrics and the sampling of the datasets are different"".
As well as helping research, Google Vizier is being put to use at Alphabet, where, its creators write, it ""has made notable improvements to production models underlying many Google products, resulting in measurably better user experiences for over a billion people"".
By
Elizabeth Stinson
These improvements include automated A/B testing of features of Google's websites, including fonts, colours and the format of search results. On Google Maps, for instance, the system is being used to optimise the trade-off between the relevance of a particular search and the distance of that result from the user (with the inevitable aim of getting higher engagement for Google).
Artificial Intelligence Google Vizier can also be used to solve black box optimisation problems in the messy physical world. And this is where the cookies come in.
To test their system, the researchers gave cookie recipes to the contractors who make the puddings in Google’s canteen. They taste-tested the result and tracked any alterations the chefs made to improve the taste. Recipes are a kind of algorithm in their own right, with similar black box properties (because you never exactly know why your bake went wrong). And this test allowed the researchers to try out their transfer learning approach: “Before starting to bake at large scale, we baked some recipes in a smaller scale run-through,” they write. “This provided useful data that we could transfer learn from when baking at scale.”
Even when things went slightly wrong – as, for instance, when the dough was allowed to sit longer, which ""unexpectedly, and somewhat dramatically, increased the subjective spiciness of the cookies for trials that involved cayenne"" – the schema was able to respond. After a number of rounds, the cookies improved significantly, the researchers say:
“Later rounds were extremely well-rated and, in the authors’ opinions, delicious."" HT: The weekly Import AI newsletter from Jack Clark of Open AI. Essential reading for anyone interested in AI
By
Matt Burgess
By
Matt Burgess
By
Matt Burgess"
5d28f50bf2,Revolutionizing Radiology with Deep Learning at Partners Healthcare--and Many Others,"Thomas H. Davenport and Randy Bean
One of the more miraculous aspects of modern medicine is its ability to peer directly inside the human body to aid in diagnosis of disease and medical conditions. Radiological imaging is one of the most effective diagnostic tools available, and its use is so pervasive that it accounts for about 10% of U.S. medical costs. The usage of medical imaging grew rapidly over the last few decades, but has plateaued as costs skyrocketed. Not only are imaging machines expensive, but images also require interpretation by radiologists. Researchers and vendors have worked for many years to automate the interpretation of images, but thus far the field of computer-aided diagnosis (CAD) has not made substantial inroads into patient care. Some institutions employ CAD as a “second set of eyes,” but the cost of imaging has yet to decline.
New cognitive technologies, however, have the potential to substantially improve CAD for radiology images and also those from pathology labs, and to combine them with other diagnostic data. These technologies are advancing quickly in research labs, but have yet to make their way into medical practice. A relatively new center at Partners Healthcare – the Center for Clinical Data Science (CCDS) – is focused on bringing these technologies to the clinical world. Based at the highly-ranked institutions Massachusetts General Hospital (MGH) and Brigham & Women’s Hospital (BWH) in Boston, the CCDS is a joint effort of MGH and BWH. Its goal is to employ machine learning and other artificial intelligence technologies to improve the healthcare delivery system; in particular, a key CCDS objective is to improve the effectiveness of imaging-based diagnosis. The CCDS is pursuing a variety of machine learning approaches, but the primary technology that it is employing is deep neural networks (also known as deep learning). These technologies have already led to breakthroughs in other areas of image recognition, and many researchers expect that they eventually will do so with medical images. A recent article in the New England Journal of Medicine, “Translating Artificial intelligence into Clinical Care,” expressed hope that this type of machine learning will lead to a breakthrough in care. As Dr. Keith Dreyer, Partners’ Chief Data Science Officer, puts it: We’ve had CAD for a couple of decades, but deep learning is a much better technology. It will provide much higher sensitivity and specificity than we have today, and radiologists will trust it. Integrating it with clinical practice offers many potential benefits. The diagnosis of a lumbar spine injury, for example, might involve up to 300 MRI images and various other test results in an electronic medical record system. A deep learning application could quickly identify the most important images for a radiologist to review and recommend treatment alternatives. The technology could save substantial time for critically injured trauma patients and could leverage the radiologist’s time for all patients."
af579873f5,Alibaba's FashionAI shows how machine learning might save the mall,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Alibaba’s sales from Saturday’s Singles Day event exceeded 25 billion dollars, more than quadruple what Americans spent last year during Black Friday. While the majority of those sales undoubtedly came via online purchases, the company also quietly experimented with an AI-powered project designed to woo offline shoppers. FashionAI was developed by Alibaba researchers in order to provide a recognizable interface for customers to use while trying on clothes. It’s a basic screen interface that uses machine-learning to make clothing and accessory suggestions to customers based on the items they are trying on. There’s no camera; it uses information embedded in the item’s tag to make the recommendations. Using the system, customers can try clothes on, receive fashion tips and suggestions from the AI, then make choices on-screen. If a user wants to try something different, or add other items, a store attendant can be summoned with the press of a button. Deep learning allows the AI to make connections in real-time by accessing massive quantities of data and making ‘smart’ decisions. When you’re on Amazon, for example, looking at a pair of cowboy boots might prompt the algorithm to recommend hiking or biker boots. But, search for a cowboy hat too, and you’re likely to be recommended other cowboy themed items, like belts, as opposed to just more hats and boots. The idea is that eventually the AI will get better at determining what you’re going to look at, what you’re going to compare it with, what you’ll want to purchase with it, and which items you’ll actually end up buying. Where no staff of humans could possibly be expected to remember the personal shopping preferences of every single customer, AI can. Alibaba’s FashionAI may have been nothing more than an experiment at this point, but it’s an exciting one. It’s taken the weirdness out of ideas like Amazon’s AI-powered shops and turned it into something more palatable. Most attempts at changing the way brick-and-mortar stores operate have revolved around trying to cram the online experience into kiosks, lobbies, and the checkout area. The trade-off is typically one where customers invest their own time and effort into a system that doesn’t quite work as well as logging on to Amazon or Alibaba with a smartphone does. By placing a screen against a wall in a changing booth, Alibaba integrated its already successful shopping system into the real-world seamlessly. If brick-and-mortar stores are to remain viable, they’ll have to leverage consumer attention by giving us all a reason to leave our phones in our pockets when we shop. People are probably getting sick of being told that Amazon and Alibaba can afford to sell items cheaper because they don’t have overhead. That doesn’t make most of us feel better about being overcharged. It feels like we’re being charged for the ‘privilege’ of standing in line and carrying our own bags through stores and parking lots, at most brick-and-mortar stores. If AI can be harnessed to bring the brick-and-mortar experience more closely in line with shopping on Alibaba.com, we all stand to benefit. Putting up AI-powered screens in dressing rooms is an important first step, and one that may ultimately save a lot of human jobs. Instead of replacing people with robots, Alibaba’s FashionAI integrates humans and machines in a way that provides the customer with everything they need. And the customer is always right.
Alibaba’s AI Fashion Consultant Helps It Set a New Singles’ Day Record
on MIT Technology Review
Read next:
5 tools for improving the efficiency (and conversion power) of your call center
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
1c005788b0,Generate your own sounds with NSynth,"Jun 19, 2017
•
Parag Mital
(
pkmital
,
pkmital
)
[Editorial Note: One of the best parts of working on the Magenta project is getting to interact with the awesome community of artists and coders. Today, we’re very happy to have a guest blog post by one of those community members, Parag Mital, who has implemented a fast sampler for NSynth to make it easier for everyone to generate their own sounds with the model.] NSynth is, in my opinion, one of the most exciting developments in audio synthesis since granular and concatenative synthesis. It is one of the only neural networks capable of learning and directly generating raw audio samples.
Since the release of WaveNet in 2016, Google Brain’s Magenta and DeepMind have gone on to explore what’s possible with this model in the musical domain. They’ve built an enormous dataset of musical notes and also released a model trained on all of this data. That means you can encode your own audio using their model, and then use the encoding to produce fun and bizarre new explorations of sound. Since NSynth is such a large model, naively generating audio would take a few minutes per sample, making it pretty much a nonstarter for creative exploration.
Luckily, there is plenty of knowledge floating out there about how to make the WaveNet decoder faster1.
As part of development for my course on Creative Applications of Deep Learning, I spent a few days implementing a faster sampler in NSynth, submitted a pull request to the Magenta folks, and it’s now part of the official repo. The whole team was incredibly supportive and guided me through the entire process, offering a ton of feedback. I’d highly encourage you to reach out via GitHub or the Magenta Google Group with any questions if you are interested in contributing to the project. If you are interested in learning more about NSynth, WaveNet, or Deep Learning in general, I highly recommend that you join our growing community of students on Kadenze. We cover everything from the basics, to Magenta, NSynth, and plenty more, including some interviews with members of Magenta. Everything I show here is also included in the Jupyter Notebook located in the Magenta demos repo.
Have a look there for more details on how these sounds were created. First, I explored resynthesizing sounds: simply encoding and decoding them without changing the encodings. Here’s an example of what an encoding looks like using NSynth’s pretrained model.
This example shows a drum beat.
You can see that the encoding seems to follow the overall pattern of the audio signal. I then tried decoding this directly. This lets me hear the bias built into the model before exploring other manipulations, and can produce some really interesting results by itself.
I found a few samples on http://freesound.org2 and generated these overnight.
You can either use the command line tool nsynth_generate (see the README for more info), or, in the Jupyter Notebook, I also show how you can save embeddings and generate audio directly in python using the new fast generation module. Here are some example sounds I tried resynthesizing using NSynth: Original: Resynthesis: Notice how much character these have!
They are really weird.
They are certainly not perfect reconstructions, as they are very different than the training data, but that is what makes them so interesting and fun to explore. Also, remember that NSynth was trained to use 8-bit mu-law 16kHz audio, so there is naturally a gritty lo-fi quality to the sound. Next, I explored a fairly simple manipulation of the embedding space of NSynth by stretching it in time.
The idea here is to perform a fairly established technique in audio synthesis called timestretching.
If you were to try the same thing naively on the raw audio signal, you’d get a pitch shift.
So making an audio signal longer would also reduce its pitch, and that might be undesirable.
But what if you want to keep it the same length?
There are plenty of other techniques out there like granular time stretching.
But I found NSynth can also perform time stretching even though it was never designed to do so, simply by scaling its embeddings.
What you do is take the encoding, so a 125 x 16 dimension matrix for a 4 second sample, and you stretch it like an image in any direction.
Then you can synthesize the stretched embeddings. Here’s the original reconstruction: And now twice as fast, by compressing it to half its length: And here it is at 1.5 times as long in duration by stretching the embeddings matrix: This is really the magical part of working with a neural audio synthesis technique.
Much like word2vec’s linear representation of a verbal semantic space, NSynth’s encoding should similarly have a good representation of its dataset in its embedding.
It would be great to see the community explore things like t-SNE visualizations, vector-based arithmetic of embeddings, and further manipulations of the embedding space that produce interesting results on the synthesized sound. There are a few ways we could think about combining sounds.
Magenta’s release of an NSynth instrument inside Ableton Live already demonstrates a really powerful way of interpolating between many different instrument sounds used in the training of NSynth.
Also their collaboration with Google Creative Labs shows how to interpolate between any two sounds using NSynth. Now we can explore interpolations of our own sounds!
In the Jupyter Notebook, I show an example of mixing the breakbeat and the cello from above by simply averaging their embeddings together.
This is unlike adding the two signals together in Ableton or simply hearing both sounds at the same time.
Instead, we’re averaging the representation of their timbres, tonality, change over time, and resulting audio signal.
This is way more powerful than a simple averaging. Here’s the result of averaging the embeddings and resynthesizing them: And here’s the result of crossfading the embeddings and resynthesizing them: NSynth produces some amazing sounds.
Once you consider its possibilities inside of your normal musical practice, you can really start to explore much more.
For me I was interested in exploring composition so I tried putting some of the generated sounds together in a sort of dub-like composition inside Ableton Live. Here I was really able to explore the kinks and peculiarities of NSynth’s synthesis method and add things like delay and reverb to accentuate them. The result uses only the samples I’ve shown you so far without very much additional post-processing: I also showed the generated sounds to a musician friend, Jordan Hochenbaum. He got pretty excited about the timestretched breakbeat synthesis and decided to use it as the foundation of a hip-hop track. The rest of the material mostly came from the string synthesis sample. He also resampled some of the generated sounds using Live’s Sampler instrument to make a bass and other polyphonic string sounds. Here’s what he came up with: His composition really demonstrates how well NSynth can be used alongside more typical elements of a musical practice.
If you are interested in learning more about sound production in Ableton Live, definitely check out his free Kadenze course on Sound Production. There is a lot to explore with NSynth. So far I’ve just shown you a taste of what’s possible when you are able to generate your own sounds. I expect the generation process will soon get much faster, especially with help from the community, and for more unexpected and interesting applications to emerge. Please keep in touch with whatever you end up creating, either personally via twitter, in our Creative Applications of Deep Learning community on Kadenze, or the Magenta Google Group. Ramachandran, P., Paine, T.L., Khorrami, P., Babaeizadeh, M., Chang, S., Zhang, Y., Hasegawa-Johnson, M.A., Campbell, R.H. and Huang, T.S., 2017. Fast generation for convolutional autoregressive models. arXiv preprint arXiv:1704.06001. ↩ Cello sample created by Xavier Serra (link, CC 3.0). ↩"
1d32c4eef1,"Dogs vs. Cats Redux Playground Competition, 3rd Place Interview: Marco Lugo | No Free Hunch","Kaggle Team|04.20.2017 The second iteration of the Dogs vs. Cats Redux playground competition challenged Kagglers to once again distinguish images of dogs from cats. This time relying on advances in computer vision and new tools like Keras. In this winner's interview, Kaggler Marco Lugo shares how he landed in 3rd place out of 1,314 teams using deep convolutional neural networks: a now classic approach. One of Marco's biggest takeaways from this for-fun competition was an improved processing pipeline for faster prototyping which he can now apply in similar image-based challenges.
I am an economist by training and have been submerged in econometrics, which I would describe as the more classical branch of statistics where the main economic focus is often in policy and therefore on causality.
Marco Lugo on Kaggle. I started programming with the C language about two decades ago and have always strived to keep learning about programming, eventually landing on R which made me discover machine learning in 2013 - I was instantly hooked on predictive modeling. I have tried various computer vision datasets in the past but nothing that had forced me to push the envelope on hyperparameter optimization. This was my first image-related competition. I believe it was one night where I was searching how to do something with R and as it turns out, the code that ended up helping me to understand how it was done was found on the Kaggle website. I explored the site at that time and decided to enter a competition for fun, applying a linear regression thinking that it would be easy but ended up obtaining a less-than-stellar outcome instead. It was that somewhat humbling result that pushed me into machine learning. I was taking the excellent deep learning course by Jeremy Howard, Kaggle’s ex-president and ex-Chief Scientist, and one of the homework assignments was to enter the competition and get a top 50% ranking. I did my homework.
The online notes for Stanford’s CS231n course by Andrej Karpathy were particularly useful. Also, the Kaggle Blog winner's interviews were good to spark new ideas when my score started to stall. I randomly partitioned the data to create a validation set containing only 8% of the training set. I also demeaned and normalized the data as needed and used data augmentation to varying degrees.
I used deep convolutional neural networks, both trained from scratch and pre-trained on the ImageNet database. My ensemble was a weighted average of the following models: Diagram of Marco's ensemble model. I was pleasantly surprised by the effect of adding relatively poor performing models into the mix. It was also interesting to play around with the different variations of rectified linear units (ReLU) as switching from standard ReLU to Leaky ReLU and Randomized Leaky ReLU had a noticeable impact. I used Keras, developed by François Chollet from Google, for all of the neural networks with both the Theano and TensorFlow back-ends depending on the type of model I had to run. While the vast majority of the work was done in Python, I did use R to run the lightGBM model. It’s a relatively old Windows 7 machine running on a AMD FX-8350 CPU, with 24GB of RAM and a NVidia GTX1060 6GB GPU. I also run an Ubuntu virtual machine on it but most of the work is done on Windows. I plan on upgrading soon. I remember that some models took over 74 hours to train as I often trained for hundreds of epochs but I cannot put an exact number on all the iterations and models that I had to run, I would estimate it at 3 or 4 weeks of running time. Predicting for the full test set took under an hour.
I learned how important it is to properly understand the evaluation function. It was worth my time to sit down with pen and paper to explore the mathematical properties of the logarithmic loss function. Understanding the formula is not the same as understanding its impact. I would have set my processing pipeline much earlier in the competition. I only did it after cracking the top 40% and, unsurprisingly, it enabled faster prototyping and thus allowed me to start to make real gains on a daily basis. It is also worth the investment as it can be easily reused. I was able to quickly recycle it for the Cervical Cancer Screening competition and land a top 10% position from the start building on the same setup. I would highly recommend trying out as many different problems as you can and getting your hands dirty even if you do not fully grasp the theory behind it at the beginning. I will steal a page here from Jeremy Howard’s deep learning course and refer you to a short essay that perfectly illustrates this point: A Mathematician’s Lament by Paul Lockhart.
Marco Lugo currently works as a Senior Analyst at Canada Mortgage and Housing Corporation. He holds a B.Sc. in Economics and Philosophy and a M.Sc. in Economics from the University of Montreal. Want to see how others have tackled the Dogs versus Cats playground competition? Check out Kaggler Bojan Tungu'z winner's interview. Can I know how do you do the weighted average? Thanks. Nice post... Are you planning to share the code ?"
84ac5d8799,Google DeepMind pairs with NHS to use machine learning to fight blindness | Technology | The Guardian,"‘Deep learning’ research company will use 1m anonymised eye scans to train a neural network to identify early signs of degenerative eye conditions ‘Deep learning’ research company will use 1m anonymised eye scans to train a neural network to identify early signs of degenerative eye conditions
Alex Hern
Tue 5 Jul ‘16 09.00 BST
Last modified on Wed 20 Sep ‘17 19.22 BST
Google DeepMind has announced its second collaboration with the NHS, working with Moorfields Eye Hospital in east London to build a machine learning system which will eventually be able to recognise sight-threatening conditions from just a digital scan of the eye. The collaboration is the second between the NHS and DeepMind, which is the artificial intelligence research arm of Google, but Deepmind’s co-founder, Mustafa Suleyman, says this is the first time the company is embarking purely on medical research. An earlier, ongoing, collaboration, with the Royal Free hospital in north London, is focused on direct patient care, using a smartphone app called Streams to monitor kidney function of patients. The Moorfields collaboration is also the first time DeepMind has used machine learning in a healthcare project. At the heart of the research is the sharing of a million anonymous eye scans, which the DeepMind researchers will use to train an algorithm to better spot the early signs of eye conditions such as wet age-related macular degeneration and diabetic retinopathy. Suleyman said: “There’s so much at stake, particularly with diabetic retinopathy. If you have diabetes you’re 25 times more likely to go blind. If we can detect this, and get in there as early as possible, then 98% of the most severe visual loss might be prevented.” Training a neural network to do the assessment of eye scans could vastly increase both the speed and accuracy of diagnosis, potentially saving the sight of thousands. The collaboration between the two organisations came about thanks to an unsolicited request from one doctor at Moorfields. Pearse Keane, a consultant ophthalmologist, contacted the Google subsidiary through its website to discuss the need to better analyse scans of the eye, and initiated the research project shortly after. “I’d been reading about deep learning and the success that technology had had in image recognition,” he said, when he came across an article about DeepMind training a machine to play Atari games – the company’s first public success. “I had the brainwave that deep learning could be really good at looking at the images of the eye. Optical Coherence Tomography is my area, and we have the largest depository of OCT images in the world. Within a couple of days I got in touch with Mustafa, and he replied.” DeepMind’s previous collaboration with the NHS had led to controversy, after it and its parter, the Royal Free hospital, were accused of not having the proper authority to share the records of patients who would be involved in the trial. At the time, Royal Free said that the arrangement “is the standard NHS information-sharing agreement set out by NHS England’s corporate information governance department and is the same as the other 1,500 agreements with third-party organisations that process NHS patient data.” Since the Moorfields collaboration involves anonymised information, the privacy hurdles are much lower. The company has been given permission for access through a research collaboration agreement with the hospital, and has published a research protocol, as is standard practice for medical trials. The company says the information shared amounts to “approximately 1m anonymous digital eye scans, along with some related anonymous information about eye condition and disease management. “This means it’s not possible to identify any individual patients from the scans. They’re also historic scans, meaning that while the results of our research may be used to improve future care, they won’t affect the care any patient receives today. The data used in this research is not personally identifiable. When research is working with such data, which is anonymous with no way for researchers to identify individual patients, explicit consent from patients for their data to be used in this way is not required.” Prof Peng Tee Khaw, the head of Moorfields’ ophthalmology research centre, said that the key to the collaboration was the huge increase in the volume of incredibly precise retinal scans available. “These scans are incredibly detailed, more detailed than any other scan of the body we do: we can see at the cellular level. But the problem for us is handling this amount of data. “It takes me my whole life experience to follow one patient’s history. And yet patients rely on my experience to predict their future. If we could use machine assisted deep learning, we could be so much better at doing this, because then I could have the experience of 10,000 lifetimes.” Somewhat oddly, the DeepMind/Moorfield collaboration is actually the second time that Google has looked at using machine learning to detect diabetic retinopathy in eye scans. An earlier, different, project was announced by Google chief executive Sundar Pichai onstage at the company’s annual developer conference, Google I/O, in May."
f6839f7d30, New tool quantifies power imbalance between female and male characters in Hollywood movie scripts  |  UW News ,"Engineering  |  News releases  |  Research  |  Science  |  Social science  |  Technology
November 13, 2017
UW News At first glance, the movie “Frozen” might seem to have two strong female protagonists — Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom. But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists. The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed. In the movie “Frozen,” only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of power as 1950s-era Cinderella.University of Washington “’Frozen’ is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,” said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies. “Anna is actually portrayed with the same low levels of power as Cinderella, which is a movie that came out more than 60 years ago. That’s a pretty sad finding,” Sap said. The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like “Heathers” to romantic comedies like “500 Days of Summer” to war films like “Apocalypse Now.” In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men.  For example, male characters spoke more in imperative sentences (“Bring me my horse”) while female characters tended to hedge their statements (“Maybe I am wrong”). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives. To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on “connotation frames” that give insights into how different verbs can empower or weaken different characters through their connotative meanings.  The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments. The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3. “For example, if a female character ‘implores’ her husband, that implies the husband has a stance where he can say no. If she ‘instructs’ her husband, that implies she has more power,” said co-author Ari Holtzman, an Allen School doctoral student. “What we found was that men systematically have more power and agency in the film script universe.” Across nearly 800 movie scripts of different genres, male characters were on average more empowered and described with higher levels of agency than female characters. Men were more likely to use forceful, imperative statements, while women were more likely to hedge their opinions.University of Washington Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose. Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb’s subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies. The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters — exposing subtle differences and biases. In 2010’s “Black Swan,” a movie centered around a female lead — a perfectionist ballerina who slowly loses grip on reality — the movie’s dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film. In the 2010 movie Black Swan, male characters (blue bars) were written with more control over their own destiny than their female counterparts, specifically in stage directions (left chart). However, the verbs used in dialogue (right chart) gave more power to female characters (red bars).University of Washington   In the 2007 movie “Juno,” about an offbeat young woman who unexpectedly gets pregnant, male characters’ scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue. The UW team’s tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man. The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers. “We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,” said co-author and Allen School doctoral student Hannah Rashkin. Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn’t limited to movies, but could be applied to books, plays or any other texts. “We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,” said senior author Yejin Choi, an associate professor in the Allen School. “We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.” The research was funded by the National Science Foundation, Google and Facebook. The other co-author is former UW Allen School undergraduate Marcella Cindy Prasetio. For more information, contact the research team at debiasing-ai@cs.washington.edu. Grant numbers: NSF: IIS – 1524371, NSF: IIS – 1714566, NSF: DGE – 1256082 13 hours ago 1 day ago 2 days ago © 2018 University of Washington
|
Seattle, WA"
eafdd9eba2,Artificial Intelligence Beats CAPTCHA - IEEE Spectrum,"Artificial intelligence software can beat the world's most widely used test of a machine's ability to act human, Google's reCAPTCHA, by copying how human vision works, a new study finds.
These new findings suggest the need for more robust automated human-checking techniques, and could help improve computer perception for robotics tasks, scientists add.
The founder of modern computing, Alan Turing, conceived of the Turing test, the most famous version of which asks if one could devise a machine capable of mimicking a human well enough in a conversation over text to be indistinguishable from human. In doing so, Turing helped give rise to the field of artificial intelligence.
The most commonly used Turing test is the CAPTCHA, an acronym for ""Completely Automated Public Turing test to tell Computers and Humans Apart."" CAPTCHAs are designed to see whether users are human, often to prevent bots from accessing computing services. They usually challenge website visitors to recognize a string of distorted letters and digits, a problem designed to be difficult for computers and easy for humans.
A CAPTCHA is considered broken if an algorithm can successfully solve it at least 1 percent of the time. Now San Francisco Bay Area startup Vicarious reveals its AI software can solve reCAPTCHAs at an accuracy rate of 66.6 percent, BotDetect at 64.4 percent, Yahoo at 57.4 percent and PayPal at 57.1 percent.
The system that Vicarious developed, known as the Recursive Cortical Network (RCN), is an artificial neural network, a computing design that mimics how the brain works. In such a system, components known as artificial neurons are fed data, and work together to solve a problem such as identifying text or recognizing speech. The neural net can then alter the pattern of connections among those neurons to change the way they interact, and the network tries solving the problem again. Over time, the neural net learns which patterns are best at computing solutions.
Previous neural nets could solve reCAPTCHAs, but required training on millions of labeled CAPTCHA image examples or handcrafted rules on how to crack each kind of image. In contrast, Vicarious' system required much less training data — compared to state-of-the-art deep-learning neural net approaches for reading text, RCN had comparable or higher accuracy while using roughly 300 times less training data.
""Our system has the ability to learn using relatively few examples, much like the human brain,""
says study lead author Dileep George, cofounder of Vicarious.
Vicarious says the key to its success was modeling RCN after the human brain's visual system. The company explains that RCN's artificial neurons are structured in a way that supports the generation of models that can quickly identify surfaces and contours to help it recognize images and objects given just a few examples.
These findings suggest ""text-based CAPTCHAs are becoming obsolete,"" George says. He notes that Google and others are already moving away from text-based CAPTCHAs toward new verification mechanisms, such as relying on image-based CAPTCHAs.
The researchers note their software could help tackle other challenges linked with computer perception. ""We're applying it toward many robotics tasks,"" George says. ""You can imagine a robot needing to not just identify an object but also interact with it, and it needs to build a model of how it behaves if it, say, has to push it.""
The scientists detailed their findings online Oct. 26 in the journal Science.  IEEE Spectrum’s general technology blog, featuring news, analysis, and opinions about engineering, consumer electronics, and technology and society, from the editorial staff and freelance contributors. Sign up for the Tech Alert newsletter and receive ground-breaking technology and science news from IEEE Spectrum every Thursday. Engineers are trying to squeeze outsize AI into mobile systems
28 Mar 2016
Digital Reasoning has trained a record-breaking artificial intelligence neural network that is 14 times larger than Google's previous record
8 Jul 2015
With short-term memory, recurrent neural networks gain some amazing abilities
26 Jan 2016
Samsung, LG, Panasonic, and TCL look to better processors and AI to move TV tech forward in 2018
9 Jan
Intel's new superconducting quantum chip called Tangle Lake has enough qubits to make things very interesting from a scientific standpoint
9 Jan
The elephant’s new protector is PAWS, a machine-⁠learning and game-theory system that predicts where poachers are likely to strike
6 Jan
An MIT student lab shows how to trick computer vision AI so it sees the wrong objects in pictures
22 Dec 2017
A dystopian future in which killer robots are massacring innocents is terrifying, but let’s be clear: It's very much science fiction
22 Dec 2017
An AI built by IBM could help organic chemists find new ways to synthesize drugs
4 Dec 2017
At the IEEE Rebooting Computing Conference, deep thinking about computing led to some wild ideas
29 Nov 2017
The agency wants ideas for turning computers into lifelong learners
21 Nov 2017
It took Stanford AI researchers just a month to beat radiologists at the pneumonia game
17 Nov 2017
The future of weaponized robots requires a reasoned discussion, not scary videos
15 Nov 2017
Industry leaders meeting at Techonomy17 say the information war has begun—and the tech community must try to fight back
14 Nov 2017
Silicon Valley teams up with a Chinese microscope manufacturer to deploy deep learning to diagnose malaria
13 Nov 2017
UC Berkeley spinoff wants to use AI and VR to teach robots new skills
8 Nov 2017
Bitmain built the majority of the computing power on the Bitcoin network. Now it wants to expand into deep learning and other AI
6 Nov 2017
A debugging method for deep learning AI pits neural networks against each other to find errors
3 Nov 2017
Researchers train algorithms to identify people with suicidal thoughts based on fMRI brain scans
2 Nov 2017
A new exhibit by experimental philosopher Jonathon Keats challenges participants to think about the future of work
1 Nov 2017
© Copyright 2018 IEEE — All rights reserved. Use of this Web site signifies your agreement to the IEEE Terms and Conditions.A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity."
1ad461a397,"
Machine-learning technology created to track drug peddlers via Twitter | The Indian Express","Researchers have developed a machine-learning technology that mined microblogging site Twitter to identify users peddling the illegal sale and marketing of prescription opioids online. The technology can be used to conduct live surveillance and detect illegal online sellers, they said. “Our study demonstrates the utility of the technology to aid in searches of social media for behaviour that poses a public threat”, said lead author Tim K Mackey, Associate Professor at the University of California-San Diego. “Social media providers can use this technology to find or prohibit content that is illegal or violates laws to ensure consumers have a safer experience online,” Mackey noted. While online sale of controlled substances is directly prohibited by federal law, social media appears to act as a conduit for increased risk to substance abuse behaviour, the researchers said. For the study, published in the American Journal of Public Health, the team used a three-step process that involved cloud-based computing to collect large volumes of tweets filtered by keywords. Machine learning to isolate tweets related to the marketing of opioids, and web forensic examination to analyse posts that included hyperlinks to external websites. The researchers collected some 619,937 tweets containing the keywords codeine, Percocet, fentanyl, Vicodin, Oxycontin, oxycodone and hydrocodone between June and November 2015. Of these, they found 1,778 posts that were marketing the sale of controlled substances, 90 per cent included hyperlinks to online sites for purchase. Forensics researchers connected marketing on Twitter to blogs, other social media platforms, user forms, online classified ads and websites. The majority of sites had foreign addresses, with many linked to Pakistan – a country identified as a source and exporter of fake, counterfeit and falsified medications, Mackey said. For all the latest Technology News, download Indian Express App"
fba83c979c,Researching for tomorrow | DeepMind,"Each scan and test result contains crucial information about whether a patient is at risk of a serious condition, and what needs to be done. Being able to interpret that information quickly and accurately is essential for hospitals to be able to save lives. Right now, 1 in 10 NHS in-patients suffers some form of avoidable harm, because this information isn't interpreted and acted on in time. AI systems could be hugely beneficial in helping with this process. Rather than programming systems by hand to recognise the potential signs of illness, which is often impossible given the number of different factors at play, AI systems can be trained to learn how to interpret test results for themselves. In time, they should also be able to learn which types of treatments are most effective for different patients. This could enable a series of benefits across the NHS, including:
We're still in the early stages of AI research in health, and our work is based on deep collaboration with our clinical partners. We currently work with two NHS hospitals to combine their clinical expertise with our machine learning technology, and together we ensure that the research we undertake will have practical benefits and save lives. The results of this work will be subject to rigorous clinical scrutiny, and will be published in peer-reviewed academic journals. Today our research is centred on a technique called deep learning. Our algorithms interpret visual information in the form of de-personalised images (head and neck scans at University College London Hospitals NHS Foundation Trust, eye scans at Moorfields Eye Hospital NHS Foundation Trust, and mammograms with the Cancer Research UK Imperial Centre). The system is learning how to identify potential issues within these images, and how to recommend the right course of action to a clinician.  As the algorithm processes more images, it refines its understanding and interpretation of the information. It then provides increasingly useful feedback, and segmentation, of the data for the clinicians to use for better diagnoses and treatment. All research projects go through rigorous regulatory and Trust approvals and are conducted only on non-identifiable patient data. You can read more about the healthcare research permissions process on our data and security page. For more details about our research collaborations with Moorfields and UCLH, and the Cancer Research UK Imperial Centre, please see the links below.
Follow
© 2018 DeepMind Technologies Limited DeepMind.com uses cookies to help give you the best possible user experience. Find Out More."
14eb8b3436,"
            Brain Imaging Technology Uses Machine Learning to Identify Suicidal Thoughts - Seeker        ","Researchers were able to identify subjects with suicidal ideation with 90 percent accuracy using a combination of brain imaging and AI algorithms.
A team of US medical researchers has developed a new and decidedly high-tech system for potentially preventing suicide. Using advanced brain-imaging technology and machine learning — a kind of subset of artificial intelligence — the system can flag patients with suicidal thoughts by essentially reading their minds.
It might sound like science fiction, but it's a technically accurate description of the new technique. The system detects and analyzes brain activity when the subject is asked to consider specific keywords and concepts related to suicide, such as “death” or “cruelty.”
When the brain activity data are processed by the system, electrical activity shows up on a map of the brain, with more intense feelings and thoughts generating specific color patterns in particular areas.
That's where the machine learning comes in. Using specifically coded algorithms, the AI system can detect significant pulses and patterns associated with suicidal thoughts. In a series of experiments using the technique, the system was able to accurately identify suicidal individuals with upwards of 90 percent accuracy.
“This type of analysis can assess a number of different component of the neural representation of a concept,” Marcel Just, professor of psychology at Carnegie Mellon University, told Seeker.
“One of the components that is measurable is the presence of various emotions,” Just said. “A classifier was able to distinguish between suicidal ideators and controls based on the how much of each emotion there was in the neural representation.”
The research, funded in part by the National Institutes of Mental Health, was published Oct. 30 in the journal Nature Human Behavior.
RELATED: AI Interviewers Entice Reluctant Soldiers to Report PTSD Symptoms
Researchers led by Just and the University of Pittsburgh's David Brent designed the experiments by preparing lists of keywords —10 related to death, 10 related to positive concepts, and 10 related to negative ideas.
The team presented the keywords to two groups of 17 people each. The first group consisted of patients with known suicidal tendencies. The second group – the control group – was made up of “neurotypical” persons, randomly selected people with no history of mental illness or suicidal behavior.
Both groups were outfitted with advanced brain scanning systems as they considered the list of keywords in various combinations. By analyzing their brain activity during this period, the system was able to distinguish between the suicidal group and the control group with 91 percent accuracy.
In a second set of experiments, the researchers used a similar approach to see if the machine learning system could distinguish between patients who had made a previous suicide attempt and those who had just thought about it. The program was able to identify those who had previously tried to take their own lives with 94 percent accuracy.
The new technique could have practical value for front-line clinicians who might be worried about potentially suicidal patients.
“We hope that information about what has been altered in a neural representation would be useful to a therapist or a therapy designer,” Just said. “Furthermore, one could subsequently assess the effectiveness of the therapy in terms of whether the alteration has been eliminated or reduced.”
RELATED: Brain-Computer Interface Allows Users to Compose Music With Only Their Thoughts
Just and other Carnegie Mellon researchers first developed the imaging system from brain activation signatures. The research has since been extended to identify emotions and multi-concept thoughts from their neural signatures.
For now, the technique requires specially calibrated brain scanning technology in the laboratory, Just said. But he hopes to change that.
“We are working on another project to determine whether suicidal ideation can be identified using [an electroencephalogram], which is a much cheaper and more widely available technology.”
WATCH: Which Countries Have the Highest Suicide Rates?"
e4aa71a007,This interactive map uses machine learning to arrange visually similar fonts - The Verge,"Typography enthusiasts likely already know how to identify fonts by name, but it’s always useful to explore visually similar fonts when you feel like changing up your options. Design consultant firm IDEO’s Font Map helps you do exactly that, with an interactive tool that lets you browse through fonts by clicking on them and seeing ones nearby that look similar, or by specifically searching for fonts by name. IDEO software designer Kevin Ho built the map using a machine learning algorithm that can sort fonts by visual characteristics, like weight, serif or san-serif, and cursive or non-cursive. “Designers need an easier way to discover alternative fonts with the same aesthetic — so I decided to see if a machine learning algorithm could sort fonts by visual characteristics, and enabling designers to explore type in a new way,” he wrote in a blog post. The resulting map organized more than 750 fonts that are available for free through Google Fonts, should you want to download them.
Even if you’re not a designer, the map is pretty fun to get lost in. Services that compare and suggest visually similar fonts already exist, like Identifont and the blog Typewolf, but IDEO’s tool makes it easy to quickly browse and at the very least, appreciate all the options out there that help make the web more beautiful.
Command Line delivers daily updates from the near-future."
9405689a01,Bringing machine learning to last mile health challenges | Devex ,"Connect SAN FRANCISCO — A new microscope will use image recognition software and machine learning technology to identify and count malaria parasites in a blood smear. The EasyScan GO, announced at MEDICA, the medical industry’s leading trade fair, is the result of a partnership between the Global Good Fund, a Seattle-based group funded by philanthropist Bill Gates, and Motic, a China-based company that specializes in manufacturing microscopes. Field tests have demonstrated that the machine learning algorithm is as reliable as an expert microscopist in fighting the spread of drug resistant malaria. EasyScan GO is the latest example of Global Good’s partnering to bring emerging technologies to health systems in low resource settings. Based at the invention company Intellectual Ventures, Global Good is focused on developing and deploying technologies for the poorest parts of the world. It partners with others to bring the benefits of technological innovation to the hardest markets in the world to reach.
The Global Good motto is “invention saving lives.” But invention means nothing if these products are developed without any consideration of the applicability of that technology in the markets they are meant to serve, said Maurizio Vecchione, senior vice president at Global Good. He emphasized that his team cannot fulfill its mission unless that technology is affordable, appropriate, accessible, and those 3 A’s, as he calls them, are at the center of every Global Good partnership.
Proponents see enormous potential for this technology. Today, machine learning can help tell us we are sick; emerging technologies like artificial intelligence will allow us to prevent illness, according to AnthroTronix founder Corinna Lathan, who wrote a World Economic Forum blog post on ways artificial intelligence and robotics will transform health care.
Several recent partnerships point to how Global Good is working to make sure the benefits of AI, machine learning, and deep learning extend to developing countries.
One in four children in the world are not registered at birth. Because they lack formal identification, it is very difficult to track their health records, including vaccinations.
Earlier this month, start-up Element Inc. announced a partnership with Global Good to develop a smartphone based platform to verify the identity of infants and children. Element Inc. focuses on developing mobile software for biometric recognition. Their aim is to deliver identity for everyone, and particularly the 1 billion people who lack proper IDs in Asia and Africa.
In Bangladesh and Cambodia, the BioNIC partnership will extend Element’s adult identity platform to children under age five, testing on fingerprints, ears and feet, irises, and more offline and on any mobile device.
“Accurately identifying patients, and linking them to their electronic medical records — which store past medical conditions, drug allergy information, medications and vaccination history — is critical to health care,” David Bell, director of the global health technologies portfolio at Global Good, said in a press release. “While adult biometrics are widely used in financial services, these have not been translated to the health sector where the need to identify a person from birth to old age presents additional challenges; namely, that infants have delicate, rapidly changing features that are difficult to capture.” This technology has only become possible in recent years, Element co-founder and CEO Adam Perold told Devex via email.
“Mobile devices now proliferate the world, at nearly three billion units; and each year, they get better and less expensive. At the same time, deep learning — through improved algorithms and increasingly powerful computers — was driving breakthroughs in how systems process video and images. We were one of the first to apply deep learning methods on mobile devices, and as a result, we have built a platform that traverses many challenges that still prevent widespread adoption of biometrics across low-resource settings,” he said.
When problems can be tackled by deep learning, the barriers in delivery are often structural, Perold continued, explaining that it is a resource heavy approach requiring lots of data, computing power, and specialized training.
“Deep learning solves key issues of access in biometric recognition. The strengths of the approach, where algorithms train themselves directly from the data, means that anyone can be recognized regardless of their features,” he said, adding that the technology “can be deployed to continuously adapt to any changes as a person develops.” The BioNIC partnership has the ultimate goal of creating a biometric solution capable of following patients from birth to old age, said Perold. Malaria kills almost half a million people every year, and one of the major challenges to eradication is the rapid spread of a multidrug-resistant strain in parts of Southeast Asia.
Currently, only analysis by a World Health Organization certified microscopist can accurately detect severe and drug-resistant cases. EasyScan GO automates that process with an intelligent microscope, simplifying and standardizing malaria detection in under-resourced countries where there is often a lack of trained personnel. “Malaria is one of the hardest diseases to identify on a microscope slide,” Bell of Global Good said in a press release. “By putting machine learning-enabled microscopes in the hands of laboratory technicians, we can overcome two major barriers to combating the mutating parasite — improving diagnosis in case management and standardizing detection across geographies and time.” The EasyScan GO uses AI-enabled software to automatically and accurately identify and count malaria parasites in a blood smear in as little as 20 minutes, Sebastian Nunnendorf, head of research and development at Motic China, said in an email to Devex. “Human capacity issues have long been a major bottleneck in low-resource settings, and the fact that we are able to address these issues through technological innovation is truly exciting,” he said. Any new medical device will need time in the market to validate, Nunnendorf said, and like the BioNIC partnership, EasyScan GO is still early in its journey. But the company hopes to go beyond diagnosing malaria and a few other parasites commonly found on a blood film, including Chagas disease, microfilaria, and sickle cell. Success with the most difficult-to-identify disease, malaria, will pave the way for EasyScan GO to excel at almost any microscopy task, Nunnendorf said. “AI and digital health are increasingly becoming the most exciting trend in healthcare, and with our product leadership and Global Good’s commitment to inventing for humanitarian impact, our innovations will be truly available everywhere,” he said.
Machine learning is the ultimate way to go faster, said Peter Norvig, director of research at Google. But speed can also lead to accidents, he told the Train AI conference in San Francisco.
“In every industry, there’s a place where AI can make things better,” Norvig told Devex. “Look at all of the AI technologies, and all problems, and it’s just a question of fitting them together and figuring out, ‘What’s the right technological match and what’s the right policy match?’”
The most obvious cases for AI use include anything a typical human can do in less than a second of thought, Andrew Ng, one of the leading thinkers on artificial intelligence, told Devex at an event at Stanford University on the role of AI in achieving the SDGs. Image recognition is one common example, but AI also has powerful applications in education and health care in both the developed and developing world, said Ng. He finds it hard to name a major industry he doesn’t think AI can transform in the next several years.
""I actually find it hard to name a major industry that I don't think AI can transform in the next several years."" - @AndrewYNg of @coursera pic.twitter.com/I9wxmr2unm “Deep learning has driven unprecedented breakthroughs across a variety of domains, spanning image processing, to drug discovery and speech recognition – there are dozens of successful use cases, Perold of Element told Devex. “Ultimately, deep learning is a tool that must be applied thoughtfully,”
Technology should bridge gaps, not drive inequalities between those at the top and those at the bottom, said Perold. “We believe that one of the best ways to do this is to meet people where they are – on the lowest common denominator of computing, the mobile device. Solving problems requires not just advanced technology, but a lot of thoughtfulness of context and application. It’s the responsibility of anyone delivering solutions to get to know the people, cultures, and delivery needs directly,” he said. Read more Devex coverage on global health."
cfdc91d3b0,Self-driving bus involved in crash less than two hours after Las Vegas launch | Technology | The Guardian,"A truck driver is blamed for the accident, which passengers say could have been avoided if the autonomous vehicle had only reversed A truck driver is blamed for the accident, which passengers say could have been avoided if the autonomous vehicle had only reversed
Samuel Gibbs
Thu 9 Nov ‘17 10.52 GMT
Last modified on Mon 27 Nov ‘17 14.04 GMT
It took less than two hours for Las Vegas’s brand new self-driving shuttle to end up in a crash on Wednesday – thanks to a human. The autonomous bus made its debut on public roads around the so called Innovation District in downtown Las Vegas in front of cameras and celebrities, dubbed America’s first self-driving shuttle pilot project geared toward the public. But within two hours it had already been involved in a minor crash with a lorry. No injuries were reported. Jenny Wong, a passenger on the shuttle at the time of the crash, told local news station KSNV: “The shuttle just stayed still. And we were like, it’s going to hit us, it’s going to hit us. And then it hit us. “The shuttle didn’t have the ability to move back. The shuttle just stayed still.” Las Vegas police officer Aden Ocampo-Gomez said the truck’s driver was at fault for the crash and was cited for illegal backing. “The shuttle did what it was supposed to do, in that its sensors registered the truck and the shuttle stopped to avoid the accident,” the city said in a statement. “Unfortunately the delivery truck did not stop and grazed the front fender of the shuttle. Had the truck had the same sensing equipment that the shuttle has, the accident would have been avoided.” The oval-shaped shuttle can seat up to eight people and has an attendant and computer monitor, but no steering wheel or brake pedals. Developed by French company Navya, it uses GPS, electronic kerb sensors and other technology to find its way at no more than 15mph. Before it crashed, dozens of people had lined up to get a free trip on a 0.6-mile loop around Fremont East, Las Vegas, including Nascar driver Danica Patrick and magic duo Penn and Teller. City spokesman Jace Radke said the shuttle took two more loops after the crash. The year-long pilot project, sponsored by AAA Northern California, Nevada and Utah, is expected to carry 250,000 people. The AAA said human error was responsible for more than 90% of the 30,000 deaths on US roads in 2016, and that robotic cars could help reduce the number of incidents. Google sibling Waymo announced on Tuesday that it is launching a fully autonomous Uber-like ride-hailing service with no human driver behind the wheel in Phoenix, Arizona in the next few months, making it the first such service accessible to the public with no one to take control in an emergency."
b727ad8b54,"Researchers can fool machine-learning vision systems with a single, well-placed pixel / Boing Boing","Three researchers from Kyushu University have published a paper describing a means of reliably fooling AI-based image classifiers with a single well-placed pixel.
It's part of a wider field of ""adversarial preturbation"" to disrupt machine-learning models; it's a field that started with some modest achievements, but has been gaining ground ever since.
But the Kyushu paper goes further than any of the research I've seen so far. The researchers use 1, 3 or 5 well-placed pixels to fool a majority of machine-classification of images, without having any access to the training data used to produce the model (a ""black box"" attack).
It's a good example of how impressive gains in ""non-adversarial"" computing (something works if no one is trying to stop it) are actually extremely fragile and almost trivial to circumvent once they exist.
According to the experimental results, the main contributions
of our work include:
• The effectiveness of conducting non-target attack
using few-pixel attack. We show that with only 1
pixel modification, there are 73.8% of the images can
be perturbed to one or more target classes, 82.0% and
87.3% in the cases of 3 and 5-pixel attacks. We show
the non-sensitive images are even much rarer than sensitive
images even if limiting the perturbation to such a
small scope, therefore few-pixel modification is an effective
method of searching adversarial images while
can be hardly recognized by human eyes in practice.
• The number of target classes that a natural image
can camouflage. In the case of 1 pixel perturbation
, each natural image can be perturbed to 2.3 other
classes on average. In specific, there are 18.4% , 17.2%
and 16.6% of the images can be perturbed to 1, 2, 3
target classes. In the case of 5-pixel perturbation, the
amounts of images that can be perturbed to from 1 to
9 target classes become almost even.
• Similar perturbation direction to a specific target
class. Effectiveness of universal perturbation has
shown that many images can be perturbed through similar
directions such that decision boundaries might leak
diversity [24] while our results show that data-points
belonging to same classes can be always more easily
perturbed to specific classes with same amount of perturbations
(i.e. 1, 3 or 5 pixel-modifications).
• Geometrical understanding of data-point distribution
in high dimensional input space. Geometrically,
the information obtained by conducting few-pixel attack
can be also regarded as a quantitative result on
changes in class labels on the cross sections obtained
by using simply low dimensional slices to cut the input
space. In particular, our results indicate that some
decision domains might have very great depths towards
many different directions but inside these deep
areas, the decision domains are quite narrow. In other
words, these domains can have many long and thin extended
synapses towards different directions in the input
space.
One pixel attack for fooling deep neural networks [Jiawei Su, Danilo Vasconcellos Vargas and Sakurai Kouichi/Arxiv]
(via 4 Short Links)
A small cohort of 31 healthy young men who took 600mg of ibuprofen twice a day for six weeks developed ""compensated hypogonadism"" (little balls), because the ibuprofen interfered with their testosterone production and their gonads had to work overtime to compensate.		 Nudging -- the idea that a well-designed ""choice architecture"" can help people make free choices that are better than the ones they would make without the nudge -- has a few well-publicized success stories: the cafeteria where frontloading veggies and other healthful options gets kids to choose carrots over pizza; and the employer-side deduction for […]		 Salvia divinorum is a plant that is legal in most of the USA and the world, a uniquely powerful psychedelic whose effects are as short-lived (5-10 minutes from first onset to the end of the experience) as they are profound (users generally need to have a ""sitter"" nearby because they lose control over their bodies […]
Read the rules you agree to by using this website in our Terms of Service. We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites. Who will be eaten first? Our forum rules are detailed in the Community Guidelines. Boing Boing is published under a Creative Commons license except where otherwise noted.
Mark Frauenfelder
Cory Doctorow
David Pescovitz
Xeni Jardin
Rob Beschizza Jason WeisbergerPublisher Ken SniderSysadmin
About Us
Contact Us
Advertise
Forums
Shop
Shop Support"
80c3dd7a4b,How PayPal Boosts Security with Artificial Intelligence - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter To PayPal, the transactions signal fraud: a U.S. user’s account is accessed in the U.K., China, and elsewhere around the world. But PayPal’s security system—thanks to a growing reliance on an artificial-­intelligence technology known as deep learning—is now able to spot possible fraud without making mistakes. That’s because algorithms mine data from the customer’s purchasing history—in addition to reviewing patterns of likely fraud stored in its databases—and can tell whether, for example, the suspect transactions were innocent actions of a globe-hopping pilot. From a cybersecurity perspective, ­PayPal has a target on its back: it processed $235 billion in payments last year from four billion transactions by its more than 170 million customers. Fraud is always possible via theft of consumer data in breaches such as “phishing” e-mails that con users into entering their credentials. To keep ahead, PayPal relies on intensive, real-time analysis of transactions. When a pattern is revealed—for example, if sudden strings of many small purchases at convenience stores turn out to be fraud—it’s turned into a “feature,” or a rule that can be applied in real time to stop purchases that fit this profile. “We now process thousands of ‘features’ in our system, compared to hundreds when the system was first put to use in 2013,” says Hui Wang, the company’s senior director of global risk sciences. As a result, PayPal can now do things like tell the difference between friends buying concert tickets together and a thief making similar purchases with a list of stolen accounts. And it’s all done in-house to avoid even the tiny latency that would occur if the company relied on a cloud provider. “Thousands of ‘features’ searching through 16 years of users’ history all needs to be done in less than a second,” Wang says. Deep learning and other artificial-intelligence approaches are quickly becoming the only way to keep up with threats, she adds. They’ve worked to help keep PayPal’s fraud rate remarkably low, at 0.32 percent of revenue—a figure far better than the 1.32 percent average that merchants see, according to a study by LexisNexis. The most recent Federal Reserve Payments Study found that $6.1 billion in fraudulent purchases were made in 2012, and the problem appears to be getting worse. PayPal isn’t the only company using deep learning to improve cybersecurity. The Israeli startup Deep Instinct has applied the technique to spotting malware, claiming that this works 20 percent better than traditional approaches. And Ashar Aziz, vice chairman and founder of the security firm FireEye, said that his company has been using deep learning for everything from detecting network intrusions to rooting out phishing attacks. Companies can further improve cybersecurity if they share data repositories on cyberattacks and fraud, says Aziz. “If you continue to get more data—and more power to process it—then you can get even better,” he says. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on AI.
PayPal,
AI,
deep learning,
Business Report,
Business Impact,
Cyber Survival,
EmTech Digital 2016
Image courtesy of PayPal; chart source: LexisNexis True Cost of Fraud report Michael Morisy
Please read our commenting guidelines.
More videos
Business Impact
Business Impact
Business Impact
Business Impact
How technology advances are changing the economy and providing new opportunities in many industries.
We haven’t stopped huge breaches. The focus now is on resilience, with smarter ways to detect attacks and faster ways to respond to them. by
David Talbot
An expert in U.S. national cybersecurity research and policy says the next generation of technology must have security built in from the very start. by
David Talbot
Investors have been pouring money into companies selling “next-generation” security products. by
Mike Orcutt
Amid a wave of corporate privacy and security pronouncements, 2014 was supposed to be the “year of encryption.” It didn’t pan out that way. by
David O’Brien
Limiting damage from attacks requires far faster reactions, quick notification of victims, and adherence to regulations. Managing all that can be tricky. by
David Talbot
More from Business Impact
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
{! insider.display.menuOptionsLabel !}
Unlimited online access including articles and video, plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
ad0013580f,Artificial intelligence produces realistic sounds that fool humans  | MIT News,"Login
or
Subscribe Newsletter
Image courtesy of the researchers. Image courtesy of the researchers. Video-trained system from MIT’s Computer Science and Artificial Intelligence Lab could help robots understand how objects interact with the world.
Watch Video
Adam Conner-Simons | CSAIL
June 13, 2016
Adam
Conner-SimonsEmail: aconner@csail.mit.eduPhone: 617-324-9135MIT Computer Science & Artificial Intelligence Lab For robots to navigate the world, they need to be able to make reasonable assumptions about their surroundings and what might happen during a sequence of events. One way that humans come to learn these things is through sound. For infants, poking and prodding objects is not just fun; some studies suggest that it’s actually how they develop an intuitive theory of physics. Could it be that we can get machines to learn the same way? Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) have demonstrated an algorithm that has effectively learned how to predict sound: When shown a silent video clip of an object being hit, the algorithm can produce a sound for the hit that is realistic enough to fool human viewers. This “Turing Test for sound” represents much more than just a clever computer trick: Researchers envision future versions of similar algorithms being used to automatically produce sound effects for movies and TV shows, as well as to help robots better understand objects’ properties. Researchers at MIT have demonstrated an algorithm that has effectively learned how to predict sound. The advance could lead to better sound effects for film and television and robots with improved understanding of objects in their surroundings. Video: MIT CSAIL “When you run your finger across a wine glass, the sound it makes reflects how much liquid is in it,” says CSAIL PhD student Andrew Owens, who was lead author on an upcoming paper describing the work. “An algorithm that simulates such sounds can reveal key information about objects’ shapes and material types, as well as the force and motion of their interactions with the world.” The team used techniques from the field of “deep learning,” which involves teaching computers to sift through huge amounts of data to find patterns on their own. Deep learning approaches are especially useful because they free computer scientists from having to hand-design algorithms and supervise their progress. The paper’s co-authors include recent PhD graduate Phillip Isola and MIT professors Edward Adelson, Bill Freeman, Josh McDermott, and Antonio Torralba. The paper will be presented later this month at the annual conference on Computer Vision and Pattern Recognition (CVPR) in Las Vegas. How it works The first step to training a sound-producing algorithm is to give it sounds to study. Over several months, the researchers recorded roughly 1,000 videos of an estimated 46,000 sounds that represent various objects being hit, scraped, and prodded with a drumstick. (They used a drumstick because it provided a consistent way to produce a sound.) Next, the team fed those videos to a deep-learning algorithm that deconstructed the sounds and analyzed their pitch, loudness and other features. “To then predict the sound of a new video, the algorithm looks at the sound properties of each frame of that video, and matches them to the most similar sounds in the database,” says Owens. “Once the system has those bits of audio, it stitches them together to create one coherent sound.” The result is that the algorithm can accurately simulate the subtleties of different hits, from the staccato taps of a rock to the longer waveforms of rustling ivy. Pitch is no problem either, as it can synthesize hit-sounds ranging from the low-pitched “thuds” of a  soft couch to the high-pitched “clicks” of a hard wood railing. “Current approaches in AI only focus on one of the five sense modalities, with vision researchers using images, speech researchers using audio, and so on,” says Abhinav Gupta, an assistant professor of robotics at Carnegie Mellon University who was not involved in the study. “This paper is a step in the right direction to mimic learning the way humans do, by integrating sound and sight.” An additional benefit of the work is that the team’s library of 46,000 sounds is free and available for other researchers to use. The name of the dataset: “Greatest Hits.” Fooling humans To test how realistic the fake sounds were, the team conducted an online study in which subjects saw two videos of collisions — one with the actual recorded sound, and one with the algorithm’s — and were asked which one was real. The result: Subjects picked the fake sound over the real one twice as often as a baseline algorithm. They were particularly fooled by materials like leaves and dirt that tend to have less “clean” sounds than, say, wood or metal.   On top of that, the team found that the materials’ sounds revealed key aspects of their physical properties: An algorithm they developed could tell the difference between hard and soft materials 67 percent of the time. The team’s work aligns with recent CSAIL research on audio and video amplification. Freeman has helped develop algorithms that amplify movements captured by video that are invisible to the naked eye, which has allowed his groups to do things like make the human pulse visible and even recover speech using nothing more than video of a potato chip bag. Looking ahead Researchers say that there’s still room to improve the system. For example, if the drumstick moves especially erratically in a video, the algorithm is more likely to miss or hallucinate a false hit. It is also limited by the fact that it applies only to “visually indicated sounds” — sounds that are directly caused by the physical interaction that is being depicted in the video. “From the gentle blowing of the wind to the buzzing of laptops, at any given moment there are so many ambient sounds that aren’t related to what we’re actually looking at,” says Owens. “What would be really exciting is to somehow simulate sound that is less directly associated to the visuals.” The team believe that future work in this area could improve robots’ abilities to interact with their surroundings. “A robot could look at a sidewalk and instinctively know that the cement is hard and the grass is soft, and therefore know what would happen if they stepped on either of them,” says Owens. “Being able to predict sound is an important first step toward being able to predict the consequences of physical interactions with the world.” The work was funded, in part, by the National Science Foundation and Shell. Owens was also supported by a Microsoft Research Fellowship. Topics: Computer Science and Artificial Intelligence Laboratory (CSAIL), Machine learning, Computer vision, Networks, Research, Algorithms, Big data, Artificial intelligence, Computer science and technology, School of Engineering, Brain and cognitive sciences, Electrical Engineering & Computer Science (eecs), School of Science CSAIL researchers recently presented an algorithm that teaches computers to predict sounds, writes Kevin Hartnett for The Boston Globe. The ability to predict sounds will help robots successfully navigate the world and “make sense of what’s in front of them and figure out how to proceed,” writes Hartnett. In an article for Wired, Tim Moynihan writes that a team of CSAIL researchers has created a machine-learning system that can produce sound effects for silent videos. The researchers hope that the system could be used to “help robots identify the materials and physical properties of an object by analyzing the sounds it makes.” Writing for the Financial Times, Clive Cookson reports that MIT researchers have developed an artificial intelligence system capable of producing realistic sounds for silent movies. Cookson explains that another application for the system could be “to help robots understand objects’ physical properties and interact better with their surroundings.""  Washington Post reporter Matt McFarland writes that MIT researchers have created an algorithm that can produce realistic sounds. “The findings are an example of the power of deep learning,” explains McFarland. “With deep learning, a computer system learns to recognize patterns in huge piles of data and applies what it learns in useful ways.” Popular Science reporter Mary Beth Griggs writes that MIT researchers have developed an algorithm that can learn how to predict sound. The algorithm “can watch a silent movie and create sounds that go along with the motions on screen. It's so good, it even fooled people into thinking they were actual, recorded sounds from the environment.” This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
c691c234ca,How Pinterest Uses Machine Learning To Keep Its Users Pinned,"New workplaces, new food sources, new medicine--even an entirely new economic system. The major tech ecosystems that battle for our attention and dollars. What’s next for hardware, software, and services. The brave new world of automation, from AI to drones. How our urban centers are building toward the future. See members of our Most Creative People in Business community: leaders who are shaping the future of business in creative ways. An award-winning team of journalists, designers, and videographers who tell brand stories through Fast Company's distinctive lens. Thanks to recent gains in machine learning, computers are getting skilled at picking out patterns and features in text and images. That’s how e-commerce giants like Amazon and eBay build sophisticated recommendation systems and how social networks like Facebook and Twitter are tweaking feeds to keep users hooked. Pinterest is no exception, with 30% of engagement tied to personalized real-time suggestions. Here’s how Pinterest engineers are leveraging artificial intelligence to keep the website’s 150 million–plus users pinning and sharing. Machine learning can not only determine the subject of an image, it can also identify visual patterns and match them to other photos. Pinterest is using this technology to process 150 million image searches per month, helping users find content that looks like pictures they’ve already pinned. Pin a photo of a cheetah-print pillow, and Pinterest will serve up animal-print decor from other users. Future iterations of the Pinterest app may let users simply point their cameras at real-world objects to get instant recommendations. If a user pins a mid-century dining-room table, the platform can now offer suggestions of other objects from the same era. The key? Metadata, such as the names of pinboards and websites where images have been posted, helps the platform understand what photos represent. While many platforms prioritize content from a user’s friends and contacts, Pinterest pays more attention to an individual’s tastes and habits—what they’ve pinned and when—enabling the site to surface more personalized recommendations. After all, friends who like the same recipes may not agree at all on fashion. Pinterest is an increasingly global platform, with more than half of its users based outside the U.S. Its recommendation engine has learned to suggest popular content from users’ local region in their native language. One finding: Slow-cooker recipes are more popular in the U.S. than the U.K., where the appliances aren’t as common. Analyzing what’s in a photo is a big factor in the site’s recommendations, but it doesn’t offer the whole story. Pinterest also looks at captions from previously pinned content and which items get pinned to the same virtual boards. That allows Pinterest to, say, link a particular dress to the pair of shoes frequently pinned alongside it, even if they look nothing alike. Steven Melendez is an independent journalist living in New Orleans.
More Fast Company Daily Newsletter"
cf6e551fb8,Information Engineering Main/Home Page,"Established by Professor Sir Michael Brady in 1985, the Robotics Research Group brought together a group of like-minded engineers working in robotics research and artificial intelligence. Now known as Information Engineering, it is currently composed of seven research groups whose interests range from machine learning to mobile robotics.
Information Engineering is located in the Engineering Science triangle, principally in the purpose-built Information Engineering Building (opened in 2004).
Other Information
Contact Us
Page last modified on October 17, 2017, at 03:23 PM
Skittlish theme adapted by David Gilbert, powered by PmWiki"
e513af13e6,An AI Ophthalmologist Shows How Machine Learning May Transform Medicine - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Google researchers got an eye-scanning algorithm to figure out on its own how to detect a common form of blindness, showing the potential for artificial intelligence to transform medicine remarkably soon. The algorithm can look at retinal images and detect diabetic retinopathy—which affects almost a third of diabetes patients—as well as a highly trained ophthalmologist can. It makes use of the same machine-learning technique that Google uses to label millions of Web images. Diabetic retinopathy is caused by damage to blood vessels in the eye and results in a gradual deterioration of vision. If caught early it can be treated, but a sufferer may experience no symptoms early on, making screening vital. It is diagnosed, in part, by having an expert examine images of a patient’s retina, captured with a specialized device, for signs of bleeding and fluid leakage. Some form of automated detection could make the diagnosis more efficient and reliable, and could be especially useful in regions where the required expertise is scarce. “One of the most intriguing things about this machine-learning approach is that it has potential to improve the objectivity and ultimately the accuracy and quality of medical care,” says Michael Chiang, a professor of ophthalmology and a clinician at Oregon Health & Science University’s Casey Eye Institute. AI has had mixed success in medicine in the past. Systems that use a database of knowledge to offer advice have been shown to outperform doctors in some settings, but there has been limited uptake. Still, the power of machine learning—especially a technique known as deep learning, may make AI more common in the future (see “10 Breakthrough Technologies 2013: Deep Learning”). A team at Google DeepMind, a subsidiary of Alphabet focused entirely on AI, is doing similar work, training computers to process optical coherence tomography scans for signs of macular degeneration and other eye disease in collaboration with researchers at Moorfields Eye Hospital in London (see “DeepMind’s First Medical Research Gig Will Use AI to Diagnose Eye Disease”). This retinal-image research, published Tuesday, marked the first time a paper about deep learning has appeared in the Journal of the American Medical Association, according to the journal’s editor-in-chief, Howard Bauchner. The paper’s authors, comprised of computer scientists at Google and medical researchers from the U.S. and India, developed an algorithm to analyze retinal images. But unlike existing ophthalmology software, it was not explicitly programmed to recognize features in images that might indicate the disease. It simply looked at thousands of healthy and diseased eyes, and figured out for itself how to spot the condition. The researchers created a training set of 128,000 retinal images classified by at least three ophthalmologists. After the algorithm had been trained, the researchers tested its performance on 12,000 images and found that it matched or exceeded the performance of experts in identifying the condition and grading its severity. The Google researchers collaborated with scientists at the Aravind Medical Research Foundation in India, where a clinical trial involving real patients is ongoing. This project involves patients receiving a normal consultation, but their images are also fed into the deep-learning system for comparison. Lily Peng, a researcher at Google and a medical doctor who was involved with the project, says results from this trial are not yet ready for publication. Deep learning could be applied in many different areas of medicine that rely on image analysis, such as radiology and cardiology. But one of the biggest challenges will be to provide convincing evidence that the systems are reliable. Brendan Frey, a professor at the University of Toronto and the CEO and cofounder of a company called Deep Genomics, warns that researchers will need to develop machine-learning systems that are capable of explaining how they reached a particular conclusion (see “AI’s Language Problem”). Peng, of Google, says this is something her team is already working on. “We understand that explaining will be very important,” she says. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on machine learning.
Google,
machine learning,
deep learning,
EmTech Digital 2017,
MIT Technology Review Events,
diabetic retinopathy
Will Knight Senior Editor, AI I am the senior editor for AI at MIT Technology Review. I mainly cover machine intelligence, robots, and automation, but I’m interested in most aspects of computing. I grew up in south London, and I wrote my first line of code (a spell-binding… More infinite loop) on a mighty Sinclair ZX Spectrum. Before joining this publication, I worked as the online editor at New Scientist magazine. If you’d like to get in touch, please send an e-mail to will.knight@technologyreview.com. More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Six issues of our award winning print magazine, unlimited online access plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year)
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
49115089c7,Smart Reply: Google's machine learning can now reply to your emails | WIRED UK,"Welcome to WIRED UK. This site uses cookies. To find out more, read our privacy policy.
By
James Temperton
Machine learning can now reply to your emails. A new feature in Google's Inbox app can recognise the content of emails and tailor responses using natural language, without a human being having to do a thing. Machine learning is used to scan emails and understand if they need replying to or not, before creating three response options. An email asking about vacation plans, for example, could be replied to with ""No plans yet"", ""I just sent them to you"" or ""I'm working on them"". The feature, dubbed Smart Reply, is only available in Google's Inbox app for Android and iOS. It has been designed for emails that can be answered with a short reply such as ""I'll send it to you"" or 'I don't, sorry'.
By
Rowland Manthorpe
Google said the system would enable users to reply to emails in just two taps -- one tap to open it, one tap to select a response and send. The responses a user chooses, or doesn't choose, will also improve future suggestions. According to Google software engineer Bálint Miklós, who first came up with the idea, the system used to suggest ""I love you"" as a suitable response to workplace emails. This was as a result of what Smart Reply had learned from the emails it had scanned. As ""Thanks"", ""Sounds good"" and ""I love you"" were common responses, it gambled on them when it was unsure of what to say. The adorable anomaly has since been removed. Smart Reply is built on a pair of recurrent neural networks, one that encodes incoming emails and one that comes up with possible responses. Each word is captured in turn to create a list of numbers, known as a thought vector, that gives the machine learning system the gist of what is being said. From this, the second network builds a grammatically correct response one word at a time. ""Amazingly, the detailed operation of each network is entirely learned, just by training the model to predict likely responses,"" explained Greg Corrado, a senior research scientist at Google. Not only does Google's machine learning system have to understand the complex and varied way people communicate, it also has to digest emails that can be hundreds of words long. To do this, Smart Reply focuses only on the most important sentences and ignores everything else. Google used a variant of a ""long short-term memory"" network to help the machine learning system hone in on what parts of incoming emails would be most useful for predicting a response. Another challenge for Google's engineers was working blind. For privacy reasons, and to make a machine learning email system truly useful, no human being has access to the data. That means everything is worked out by a computer. ""This means researchers have to get machine learning to work on a data set that they themselves cannot read, which is a little like trying to solve a puzzle while blindfolded,"" Corrado explained. Smart Reply will be introduced later this week as an update for Google Inbox for Android and iOS.
By
Matt Burgess
By
Emily Reynolds
By
WIRED"
bd57e384ad,How Machine Learning Fuels Your Netflix Addiction - RTInsights,"Every recommended video you see on Netflix was selected by an algorithm. The company estimates its algorithms produce $1 billion a year in value from customer retention. When intuition fails, data from machine learning can win, according to a recent paper describing Netflix’s  recommendations system. On a Netflix screen, a user is presented with about 40 rows of video categories, with each row containing up to 75 videos, according to the paper, which was published in the Dec. 2015 issue of ACM Transactions on Management Information Systems (TMIS). While one would think such a large selection would keep viewers browsing, too much choice can be counterproductive. Typically a viewer loses interest after perhaps 60 to 90 seconds of trying to find a video to watch, write authors Carlos A. Gomez-Uribe and Neil Hunt of Netflix. Like any digital business, or even a supermarket with rows of breakfast cereal, Netflix has little time to catch the customer’s attention. The training and evaluation of algorithms takes place offline, while A/B testing takes place online. Image adapted from ACM Historically, Netflix relied heavily on customer ratings of videos when shipping DVDs by mail. Now Netflix has access to a much broader set of data—what each member watches, when they watch, the place on the Netflix screen the customer found the video, recommendations the customer didn’t choose, and the popularity of videos in the catalog. All of this data gets fed into several algorithms powered by statistical and machine-learning techniques. Approaches use both supervised (classification, regression) and unsupervised (dimensionality reduction through clustering or compression) approaches, according to the authors. A video-to-video similarity algorithm, or Sims, makes recommendations in the “Because You Watched” row. A personalized video ranker algorithm, or PVR, selects the order of videos in genre rows, using an arbitrary subset of the Netflix catalog. As Gomez-Uribe told Wired, “The closer to the first position on a row a title is, the more likely it will get played.” But PVR works better when it’s mixed with “unpersonalized popularity,” he and Hunt wrote in their paper. As an example, the authors describe recommendations for shows similar to “House of Cards.” While one might think that political or business dramas such as “The West Wing” or “Mad Men” would increase customer engagement, it turns out that popular but outside-of-genre titles such as “Parks and Recreation” and “Orange Is the New Black” fared better. The authors call this a case of “intuition failure.” Another algorithm is the “Top N ranker,” which makes recommendations in the “Top Picks” row. A “Trending Now” row uses short-term trends, such as an interest in holiday movies or films driven by weather events. These short-term trends are “powerful predictors of videos that our members will watch, especially when combined with the right dose of personalization,” the authors write.
The authors write that over the years, the recommendation system has decreased customer churn by several percentage points and saves the company about $1 billion a year. Personalized content also helps “find an audience even for relatively niche videos that would not make sense for broadcast TV models because their audiences would be too small to support significant advertising revenue, or to occupy a broadcast or cable channel time slot,” according to the paper. “This is very evident in our data, which show that our recommender system spreads viewing across many more videos much more evenly than would an unpersonalized system.” The paper also describes how Netflix uses “evidence” algorithms, which focus on what information to show a viewer about a movie (such as whether it won an Oscar); search algorithms; how Netflix performs A/B testing on its algorithms, and opportunities for improvement in testing. A copy of the paper can be found here.   Chris Raphael (full bio)
covers fast data technologies and business use cases for real-time analytics. Follow him on Twitter at raphaelc44."
d005df457a,Amazon Web Services Getting Smarter At AI and Machine Learning | Fortune,"Amazon is improving the artificial intelligence and machine learning capabilities of its popular cloud services in order to better compete with rival services from Google and Microsoft. Amazon Web Services plans to announce the upgrades at its upcoming annual customer conference in Las Vegas, the tech news site The Information reported on Monday. Amazon has been working with several AI-oriented startups including DataRobot and Domino Data Lab, to improve its services, the web site said. Cloud computing is largely used to run standard business applications and store digital data for corporations and consumers. But with so much data already stored in cloud servers, now corporations are looking to conduct big data analysis and machine learning tasks like image recognition directly in the cloud as well. Amazon’s competitors have already pushed into similar territory. Google (googl) this year showed off its own custom microchip, dubbed the Tensor Processing Unit, that can be used by cloud customers that want to run machine learning programs or other AI-related tasks. Microsoft (msft) fired back in August, unveiling its Project Brainwave that relies on reprogrammable chips to speed AI applications. Get Data Sheet, Fortune’s technology newsletter. Fortune contacted Amazon for comment and will update this story if it replies. Amazon’s (amzn) new effort is code-named Ironman and is aimed at completing tasks for companies focused on insurance, energy, fraud detection, and drug discovery, The Information reported. The services will be offered to run on graphic processing chips made by Nvidia as well as so-called field programmable gate array chips, which can be reprogrammed as needed for different kinds of software. Nvidia, Advanced Micro Devices, and Intel, among others, have been battling to supply cloud operators like Amazon with the highest-performing chips for AI and machine learning apps. Nvidia (nvda) is far in the lead, but Intel (intc) said last week that it planned to start making its own high-end graphics chips for the AI market under the leadership of former top AMD (amd) executive Raja Koduri. Even before the AI upgrades, Amazon Web Services is among the fastest growing parts of the e-commerce giant’s businesses. The unit has brought in $12.3 billion in revenue in the first nine months of 2017, up 42% from the same period last year."
908eed2e7e,"CAPTCHAs may be a thing of the past, thanks to new machine learning research","CAPTCHA is an acronym for Completely Automated Public Turing test to tell Computers and Humans Apart. The term was coined in 2003, when the use of automated bots was becoming commonplace, and it refers to those annoying squiggly distorted letters that you have to type in when creating an online account. Although some companies have found ways around them, CAPTCHAs are still ubiquitous online. Researchers at the AI company Vicarious may have just made CAPTCHAs obsolete, however, by creating a machine-learning algorithm that mimics the human brain. To simulate the human capacity for what is often described as “common sense,” the scientists built a computer vision model dubbed the Recursive Cortical Network. “For common sense to be effective it needs to be amenable to answer a variety of hypotheticals — a faculty that we call imagination,” they noted in a post at their blog. The ability to decipher CAPTCHAs has become something of a benchmark for artificial intelligence research. The new Vicarious model, published in the journal Science, cracks the fundamentals of the CATCHPA code by parsing the text using techniques that are derived from human reasoning. We can easily recognize the letter A for example, even if it’s partly obscured or turned upside down. As Dileep George, the co-founder of the company explained to NPR, the RCN takes far less training and repetition to learn to recognize characters by building its own version of a neural network. “So if you expose it to As and Bs and different characters, it will build its own internal model of what those characters are supposed to look like,” he said. “So it would say, these are the contours of the letter, this is the interior of the letter, this is the background, etc.” These various features get put into groups, creating a hierarchal “tree” of related features. After several passes, the data is given a score for evaluation. CAPTCHAs can be identified with a high degree of accuracy. The RCN was able to crack the BotDetect system with 57 percent accuracy with far less training than conventional “deep learning” algorithms, which rely more on brute force and require tens of thousands of images before they can understand CAPTCHAs with any degree of accuracy. Solving CATCHPAs is not the goal of the research, but it provides insight into how our brains work and how computers can replicate it, NYU’s Brenden Lake told Axios. “It’s an application that not everybody needs,” he said. “Whereas object recognition is something that our minds do every second of every day.” “Biology has put a scaffolding in our brain that is suitable for working with this world. It makes the brain learn quickly,” George said. “So we copy those insights from nature and put it in our model. Similar things can be done in neural networks.”"
3b316ac3c9,DeepMind and Blizzard release new tools to train AI using Starcraft - The Verge,"Teaching computers to play games has always been a useful (if somewhat crude) measure of their intelligence. But as our machines have gotten smarter, we’ve had to find new challenges for them. First it was chess, then Atari, then the board game Go, and now they’re taking on their biggest challenge yet: Starcraft. To be precise, Starcraft II, which researchers at Google’s AI subsidiary DeepMind say is the perfect environment for teaching computers advanced skills like memory and planning. Last year, DeepMind said it was going to work with Starcraft creator Blizzard to turn the space-based strategy game into a proper research environment for AI engineers, and today, that software is being released to the public.
The toolkit from DeepMind and Blizzard bundles in various aids, including a large dataset of Starcraft II replays collected from professional matches (which AI can watch to learn human tactics); and a set of mini-games that isolate certain gameplay elements (like map exploration and resource collection) and can be used to hone particular skills. The most important bit of kit, though, is an API that lets AI agents play the game like a human would and feed back data to researchers. This means that the agents can be given the same constraints as humans (so they can’t see all of the map at once, or can’t click the mouse infinitely fast) while learning through trial and error — a process known as “reinforcement learning” in AI. But why is Starcraft such a good way to train artificial intelligence? It’s not because we want computers to learn military tactics, but because we need to teach them certain abstract skills, and video games happen to be a good way of doing so. Video games are virtual environments, which means games can quickly be repeated over and over; there’s lots of training data available, helpfully generated by humans playing the game; and Starcraft
itself has a number of gameplay mechanics that are particularly challenging for computers.
Oriol Vinyals, a researcher at DeepMind who’s working on the project (and who happens to be a former top-rank Starcraft player himself) explains that one of the interesting constraints the game offers is the “fog of war” mechanic, which covers up the map, and forces players to explore to find out what their enemy is up to. “So it might be critical for an AI agent to remember ‘Ah, I saw a unit over there before, but I don’t see it now, so I should go back and scout and see if they have a base near that location,’” Vinyals tells The Verge.
To a human, this is such an obvious idea that it’s barely worth thinking about, but it’s the sort of common sense insight that AI need to learn in order to be useful. In Starcraft, thinking about what a player can’t see is essential to winning — and it’s a challenge that doesn’t exist in games like chess or Go, where both players have complete knowledge of their environment at all times.
Vinyals says that this sort of memory skill can then be applied in all sorts of environments, and gives the example of a computer managing power in a data center to decrease electricity costs. “It might see that on a Sunday there’s a power spike for whatever reason, and it will have to remember this information next Sunday to account for it,” he says. “Memory plays a key role here, and teaching computers to infer what the state of the world might be is super interesting for us.”
As well as teaching AI certain skills, the newly released API sets the stage for a human vs. computer Starcraft showdown. Neither Blizzard nor DeepMind have said they plan to stage matches similar to those AlphaGo played against human champions, but Starcraft II’s finest players are certainly keen. Speaking to MIT Technology Review earlier this year, pro Starcraft player Byun Hyun Woo was fairly confident about his chances. “I don’t think AI can beat [a professional player], at least not in my lifetime,” he said.
The problem is that artificial intelligence has a way of surprising humans, as when DeepMind’s AlphaGo AI made moves that commentators thought nonsensical during its matches with Go master Lee Sedol (but that later turned out to be crucial to its success).
So will DeepMind’s AI surprise Starcraft players? Vinyals says it’s already happening, and gives the examples of an agent that was tasked with exploring a section of a map as quickly as possible using just two units. Usually, says Vinyals, a human player would select the units and use the “move” command to cover the ground as quickly as possible. “But it turns out that instead of using ‘move’ you can use another command called ‘patrol,’” he explains. Unlike ‘move,’ this forces the units to keep their distance from one another “and in that way they covered more of the map and collected resources faster.”
It’s not a breakthrough, but it shows how computers can get the upper hand just by taking new approaches to familiar problems. “I thought it was funny,” says Vinyals. “I just didn’t remember — or maybe didn’t know — this behavior.” It’s likely there’ll be more surprises to come.
Command Line delivers daily updates from the near-future."
f3000bceff,Fruity or fermented? Algorithm predicts how molecules smell | New Scientist,"Daily news
28 October 2016
Eric Feferberg/AFP/Getty Images By Bob Holmes
It’s not something to be sniffed at. Computers have cracked a problem that has stumped chemists for centuries: predicting a molecule’s odour from its structure. The feat may allow perfumers and flavour specialists to create new products with much less trial and error. Unlike vision and hearing, the result of which can be predicted by analysing wavelengths of light or sound, our sense of smell has long remained inscrutable. Olfactory chemists have never been able to predict how a given molecule will smell, except in a few special cases, because so many aspects of a molecule’s structure could be important in determining its odour.
Andreas Keller and Leslie Vosshall at Rockefeller University in New York City decided to crowdsource the power of machine learning to address the problem. First, they had 49 volunteers rate the odour of 476 chemicals according to how intense and how pleasant the smell was, and how well it matched 19 other descriptors, such as garlic, spice or fruit. Then they released the data for 407 of the chemicals, along with 4884 different variables measuring chemical structure, and invited anyone to develop machine-learning algorithms that would make sense of the patterns. They used the remaining 69 chemicals to evaluate the accuracy of the algorithms of the 22 teams that took up the challenge. The best algorithms proved far more accurate than any previous efforts in predicting the volunteers’ descriptions of the test chemicals. They were not perfect, partly because people rarely rate the same odour identically when tested a second time. “If you ask someone how burnt a smell is and they give it a 17, and then you come back half an hour later and ask again and they give it a 10,” says contest winner Rick Gerkin, a neuroscientist at Arizona State University in Tempe. “The best a model can do is be a little bit wrong in both cases.” Even so, Gerkin’s algorithm predicted the volunteers’ scores nearly as well as their previous ratings of a given odour did. Real odours span many more than just 21 descriptors, of course, but Gerkin thinks it would be straightforward, though time-consuming, to tackle a wider set of descriptors. This could help perfumers and flavour specialists sort through the billions of scented molecules to find ones with a particular, desired odour, says Robert Sobel, vice president for research at FONA International, a flavour company in Geneva, Illinois. Even if the predictions aren’t perfect, they can help narrow the field when you’re after a particular scent or flavour, says Gerkin. “Eventually, you can use a database like that and say OK, pick out the top 100 hits out of a billion molecules. A hundred molecules are easier to test than a billion.” The next challenge is working out what scents will arise from mixtures of chemicals. “What you’re doing here is rating individual molecules, says Avery Gilbert at Synesthetics, a sensory consultancy in Fort Collins, Colorado. “What’s more useful is knowing which ingredients play nicely together.” Journal reference: bioRxiv, DOI: 10.1101/082495 More on these topics:
A shorter version of this article was published in New Scientist magazine on 5 November 2016"
578ef1ef31,Q&A: Uber’s machine learning chief says pattern-finding computing fuels ride-hailing giant – GeekWire,"by Dan Richman on October 19, 2016 at 2:45 pmOctober 19, 2016 at 3:19 pm Danny Lange, Uber’s head of machine learning. (Uber photo.)
Under the simple skin of Uber lies complexity you may not have considered: the logistics of predicting how long it will take rides (or meals) to arrive, setting pricing and even where to wait to give a driver the best odds of finding you for pick-up. Underlying those decisions is machine learning, using computers to find patterns and make predictions without explicitly programming them to do so. And that very important job falls to Danny Lange, a Danish-born researcher who joined the ride-hailing giant 11 months ago after a nearly two-year stint leading machine learning efforts for Amazon Web Services. Before that, Lange wrangled big data for Microsoft and even launched a Silicon Valley startup. He heads a growing team of researchers in San Francisco and Seattle. Uber opened an engineering office in Seattle last year, and that office now houses about 150 people working on machine learning, as well as product engineering and operations. GeekWire caught up with Lange recently to talk about how Uber uses machine learning. Here are edited excerpts from the conversation. Q: Thanks for chatting with us. Can you start by saying a bit about how Uber is using machine learning? Lange: My team and I are making machine learning available as a service to everyone in the company. Traditionally, a company would hire Ph.D.’s and data scientists, and each team would have to figure out its own algorithms. We’re creating a way to access machine learning just using web interfaces, APIs and SDKs (software development kits). Our customers inside the company can expect this service to run 24/7 for them, with no need to become specialists in machine learning themselves. Q: How widely is machine learning being used within Uber? Lange: Many of program teams are using it, including Uber Eats (a food delivery service), UberX (the basic ride-hailing service), UberPOOL (a ride-sharing variant on UberX), and Uber Maps (an as-yet unreleased service). For example, Uber Eats uses machine learning to estimate the delivery time for your meals. We went from a finite approach — where you compute the time using the distance between you and the restaurant, the average speed and the time to prepare the meal — to taking the delivery times for thousands and thousands of meals and basing the prediction on that. Overnight, that improved our estimates by 26 percent. Q: What are some other examples of machine learning underlying Uber’s services?  Lange: The ETA for Uber X rides. There’s no one sitting there saying: ‘There’s 7 miles between you and the car and it will take the car 14 minutes to get to you.’ It’s based on data from millions of trips, which lets us take into consideration normal patterns that occur day after day. Also, sometimes the Uber app will advise customers to move a few yards to or around a corner for pick-up. That’s done by an algorithm that detects patterns of successful pick-ups, versus those where there have been challenges. We use the experience of millions and millions of pick-ups. It’s something the system learns. And we’re now using machine learning to detect fraud, such as account take-overs and stolen credit cards. Q: What technologies underlie Uber’s machine learning?  Lange: We offer about 10 different algorithms, including boosted trees, linear learners and neural networks. We give our internal customers the benefit of taking the algorithm that suits their problems best. Q. How would Uber be different if it had no machine learning? Would it be able to function as a company? Lange: I actually don’t think so. The concept of machine learning is that we have this constant feedback loop and we learn every day from it. Q: Why don’t you just use the machine-learning services you built for Amazon Web Services? Lange: We have lots of special needs. We have scale issues and challenges around being a global service. We run our own data centers and built a lot of the apps ourselves. Q: Machine learning is very compute- and data-intensive. Does Uber use any public-cloud services at all, or any external data? Lange: To some extent (we use cloud computing), but most of what we’re doing is in-house. All the data we use is internal. Q: Are you building on what you created at AWS, or have you gone in a different direction? Lange: We are pursuing an open-source path. We’re using Hadoop (massively distributed computing technology), Spark (a large-scale data processing engine from Apache) and MLLib (a scalable machine-learning library). We have created a centralized repository of data, so if you’re launching a new service within Uber, you can use this data that has been gathered from other services and bootstrap yours. We pioneered the global shift to cloud, social, mobile, and data science technologies, and we’ve been disrupting enterprise software ever since. Are you ready to push your limits at the world’s fastest-growing enterprise software company? We’re looking for the industry’s best technical talent—we’re looking for you! Join us in our Bellevue office. Intel: Our initial fix for the Meltdown and Spectre chip design flaws had flaws Who will pay for Meltdown and Spectre? Chip flaws could leave a financial mess in their wake Report: Dropbox submits confidential filing for an IPO Subscribe to GeekWire's free newsletters to catch every headline Have a scoop that you'd like GeekWire to cover? Let us know. Uber on track to post a $3B loss for 2016 as revenue and bookings soar, new report says Study shows how Uber and Lyft drivers discriminate based on rider race Lyft expanding Seattle presence with new office lease as it battles ride-hailing rival Uber UberEats restaurant delivery service now live in 50 cities after launching in Portland The rapid rise of new Seattle: Time-lapse video shot over 3 years captures city’s massive growth Lights out at CES: Giant trade show plunges into darkness, knocks out Samsung, LG, and other booths From cryptocurrency to IPOs, 6 venture capitalists make tech predictions for 2018 Major bummer: Nirvana’s ‘Smells Like Teen Spirit’ auto-tuned in wrong key is grunge blasphemy Catch every headline in your inbox"
58309c1905,An AI Ophthalmologist Shows How Machine Learning May Transform Medicine - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Google researchers got an eye-scanning algorithm to figure out on its own how to detect a common form of blindness, showing the potential for artificial intelligence to transform medicine remarkably soon. The algorithm can look at retinal images and detect diabetic retinopathy—which affects almost a third of diabetes patients—as well as a highly trained ophthalmologist can. It makes use of the same machine-learning technique that Google uses to label millions of Web images. Diabetic retinopathy is caused by damage to blood vessels in the eye and results in a gradual deterioration of vision. If caught early it can be treated, but a sufferer may experience no symptoms early on, making screening vital. It is diagnosed, in part, by having an expert examine images of a patient’s retina, captured with a specialized device, for signs of bleeding and fluid leakage. Some form of automated detection could make the diagnosis more efficient and reliable, and could be especially useful in regions where the required expertise is scarce. “One of the most intriguing things about this machine-learning approach is that it has potential to improve the objectivity and ultimately the accuracy and quality of medical care,” says Michael Chiang, a professor of ophthalmology and a clinician at Oregon Health & Science University’s Casey Eye Institute. AI has had mixed success in medicine in the past. Systems that use a database of knowledge to offer advice have been shown to outperform doctors in some settings, but there has been limited uptake. Still, the power of machine learning—especially a technique known as deep learning, may make AI more common in the future (see “10 Breakthrough Technologies 2013: Deep Learning”). A team at Google DeepMind, a subsidiary of Alphabet focused entirely on AI, is doing similar work, training computers to process optical coherence tomography scans for signs of macular degeneration and other eye disease in collaboration with researchers at Moorfields Eye Hospital in London (see “DeepMind’s First Medical Research Gig Will Use AI to Diagnose Eye Disease”). This retinal-image research, published Tuesday, marked the first time a paper about deep learning has appeared in the Journal of the American Medical Association, according to the journal’s editor-in-chief, Howard Bauchner. The paper’s authors, comprised of computer scientists at Google and medical researchers from the U.S. and India, developed an algorithm to analyze retinal images. But unlike existing ophthalmology software, it was not explicitly programmed to recognize features in images that might indicate the disease. It simply looked at thousands of healthy and diseased eyes, and figured out for itself how to spot the condition. The researchers created a training set of 128,000 retinal images classified by at least three ophthalmologists. After the algorithm had been trained, the researchers tested its performance on 12,000 images and found that it matched or exceeded the performance of experts in identifying the condition and grading its severity. The Google researchers collaborated with scientists at the Aravind Medical Research Foundation in India, where a clinical trial involving real patients is ongoing. This project involves patients receiving a normal consultation, but their images are also fed into the deep-learning system for comparison. Lily Peng, a researcher at Google and a medical doctor who was involved with the project, says results from this trial are not yet ready for publication. Deep learning could be applied in many different areas of medicine that rely on image analysis, such as radiology and cardiology. But one of the biggest challenges will be to provide convincing evidence that the systems are reliable. Brendan Frey, a professor at the University of Toronto and the CEO and cofounder of a company called Deep Genomics, warns that researchers will need to develop machine-learning systems that are capable of explaining how they reached a particular conclusion (see “AI’s Language Problem”). Peng, of Google, says this is something her team is already working on. “We understand that explaining will be very important,” she says. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on machine learning.
Google,
machine learning,
deep learning,
EmTech Digital 2017,
MIT Technology Review Events,
diabetic retinopathy
Will Knight Senior Editor, AI I am the senior editor for AI at MIT Technology Review. I mainly cover machine intelligence, robots, and automation, but I’m interested in most aspects of computing. I grew up in south London, and I wrote my first line of code (a spell-binding… More infinite loop) on a mighty Sinclair ZX Spectrum. Before joining this publication, I worked as the online editor at New Scientist magazine. If you’d like to get in touch, please send an e-mail to will.knight@technologyreview.com. More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Unlimited online access including articles and video, plus The Download with the top tech stories delivered daily to your inbox.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
d860b01a4d,Skype launches Photo Effects – sticker suggestions powered by machine learning  |  TechCrunch,"You are about to activate our Facebook Messenger news bot. Once subscribed, the bot will send you a digest of trending stories once a day. You can also customize the types of stories it sends you.
Click on the button below to subscribe and wait for a new Facebook message from the TC Messenger news bot.
Thanks,
TC Team
Not content with merely launching its own take on Instagram and Snapchat’s Stories, Skype today is adding another copycat-like feature to its app: photo stickers. The company says it’s introducing new “Photo Effects” (as it’s calling these stickers), which include things like face masks, decorative borders, witty captions, and more. However, unlike the photo stickers you’ll find in other social apps today, Skype will actually suggest the stickers to use based on the photo’s content, day of the week, and other options. The new feature is based on technology Microsoft introduced earlier this year in a silly camera app called Sprinkles. The Sprinkles app leverages Microsoft’s machine learning and A.I. capabilities to do things like detect faces in photos, determine the subject’s age and emotion, figure out your celebrity look-a-like, and suggest captions. It then lets you swipe through its suggestions – for example, various props to add to your photo, funny captions, and stickers displaying its guess about your age, among other things. Similarly, Skype will suggest its photo effects automatically with a press of a button. To use the feature, you’ll first snap a photo then tap the magic wand icon at the top of the screen to access the photo effects. As you swipe right through the suggestions, you’ll be prompted to add things to your photo like a smart face sticker, the weather, your location, a caption that references the day (e.g. “turn that frown upside down, it’s taco Tuesday!”), face masks, a celebrity look-a-like, or even a mystery face swap.
Microsoft says these photo effects will change often – like on different days of the week or holidays, for instance. The resulting image can be shared with Skype friends in a conversation or posted to Skype’s new Highlights feature, which is the Instagram/Snapchat Stories clone introduced earlier this year. Like Stories on other platforms, Highlights are somewhat ephemeral. But instead of lasting a day, they’re available for a week. They’re also not shared with your entire Skype network – only those who have opted to follow your Highlights directly. Highlights remains a mobile-only feature for now. When Skype’s revamped interface launched to desktop users in October, Microsoft told us Highlights was not a priority for desktop integration at this time, based on user feedback. However, the company insisted it still aims to bring Highlights to the desktop in a later release. The addition of Photo Effects is arriving on Skype for mobile users in the latest update. Skype’s release notes list Photo Effects as “upcoming” in Android version 8.10.0.4 and iOS 8.10.0.5. This version began rolling out on Monday, but will gradually release to the install base over the next week. Latest headlines delivered to you daily"
d9d058bc12,"Google's AI Can Dream, and Here's What it Looks Like | IFLScience","Software engineers at Google have been analyzing the 'dreams' of their computers. And it turns out that androids do dream of electric sheep... and also pig-snails, camel-birds and dog-fish. This conclusion has been made after testing the ability of Google's servers to recognize and create images of commonplace objects – for example, bananas and measuring cups. The result of this experiment is some tessellating Escher-esque artwork with Dali-like quirks.
Google's artificial neural network creates its own images from keywords. Michael Tyka/Google. So, what's the point in creating these bizarre images? Is it purely to find out our future robot overlord's artistic potential, or is there a more scientific reason? As it turns out, it is for science: Google wants to know how effectively computers are learning. The Google artificial neural network is like a computer brain, inspired by the central nervous system of animals. When the engineers feed the network an image, the first layer of 'neurons' have a look at it. This layer then 'talks' to the next layer, which then has a go at processing the image. This process is repeated 10 to 30 times, with each layer identifying key features and isolating them until it has figured out what the image is. The neural network then tells us what it has valiantly identified the object to be, sometimes with little success. This is the process behind image-recognition. The Google team then realized that they could reverse the process. They gave the artificial neural network an object and asked it to create an image of that object. The computer then tries to associate it with specific features. When we want a picture of a fork, the computer should figure out that the defining features of a fork are two to four tines and a handle. But things like size, color and orientation aren't as important. The images in the picture above were created in order to ascertain whether the computer has understood this sort of distinction. Sometimes, the resulting images are not quite what you'd expect... Take this picture of a dumbbell, for example:
Google's artificial neural network's interpretation of a dumbbell. Michael Tyka/Google. You may have noticed that no computer-generated dumbbell is complete without a muscular weightlifting arm. While the computer is clearly incorrect in thinking that dumbbells are sold complete with a human arm, it can be forgiven for thinking that this is the case. If you type ""dumbbell"" into Google, images of these objects are intermingled with pictures of weightlifters holding them. Therefore, to the computer, one of the defining features of a dumbbell is the arm that lifts it.  The computers have the ability to see images in objects in a way that artists can only dream of replicating. It sees buildings within clouds, temples in trees and birds in leaves.
Google's artificial neural network finds images of birds and buildings within landscapes. Michael Tyka/Google. Highly detailed elements seem to pop up out of nowhere. This processed image of a cloudy sky proves that Google's artificial neural network is the champion of finding pictures in a cloudy sky.
Google's artificial neural network's attempt at finding images in a cloudy sky. Michael Tyka/Google.
A close-up of some of the stranger creatures that the network found in the sky. Michael Tyka/Google. This technique, which creates images where there aren't any, is aptly called 'inceptionism.' There is an inceptionism gallery where you can explore the computer's artwork. Finally, the designers gave the computer full, free reign over its artwork. The final pieces were beautiful pictures derived from a mechanical mind – what the engineers are calling 'dreams.' The 'blank canvas' was simply an image of white noise. The computer pulled out patterns from the noise and created dreamscapes: pictures that could only come from an infinite imagination.
The product of an artificial neural network being asked to amplify and pull patterns out of white noise. Michael Tyka/Google
Pulling patterns out of white noise. Michael Tyka/Google. [Via Google] Sign up today to get weekly science coverage direct to your inbox This website uses cookies This website uses cookies to improve user experience. By continuing to use our website you consent to all cookies in accordance with our cookie policy."
f90e1bb3a4,IBM's Watson AI Recommends Same Treatment as Doctors in 99% of Cancer Cases,"Artificial intelligence (AI) is about more than just the promise of a robot butler — it can actually save lives. AI’s contribution to the healthcare industry and in medical research could be hugely significant. IBM sees that and wants Watson, its AI technology, at the forefront of this development. Watson is far more than just that AI that beat human champions at Jeopardy in 2011. Currently, Watson dabbles in genomics and assists with cancer diagnoses, making use of its strength with words. The AI is able to read and interpret words through natural language processing and is pretty good at it. Human experts at the University of North Carolina School of Medicine tested Watson by having the AI analyze 1,000 cancer diagnoses. In 99 percent of the cases, Watson was able to recommend treatment plans that matched actual suggestions from oncologists. Not only that, but because it can read and digest thousands of documents in minutes, Watson found treatment options human doctors missed in 30 percent of the cases. The AI’s processing power allowed it to take into account all of the research papers or clinical trials that the human oncologists might not have read at the time of diagnosis. IBM is also working with medical lab company Quest Diagnostics to provide gene sequencing matched with diagnostic analysis courtesy of Watson, which would be made available as a cloud service oncologists could access. “This is the broad commercialization of Watson in oncology,” John E. Kelly, a senior VP who oversees IBM’s research labs and the Watson business, tells the New York Times. IBM’s serious about bringing Watson into the healthcare industry. Like, billions of dollars serious. To build Watson’s medical credibility, IBM spent $4 billion dollars purchasing companies that had huge stores of medical data, from billing records to MRI images. And it looks like the investment is paying off. The medical field counts for two-thirds of the employment of Watson AI units. Moreover, Watson may generate $500 million in revenue this year, according to estimates by the securities research arm of Swiss bank UBS. That number could reach nearly $6 billion by 2020 and almost $17 billion by 2022. “IBM has taken a very different approach from Google, Amazon, and Microsoft. It’s leveraging data and the knowledge of experts,” said independent analyst Judith Hurwitz in the Times article. Watson is proving that AI is the future of genomics and potentially the future of IBM as a whole.
References:
NY Times, IBM Watson
Related Articles Over 350,000 people subscribe to our newsletter.
Our mission is to empower our readers and drive the development of transformative technologies towards maximizing human potential
Sign in to join the conversation."
fefa6cb030,[1710.01931] Forecasting Player Behavioral Data and Simulating in-Game Events,"Link back to: arXiv, form interface, contact."
991be4626f,Machine learning's next trick is generating videos from photos - The Verge,"Show a human any photograph and they’ll able to predict what happens next with pretty decent accuracy. The woman riding her bike will keep on moving. The dog will catch the frisbee. The man is going to have a pratfall. And so on. It’s such a basic skill that we don’t consider the vast amount of information that is used to make these predictions — concerning gravity, inertia, the nature of pratfalls, etc. — and teaching computers to do the same is proving to be a key challenge in machine vision. The videos are short, small, and often nightmarish Researchers from MIT attempting to solve this problem have come up with some very impressive results, using specially trained neural networks to turn images into videos and getting the computer to essentially predict what happens next. Their model has plenty of limitations — the videos are seconds long, tiny, and often nightmarish — but it’s still an impressive feat of machine imagination, and another step toward computers that understand the world a little more like humans. The neural net was trained using more than 2 million videos downloaded from Flickr. These were sorted into four types of scenes; golf courses, beaches, train stations, and hospitals (this latter category made up of images of babies), and the footage was stabilized to remove camera shake. Using this data, the team’s neural nets were able to not only generate short videos that resembled these scenes (that's the GIF at the top of the page), but to also look at a still and create footage that might follow (that's the GIF below). This essentially predicting what will happen next, albeit in a limited manner that is only guessing how pixels might change, rather than understanding the scene. Here’s how that looks:
It’s quite easy to see what’s being achieved here and where it falls short. In the beach videos for example, you can see the waves crashing, and at the train station, the model knows that the train is likely to keep moving past the camera. However, when asked to predict how a human will walk across a golf course, the end results don't actually look anything like a human. They're blurred, smeared, and unrealistic. The researchers themselves note that the computer’s predictions don’t usually follow ""the correct video,"" but that at least ""the motions are plausible."" Getting beyond these plausible-but-obviously-fake videos is going to be tough, but other machine learning systems have made progress in related areas, predicting actions like handshakes and hugs, and even generating sounds that match videos. Facebook’s head of AI Yann LeCun addressed this topic in an interview last year, saying that being able to generate future movement like the research above is a ""piece of the puzzle"" in creating predictive computers, but that true understanding of a video or image and its possible futures will take much more work. ""If you’re watching a Hitchcock movie and I ask, ‘15 minutes from now, what is it going to look like in the movie?’ You have to figure out who the murderer is,"" said LeCun. ""Solving this problem completely will require knowing everything about the world and human nature. That’s what’s interesting about it."" Command Line delivers daily updates from the near-future."
236297047e,Scientists Use Machine-learning to Analyze Language in Movies  | Technology Networks,"News   Nov 14, 2017
| Original story from University of Washington
In the movie Frozen, only the princess Elsa is portrayed with high power and positive agency, according to a new analysis of gender bias in movies. Her sister, Anna, is portrayed with similarly low levels of agency and power as Cinderella, a movie character that debuted in 1950. Credit: University of Washington
At first glance, the movie ""Frozen"" might seem to have two strong female protagonists -- Elsa, the elder princess with unruly powers over snow and ice, and her sister, Anna, who spends much of the film on a quest to save their kingdom.But the two princesses actually exert very different levels of power and control over their own destinies, according to new research from University of Washington computer scientists.The team used machine-learning-based tools to analyze the language in nearly 800 movie scripts, quantifying how much power and agency those scripts give to individual characters. In their study, recently presented in Denmark at the 2017 Conference on Empirical Methods in Natural Language Processing, the researchers found subtle but widespread gender bias in the way male and female characters are portrayed.""'Frozen' is an interesting example because Elsa really does make her own decisions and is able to drive her own destiny forward, while Anna consistently fails in trying to rescue her sister and often needs the help of a man,"" said lead author and Paul G. Allen School of Computer Science & Engineering doctoral student Maarten Sap, whose team also applied the tool to Wikipedia plot summaries of several classic Disney princess movies.""Anna is actually portrayed with the same low levels of power and agency as Cinderella, which is a movie that came out more than 60 years ago. That's a pretty sad finding,"" Sap said.The team also created a searchable online database showing the subtle gender biases in hundreds of Hollywood movie scripts, which range from late 80s cult classics like ""Heathers"" to romantic comedies like ""500 Days of Summer"" to war films like ""Apocalypse Now.""In their analysis, the researchers found that women were consistently portrayed in ways that reinforce gender stereotypes, such as in more submissive positions and with less agency than men. For example, male characters spoke more in imperative sentences (""Bring me my horse"") while female characters tended to hedge their statements (""Maybe I am wrong""). However, the bias is not just in the words these characters speak, but also in the way they are portrayed through narratives.To study the nuanced biases in narratives, the UW researchers expanded prior work presented in 2016 on ""connotation frames"" that give insights into how different verbs can empower or weaken different characters through their connotative meanings. The study evaluated the power and agency implicit in 2,000 commonly used verbs, where the connotative meanings were obtained from Amazon Mechanical Turk crowdsourcing experiments.The power dimension denotes whether a character has authority over another character, while the agency dimension denotes whether a character has control over his or her own life or storyline. For each verb, turkers were asked to rank the implied level of power differentials and agency on a scale of 1 to 3.""For example, if a female character 'implores' her husband, that implies the husband has a stance where he can say no. If she 'instructs' her husband, that implies she has more power,"" said co-author Ari Holtzman, an Allen School doctoral student. ""What we found was that men systematically have more power and agency in the film script universe.""Verbs that imply low power or agency include words like ask, experience, happen, wait, relax, need or apologize. Verbs that confer high power or agency include words like finish, prepare, betray, construct, destroy, assign or compose.Using the movie scripts, the researchers automatically identified genders of 21,000 characters based on names and descriptions. Using natural language processing tools, which employ machine learning, they looked at which characters appeared as a verb's subject and object. They then computed how much agency and power were ascribed to these characters, using their crowdsourced connotation frames. The researchers also accounted for the fact that male actors spent more time on screen than female actors and also spoke more, accounting for 71.8 percent of the words spoken across all movies.The team calculated separate power and agency scores for male and female characters in each movie. They also created scores based on words that the characters spoke in dialogue and on words that were used in narration or stage direction to describe those characters -- exposing subtle differences and biases.In 2010's ""Black Swan,"" a movie centered around a female lead -- a perfectionist ballerina who slowly loses grip on reality -- the movie's dialogue gives more agency to female characters. But the language used to describe the characters in stage direction and narration gave male characters more power and agency in that film.In the 2007 movie ""Juno,"" about an offbeat young woman who unexpectedly gets pregnant, male characters' scene descriptions and narratives also consistently score higher in power and agency, though the two genders come closer in their dialogue.The UW team's tool yields a much more nuanced analysis of gender bias in fictional works than the Bechdel Test, which only evaluates whether at least two female characters have a conversation about something other than a man.The tendency for male characters to score higher on both power and agency dimensions held true throughout all genres: comedy, drama, horror, sci-fi, thrillers. Interestingly, the team found the same gender bias even for movies with female casting directors or script writers.""We controlled for this. Even when women play a significant role in shaping a film, implicit gender biases are still there in the script,"" said co-author and Allen School doctoral student Hannah Rashkin.Next steps for the team include broadening the tool to not only identify gender bias in texts but also to correct for it by offering rephrasing suggestions or ways to make language more equal across characters of different genders. The methodology isn't limited to movies, but could be applied to books, plays or any other texts.""We developed this tool to help people understand how they may be perpetuating these subtle but prevalent biases that are deeply integrated in our language,"" said senior author Yejin Choi, an associate professor in the Allen School. ""We believe it will help to have this diagnostic tool that can tell writers how much power they are implicitly giving to women versus men.""This article has been republished from materials provided by University of Washington. Note: material may have been edited for length and content. For further information, please contact the cited source.
SLIMS Users Can Now Connect to the iSpecimen Marketplace
Genohm and iSpecimen have jointly announced that Genohm has established a partnership with iSpecimen to enable users of the SLIMS laboratory information management system to seamlessly connect to the iSpecimen Marketplace.
Ovation and Coriell Life Sciences Partner to Streamline Genetic Reporting Workflows
Ovation.io, Inc. (Ovation), makers of the fastest-growing clinical laboratory information and commercialization platform, and Coriell Life Sciences, Inc., an innovative provider of clinical genetic reporting solutions, announced a partnership and platform integration designed to create a seamless ecosystem for sample and workflow management, genetic data interpretation, and external client communication.
Blue Brain Nexus: An Open-Source Tool for Data-Driven Science
The Blue Brain Nexus will help enable data-driven neuroscience through searching, integrating and tracking large-scale data and models.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 11, 2018 Video News   Jan 11, 2018 News   Jan 11, 2018 Article Article"
bf992b4a86,Lessons from Alexa: Artificial Intelligence and Machine Learning Use Cases | Earley Information Science,"I was just given an Amazon Echo as a gift and was very excited to connect it and set it up. For those of you less familiar with Echo, it is Amazon’s cloud based intelligent agent, which interacts using voice recognition and performs some useful tasks using voice commands.  The hardware is a cylinder about 9 inches tall and 3 inches in diameter.  The “wake up phrase,” and therefore the name of the agent, is “Alexa.”  After setting it up I walked through some of the features – asking about the weather, setting a timer, listening to a radio station, asking about specific facts (like capitals of states or countries, measures, math problems), asking about local movies, getting an NPR “flash briefing” and a few other interesting functions. These functions all worked well, but it quickly became apparent that Alexa was not robust when one goes off script or makes requests with even slight phrase variations.  For example, I asked Alexa how large the Echo hardware was. I tried several variations of this. “How large are you?” and “What are your dimensions?” elicited: “Sorry, I didn’t understand the question I heard.” (Of course, Alexa has no dimensions but the Echo does.) When I asked “What are the physical dimensions of the Echo?” Alex still could not understand.  When I asked “How tall is the Echo?” Alexa finally was able to tell me. I also had success with “How large is the Echo?” and “What are the dimensions of the Echo?” As it turned out, adding the additional term “physical” to “dimensions” caused the algorithm to fail.  I was also able to connect the Echo to my Hue wireless LED lights as well as my NEST learning thermostat.  Pretty cool. However, I had to use precise language to make the functions work.  I made lots of mistakes but eventually learned how to control my home automation with my voice.  In this case, “machine learning” (the technology behind many intelligent agents) turned out to be a human learning to talk to the machine rather than the machine learning to interpret the user.  These emerging intelligent agents have many limitations.  That does not diminish their value, and the improvement in quality will accelerate. Alexa can already be programmed with “skills” that are specific use cases regarding specialized domains of knowledge.  They range from the whimsical “Magic Eight Ball”, to trivia games about Cricket, self-help and daily meditations, and the more utilitarian Capital One Hands-Free Banking. The concept of “skills” points to the vast problems of terminology, meaning and interactions.  Intelligent assistants are programmed with specific users and use cases in mind.  The same will be true of enterprise uses for intelligent assistants.  The more carefully designed and scoped, the more valuable the application.  Artificial Intelligence, which include applications like Alexa, is a class of application that requires a defined set of use cases and an information architecture. You can’t have artificial intelligence without information architecture.  (No AI without IA.) News about  artificial intelligence shows up practically daily in the popular media – how it will change our world and every aspect of work and our daily lives.  There is no doubt that this is happening.  It is happening on multiple levels. Some of the applications are glitzy, some are sexy, and many more are under the radar.   Artificial intelligence has only made limited and specialized inroads into most organizations. In some cases, these are high end, costly “science projects” where management is convinced that they need to spend vast sums of money with specialized vendors that work in secrecy and promise black box algorithms that will allow them to dominate their competitors and the marketplace.  In other cases they are promised tools that “define their own algorithms” to personalize interactions with users and use machine learning to determine what to offer whom, when and under what circumstances.  Speak with any of the leading analyst firms before buying into such claims.  More often than not, the claims and hype are outpacing the reality and practicality.  Well-funded startups with large marketing budgets, professional sales teams selling to the c suite and competent technical staffs will eventually figure out their space in the market, but after spending large amounts of client and VC money experimenting and “pivoting” (the venture term for making mistakes with large amounts of VC and client money) until they solve some customer problems and find a way to repeat what is very often a services based solution.  Far more organizations are trying to deal with the “we can’t find our stuff” types of problems or “our customers can’t find the right stuff” problems. These include knowledge base organization, call centers and support centers, customer self-service, product search, product content, and overall web content management.  Problems than are trying to develop sophisticated applications.  Some  artificial intelligence vendors claim to solve this class of problem, too; however, looking under the hood we see a lot of work that is not necessarily  artificial intelligence as it is promoted, but is actually more about good information architecture  practices combined with certain types of learning algorithms, metrics based processes and natural language interfaces.  [White Paper] Making Intelligent Virtual Assistants a Reality [Article] There is no AI without IA Artificial intelligence encompasses a class of application that allows for easier interaction with computers and also allows computers to take on more of the types of problems that were typically in the realm of human cognition.  Every  artificial intelligence program interfaces with information, and the better that information is structured, the more effective the program is. A “corpus” of information contains the answers that the program is attempting to process and interpret.  Structuring that information for retrieval is referred to as “knowledge engineering” and the structures are called “knowledge representation.”  Knowledge representation consists of taxonomies, controlled vocabularies, thesaurus structures, and all of the relationships between terms and concepts. These elements collectively make up the “ontology.”  An ontology represents a domain of knowledge and the information architecture structures and mechanisms for accessing and retrieving answers in specific contexts.  Ontologies can also capture “common sense” knowledge of objects, processes, materials, actions, events, and myriad more classes of real world logical relationships.  In this way, an ontology forms a foundation for computer reasoning even if the answer to a question is not explicitly contained in the corpus.  The answer can be inferred from the facts, terms and relationships in the ontology.  In a practical sense, this can make the system more user friendly and forgiving when the user makes requests using phrase variations, and more capable when encountering use cases that were not completely defined when the system was developed.  In effect, the system can “reason” and make logical deductions.  Organizations already have ontologies in the form of master data, customer attributes, product catalogs, dictionaries, glossaries, structured taxonomies, metadata, transaction systems and other sources of information.  These need to be integrated, structured, managed, and normalized to be most useful. They then become the knowledge architecture for internal and customer-facing applications, ecommerce systems, marketing technologies, content management tools, social media listening applications, and customer engagement platforms. Adding  artificial intelligence and machine intelligence tools is a natural evolution of the technology ecosystem and will begin for most enterprises as an extension of their customer facing information systems such as search and content personalization mechanisms.  At its core, search is a recommendation engine that processes user intent signals (the search term or phrase itself, but also the user context: where they came from and any information we may have about their preferences) and presents a “recommended” result. Personalization tools make content and product recommendations.  (Content can be in the form of promotions, offers, sales, next best action, products for cross sell and upsell, answers to questions, etc.)     In the case of  artificial-intelligence-driven search and user experience, the results are specific answers instead of lists of documents or more appropriate selections of products that suite the user’s needs. The interface is more conversational and more forgiving of ambiguous requests, or variations in phrases and terms than typical approaches.  Artificial-intelligence-driven search leverages context of users, their characteristics and where they are in their journey.  This is why  artificial intelligence and machine learning is behind personalization technologies and also makes use of big data sources, transactional information, social graph data, real time user behaviors on web sites and mobile devices and other signals that the system processes in order to infer user intent To learn more about the data architecture approaches that support artificial intelligence see the following additional content from Earley Information Science: [Webinar] Training AI-driven Support Bots to Deliver the Next Generation of Customer Experience [White Paper] Making Intelligent Virtual Assistants a Reality [Article] There is no AI without IA [Blog] The Coming Chatbot Craze (and what you need to do about it)     Seth Earley
Founder & CEO Phone: 781.812.5551 info@earley.com Contact Us © 2017 Earley Information Science, Inc. All rights reserved. Privacy Policy    |     Terms of Service"
4526403763,An AI Ophthalmologist Shows How Machine Learning May Transform Medicine - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Google researchers got an eye-scanning algorithm to figure out on its own how to detect a common form of blindness, showing the potential for artificial intelligence to transform medicine remarkably soon. The algorithm can look at retinal images and detect diabetic retinopathy—which affects almost a third of diabetes patients—as well as a highly trained ophthalmologist can. It makes use of the same machine-learning technique that Google uses to label millions of Web images. Diabetic retinopathy is caused by damage to blood vessels in the eye and results in a gradual deterioration of vision. If caught early it can be treated, but a sufferer may experience no symptoms early on, making screening vital. It is diagnosed, in part, by having an expert examine images of a patient’s retina, captured with a specialized device, for signs of bleeding and fluid leakage. Some form of automated detection could make the diagnosis more efficient and reliable, and could be especially useful in regions where the required expertise is scarce. “One of the most intriguing things about this machine-learning approach is that it has potential to improve the objectivity and ultimately the accuracy and quality of medical care,” says Michael Chiang, a professor of ophthalmology and a clinician at Oregon Health & Science University’s Casey Eye Institute. AI has had mixed success in medicine in the past. Systems that use a database of knowledge to offer advice have been shown to outperform doctors in some settings, but there has been limited uptake. Still, the power of machine learning—especially a technique known as deep learning, may make AI more common in the future (see “10 Breakthrough Technologies 2013: Deep Learning”). A team at Google DeepMind, a subsidiary of Alphabet focused entirely on AI, is doing similar work, training computers to process optical coherence tomography scans for signs of macular degeneration and other eye disease in collaboration with researchers at Moorfields Eye Hospital in London (see “DeepMind’s First Medical Research Gig Will Use AI to Diagnose Eye Disease”). This retinal-image research, published Tuesday, marked the first time a paper about deep learning has appeared in the Journal of the American Medical Association, according to the journal’s editor-in-chief, Howard Bauchner. The paper’s authors, comprised of computer scientists at Google and medical researchers from the U.S. and India, developed an algorithm to analyze retinal images. But unlike existing ophthalmology software, it was not explicitly programmed to recognize features in images that might indicate the disease. It simply looked at thousands of healthy and diseased eyes, and figured out for itself how to spot the condition. The researchers created a training set of 128,000 retinal images classified by at least three ophthalmologists. After the algorithm had been trained, the researchers tested its performance on 12,000 images and found that it matched or exceeded the performance of experts in identifying the condition and grading its severity. The Google researchers collaborated with scientists at the Aravind Medical Research Foundation in India, where a clinical trial involving real patients is ongoing. This project involves patients receiving a normal consultation, but their images are also fed into the deep-learning system for comparison. Lily Peng, a researcher at Google and a medical doctor who was involved with the project, says results from this trial are not yet ready for publication. Deep learning could be applied in many different areas of medicine that rely on image analysis, such as radiology and cardiology. But one of the biggest challenges will be to provide convincing evidence that the systems are reliable. Brendan Frey, a professor at the University of Toronto and the CEO and cofounder of a company called Deep Genomics, warns that researchers will need to develop machine-learning systems that are capable of explaining how they reached a particular conclusion (see “AI’s Language Problem”). Peng, of Google, says this is something her team is already working on. “We understand that explaining will be very important,” she says. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on machine learning.
Google,
machine learning,
deep learning,
EmTech Digital 2017,
MIT Technology Review Events,
diabetic retinopathy
Will Knight Senior Editor, AI I am the senior editor for AI at MIT Technology Review. I mainly cover machine intelligence, robots, and automation, but I’m interested in most aspects of computing. I grew up in south London, and I wrote my first line of code (a spell-binding… More infinite loop) on a mighty Sinclair ZX Spectrum. Before joining this publication, I worked as the online editor at New Scientist magazine. If you’d like to get in touch, please send an e-mail to will.knight@technologyreview.com. More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
a2686b8661,An AI has learned how to pick a single voice out of a crowd | New Scientist,"News & Technology
24 October 2017
Christopher Anderson/Magnum Photos By Richard Gray Devices like Amazon’s Echo and Google Home can usually deal with requests from a lone person, but like us they struggle in situations such as a noisy cocktail party, where several people are speaking at once. Now an AI that is able to separate the voices of multiple speakers in real time promises to give automatic speech recognition a big boost, and could soon find its way into an elevator near you. The technology, developed by researchers at the Mitsubishi Electric Research Laboratory in Cambridge, Massachusetts, was demonstrated in public for the first time at this month’s Combined Exhibition of Advanced Technologies show in Tokyo.
It uses a machine learning technique the team calls “deep clustering” to identifies unique features in the “voiceprint” of multiple speakers. It then groups the distinct features from each speaker’s voice together, allowing it to disentangle multiple voices and then reconstruct what each person was saying. “It was trained using 100 English speakers, but it can separate voices even if a speaker is Japanese,” says Niels Meinke, a spokesperson for Mitsubishi Electric. Meinke says the system can separate and reconstruct the speech of two people speaking into a single microphone with up to 90 per cent accuracy. If there are three speakers the accuracy dips, but is still up to 80 per cent. In both cases, this was with speakers the system had never encountered before. Conventional approaches to this problem – such as using two microphones to replicate the position of a listener’s ears – have only managed 51 per cent accuracy. In overcoming the “cocktail party effect” that has dogged AI research for decades, the new technology could help smart assistants in homes and cars work better. It could also improve automatic speech transcription, and be used to help law enforcement agencies reconstruct recordings of conversations that had been muddied by music, for example. In preliminary tests the system was able to separate the voices of up to five people at once. “The system could be used to separate speech in a range of products including lifts, air-conditioning units and household products,” says Meinke. Indeed, Mitsubishi is now in the process of building its voice recognition technology into lifts and air-conditioners, among other products. Reference: arxiv.org/abs/1508.04306 More on these topics:
A shorter version of this article was published in New Scientist magazine on 28 October 2017"
fdc3620d68,Machine Learning Is Redefining The Enterprise In 2016,"Bottom line: Machine learning is providing the needed algorithms, applications, and frameworks to bring greater predictive accuracy and value to enterprises’ data, leading to diverse company-wide strategies succeeding faster and more profitably than before. Industries Where Machine Learning Is Making An Impact   The good news for businesses is that all the data they have been saving for years can now be turned into a competitive advantage and lead to strategic goals being accomplished. Revenue teams are using machine learning to optimize promotions, compensation and rebates drive the desired behavior across selling channels. Predicting propensity to buy across all channels, making personalized recommendations to customers, forecasting long-term customer loyalty and anticipating potential credit risks of suppliers and buyers are Figure 1 provides an overview of machine learning applications by industry.
Source: Tata Consultancy Services, Using Big Data for Machine Learning Analytics in Manufacturing – TCS
Machine Learning Is Revolutionizing Sales and Marketing   Unlike advanced analytics techniques that seek out causality first, machine learning techniques are designed to seek out opportunities to optimize decisions based on the predictive value of large-scale data sets. And increasingly data sets are comprised of structured and unstructured data, with the global proliferation of social networks fueling the growth of the latter type of data.  Machine learning is proving to be efficient at handling predictive tasks including defining which behaviors have the highest propensity to drive desired sales and marketing outcomes. Businesses eager to compete and win more customers are applying machine learning to sales and marketing challenges first.  In the MIT Sloan Management Review article, Sales Gets a Machine-Learning Makeover the Accenture Institute for High Performance shared the results of a recent survey of enterprises with at least $500M in sales that are targeting higher sales growth with machine learning. Key takeaways from their study results include the following: Why Machine Learning Adoption Is Accelerating
Machine learning's ability to scale across the broad spectrum of contract management, customer service, finance, legal, sales, quote-to-cash, quality, pricing and production challenges enterprises face is attributable to its ability to continually learn and improve. Machine learning algorithms are iterative in nature, constantly learning and seeking to optimize outcomes.  Every time a miscalculation is made, machine learning algorithms correct the error and begin another iteration of the data analysis. These calculations happen in milliseconds which makes machine learning exceptionally efficient at optimizing decisions and predicting outcomes. The economics of cloud computing, cloud storage, the proliferation of sensors driving Internet of Things (IoT) connected devices growth, pervasive use of mobile devices that consume gigabytes of data in minutes are a few of the several factors accelerating machine learning adoption. Add to these the many challenges of creating context in search engines and the complicated problems companies face in optimizing operations while predicting most likely outcomes, and the perfect conditions exist for machine learning to proliferate."
d6e5f2be40,Cultural Diffusion and Trends in Facebook Photographs – Facebook Research,"By: Quenzeng You, Dario Garcia, Manohar Paluri, Jiebo Luo, Jungseock Joo Yuhang Zhao, Shaomei Wu, Lindsay Reynolds, Shiri Azenkot Yuhang Zhao, Shaomei Wu, Lindsay Reynolds, Shiri Azenkot David Holtz, Diana Lynn MacLean, Sinan Aral Sagie Benaim, Lior Wolf Peter Hedman, Suhib Alsisan, Richard Szeliski, Johannes Kopf Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, Michael Cohen Online social media is a social vehicle in which people share various moments of their lives with their friends, such as playing sports, cooking dinner or just taking a selfie for fun, via visual means, i.e., photographs. Our study takes a closer look at the popular visual concepts illustrating various cultural lifestyles from aggregated, de-identified photographs. We perform analysis both at macroscopic and microscopic levels, to gain novel insights about global and local visual trends as well as the dynamics of interpersonal cultural exchange and diffusion among Facebook friends. We processed images by automatically classifying the visual content by a convolutional neural network (CNN). Through various statistical tests, we find that socially tied individuals more likely post images showing similar cultural lifestyles. To further identify the main cause of the observed social correlation, we use the Shuffle test and the Preference-based Matched Estimation (PME) test to distinguish the effects of influence and homophily. The results indicate that the visual content of each user’s photographs are temporally, although not necessarily causally, correlated with the photographs of their friends, which may suggest the effect of influence. Our paper demonstrates that Facebook photographs exhibit diverse cultural lifestyles and preferences and that the social interaction mediated through the visual channel in social media can be an effective mechanism for cultural diffusion. Areas
Applied Machine Learning, Computer Vision, Data Science
© 2018 Facebook"
963ed18745,[1710.03748] Emergent Complexity via Multi-Agent Competition,"Link back to: arXiv, form interface, contact."
32327e2e53,Google brings on-device machine learning to mobile with TensorFlow Lite,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Google’s recent preview release of its open-source TensorFlow Lite software for machine learning developers signifies an exciting shift in the field of AI. The company’s dedication to developing AI capable of running algorithms on a mobile device — without connecting to the cloud — is laying the groundwork for the artificial intelligence of things (AioT) of the future. As far as consumer products go, Google Assistant, Alexa, and Siri are among the most popular uses of AI in the mainstream. For as little as $30 or $40 a person can get their own interactive artificial intelligence – as long as they also have WiFi and somewhere to plug in a charger. TensorFlow Lite represents the nascent steps on the path toward making AI-powered devices not just accessible, but disposable. It’s the death of buttons. Developers now have preview access to TensorFlow Lite for Android or iOS. Instead of providing new functionality for AI applications, it’s designed to leverage existing hardware – like the Snapdragon processors inside many smartphones – to run algorithms typically impossible for mobile devices without connecting to the cloud. With Google’s new Lite AI platform you can run AI models on a smartphone and, upon adding new data, run those algorithms to determine new outcomes. It’s machine-learning on the go, without the need for connectivity. If you’re one of the people terrified at the prospect of hundreds of devices in your home spying on you through your internet connection, you’ll be happy to know that the researchers at Google are specifically designing TensorFlow Lite to, eventually, address those kinds of concerns. According to the TensorFlow Lite website the software is designed with the following criteria in mind: It’ll be interesting to see where Google’s ‘AI platform miniaturization’ project goes next. It’s paving the way for voice-controlled disposables built on cheap chips and AI-powered appliances that don’t expose your entire network to hackers. If Google can continue to squeeze more usefulness into less powerful devices we’ll eventually live in a world where AI can be injected cheaply into any gadget, even disposable ones. Google engineer and TensorFlow technical lead Pete Warden told MIT “What I want is a 50-cent chip that can do simple voice recognition and run for a year on a coin battery.” TensorFlow Lite brings the company one step closer to realizing Warden’s vision.
Read next:
Motorola's new Polaroid-printing Mod might be the best reason to buy a Moto Z
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
35108dd09d,"
	
	Machine Learning Detects Marketing and Sale of Opioids on Twitter

","​Using advanced machine learning, a cross disciplinary team of University of California San Diego researchers developed technology that mined Twitter to identify entities illegally selling prescription opioids online. Between June and November 2015, some 619,937 tweets containing the keywords codeine, Percocet, fentanyl, Vicodin, Oxycontin, oxycodone and hydrocodone were collected. The findings, published online in the
American Journal of Public Health in October, detected 1,778 posts that were marketing the sale of controlled substances, 90 percent included hyperlinks to online sites for purchase.
“An unhealthy use of prescription and non-prescription opioid drugs continues to rise in the United States. Public policy and law enforcement efforts are trying to address this crisis, but closer attention to the potential negative influence of digital technologies is needed,” said Tim K. Mackey, PhD, UC San Diego School of Medicine associate professor of anesthesiology and global public health and first author. “Our study demonstrates the utility of a technology to aid in these efforts that searches social media for behavior that poses a public threat, such as the illegal sale of controlled substances.” Mackey and researchers with UC San Diego School of Medicine and Jacobs School of Engineering used a three-step process that involved cloud-based computing to collect large volumes of tweets filtered by keywords, machine learning to isolate tweets related to the marketing of opioids, and web forensic examination to analyze posts that included hyperlinks to external websites. During the five-month study period, less than 1 percent of tweets mentioning opioids were marketing prescription opioids, and only 46 of the hyperlinks included in those tweets were still live when the team analyzed the data eight months later. However, if the technology were used for active surveillance the data could be used to find more live links and could also be used for surveillance and detection of other health-related illegal online activities, said Mackey. The Ryan Haight Online Pharmacy Consumer Protection Act prohibits the sale and marketing of controlled substances directly to consumers online. It was passed in 2008 after 18-year-old Ryan Haight of San Diego died after purchasing Vicodin online.
“This technology could help improve enforcement of the Ryan Haight Act,” said Mackey. “In addition, social media providers can use it to find or prohibit content that is illegal or violates laws to ensure consumers have a safer experience online.”
Forensics researchers connected marketing on Twitter to blogs, other social media platforms, user forms, online classified ads and websites. The majority of sites had foreign addresses, with many linked to Pakistan — a country recently identified as a source and exporter of fake, counterfeit and falsified medications, said Mackey. “The online sale of controlled substances is directly prohibited by federal law. However, social media appears to act as a conduit for increased risk to substance abuse behavior,” said Mackey. An interactive map of study findings is available here:
www.google.com/maps/d/u/0/edit?mid=13qnaNCAKlVJVOezTUObv3Szto1Q&ll=12.624085123383825%2C0&z=3
This study builds on a February 2017 study in the journal
Addictive Behaviors, which used the same machine learning technology to filter Twitter content to identify nonmedical uses of prescription drugs and behavioral trends.
Co-authors include: Janani Kalyanam, Takeo Katsuki, and Gert Lanckriet, UC San Diego. This research was funded, in part, by Alliance for Safe Online Pharmacies.
Disclosure: Mackey is a non-compensated member of the Alliance for Safe Online Pharmacies academic advisory panel. Francesca Torriani, MD
Medical Director, Infection Prevention and Clinical Epidemiology
619-471-9045
UC San Diego | School of Medicine | Health Sciences
Regents of the University of California. All rights reserved."
27cf1de6b0,EasyScanGo,"Global Good and Motic Introduce a Breakthrough AI-Powered Microscope to Fight Drug-Resistant Malaria.  We are proud to announce our collaboration with Global Good to create and distribute the new EasyScan_GO, a breakthrough AI-powered microscope to fight the spread of drug-resistant malaria and assist in case management. Using custom image recognition software, EasyScan_GO is capable of identifying and counting malaria parasites in a blood smear in as little as 20 minutes. Every year, malaria kills almost half a million people, and researchers estimate nearly half the world’s population is at risk of contracting it. The disease is typically found in tropical and sub-tropical countries in Asia, sub-Saharan Africa, and the Americas. Even in Europe and North America, thousands of cases are diagnosed from infected travelers returning home. “Malaria is one of the hardest diseases to identify on a microscope slide. By putting machine learning-enabled microscopes in the hands of laboratory technicians, we can overcome two major barriers to combating the mutating parasite—improving diagnosis in case management and standardizing detection across geographies and time.”  David Bell Director of Global Health Technologies supporting Global Good. “Microscopic detection, specification and counting of Malaria causing organisms in human red blood cells needs skilled personnel, is subjective and time consuming. EasyScan Go instead allows a rapid automated identification, specification and quantification of different types of plasmodiae in red blood cells. Machine detection and classification of different types of microorganisms can be controlled on an image gallery and corrected. Conventionally stained slides can be subjectively reassessed, if necessary. Thus, a reliable, objective early diagnosis of different types of Malaria can be achieved even in the absence of specialists within a few minutes. Similar to automated screening smears for cervical cancer cells the device thus further improves the availability and quality of microscopical diagnostics.”   Prof. Dr. Alfred Böcking Emeritus Director Institute of Cytopathology, University Düsseldorf, Germany Consultant Institute of Pathology, City Hospital Düren, Germany “Our goal in integrating Global Good’s advanced software into Motic’s high-quality, affordable digital slide scanner is to simplify and standardize malaria detection. Success with the most difficult-to-identify disease paves the way for the EasyScan product line to excel at almost any microscopy task and to detect other major diseases that affect developed and emerging markets alike.” Richard Yeung Vice President of Motic China. “Microscopy still remains as gold standard for Tuberculosis and Malaria detection. With development of AI, automatic diagnosis system could be a practical and useful solution. Motic has been concentrating in AI technology for years to free laboratory workers and treating physicians from burdensome subjective manual Microscopy observation.” Dr. Paramasivan, C.N. Senior Scientific Advisor, Foundation for Innovative New Diagnostics (FIND), India. ""This collaboration, combining Global Good’s impact invention focus with Motic’s engineering, manufacturing and distribution capabilities, represents the type of innovative healthcare solution that is needed to improve health in emerging and low-income markets. By distributing and commercializing an intelligent microscope, Global Good and Motic are creating a future where quality diagnosis of multiple diseases is within reach for everyone everywhere.”  Maurizio Vecchione Intellectual Ventures’ Executive Vice President of Global Good and Research. “Microscopy is gold standard for laboratory confirmation of malaria. However, it is labour-intensive and time-consuming, required a lot of training and refresh training. Therefore, it is becoming a big challenge to maintain the skill of microscopists in the setting of malaria elimination, such as in China. Hopefully, artificial intelligence system, which using machine deep learning technology, could be a helpful assistant for malaria diagnosis. As an intelligent system, I wish EasyScan GO will be an powerful tool for malaria surveillance with its capacity of automatically parasite identification and species classification.”  Jun (Joe) Cao, PhD. Deputy Director of Jiangsu Institute of Parasitic Diseases (JIPD). Executive Director of WHO Collaborating Centre for Research and Training on Malaria Elimination. Motic is a groundbreaking company specialized in manufacturing conventional compound microscopes. Thanks to our optical expertise, thorough and heartfelt customer service, and aim to enhance the microscopy experience, we have grown into a global brand within everyone's reach. We are committed to standing by our young scientists from their early steps, improving healthcare, and supporting the progress of scientific research by adapting to the market trends, and by focusing on advanced digital solutions aimed at fulfilling the needs of today and exceeding the expectations of tomorrow. Global Good is dedicated to inventing technology for humanitarian impact. Millions of people suffer and die each year in poor countries from causes that humanity has the scientific and technical ability to solve. Funded by Bill Gates and focused on a shared vision with Nathan Myhrvold, Global Good invents technology to solve some of humanity's most daunting problems. Global Good does this by collaborating with leading humanitarian organizations, forward-looking governments, research institutions, and corporate and private sector partners that bring our inventions to market."
22b7f10725,Bug-repair system learns from example | MIT News,"Login
or
Subscribe Newsletter
A new machine-learning system analyzes successful repairs to buggy software and learns how to repair new bugs.
Automatic code-patching system corrects nearly twice as many errors as its predecessors.
Larry Hardesty | MIT News Office
September 28, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office Anyone who’s downloaded an update to a computer program or phone app knows that most commercial software has bugs and security holes that require regular “patching.” Often, those bugs are simple oversights. For example, the program tries to read data that have already been deleted. The patches, too, are often simple — such as a single line of code that verifies that a data object still exists. That simplicity has encouraged computer scientists to explore the possibility of automatic patch generation. Several research groups, including that of Martin Rinard, an MIT professor of electrical engineering and computer science, have developed templates that indicate the general forms that patches tend to take. Algorithms can then use the templates to generate and evaluate a host of candidate patches. Recently, at the Association for Computing Machinery’s Symposium on the Foundations of Software Engineering, Rinard, his student Fan Long, and Peter Amidon of the University of California at San Diego presented a new system that learns its own templates by analyzing successful patches to real software. Where a hand-coded patch-generation system might feature five or 10 templates, the new system created 85, which makes it more diverse but also more precise. Its templates are more narrowly tailored to specific types of real-world patches, so it doesn’t generate as many useless candidates. In tests, the new system, dubbed Genesis, repaired nearly twice as many bugs as the best-performing hand-coded template system. Thinning the herd “You are navigating a tradeoff,” says Long, an MIT graduate student in electrical engineering and computer science and first author on the paper. “On one hand, you want to generate enough candidates that the set you’re looking through actually contains useful patches. On the other hand, you don’t want the set to include so many candidates that you can’t search through it.” Every item in the data set on which Genesis was trained includes two blocks of code: the original, buggy code and the patch that repaired it. Genesis begins by constructing pairs of training examples, such that every item in the data set is paired off with every other item. Genesis then analyzes each pair and creates a generic representation — a draft template — that will enable it to synthesize both patches from both originals. It may synthesize other, useless candidates, too. But the representation has to be general enough that among the candidates are the successful patches. Next, Genesis tests each of its draft templates on all the examples in the training set. Each of the templates is based on only two examples, but it might work for several others. Each template is scored on two criteria: the number of errors that it can correct and the number of useless candidates it generates. For instance, a template that generates 10 candidates, four of which patch errors in the training data, might score higher than one that generates 1,000 candidates and five correct patches. On the basis of those scores, Genesis selects the 500 most promising templates. For each of them, it augments the initial two-example training set with each of the other examples in turn, creating a huge set of three-example training sets. For each of those, it then varies the draft template, to produce a still more general template. Then it performs the same evaluation procedure, extracting the 500 most promising templates. Covering the bases After four rounds of this process, each of the 500 top-ranking templates has been trained on five examples. The final winnowing uses slightly different evaluation criteria, ensuring that every error in the training set that can be corrected will be. That is, there may be a template among the final 500 that patches only one bug, earning a comparatively low score in the preceding round of evaluation. But if it’s the only template that patches that bug, it will make the final cut. In the researchers’ experiments, the final winnowing reduced the number of templates from 500 to 85. Genesis works with programs written in the Java programming language, and the MIT researchers compared its performance with that of the best-performing hand-coded Java patch generator. Genesis correctly patched defects in 21 of 49 test cases drawn from 41 open-source programming projects, while the previous system patched 11. It’s possible that more training data and more computational power — to evaluate more candidate templates — could yield still better results. But a system that allows programmers to spend only half as much time trying to repair bugs in their code would be useful nonetheless. Topics: Research, School of Engineering, Computer science and technology, Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical Engineering & Computer Science (eecs), Machine learning, Software This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
d147f6af24,Revolutionizing Radiology with Deep Learning at Partners Healthcare--and Many Others,"Thomas H. Davenport and Randy Bean
One of the more miraculous aspects of modern medicine is its ability to peer directly inside the human body to aid in diagnosis of disease and medical conditions. Radiological imaging is one of the most effective diagnostic tools available, and its use is so pervasive that it accounts for about 10% of U.S. medical costs. The usage of medical imaging grew rapidly over the last few decades, but has plateaued as costs skyrocketed. Not only are imaging machines expensive, but images also require interpretation by radiologists. Researchers and vendors have worked for many years to automate the interpretation of images, but thus far the field of computer-aided diagnosis (CAD) has not made substantial inroads into patient care. Some institutions employ CAD as a “second set of eyes,” but the cost of imaging has yet to decline.
New cognitive technologies, however, have the potential to substantially improve CAD for radiology images and also those from pathology labs, and to combine them with other diagnostic data. These technologies are advancing quickly in research labs, but have yet to make their way into medical practice. A relatively new center at Partners Healthcare – the Center for Clinical Data Science (CCDS) – is focused on bringing these technologies to the clinical world. Based at the highly-ranked institutions Massachusetts General Hospital (MGH) and Brigham & Women’s Hospital (BWH) in Boston, the CCDS is a joint effort of MGH and BWH. Its goal is to employ machine learning and other artificial intelligence technologies to improve the healthcare delivery system; in particular, a key CCDS objective is to improve the effectiveness of imaging-based diagnosis. The CCDS is pursuing a variety of machine learning approaches, but the primary technology that it is employing is deep neural networks (also known as deep learning). These technologies have already led to breakthroughs in other areas of image recognition, and many researchers expect that they eventually will do so with medical images. A recent article in the New England Journal of Medicine, “Translating Artificial intelligence into Clinical Care,” expressed hope that this type of machine learning will lead to a breakthrough in care. As Dr. Keith Dreyer, Partners’ Chief Data Science Officer, puts it: We’ve had CAD for a couple of decades, but deep learning is a much better technology. It will provide much higher sensitivity and specificity than we have today, and radiologists will trust it. Integrating it with clinical practice offers many potential benefits. The diagnosis of a lumbar spine injury, for example, might involve up to 300 MRI images and various other test results in an electronic medical record system. A deep learning application could quickly identify the most important images for a radiologist to review and recommend treatment alternatives. The technology could save substantial time for critically injured trauma patients and could leverage the radiologist’s time for all patients."
1cc0993162,"Dstl Satellite Imagery Competition, 3rd Place Winners’ Interview: Vladimir & Sergey | No Free Hunch","Kaggle Team|05.09.2017 In their satellite imagery competition, the Defence Science and Technology Laboratory (Dstl) challenged Kagglers to apply novel techniques to ""train an eye in the sky"". From December 2016 to March 2017, 419 teams competed in this image segmentation challenge to detect and label 10 classes of objects including waterways, vehicles, and buildings. In this winners' interview, Vladimir and Sergey provide detailed insight into their 3rd place solution.
My name is Vladimir Iglovikov, I work as a Sr. Data Scientist at TrueAccord. A couple years ago I got my PhD in theoretical physics at UC Davis, but choose not to go for postdoctoral position as most of my colleagues. I love science, I love doing research, but I needed something new, I needed a challenge. Another thing is that I wanted to get deeper experience with software engineering and data science which would aid me in my research. A few months before my graduation, in one of the online classes at Coursera lecturer mentioned Kaggle as a platform to practice your machine learning skills. First few competitions were epically failed, but I learned a lot. Slowly, piece by piece, I was able to merge theoretical knowledge from courses, books and papers with actual data cleaning and model training. During the first year I figured out how and when to apply classical methods like Logistic Regression, SVM, kMeans, Random Forest, xgboost, etc. Neural Network type competitions were rather rare and I did not have a chance to practice deep learning that much, but starting from December of the last year we already had seven computer vision problems. DSTL problem had the closest deadline and that is why I decided to join. Vladimir on Kaggle. Sergey Mushinskiy
I had 10+ years experience in IT, until recent switch to machine learning and software development.
I was in search for a great project to showcase my machine learning skills (especially deep learning part of it). And what could be better than winning a tough competition like this? There were several competitions on Kaggle at that time and it was a difficult choice to make – everything looked very interesting.
Sergey on Kaggle. DSTL competition had the closest deadline, there were some technical challenges, and until the very late, notably less people than usual participated in the competition.. It was interesting to take this challenge that scared off even the top kagglers. However, during this competition it became obvious that there were a lot of extremely talented participants who did a good job and shared endless insights and solved a lot of challenging aspects.
But all this pale in comparison with encouragement coming from friends in our Russian Open Data Science community. It is a group of like-minded and extremely dedicated people, who decided to go all-in into this competition and who gave me support and motivation to start and persevere through all the challenges.
We tried to tackle this problem in a variety of different ways and, as expected, most of them didn’t work. We went through a ton of a literature and implemented a set of different network architectures before we eventually settled on a small set of relatively simple yet powerful ideas which we are going to describe. On the high level we had to solve an image segmentation problem. Ways to approach it are well known and there is number of papers regarding the topic. Also Vladimir already had experience with this kind of tasks when he participated in an Ultrasound Nerve Segmentation competition and placed 10th out of 923.
What was the data? Unlike the other computer vision problems where you are given RGB or grayscale images, we had to deal with the satellite data that is given both in visual and
lower frequency regions. On one hand it carries more information, on another it is not really obvious how to use this extra data properly.
Data was divided into train (25 images) and test (32 images) sets. Each image covers 1 square kilometer of
the earth surface. The participants were provided with three types of images of the same area: a high-resolution panchromatic (P), an 8-band image with a lower resolution (M-band), and a longwave (A-band) that has the lowest resolution of all. As you can see from the image above, RGB and M-band partially overlap in the optical spectral range. Turns out that RGB was itself reconstructed at the postprocessing step from a combination of a low resolution M-band image and high resolution panchromatic image.
Another interesting fact is that our images have color depth of 11 and 14-bit instead of a more common 8-bit. From a neural network perspective it is better, each pixel carries more information but for a human it introduces additional steps required for visualization.
As you can see input data contains a lot of interesting information. But what is our output? We want to assign one or more class labels to the each pixel of the input image.
The prediction for each class was evaluated independently using Average Jaccard Index (also known in literature as Intersection-over-Union), and the class-wise scores were averaged over all ten classes with equal weights.
Overall, the problem looks like a standard image segmentation problem with some multispectral input specificities.
One of the issues that participants needed to overcome during competition is a lack of training data. We were provided 25 pictures, covering 25 square kilometers. This may sound like a lot, but these images are pretty diverse: jungles, villages and farmland. And they are really different. This made our life harder. Clouds are a major challenge for satellite imaging. In the provided data, however, clouds were presented rather as an exception than a rule. So the issue did not affect us as much. The fact that we did not have access to the images of the same area at different time was a big complication. We believe that temporal information could significantly improve our model performance. For instance pixel wise difference between images of the same area corresponds to an object that can change its position with time making it possible to identify moving cars. Another interesting thing is that satellite takes a shot in such a way that in the M-band, channels
2, 3, 5 and 7 come a few seconds later than 1, 4, 6 and 8 which leads to a ghosts of a moving objects:
Speaking of cars, they were often labeled unreliably. The image below shows the reference annotation of cars. Needless to say, that a fraction of this reference data corresponds to unrelated objects: debris, random road pixels, etc:
As a result of all these complications we gave up on the both vehicle classes and did not predict them in our final solution. There were a few approaches that we wanted to try, like constrain the phase space to villages and roads and run FasterRNN or SSD to localize the cars, but we did not have time to implement it. Now when we are done with an introduction we would like to present a set of ideas that helped us to finish at the third place. A stable local validation is 90% of the success. In every ML textbook one can find the following words: “Assuming that train and test set are generated from the same iid…”, followed by a discussion on how to perform cross-validation. In practice, both in industry and in competitions, this assumption is satisfied only approximately, and it is very important to know how accurate this approximation is. Class distribution in the training set can be easily obtained from the annotation data / labels. To find out how much area each class occupies in the test set we used the following trick:
For each class we made a dedicated submission in which all pixels were attributed to that class only. A jaccard score returned by the system for such submission gave us the relative area of the class in the public fraction of the test set. This approach worked because of the specific submission and evaluation formats of this particular competition
(recall that the final score is an average of the classes scores, so a submission with just one class presented would give a score for this class). In addition, when the competition was over, we used the same trick to obtain distributions of the classes in images of the private fraction of the test set.
First of all, this plot tells us, that classes are heavily imbalanced within each set of images. For example, one pixel of both large and small vehicle classes corresponds to a 60,000 pixels with crops. This lead us to a conclusion that training a separate model per class will work much better than a one model that predicts all classes at a time. Of course we tried to train such a model and it works surprisingly well. We we able to get in a top 10% but not more. Secondly, the distributions themselves vary significantly from one set of images to another. For instance, an initial version of the train set did not contain any images with waterways, and only after competitors pointed that out, the organizers moved two images of rivers from the test into the train set. One can see that this particular class was still under-represented in the train set compared to the public and the private test sets. As a result simple unsupervised methods for water classes worked better than neural network approaches. It is also important to mention that when train and test sets are relatively different those that participated in the competition after results on the Private Leaderboard are released change their standing a lot (in this problem participants in the middle of the Leaderboard easily moved +- 100 places). And those that did not, as usual, claimed that participants are overfitters that do not understand machine learning and do not know how to perform cross validation properly. These days for the most of the computer vision problems neural networks are the most promising approach. What other methods can one use for the satellite image segmentation? Deep Learning became popular only in the last few years, and people have worked with satellite images for a much longer time. We discussed this problem with former and current employees of Orbital Insight and Descartes Labs plus we read a ton of literature on the subject. Apparently fact that we have infrared and other channels from non-optical frequency range allows to identify some classes purely from the pixel values, without any contextual information. Using this approach, the best results were obtained for water and vegetation classes.
For instance, in our final solution both water classes were segmented using NDWI, which is just a ratio of the difference and sum of the pixel values in the green and infrared channels.
This image demonstrates high intensity values for waterways, but it also shows false positives on some buildings – perhaps due to the relative similarity of the specific heat of metal roofs and water. We expected a deep learning approach to perform as well as or even better than index thresholding and, in vegetation prediction, neural networks did indeed outperform indices. However, we found that indices allow us to achieve better results for under-represented classes such as waterways and standing water. In the provided images, ponds were smaller than rivers, so we additionally thresholded our predictions by area of water body to distinguish waterways from standing water. As we mentioned before classes are very different and given the lack of data beating top 10% score with a single model would be very tough. Instead, we decided to train a separate network for each class, except water and cars. We tried a lot of various network architectures and settled on a modified U-net that previously had shown very good results in the problem of Ultrasound Nerve Segmentation. We had a lot of hope for Tiramisu, but its convergence was slow and performance wasn’t satisfactory enough. We used Nadam Optimizer (Adam with Nesterov momentum) and trained the network for 50 epochs with a learning rate of 1e-3 and additional 50 epochs with a learning rate of 1e-4. Each epoch was trained on 400 batches, each batch containing 128 image patches. Each batch was created randomly cropping 112x112 patches from original images. In addition each patch was modified by applying a random transformation from group D4. Initially we tried 224x224 but due to limited GPU memory this would
significantly reduce the batch size from 128 to 32. Larger batches proved to be more important than a larger receptive field. We believe that was due to the train set containing 25 images only, which differ from one another quite heavily. As a result, we decided to trade-off receptive field size in favour of a larger batch size. If we just had RGB data and nothing else the problem would be much simpler. We would just crop big images into batches and feed them into a network. But we have RGB, P, M and A bands which have different color depth, resolution and could be shifted in time and space. All of them may contain unique and useful information that needs to be used. After tackling this problem in a bunch of different ways we ended up dropping A-band, M was stretched from 800x800 to 3600x3600 and stacked it with RGB and P images. Deep neural networks can find the interactions between different features when the amount of the data is sufficient, but we suspected that for this problem it was not the case. In the recent Allstate competition, quadratic features improved performance of xgboost, which is very efficient at finding feature interactions as well. We followed a similar path and added the indices CCCI, EVI, SAVI, NDWI as four extra channels. We like to think of these extra channels as an added domain knowledge. We suspect that these indices would not enhance performance on larger dataset.
As already mentioned, the evaluation metric for this competition was an Average Jaccard Index. A common loss function for classification tasks is categorical cross entropy but in our case classes are not mutually exclusive and using binary cross entropy makes more sense. But we can go deeper. It is well known that in order to get better results your evaluation metric and your loss function need to be as similar as possible. The problem here however is that Jaccard Index is not differentiable. One can generalize it for probability prediction, which on one hand, in the limit of the very confident predictions, turns into normal Jaccard and on the other hand is differentiable –
allowing the usage of it in the algorithms that are optimized with gradient descent. That logic led us to the following loss function:
It is pretty clear how to prepare the patches for training - we just crop them from the original images, augment and feed into the network with the structure that we described above.
What about predictions? In the zeroth order approximation everything looks straightforward, partition (3600, 3600) image into small patches, make prediction and stitch them together. We did this and got something like this:
Why do we have this weird square structure? And the answer is that not all outputs in the Fully Connected Networks are equally good. Number of ways that you can get from any input pixel to the central part of the output in a network is much higher than to the edge ones. As a result prediction quality is decreasing when you move away from center. We checked this hypothesis for a few classes, and, for example, for buildings logloss vs distance from center looks like this:
One way to deal with such issue was to make the predictions on overlapping patches, and crop them on the edges, but we came out with a better way. We added Cropping2D layer to the output layers of our networks, which solved two problem simultaneously: As a bonus, the trick slightly decreased the computation time. To summarize, we trained a separate model for each of the first six classes. Each of them took matrix with a shape (128, 16, 112, 112) as an input and returned the mask for a central region of the input images (128, 1, 80, 80) as an output.
Are we done with the boundary effects? Not yet. To partition original images into (112, 112) tiles we added zeros to the edges of the original (3600, 3600) images, classical ZeroPadding. This added some problems at the prediction time. For example, sharp change in pixel values from central to zero padded area was probably interpreted by a network as a wall of the building and as a result we got a layer of building all over the perimeter in the predicted mask.
This issue was addressed with the same trick as in the original U-net paper. We added reflections of the central part to the padded areas.
We also considered modifying zero padding layers within a network to pad with reflections instead of zeros in a similar manner. Theoretically, this micro optimization may improve overall network performance, but we did not check it. At this stage we already had a strong solution that allowed to get in top 10. But you can always go deeper. If it is possible to augment your train set to increase its size, you can always perform test time augmentation to decrease variance of the predictions. We did it in the following way:
1. Original (3600, 3600) image is rotated by 90 degrees and we get 2 images: original and rotated.
2. Both are padded.
3. Split into tiles.
4. We perform predictions on each tile.
5. Combine predictions back into the original size.
6. Crop padding areas.
7. Prediction of the rotated image is rotated back to the original orientation.
8. Results of the both prediction pipelines averaged with geometric mean. A schematic representation of the prediction pipeline:
How does it improve our predictions?
1. We decrease variance of the predictions.
2. Images are split in tiles in a different way and this helps to decrease local boundary effects. We used 4 image orientations for test time augmentation, but one can do it with all eight elements of the D4 group. In general, any augmentation that can be used at the train time can be also used at the test time. After all of these steps we managed to get something which looks surprisingly nice:
We finished 8th on the Public Leaderboard and 3rd on the Private. Many thanks to Artem Yankov, Egor Panfilov, Alexey Romanov, Sarah Bieszad and Galina Malovichko for help in preparation of this article. Vladimir gave a talk based on his team's winning approach in this competition at the SF Kagglers meetup in April. Check out the video below!
Nice article... Could you share the code ? DSTL, organizers of this competition were impressed by the quality of the solutions in the problem so much that they created their own copy of Kaggle at https://datasciencechallenge.org and started two competitions. First was Computer Vision / Detection, second was something about natural language processing. Computer vision problem looked pretty interesting and invested time in it. I finished second. Prize for the second place is 12,000 pounds. Couple minutes ago I got an email: ```
I'm sorry to say that due to your Russian citizenship you do not meet Clause 2.3b because Russia has a score below 37.
As a result the company running the challenges is unable to award you any prize money as you do not meet the criteria that they need to follow due to their legal obligations in the UK.
``` Politics should not get in the way of science. I really do not like a concept of judging scientists not by their merit, but by age, gender, ethnicity or country of origin. And I am pretty sad by the mentality of the British Government representatives. I agree with you, since the competition is public there should be no restrictions for participants We summarized our ideas in: https://arxiv.org/abs/1706.06169 Hello, you've mentioned that 25 images are available to you for training and 32 for testing. Whereas, when I downloaded the data, I got 150 RGB images (The multi-channel images for the same images also exist in separate folder).
Is there anything I am missing? There are 6 images in Public test and 26 in Private test. There are also ~400 images in the test set that are not used in the evaluation and added by organizers to prevent hand labeling. Thanks for this article, this is really awesome. I have one idea I would like to share and have you opinion on. You said ""Another interesting thing is that satellite takes a shot in such a way that in the M-band, channels 2, 3, 5 and 7 come a few seconds later than 1, 4, 6 and 8 which leads to a ghosts of a moving objects.""
Do you think we can use this to compute the speed of the vehicles? Do we know the exact value of the delay between bands? It may work, but it is hard to tell if it will work or not. We do not know what is the exact time shift between bands but, I believe, this information should be in the satellite specifications."
e970df28e1,Salesforce created an algorithm that automatically summarizes text using machine learning - The Verge,"This year, people are expected to spend more than half their day reading email, articles, or posts on social media, and it’s only going to get worse. To help solve this problem, researchers at Salesforce have developed an algorithm that uses machine learning to produce “surprisingly coherent and accurate” summaries according to MIT Technology Review.
Automatic summarization would be a particularly useful technology for Salesforce, which produces a variety of customer-service focused products. The company notes that the resulting summaries could be used by sales or customer service representatives to quickly digest emails and information, which would allow them to spend more time focused on their customers.
To that end, Salesforce is turning to machine learning to find ways to summarize longer blocks of texts, which it could eventually incorporate into its products. The company announced that it made two breakthroughs in natural language processing, introducing a new, “contextual word generation model,” and a “new way of training summarization models.” Together, the two advances allow researchers to automatically create summaries of longer texts that are accurate and readable. The company acquired a deep learning outfit MetaMind last year, which was behind the research.
The researchers explain that automatic text summarization works in two ways: extraction or abstraction. With extraction, computer can draw from preexisting wording in a text, but it’s not very flexible. Abstraction allows the computer to introduce new words, but the system has to understand the original article enough to be able to introduce the right words.
This is where deep learning neural networks come into play. They process numerous examples of sentences and words to spit out new representations of each phrase, which allows the system to interpret texts and introduce its own words. The researchers let their model to look back at the text it’s working off of for additional context. It also looks back at earlier generated examples, to ensure that it’s not repeating itself.
The other breakthrough concerns how the researchers train the system to learn and improve itself. They used two approaches: teacher forcing and reinforcement learning. Reinforcement learning is a method that draws inspiration from how animals learn, and been used to teach Google’s DeepMind how to play video games. In this instance, the model is allowed to generate a sequence of words, and the result is then scored with an automated evaluation metric known as ROUGE (Recall-Oriented Understudy for Gisting Evaluation). The algorithm updates itself with higher scores, leading to better outcomes with future summaries. Teacher forcing is when the results are scored word by word off of an established reference, which provide “very decent results,” but which doesn’t allow for much flexibility.
Researchers found that “ROUGE-optimized RL helps improve recall...and word level learning supervision ensures good language flow, making the summary more coherent and readable.” Scored against this system, they found that their joint model scored higher than other approaches, and Richard Socher, Salesforce’s chief scientist, noted that he didn’t think that he’d ever seen “such a large improvement in any [natural-language-processing] task.”
The results are pretty astonishing: the researchers provided several examples, showing the original article, a human-generated summary, and a summary generated by their own model, and in each case, the summaries are considerably shorter than the original text, but contain the essentials in a readable form. Despite their advances, there’s still considerable work to be done in this field: MIT Technology Review spoke with Kristian Hammond, a professor at Northwestern University, who noted that the advance “shows the limits of relying purely on statistical machine learning,” but that it’s a step in the right direction.
Command Line delivers daily updates from the near-future."
78df5cc69c,Enhancing Google Maps with Deep Learning and Street View,"1,190,464 Dec unique visitors
Given the distributed nature of the software systems we’re now building, and the distributed nature of the teams building them, it's more important than ever to understand the basics of software architecture. As a short introduction to the topic and to debunk some myths, here are five things that every software developer should know about software architecture.
Eric Brewer explores continuous evolution, which is the ability to add features easily to a running service and is key to high velocity in development, focusing on immutability to decouple specification from instantiation. Similarly, declarative techniques enable automation, and specialization of APIs enables better support for classes of tasks, including those that are stateless.
This InfoQ emag aims to introduce you to core stream processing concepts like the log, the dataflow model, and implementing fault-tolerant streaming systems.
The book Fit for Purpose by David Anderson and Alexei Zheglov explores how companies can understand their customers and develop products that fit with the purpose(s) their customers have. It provides a framework to help you understand customers’ purposes, segment your market according to purpose, and manage the portfolio of products and services to create happy customers.
Consumer-driven contracts enable our teams at Rightmove to work independently, and be confident that their changes won’t break other services when deploying their own. It also improves communication between teams, and helps to get developers thinking about API design early on.
Software Development Conference
LondonMar 5-9, 2018
AI & ML SFApr 9-11, 2018
New YorkJun 25-29, 2018
A note to our readers: As per your request we have developed a set of features that allow you to reduce the noise, while not losing sight of anything that is important. Get email and web notifications by choosing the topics you are interested in. Google's Ground Truth team recently announced a new deep learning model for the automatic extraction of information from geo-located image files to improve Google Maps. This neural network model achieved a higher accuracy in processing the challenging French Street Name Signs (FSNS) dataset. Julian Ibarz (Google Brain Team) and Sujoy Banerjee (Ground Truth Team) wrote on Google Research Blog website about this TensorFlow model used for solving real-world image text extraction problems. Google Maps software is used for directions, real-time traffic information and information on businesses, however to provide a better experience to its over one billion users, the information has to reflect the changing world. Street View cars have collected 80 billion images to date and it's impossible to manually analyze this very large image data set to find new or updated information for Google Maps. So one of the goals for the team is to automatically extract structured information from the geo-located images. The new deep neural network model, now publicly available for use by developers, achieved a higher deep neural network (84.2%) in reading street names out of Street View images from the French Street Name Signs (FSNS) dataset. This model is extensible to extract other types of information out of Street View images like the business names from store fronts. Text recognition in a natural environment like cities, roads and businesses is a challenging computer vision (CV) and machine learning problem. Factors like distortion, occlusions, directional blur, cluttered background or different viewpoints make the extraction of text from natural scenes more challenging. The Google team used a neural network based model back in 2008 to blur faces and license plates in Street View images to protect the privacy of their users. Based on this research, they have been able to use machine learning to automatically improve Google Maps with relevant up-to-date information. The deep learning model also automatically labels new Street View imagery, normalizes the text to be consistent with the naming conventions and ignores extraneous text that's not relevant for the data analytics. This allows the team to create new addresses directly from images without even knowing the name of the street or the location of the addresses. For example, when a Street View car drives on a newly built road, the model can analyze the captured images, extract the street names and numbers, and properly create and locate the new addresses automatically on Google Maps. To apply these models across the large Street View image datasets, the Ground Truth team uses the machine learning chip Tensor Processing Unit (TPU) to reduce the computational cost of the inferences of the pipeline.   Rate this Article Related Editorial Related Vendor Content Related Sponsor
Allowed html: a,b,br,blockquote,i,li,pre,u,ul,p
Good Information
by
Anita Shah
Allowed html: a,b,br,blockquote,i,li,pre,u,ul,p
Allowed html: a,b,br,blockquote,i,li,pre,u,ul,p
Redpoint Games Launch NPM Package Signing Tool
restQL, a Microservices Query Language, Released on GitHub
Parcel.js Launch Brings a Zero-Configuration Option to JavaScript Module Bundling
Buoyant Release New Kubernetes Service Mesh ""Conduit"" Written in Rust and Golang
HashiCorp and Contino Share Enterprise Terraform Recommended Practices
Google Introduces Low-Priced Preemptible GPUs for Their Customers
Q&A on the Book Fit for Purpose
How the Dutch Railways Applies Agile and Lean
Oracle Announces New Java Champions
Deep Image Priors on Neural Networks with No Training
Hazelcast Joins the Eclipse Foundation
Modern Big Data Pipelines over Kubernetes
Buoyant Release New Kubernetes Service Mesh ""Conduit"" Written in Rust and Golang
HashiCorp and Contino Share Enterprise Terraform Recommended Practices
Redpoint Games Launch NPM Package Signing Tool
Join a community of over 250 K senior developers by signing up for our newsletter Login to InfoQ to interact with what matters most to you. Recover your password... Quick overview of most important highlights in the industry and on the site. Build your own feed by choosing topics you want to read about and editors you want to hear from. Set up your notifications and don't miss out on content that matters to you
Forgot password ?
Back to login
Back to login
Don't have a username ? REGISTER HERE Is your profile up-to-date? Please take a moment to review and update. Note: If updating/changing your email, a validation request will be sent
Keep current company name
Keep current company role
Keep current company Size
Keep current country/zone
Keep current state/province/region
Subscribe to our newsletter?
Subscribe to our architect newsletter?
Subscribe to our industry email notices? We understand why you use ad blockers. However to keep InfoQ free we need your support. InfoQ will not provide your data to third parties without individual opt-in consent. We only work with advertisers relevant to our readers. Please consider whitelisting us."
c17d5ede4b,Uber Data Determine The Best Food Places In New York City,"Anyone who lives in the Ditmars neighborhood of Astoria, Queens will speak fondly of El Rey Del Taco, a neighborhood favorite food late night food truck that often has a line down the block. However, it isn’t the only place in the area; in fact, there is another much less popular taco place 50 yards away. What makes El Rey del Taco great (besides the life-changing carne asada) is the location. You can find it on the corner of Ditmars and 31st St where the Q train lets out, perfectly poised to capture party-goers trudging back from Manhattan bars.
Despite how much New Yorkers love their favorite food trucks, the City of New York has set hard limits on the number of them allowed, to curb noise and air pollution from their gas-powered generators. This poses the question: If only a few thousand food trucks can legally operate at any given time, what is the best real estate. How can they best optimize their locations to maximize sales? While spots like the major subway stops are no-brainers, most of these sites have already been claimed. On top of that, the city plans to increase the number of permits substantially over the next several years, and these carts will need to find new spots.
Can location data from mobile devices provide a reasonable proxy for the concentrated volume of potential customers? Using a public Uber dataset, we looked at all the pickup and drop off points that occurred over a 3-month time window within Manhattan. Obviously, this data set doesn’t capture all people on the move (pedestrians, Yellow cab riders, bikes, etc.,) but it roughly reflects high traffic locations in NY and thus can be a primer for food truck locations. Best Food Truck Locations in NYC can be determined by data.
The data set comes in the form of spatial coordinates. A heat-map of all pickups will show the expected: Manhattan is a very busy place. To yield more exact results, we used a K-means clustering algorithm to pinpoint cluster centers within the traffic data. The underlying idea is that a cluster center generated from this dataset would generate spots on the map that minimize distances between pickup points, indicating locations with ideal points to set up food carts to access the highest number of customers. Once we assigned each pick-up data point to a cluster (Figure 1), we ranked the clusters based on pickup volume (Figure 2).
Figure 1: Pickups colored by cluster assignment. As you can see from Figure 1, there are significant differences between the cluster centers and the top-ranked points at different times. While the main centers of pickups are around Greenwich Village and Lower Manhattan on Thursday evenings, late on Saturday night the traffic centers around Midtown. Especially for smaller, fully mobile carts (think ice cream trucks with a driver or Italian ice carts), this kind of information could help tell operators where to go to take advantage of Uber customers. Nevertheless, using a k-means has shortcomings. The distance is Euclidean and not along the actual roads, so it might be that a center seems close but in reality, it is not. Moreover, this assumes that Uber users are good food truck customers. Figure 2: Thursday evening vs. Saturday late night top-ranked cluster centers. To test the hypothesis that there is a relationship between Uber pickups and food truck locations, we triangulated our Uber data with food truck location data scraped from Yelp. We then divided the city into a grid and determined how many total pickups and food trucks occurred in a given square kilometer. In each grid square, we calculated a ratio of Uber pickups to the number of food carts."
39aebda7ef,Learning language by playing games | MIT News,"Login
or
Subscribe Newsletter
Illustration: Jose-Luis Olivares/MIT System learns to play text-based computer game using only linguistic information.
Larry Hardesty | MIT News Office
September 23, 2015
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
MIT researchers have designed a computer system that learns how to play a text-based computer game with no prior assumptions about how language works. Although the system can’t complete the game as a whole, its ability to complete sections of it suggests that, in some sense, it discovers the meanings of words during its training. In 2011, professor of computer science and engineering Regina Barzilay and her students reported a system that learned to play a computer game called “Civilization” by analyzing the game manual. But in the new work, on which Barzilay is again a co-author, the machine-learning system has no direct access to the underlying “state” of the game program — the data the program is tracking and how it’s being modified. “When you play these games, every interaction is through text,” says Karthik Narasimhan, an MIT graduate student in computer science and engineering and one of the new paper’s two first authors. “For instance, you get the state of the game through text, and whatever you enter is also a command. It’s not like a console with buttons. So you really need to understand the text to play these games, and you also have more variability in the types of actions you can take.” Narasimhan is joined on the paper by Barzilay, who’s his thesis advisor, and by fellow first author Tejas Kulkarni, a graduate student in the group of Josh Tenenbaum, a professor in the Department of Brain and Cognitive Sciences. They presented the paper last week at the Empirical Methods in Natural Language Processing conference. Gordian “not” The researchers were particularly concerned with designing a system that could make inferences about syntax, which has been a perennial problem in the field of natural-language processing. Take negation, for example: In a text-based fantasy game, there’s a world of difference between being told “you’re hurt” and “you’re not hurt.” But a system that just relied on collections of keywords as a guide to action would miss that distinction. So the researchers designed their own text-based computer game that, though very simple, tended to describe states of affairs using troublesome syntactical constructions such as negation and conjunction. They also tested their system against a demonstration game built by the developers of Evennia, a game-creation toolkit. “A human could probably complete it in about 15 minutes,” Kulkarni says. To evaluate their system, the researchers compared its performance to that of two others, which use variants of a technique standard in the field of natural-language processing. The basic technique is called the “bag of words,” in which a machine-learning algorithm bases its outputs on the co-occurrence of words. The variation, called the “bag of bigrams,” looks for the co-occurrence of two-word units. On the Evennia game, the MIT researchers’ system outperformed systems based on both bags of words and bags of bigrams. But on the homebrewed game, with its syntactical ambiguities, the difference in performance was even more dramatic. “What we created is adversarial, to actually test language understanding,” Narasimhan says. Deep learning The MIT researchers used an approach to machine learning called deep learning, a revival of the concept of neural networks, which was a staple of early artificial-intelligence research. Typically, a machine-learning system will begin with some assumptions about the data it’s examining, to prevent wasted time on fruitless hypotheses. A natural-language-processing system could, for example, assume that some of the words it encounters will be negation words — though it has no idea which words those are. Neural networks make no such assumptions. Instead, they derive a sense of direction from their organization into layers. Data are fed into an array of processing nodes in the bottom layer of the network, each of which modifies the data in a different way before passing it to the next layer, which modifies it before passing it to the next layer, and so on. The output of the final layer is measured against some performance criterion, and then the process repeats, to see whether different modifications improve performance. In their experiments, the researchers used two performance criteria. One was completion of a task — in the Evennia game, crossing a bridge without falling off, for instance. The other was maximization of a score that factored in several player attributes tracked by the game, such as “health points” and “magic points.” On both measures, the deep-learning system outperformed bags of words and bags of bigrams. Successfully completing the Evennia game, however, requires the player to remember a verbal description of an engraving encountered in one room and then, after navigating several intervening challenges, match it up with a different description of the same engraving in a different room. “We don’t know how to do that at all,” Kulkarni says. “I think this paper is quite nice and that the general area of mapping natural language to actions is an interesting and important area,” says Percy Liang, an assistant professor of computer science and statistics at Stanford University who was not involved in the work. “It would be interesting to see how far you can scale up these approaches to more complex domains.” Topics: Research, School of Engineering, Artificial intelligence, Computer science and technology, Computer Science and Artificial Intelligence Laboratory (CSAIL), Electrical engineering and computer science (EECS), Machine learning
Michael Winthrop
October 3, 2015
Since the text entries from the player are taken both as input and commands the game action responses require significant evaluation. I propose that an additional sifting mechanism might have a low priority task of ""similar subject statements of uncertain value"". This is analogous to a ""detective list of clues"". As with short and long term memory, the uncertain value subject statements are either in a rotating file or a permanent file. The permanent file is the understood knowledge (lacking useful purpose) and the short term memory file is the uncertainty list. The short term file should include possibility pointers into the long term file and retrieve pointers from that file of other short term items to that long term item, etc. Since these are tenuous inferences that exceed the use of language, we might create a sort of insight.
Mitagen59
October 12, 2015
Why they do not play World of Warcraft in foreign language? This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
d91703d4f6,"UberEats gives you restaurant recommendations, ratings and favorites - CNET","CNET también está disponible en español. Don't show this again The ride-hailing company's food-delivery service rolls out restaurant ratings, favorites and menu suggestions based on machine learning. Now you'll see crowd sourced restaurant ratings in the UberEats app. UberEats wants to get to know you. The food-delivery service added three new features to its app on Thursday that aim to learn your tastes, preferences and likes. It'll now show ""recommended for you"" menu items, eater-generated restaurant ratings and also let you ""favorite"" certain dining spots. Essentially, UberEats is getting more personal. ""With such a large and growing selection of restaurants on the platform, we're thinking how can we show you what you want,"" said Ambika Krishnamachar, a product manager for UberEats. UberEats is the food-delivery branch of the ride-hailing service Uber. Similar to how people can hail an Uber ride with the push of a button on their smartphone, they can also order food from dozens of local restaurants using the UberEats app. UberEats suggests menu items based on what you've ordered in the past. On-demand food is a competitive space. Besides UberEats, people have a multitude of apps to choose from. There's Caviar, Grubhub, Seamless, DoorDash and Postmates. There's also the old-fashioned choice of simply calling a restaurant and ordering food, although not all restaurants deliver. By making its service more personalized, UberEats aims to stand above the rest. The food delivery service first launched in 2014 in Los Angeles under the moniker UberFresh. Then eventually it morphed into UberEats with an official launch in Toronto, Canada, in December 2015. By the end of 2016, UberEats was is 56 cities worldwide. Today it's in 130 cities and is slated to be in 200 by the end of the year. UberEats is even in some cities that don't have the ride-hailing service, such as Tokyo, Taipei and Seoul. UberEats partners with restaurants, so restaurants can decide what they want to sell on the app. The company charges restaurants a service fee that's calculated as a percentage of its sales on the platform. Uber now has partnerships with 80,000 restaurants, which include everything from upscale eateries to McDonald's. ""With an increasing choice of selection it becomes a little bit more difficult to choose what you want to order,"" said Calvin Lee, UberEats product manager. Uber has created ""technical solutions to solve some of the pain points for our customers."" UberEats' new ""recommended for you"" feature uses machine learning to personalize the app based on what each person is ordering and favoriting. It looks at past restaurant and menu item choices to make suggestions -- much like how Netflix makes movie recommendations. So, if you tend to order pasta, UberEats will likely show you a restaurant's spaghetti, lasagna or tortellini dishes.
""Every single consumer on the platform is super different,"" Krishnamachar said. ""By learning your preferences we can more intelligently suggest food that you like."" UberEats' new restaurant ratings work a little differently. Krishnamachar said ratings aim to better guide eaters in picking restaurants and to help restaurants better learn what dishes are resonating with people. The feature will have a five-star rating system and show feedback over the last 90 days. Sometimes negative ratings can hurt restaurants. Businesses have complained that they've lost customers after receiving bad ratings on sites like Yelp, OpenTable and MenuPages. Krishnamachar said negative ratings shouldn't harm UberEats' partner restaurants since they refresh on a rolling 90-day basis. ""It motivates a high-quality experience across the board,"" she said. ""At the end of the day, we're reflecting what customers are saying."" 'Alexa, be more human': Inside Amazon's effort to make its voice assistant smarter, chattier and more like you. Rebooting the Reef: CNET dives deep into how tech can help save Australia's Great Barrier Reef. Be respectful, keep it clean and stay on topic.
We'll remove comments that violate our policy.Please read our Comment Policy before commenting."
2af9a725ed,How a Math Genius Hacked OkCupid to Find True Love | WIRED,"Climate change means warmer springs and summers, and possibly increases in precipitation as well. Then the rains wi… twitter.com/i/web/status/9… Mathematician Chris McKinlay hacked OKCupid to find the girl of his dreams.
Emily Shur Chris McKinlay was folded into a cramped fifth-floor cubicle in UCLA’s math sciences building, lit by a single bulb and the glow from his monitor. It was 3 in the morn­ing, the optimal time to squeeze cycles out of the supercomputer in Colorado that he was using for his PhD dissertation. (The subject: large-scale data processing and parallel numerical methods.) While the computer chugged, he clicked open a second window to check his OkCupid inbox. McKinlay, a lanky 35-year-old with tousled hair, was one of about 40 million Americans looking for romance through websites like Match.com, J-Date, and e-Harmony, and he’d been searching in vain since his last breakup nine months earlier. He’d sent dozens of cutesy introductory messages to women touted as potential matches by OkCupid’s algorithms. Most were ignored; he’d gone on a total of six first dates. On that early morning in June 2012, his compiler crunching out machine code in one window, his forlorn dating profile sitting idle in the other, it dawned on him that he was doing it wrong. He’d been approaching online matchmaking like any other user. Instead, he realized, he should be dating like a mathematician. OkCupid was founded by Harvard math majors in 2004, and it first caught daters’ attention because of its computational approach to matchmaking. Members answer droves of multiple-choice survey questions on everything from politics, religion, and family to love, sex, and smartphones. On average, respondents select 350 questions from a pool of thousands—“Which of the following is most likely to draw you to a movie?” or “How important is religion/God in your life?” For each, the user records an answer, specifies which responses they’d find acceptable in a mate, and rates how important the question is to them on a five-point scale from “irrelevant” to “mandatory.” OkCupid’s matching engine uses that data to calculate a couple’s compatibility. The closer to 100 percent—mathematical soul mate—the better. But mathematically, McKinlay’s compatibility with women in Los Angeles was abysmal. OkCupid’s algorithms use only the questions that both potential matches decide to answer, and the match questions McKinlay had chosen—more or less at random—had proven unpopular. When he scrolled through his matches, fewer than 100 women would appear above the 90 percent compatibility mark. And that was in a city containing some 2 million women (approximately 80,000 of them on OkCupid). On a site where compatibility equals visibility, he was practically a ghost. He realized he’d have to boost that number. If, through statistical sampling, McKinlay could ascertain which questions mattered to the kind of women he liked, he could construct a new profile that honestly answered those questions and ignored the rest. He could match every woman in LA who might be right for him, and none that weren’t.
Chris McKinlay used Python scripts to riffle through hundreds of OkCupid survey questions. He then sorted female daters into seven clusters, like “Diverse” and “Mindful,” each with distinct characteristics.
Maurico Alejo
Even for a mathematician, McKinlay is unusual. Raised in a Boston suburb, he graduated from Middlebury College in 2001 with a degree in Chinese. In August of that year he took a part-time job in New York translating Chinese into English for a company on the 91st floor of the north tower of the World Trade Center. The towers fell five weeks later. (McKinlay wasn’t due at the office until 2 o’clock that day. He was asleep when the first plane hit the north tower at 8:46 am.) “After that I asked myself what I really wanted to be doing,” he says. A friend at Columbia recruited him into an offshoot of MIT’s famed professional blackjack team, and he spent the next few years bouncing between New York and Las Vegas, counting cards and earning up to $60,000 a year. The experience kindled his interest in applied math, ultimately inspiring him to earn a master’s and then a PhD in the field. “They were capable of using mathema­tics in lots of different situations,” he says. “They could see some new game—like Three Card Pai Gow Poker—then go home, write some code, and come up with a strategy to beat it.” Now he’d do the same for love. First he’d need data. While his dissertation work continued to run on the side, he set up 12 fake OkCupid accounts and wrote a Python script to manage them. The script would search his target demographic (heterosexual and bisexual women between the ages of 25 and 45), visit their pages, and scrape their profiles for every scrap of available information: ethnicity, height, smoker or nonsmoker, astrological sign—“all that crap,” he says. To find the survey answers, he had to do a bit of extra sleuthing. OkCupid lets users see the responses of others, but only to questions they’ve answered themselves. McKinlay set up his bots to simply answer each question randomly—he wasn’t using the dummy profiles to attract any of the women, so the answers didn’t mat­ter—then scooped the women’s answers into a database. McKinlay watched with satisfaction as his bots purred along. Then, after about a thousand profiles were collected, he hit his first roadblock. OkCupid has a system in place to prevent exactly this kind of data harvesting: It can spot rapid-fire use easily. One by one, his bots started getting banned. He would have to train them to act human. He turned to his friend Sam Torrisi, a neuroscientist who’d recently taught McKinlay music theory in exchange for advanced math lessons. Torrisi was also on OkCupid, and he agreed to install spyware on his computer to monitor his use of the site. With the data in hand, McKinlay programmed his bots to simulate Torrisi’s click-rates and typing speed. He brought in a second computer from home and plugged it into the math department’s broadband line so it could run uninterrupted 24 hours a day. After three weeks he’d harvested 6 million questions and answers from 20,000 women all over the country. McKinlay’s dissertation was relegated to a side project as he dove into the data. He was already sleeping in his cubicle most nights. Now he gave up his apartment entirely and moved into the dingy beige cell, laying a thin mattress across his desk when it was time to sleep. For McKinlay’s plan to work, he’d have to find a pattern in the survey data—a way to roughly group the women according to their similarities. The breakthrough came when he coded up a modified Bell Labs algorithm called K-Modes. First used in 1998 to analyze diseased soybean crops, it takes categorical data and clumps it like the colored wax swimming in a Lava Lamp. With some fine-tuning he could adjust the viscosity of the results, thinning it into a slick or coagulating it into a single, solid glob. He played with the dial and found a natural resting point where the 20,000 women clumped into seven statistically distinct clusters based on their questions and answers. “I was ecstatic,” he says. “That was the high point of June.” He retasked his bots to gather another sample: 5,000 women in Los Angeles and San Francisco who’d logged on to OkCupid in the past month. Another pass through K-Modes confirmed that they clustered in a similar way. His statistical sampling had worked. Now he just had to decide which cluster best suited him. He checked out some profiles from each. One cluster was too young, two were too old, another was too Christian. But he lingered over a cluster dominated by women in their mid-twenties who looked like indie types, musicians and artists. This was the golden cluster. The haystack in which he’d find his needle. Somewhere within, he’d find true love. Actually, a neighboring cluster looked pretty cool too—slightly older women who held professional creative jobs, like editors and designers. He decided to go for both. He’d set up two profiles and optimize one for the A group and one for the B group. He text-mined the two clusters to learn what interested them; teaching turned out to be a popular topic, so he wrote a bio that emphasized his work as a math professor. The important part, though, would be the survey. He picked out the 500 questions that were most popular with both clusters. He’d already decided he would fill out his answers honestly—he didn’t want to build his future relationship on a foundation of computer-generated lies. But he’d let his computer figure out how much importance to assign each question, using a machine-learning algorithm called adaptive boosting to derive the best weightings.
Emily Shur (Grooming by Andrea Pezzillo/Artmix Beauty) With that, he created two profiles, one with a photo of him rock climbing and the other of him playing guitar at a music gig. “Regardless of future plans, what’s more interesting to you right now? Sex or love?” went one question. Answer: Love, obviously. But for the younger A cluster, he followed his computer’s direction and rated the question “very important.” For the B cluster, it was “mandatory.” When the last question was answered and ranked, he ran a search on OkCupid for women in Los Angeles sorted by match percentage. At the top: a page of women matched at 99 percent. He scrolled down … and down … and down. Ten thousand women scrolled by, from all over Los Angeles, and he was still in the 90s. He needed one more step to get noticed. OkCupid members are notified when some­one views their pages, so he wrote a new program to visit the pages of his top-rated matches, cycling by age: a thousand 41-year-old women on Monday, another thousand 40-year-old women on Tuesday, looping back through when he reached 27-year-olds two weeks later. Women reciprocated by visiting his profiles, some 400 a day. And messages began to roll in. “I haven’t until now come across anyone with such winning numbers, AND I find your profile intriguing,” one woman wrote. “Also, something about a rugged man who’s really good with numbers … Thought I’d say hi.” “Hey there—your profile really struck me and I wanted to say hi,” another wrote. “I think we have quite a lot in common, maybe not the math but certainly a lot of other good stuff!” “Can you really translate Chinese?” yet another asked. “I took a class briefly but it didn’t go well.” The math portion of McKinlay’s search was done. Only one thing remained. He’d have to leave his cubicle and take his research into the field. He’d have to go on dates. On June 30, McKinlay showered at the UCLA gym and drove his beat-up Nissan across town for his first data-mined date. Sheila was a web designer from the A cluster of young artist types. They met for lunch at a cafe in Echo Park. “It was scary,” McKinlay says. “Up until this point it had almost been an academic exercise.” By the end of his date with Sheila, it was clear to both that the attraction wasn’t there. He went on his second date the next day—an attractive blog editor from the B cluster. He’d planned a romantic walk around Echo Park Lake but found it was being dredged. She’d been reading Proust and feeling down about her life. “It was kind of depressing,” he says. Date three was also from the B group. He met Alison at a bar in Koreatown. She was a screenwriting student with a tattoo of a Fibonacci spiral on her shoulder. McKinlay got drunk on Korean beer and woke up in his cubicle the next day with a painful hangover. He sent Alison a follow- up message on OkCupid, but she didn’t write back. The rejection stung, but he was still getting 20 messages a day. Dating with his computer-endowed profiles was a completely different game. He could ignore messages consisting of bad one-liners. He responded to the ones that showed a sense of humor or displayed something interesting in their bios. Back when he was the pursuer, he’d swapped three to five messages to get a single date. Now he’d send just one reply. “You seem really cool. Want to meet?” By date 20, he noticed latent variables emerging. In the younger cluster, the women invariably had two or more tattoos and lived on the east side of Los Angeles. In the other, a disproportionate number owned midsize dogs that they adored. His earliest dates were carefully planned. But as he worked feverishly through his queue, he resorted to casual afternoon meetups over lunch or coffee, often stacking two dates in a day. He developed a set of personal rules to get through his mara­thon love search. No more drinking, for one. End the date when it’s over, don’t let it trail off. And no concerts or movies. “Nothing where your attention is directed at a third object instead of each other,” he says. “It’s inefficient.”
Love is a Data Field McKinlay’s code found that the women clustered into statistically identifiable groups who tended to answer their OkCupid survey questions in similar ways. One group, which he dubbed the Greens, were online dating newbies; another, the Samanthas, tended to be older and more adventuresome. Here’s how each cluster answered four of the most popular questions. The Questions (1) About how long do you want your next relationship to last?
One night
A few months to a year
Several years
The rest of my life (2) Say you’ve started seeing someone you really like. As far as you’re concerned, how long will it take before you have sex?
1-2 dates
3-5 dates
6 or more dates
Only after the wedding (3) Have you ever had a sexual encounter with someone of the same sex?
Yes, and I enjoyed myself
Yes, and I did not enjoy myself
No, and I would never
No, but I’d like to (4) How important is religion/God in your life?
Extremely important
Somewhat important
Not very important
Not important at all After a month of dating equally from both of his profiles, he decided he was spending too much time on the freeway reaching east-side women from the tattoo cluster. He deleted his A-group profile. His efficiency improved, but the results were the same. As summer drew to a close, he’d been on more than 55 dates, each one dutifully logged in a lab notebook. Only three had led to second dates; only one had led to a third. Most unsuccessful daters confront self-esteem issues. For McKinlay it was worse. He had to question his calculations. Then came the message from Christine Tien Wang, a 28-year-old artist and prison abolition activist. McKinlay had popped up in her search for 6-foot guys with blue eyes near UCLA, where she was pursuing her master’s in fine arts. They were a 91 percent match. He met her at the sculpture garden on campus. From there they walked to a college sushi joint. He felt it immediately. They talked about books, art, music. When she confessed that she’d made some tweaks to her profile before messaging him, he responded by telling her all about his love hacking. The whole story. “I thought it was dark and cynical,” she says. “I liked it.” It was first date number 88. A second date followed, then a third. After two weeks they both suspended their OkCupid accounts. “I think that what I did is just a slightly more algorithmic, large-scale, and machine-learning-based version of what everyone does on the site,” McKinlay says. Everyone tries to create an optimal profile—he just had the data to engineer one. It’s one year after their first date, and McKinlay and Tien Wang have met me at the Westwood sushi bar where their relationship began. McKinlay has his PhD; he’s teaching math and is now working on a postgraduate degree in music. Tien Wang was accepted into a one-year art fellowship in Qatar. She’s in California to visit McKinlay. They’ve been staying connected on Skype, and she has returned for a couple of visits. At my request, McKinlay has brought his lab notebook. Tien Wang hasn’t seen it before today. It’s page after page of formulas and equations in McKinlay’s tight handwriting, ending in a neatly ordered list of women and dates, a few terse notes about each. Tien Wang leafs through it, laughing at some of the highlights. On August 24, she notices, he took two women to the same beach on the same day. “That’s horrible,” she says. To Tien Wang, McKinlay’s OkCupid hacking is a funny story to tell. But all the math and coding is merely prologue to their story together. The real hacking in a relationship comes after you meet. “People are much more complicated than their profiles,” she says. “So the way we met was kind of superficial, but everything that happened after is not superficial at all. It’s been cultivated through a lot of work.” “It’s not like, we matched and therefore we have a great relationship,” McKinlay agrees. “It was just a mechanism to put us in the same room. I was able to use OkCupid to find someone.” She bristles at that. “You didn’t find me. I found you,” she says, touching his elbow. McKinlay pauses to think, then admits she’s right. A week later Tien Wang is back in Qatar, and the couple is on one of their daily Skype calls when McKinlay pulls out a diamond ring and holds it up to the webcam. She says yes. They’re not entirely sure when they’ll get married. There’s research to be done to determine the optimal wedding day. Climate change means warmer springs and summers, and possibly increases in precipitation as well. Then the rains wi… twitter.com/i/web/status/9… Climate change means warmer springs and summers, and possibly increases in precipitation as well. Then the rains wi… twitter.com/i/web/status/9… We get it: Ads aren’t what you’re here for. But ads help us keep the lights on. So, add us to your ad blocker’s whitelist or pay $1 per week for an ad-free version of WIRED. Either way, you are supporting our journalism. We’d really appreciate it. Already a member? Log in All of us at WIRED appreciate your support! CNMN Collection Use of this site constitutes acceptance of our user agreement (effective 3/21/12) and privacy policy (effective 3/21/12). Affiliate link policy. Your California privacy rights. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast."
b5d8f9ebc3,Mckinsey used machine learning to discover the best way to teach science — Quartz,"There is a long-standing, red-hot debate in educational circles about the most effective way to teach kids. Some favor more traditional teacher-directed methods, with the teacher presenting materials and responding to questions about it. Others advocate for inquiry-based learning—where students drive their own learning (pdf) through discovery and exploration, working with peers and developing their own ideas—arguing it results in deeper, and more meaningful learning. The two are sometimes pitted against each other as “sage on a stage” (teacher directed) vs. “guide on the side” (student-led, or inquiry based). Both cite ample evidence to prove the superiority of their method (see here for teacher-directed, and here for inquiry-based). McKinsey applied machine learning to the world’s largest student database to try and come up with a more scientific answer. The bottom line: A mixture of the two methods is best, but between the two, teacher-directed came out stronger. In all five regions of the world, scores were generally higher when teachers took the lead. “The more frequently teacher-directed happens, the better students do,” said Marc Krawitz,
an associate partner at McKinsey. Conversely, “Student outcomes tend to decline with inquiry-based, as it is increased in isolation.” The data come from the Organisation for Economic Co-operation and Development, which tests 15-year-olds around the world on mathematics, reading, and science every three years (the Programme for International Student Assessment, or PISA). McKinsey used the 2015 test, which focused on science and covered more than half a million students across 72 countries. The greatest learning gains—+26 points on the PISA test—happened when “many-to-all” of the lessons were teacher-directed and “some to many” were inquiry-based. In other words, a healthy combination was best. But the bottom left quadrant shows that inquiry-based without a lot of teacher direction is definitively not helpful, at least as far as the PISA test goes, whereas students in all-teacher-led systems still scored moderately higher than the baseline. The authors offers two potential explanations for why teacher-directed produces better scores. First, “students cannot progress to inquiry-based methods without a strong foundation of knowledge, gained through teacher-directed learning,” the report says. Second, inquiry-based teaching is harder to do, and teachers who try it without adequate training and support will struggle. McKinsey’s report won’t settle the debate. A PISA score is not a perfect measure of a good education. The test is only administered in 72 countries, and plenty of people argue PISA itself is flawed. And there’s an important caveat: The report notes that inquiry-based teaching increases students’ joy in science significantly more than teacher-directed learning does. (Teacher-directed is also positively correlated with joy, though the impact is less.) Inquiry-based also helps convince students that science is worthwhile for their future careers. Since passion often results in perseverance, which can lead to better student (and life) outcomes, joy matters. Also, joy just matters. “Inquiry-based practices have a stronger positive affect on students’ joy in science and their beliefs that doing well in school will help them have a brighter future,” said Mona Mourshed, lead author of the report and global head of the education practice at McKinsey. “That’s why, across all regions, blending teacher-directed instruction with inquiry-based learning produces the greatest overall benefit.” Good teachers of course draw on multiple methods, depending on the subject and the learner. And good teachers also know that most things in life—from the value of standardized tests, to ideological debates about teaching—are not black and white."
2362ee3540,How Quantum Machine Learning will solve problems once thought out of reach | BetaKit,"Quantum computers will transform many fields, and the impact on optimization and machine learning (ML) will be among the most profound. In recent years, machine learning has taken off thanks partly to the acceleration in computing hardware, but there is a long list of valuable technological problems that cannot be solved simply because we are too limited by the computer power of our contemporary digital computers. Quantum machine learning sits at the intersection of quantum information processing and machine learning to solve complex problems inefficient to work on with a classical system. Quantum machine learning is a new field that has recently emerged and may have an answer to some of these problems. It is the science and technology at the intersection of quantum information processing and machine learning. Powered by quantum hardware systems, quantum-enhanced machine learning can solve complex problems that are inefficient to work on with a classical system. In other words, quantum-enhanced learning algorithms will complement current deep learning and other machine learning approaches, as opposed to competing with them or replacing them. This will lead to more accurate and sometimes “explainable” predictions. Take the three-billion-dollar Human Genome Project, designed to determine the DNA sequence of the entire human genome. Although the project was declared complete in 2003, scientists only had the faintest glimmer of what their information was really telling them, especially because they were immersed with large volumes of data that could take years to process and analyze. Diseases such as cancer each may have millions of DNA rearrangements compared to a normal cell, and even today we cannot fully read these rearrangements and process our data into useful medical insight. A major barrier to sequencing more personal human genomes and analyzing this data has been computational power. This is an example of how the slope of progress in many fields of science that rely on computers, including genetics in the example above, have been characterized by an increase in computational power of computers. At Creative Destruction Lab’s Quantum Machine Learning program, companies are solving a mixture of problems: quantum-assisted semantic search engines, computationally designed drugs, simulated new chemical molecules, and redesigning the video rendering industry to name a few.
Building a quantum machine learning company has its obstacles. Applying quantum-enhanced machine learning is not equivalent to running our current algorithms on a quantum computer. There are some major obstacles that need to be overcome. First, we need to prepare the classical data before being able to use it on a quantum computer, a process that is called quantum state preparation. Second, we have to develop appropriate programming languages for quantum computers to interact with them. Finally, the output produced by quantum computers is not immediately interpretable; for us to have a classical understanding of these results we have to perform an operation called “measurement” of the quantum state that is the result of a computation.
The execution of quantum algorithms requires hardware that is not yet available. The technical complexities and software issues make implementing the technology daunting. Besides these specific challenges, the execution of quantum algorithms requires hardware that is not yet available. Current hardware cannot run QML algorithms at a scale required to commercialize the technology. Even though smaller quantum computers, larger special-purpose quantum simulators, and annealers may have potential use in machine learning and data analysis, the hardware challenges taken together with software issues may take three to five years to solve. These and many other technical complexities make implementing such technologies daunting.
That said, quantum machine learning can have significant benefits. One example is in the materials space. Designing new chemical compounds or materials is a time consuming and extremely expensive process: it can take anywhere between a few months to a few decades depending on the complexity of product being designed. Even the most advanced computational methods fail to accurately predict all the properties of the final product. The difficulty is due to a concept that is called the “many-body problem” in physics. This problem arises when you have an assembly of objects such as electrons or nuclei whose interaction is so complex that it is almost impossible to calculate these interactions analytically and simulate the chemical compounds on a computer.
Just recently, a group of researchers from Switzerland, showed that, in fact, a quantum computer with just 100 to 200 quantum bits can calculate reaction mechanism of a complex chemical reaction in a few days. The researchers chose an enzyme called nitrogenase, which enables certain microorganisms to split atmospheric nitrogen molecules. Adding quantum machine learning to this kind of quantum simulation could lead to the discovery of new chemical compounds. Another example of a problem where quantum technologies may yield a significant benefit is probabilistic sampling, which is the most natural operation to do on a quantum system. This allows us to build companies that use probabilistic graphical models, which would provide a construction that would tell you exactly why they arrived at a certain condition, leading towards explainable AI. For instance, instead of just predicting which way the price of a stock will move, a probabilistic model is able to justify the prediction with a confidence rate and possibly with an explanation. This is achieved by explicit modelling of random variables and by performing probabilistic inference, the latter which relies on sampling that can be boosted by a quantum device. Yet these examples are just a glimpse of where quantum enhanced machine learning algorithms can provide a significant lift. Machine learning relies heavily on the computing power and quantum hardware that can enable solving complex problems across a range of industries such as financial services, molecular biology, quantum chemistry, security, and logistics to name a few. At the Creative Destruction Lab Quantum Machine Learning initiative we are starting early to foster the startups that will help solve complex problems once considered out of our reach. This article was jointly authored by members of Creative Destruction Lab.
NYC-based Movenbank launched its financial scoring system in public beta today to give consumers a real-time measure of their financial health, and announced that…"
dc0a94cb7d,Applying Machine Learning Clustering and Classification to Predict Banana Ripeness States and Shelf Life | Thor | International Journal of Advanced Food Science and Technology,"Nandan Thor
Master of Science, Industrial Engineering Candidate, California Polytechnic State University, San Luis Obispo		United States
Food waste accounts for over $15 billion annually. Not only is food waste a financial problem, it is also an ethical problem. The use of machine learning algorithms for clustering and classification provide an opportunity to help reduce food waste. K-Means clustering is proposed to determine banana ripeness states and the Decision Tree Classifier algorithm is proposed to classify banana shelf-life. An experiment is undertaken to provide data by imaging bananas and extracting color features using computer vision. The resultant data is then clustered to determine banana ripeness states. The states are used to determine the end-point of data collection. Seven different machine learning classification algorithms are tested to classify fruit shelf-life. The most accurate classifier is the Decision Tree Classifier which has an accuracy around 52%. The combination of machine learning algorithms and big data analysis becomes a powerful tool in working to reduce food waste."
59c653d9fe,How Machine Learning Fuels Your Netflix Addiction - RTInsights,"Every recommended video you see on Netflix was selected by an algorithm. The company estimates its algorithms produce $1 billion a year in value from customer retention. When intuition fails, data from machine learning can win, according to a recent paper describing Netflix’s  recommendations system. On a Netflix screen, a user is presented with about 40 rows of video categories, with each row containing up to 75 videos, according to the paper, which was published in the Dec. 2015 issue of ACM Transactions on Management Information Systems (TMIS). While one would think such a large selection would keep viewers browsing, too much choice can be counterproductive. Typically a viewer loses interest after perhaps 60 to 90 seconds of trying to find a video to watch, write authors Carlos A. Gomez-Uribe and Neil Hunt of Netflix. Like any digital business, or even a supermarket with rows of breakfast cereal, Netflix has little time to catch the customer’s attention. The training and evaluation of algorithms takes place offline, while A/B testing takes place online. Image adapted from ACM Historically, Netflix relied heavily on customer ratings of videos when shipping DVDs by mail. Now Netflix has access to a much broader set of data—what each member watches, when they watch, the place on the Netflix screen the customer found the video, recommendations the customer didn’t choose, and the popularity of videos in the catalog. All of this data gets fed into several algorithms powered by statistical and machine-learning techniques. Approaches use both supervised (classification, regression) and unsupervised (dimensionality reduction through clustering or compression) approaches, according to the authors. A video-to-video similarity algorithm, or Sims, makes recommendations in the “Because You Watched” row. A personalized video ranker algorithm, or PVR, selects the order of videos in genre rows, using an arbitrary subset of the Netflix catalog. As Gomez-Uribe told Wired, “The closer to the first position on a row a title is, the more likely it will get played.” But PVR works better when it’s mixed with “unpersonalized popularity,” he and Hunt wrote in their paper. As an example, the authors describe recommendations for shows similar to “House of Cards.” While one might think that political or business dramas such as “The West Wing” or “Mad Men” would increase customer engagement, it turns out that popular but outside-of-genre titles such as “Parks and Recreation” and “Orange Is the New Black” fared better. The authors call this a case of “intuition failure.” Another algorithm is the “Top N ranker,” which makes recommendations in the “Top Picks” row. A “Trending Now” row uses short-term trends, such as an interest in holiday movies or films driven by weather events. These short-term trends are “powerful predictors of videos that our members will watch, especially when combined with the right dose of personalization,” the authors write.
The authors write that over the years, the recommendation system has decreased customer churn by several percentage points and saves the company about $1 billion a year. Personalized content also helps “find an audience even for relatively niche videos that would not make sense for broadcast TV models because their audiences would be too small to support significant advertising revenue, or to occupy a broadcast or cable channel time slot,” according to the paper. “This is very evident in our data, which show that our recommender system spreads viewing across many more videos much more evenly than would an unpersonalized system.” The paper also describes how Netflix uses “evidence” algorithms, which focus on what information to show a viewer about a movie (such as whether it won an Oscar); search algorithms; how Netflix performs A/B testing on its algorithms, and opportunities for improvement in testing. A copy of the paper can be found here.   Chris Raphael (full bio)
covers fast data technologies and business use cases for real-time analytics. Follow him on Twitter at raphaelc44."
f11ff176c5,Alibaba's FashionAI shows how machine learning might save the mall,"TNW uses cookies to personalize content and ads to
make our site easier for you to use.
We do also share that information with third parties for
advertising & analytics.
by Tristan Greene
—
in Artificial Intelligence
Alibaba’s sales from Saturday’s Singles Day event exceeded 25 billion dollars, more than quadruple what Americans spent last year during Black Friday. While the majority of those sales undoubtedly came via online purchases, the company also quietly experimented with an AI-powered project designed to woo offline shoppers. FashionAI was developed by Alibaba researchers in order to provide a recognizable interface for customers to use while trying on clothes. It’s a basic screen interface that uses machine-learning to make clothing and accessory suggestions to customers based on the items they are trying on. There’s no camera; it uses information embedded in the item’s tag to make the recommendations. Using the system, customers can try clothes on, receive fashion tips and suggestions from the AI, then make choices on-screen. If a user wants to try something different, or add other items, a store attendant can be summoned with the press of a button. Deep learning allows the AI to make connections in real-time by accessing massive quantities of data and making ‘smart’ decisions. When you’re on Amazon, for example, looking at a pair of cowboy boots might prompt the algorithm to recommend hiking or biker boots. But, search for a cowboy hat too, and you’re likely to be recommended other cowboy themed items, like belts, as opposed to just more hats and boots. The idea is that eventually the AI will get better at determining what you’re going to look at, what you’re going to compare it with, what you’ll want to purchase with it, and which items you’ll actually end up buying. Where no staff of humans could possibly be expected to remember the personal shopping preferences of every single customer, AI can. Alibaba’s FashionAI may have been nothing more than an experiment at this point, but it’s an exciting one. It’s taken the weirdness out of ideas like Amazon’s AI-powered shops and turned it into something more palatable. Most attempts at changing the way brick-and-mortar stores operate have revolved around trying to cram the online experience into kiosks, lobbies, and the checkout area. The trade-off is typically one where customers invest their own time and effort into a system that doesn’t quite work as well as logging on to Amazon or Alibaba with a smartphone does. By placing a screen against a wall in a changing booth, Alibaba integrated its already successful shopping system into the real-world seamlessly. If brick-and-mortar stores are to remain viable, they’ll have to leverage consumer attention by giving us all a reason to leave our phones in our pockets when we shop. People are probably getting sick of being told that Amazon and Alibaba can afford to sell items cheaper because they don’t have overhead. That doesn’t make most of us feel better about being overcharged. It feels like we’re being charged for the ‘privilege’ of standing in line and carrying our own bags through stores and parking lots, at most brick-and-mortar stores. If AI can be harnessed to bring the brick-and-mortar experience more closely in line with shopping on Alibaba.com, we all stand to benefit. Putting up AI-powered screens in dressing rooms is an important first step, and one that may ultimately save a lot of human jobs. Instead of replacing people with robots, Alibaba’s FashionAI integrates humans and machines in a way that provides the customer with everything they need. And the customer is always right.
Alibaba’s AI Fashion Consultant Helps It Set a New Singles’ Day Record
on MIT Technology Review
Read next:
5 tools for improving the efficiency (and conversion power) of your call center
Stay tuned with our weekly recap of what’s hot & cool by our CEO Boris.
Join over 260,000 subscribers!
Sit back and let the hottest tech news come to you by the magic of electronic mail.
Prefer to get the news as it happens? Follow us on social media.
1.76M followers
1M likes
Got two minutes to spare? We'd love to know a bit more about our readers.
Start!
All data collected in the survey is anonymous."
d5c9213078,Machine Learning: Helping Determine How a Drug Affects the Brain | Technology Networks,"News   Nov 16, 2017
| Original story from University College London
Gaze recovery high-dimensional classifier weights. Represented as 3D cubic glyphs varying in colour and scale are the weights of a transductive linear support vector machine classifier trained to relate the high-dimensional pattern of damage to gaze outcome, achieving k-fold cross-validation performance of 78.33% (SE = 1.70%) sensitivity and 82.78% (SE = 0.56%) specificity for distinguishing between patients who recovered from a leftward deviation of gaze and those who did not. Positive weights (dark blue to cyan) favour recovery, negative weights (dark red to yellow) persistence of symptoms. Though hemispheric asymmetry is prominent, note the distribution of weights is highly complex, as one would expect from the complexity of the functional and lesional architectures that generate the critical pattern. Credit: https://doi.org/10.1093/brain/awx288
Machine learning could improve our ability to determine whether a new drug works in the brain, potentially enabling researchers to detect drug effects that would be missed entirely by conventional statistical tests, finds a new UCL study published in Brain.""Current statistical models are too simple. They fail to capture complex biological variations across people, discarding them as mere noise. We suspected this could partly explain why so many drug trials work in simple animals but fail in the complex brains of humans. If so, machine learning capable of modelling the human brain in its full complexity may uncover treatment effects that would otherwise be missed,"" said the study's lead author, Dr Parashkev Nachev (UCL Institute of Neurology).To test the concept, the research team looked at large-scale data from patients with stroke, extracting the complex anatomical pattern of brain damage caused by the stroke in each patient, creating in the process the largest collection of anatomically registered images of stroke ever assembled. As an index of the impact of stroke, they used gaze direction, objectively measured from the eyes as seen on head CT scans upon hospital admission, and from MRI scans typically done 1-3 days later.They then simulated a large-scale meta-analysis of a set of hypothetical drugs, to see if treatment effects of different magnitudes that would have been missed by conventional statistical analysis could be identified with machine learning. For example, given a drug treatment that shrinks a brain lesion by 70%, they tested for a significant effect using conventional (low-dimensional) statistical tests as well as by using high-dimensional machine learning methods.The machine learning technique took into account the presence or absence of damage across the entire brain, treating the stroke as a complex ""fingerprint"", described by a multitude of variables.""Stroke trials tend to use relatively few, crude variables, such as the size of the lesion, ignoring whether the lesion is centred on a critical area or at the edge of it. Our algorithm learned the entire pattern of damage across the brain instead, employing thousands of variables at high anatomical resolution. By illuminating the complex relationship between anatomy and clinical outcome, it enabled us to detect therapeutic effects with far greater sensitivity than conventional techniques,"" explained the study's first author, Tianbo Xu (UCL Institute of Neurology).The advantage of the machine learning approach was particularly strong when looking at interventions that reduce the volume of the lesion itself. With conventional low-dimensional models, the intervention would need to shrink the lesion by 78.4% of its volume for the effect to be detected in a trial more often than not, while the high-dimensional model would more than likely detect an effect when the lesion was shrunk by only 55%.""Conventional statistical models will miss an effect even if the drug typically reduces the size of the lesion by half, or more, simply because the complexity of the brain's functional anatomy--when left unaccounted for--introduces so much individual variability in measured clinical outcomes. Yet saving 50% of the affected brain area is meaningful even if it doesn't have a clear impact on behaviour. There's no such thing as redundant brain,"" said Dr Nachev.The researchers say their findings demonstrate that machine learning could be invaluable to medical science, especially when the system under study--such as the brain--is highly complex.""The real value of machine learning lies not so much in automating things we find easy to do naturally, but formalising very complex decisions. Machine learning can combine the intuitive flexibility of a clinician with the formality of the statistics that drive evidence-based medicine. Models that pull together 1000s of variables can still be rigorous and mathematically sound. We can now capture the complex relationship between anatomy and outcome with high precision,"" said Dr Nachev.""We hope that researchers and clinicians begin using our methods the next time they need to run a clinical trial,"" said co-author Professor Geraint Rees (Dean, UCL Faculty of Life Sciences). This article has been republished from materials provided by University College London. Note: material may have been edited for length and content. For further information, please contact the cited source.ReferenceXu, T., Jäger, H. R., Husain, M., Rees, G., & Nachev, P. (2017). High-dimensional therapeutic inference in the focally damaged human brain. Brain. doi:10.1093/brain/awx288
A Virus-Like Protein is Important for Cognition and Memory
A protein involved in cognition and storing long-term memories looks and acts like a protein from viruses.
Genetic ‘Switches’ that Guide Human Brain Development Mapped
Mapping the gene regulation in human neurogenesis reveals factors that govern the growth of our brains and, in some cases, set the stage for several brain disorders that appear later in life.
Sensory Interneurons Created from Stem Cells
Researchers at the Eli and Edythe Broad Center of Regenerative Medicine and Stem Cell Research at UCLA have, for the first time, coaxed human stem cells to become sensory interneurons — the cells that give us our sense of touch.
Like what you just read? You can find similar content on the communities below.
To personalize the content you see on Technology Networks homepage, Log In or Subscribe for Free
News   Jan 12, 2018 News   Jan 12, 2018 Poster Article News   Jan 11, 2018 News   Jan 11, 2018"
71e9a453e0,"Gaussian Processes for Timeseries Modelling | Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences","Institution: National University of Ireland, Maynooth In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches. If we are to take full advantage of the richness of scientific data available to us, we must consider a principled framework under which we may reason and infer. To fail to do this is to ignore uncertainty and risk false analysis, decision-making and forecasting. What we regard as a prerequisite for intelligent data analysis is ultimately concerned with the problem of computing in the presence of uncertainty. Considering data analysis under the mathematics of modern probability theory allows us to exploit a profound framework under which information, uncertainty and risk for actions, events and outcomes may be readily defined. Much recent research hence focuses on the principled handling of uncertainty for modelling in environments that are dynamic, noisy, observation costly and time sensitive. The machinery of probabilistic inference brings to the field of time-series analysis and monitoring robust, stable, computationally practical and principled approaches that naturally accommodate these real-world challenges. As a framework for reasoning in the presence of uncertain, incomplete and delayed information, we appeal to Bayesian inference. This allows us to perform robust modelling even in highly uncertain situations, and has a long pedigree in inference. Being able to include measures of uncertainty allows, for example, us to actively select where and when we would like to observe samples and offers approaches by which we may readily combine information from multiple noisy sources. This paper favours the conceptual over the mathematical (of course, the mathematical details are important and elegant but would obscure the aims of this paper; the interested reader is encouraged to read the cited material and a canonical text such as Rasmussen & Williams [1]). We start §2 with a short overview of why Bayesian modelling is important in time-series analysis, culminating in arguments that provoke us to use non-parametric models. Section 3 presents a conceptual overview of a particular flavour of non-parametric model, the Gaussian process (GP), which is well suited to time-series modelling [1]. We discuss in more detail the role of covariance functions, the influence they have on our models and explore, by example, how the (apparently subjective) function choices we make are in fact motivated by domain knowledge. Section 5 presents real-world time-series examples, from sensor networks, changepoint data and astronomy, to highlight the practical application of GP models. The more mathematical framework of inference is detailed in §4. We start by casting time-series analysis into the format of a regression problem, of the form y(x)=f(x)+η, in which f() is a (typically) unknown function and η is a (typically white) additive noise process. The goal of inference in such problems is twofold: firstly to evaluate the putative form of f() and secondly to evaluate the probability distribution of y* for some x*, i.e. p(y*|x*). To enable us to perform this inference, we assume the existence of a dataset of observations, typically obtained as input–output pairs,
for example. For the purposes of this study, we make the tacit assumption that the inputs xi (representing, e.g. time locations of samples) are known precisely, i.e. there is no input noise, but that observation noise is present on the yi. When we come to analyse time-series data, there are two approaches we might consider. The first function mapping and the second curve fitting. The mapping approach considers inference of a function f that maps some observed x to an outcome variable y without explicit reference to the (time) ordering of the data. For example, if we choose x to be a datum in a time series, and y to be the next datum, then inferring f(x) models the relationship between one datum and its successor. Problematically, the mapping is (typically) static, so poorly models non-stationary time series, and there is difficulty in incorporating time-series domain knowledge, such as beliefs about smoothness and continuity. Furthermore, if the periods between samples are uneven, this approach fails to accommodate this knowledge with ease. Curve fitting, on the other hand, makes the tacit assumption that y is ordered by x, the latter normally taken to be the time variable, with inference proceeding by fitting a curve to the set of x,y points. Prediction, for example, is thence achieved by extrapolating the curve that models the observed past data. The relationship between x and y is hence not fixed, but conditioned on observed data that (typically) lies close, in time, to the point we are investigating. In this study, we make the decision to concentrate on this approach, as we believe it offers a more profound model for much of the time-series data we are concerned with. As a simple example to introduce the canonical concepts of Bayesian modelling, we consider a small set of data samples, located at x=0,1,2, and associated observed target values. Least-squares regression on this data using a simple model (based on polynomial splines) gives rise to the curve shown as the line in figure 1a. We see that, naturally, this curve fits our observed data very well. What about the credibility of the model in regions where we see no data, importantly x>2? If we look at a larger set of example curves from the same model, we obtain a family of curves that explains the observed data identically yet differ very significantly in regions where we have no observations, both interpolating between sample points, and in extrapolation. This simple example leads naturally to us considering a distribution of curves. Working with some distribution over the curves, each of which offers an explanation for the observed data, is central to Bayesian modelling. We note that curves which lie towards the edges of this distribution have higher average curvature than those which lie close to the middle. In the simple example under consideration, there is an intimate relationship between curvature, complexity and Bayesian inference, leading naturally to posterior beliefs over models being a combination of how well observed data are explained and how complex the explanatory functions are. This elegant formalism encodes in a single mathematical framework such ideas as Occam’s razor, such that simple explanations of observed data are favoured.
A simple example of curve fitting. (a) The least-squares fit of a simple spline to the observed data (circles). (b) Example curves with identical fit to the data as the least-squares spline. These curves have high similarity close to the data yet high variability in regions of no observations, both interpolating and, importantly for time series, as we extrapolate beyond x=2. (Online version in colour.) The simple example above showed that there are many functions which can equally well explain data that we have observed. How should we choose from the bewildering array of mathematical functions that give rise to such explanatory curves? If we have strong prior knowledge regarding a system, then this (infinite-dimensional) function space may be reduced to a single family; perhaps the family of quartic polynomials may be the right choice. Such models are considered to be parametric, in the sense that a finite number of unknown parameters (in our polynomial example, these are the coefficients of the model) need to be inferred as part of the data modelling process. Although there is a very large literature (rightly so) on such parametric modelling methods, there are many scenarios in which we have little, or no, prior knowledge regarding appropriate models to use. We may, however, have seemingly less specific domain knowledge; for example, we may know that our observations are visible examples from an underlying process that is smooth, continuous and variations in the function take place over characteristic time scales (not too slowly yet not so fast) and have typical amplitude. Surprisingly, we may work mathematically with the infinite space of all functions that have these characteristics. Furthermore, we may even contemplate probability distributions over this function space, such that the work of modelling, explaining and forecasting data is performed by refining these distributions, so focusing on regions of the function space that are excellent contenders to model our data. As these functions are not characterized with explicit sets of parameters to be inferred (unlike our simple polynomial example, in which sets of coefficients need to be evaluated), this approach is referred to as a branch of non-parametric modelling.1 As the dominant machinery for working with these models is that of probability theory, they are often referred to as Bayesian non-parametric models. We now focus on a particular member, namely the GP. We start this introduction to GPs by considering a simple two-variable Gaussian distribution, which is defined for variables x1,x2 say, by a mean and a 2×2 covariance matrix, which we may visualize as a covariance ellipse corresponding to equal probability contours of the joint distribution p(x1,x2). Figure 2 shows an example of a two-dimensional distribution as a series of elliptical contours. The corresponding marginal distributions p(x1) and p(x2) are shown as ‘projections’ of this along the x1 and x2 axes (solid black lines). We now consider the effect of observing one of the variables such that, for example, we observe x1 at the location of the dashed vertical line in the figure. The resultant conditional distribution p(x2|x1=known), indicated by the dash-dotted curve, now deviates significantly from the marginal p(x2). Because of the relationship between the variables implied by the covariance, knowledge of one shrinks our uncertainty in the other.
The conceptual basis of GPs starts with an appeal to simple multi-variate Gaussian distributions. A joint distribution (covariance ellipse) forms marginal distributions p(x1),p(x2) that are vague (black solid line). Observing x1 at a value indicated by the vertical dashed line changes our beliefs about x2, giving rise to a conditional distribution (black dashed-dot line). Knowledge of the covariance lets us shrink uncertainty in one variable, based on observation of the other. (Online version in colour.) To see the intimate link between this simple example and time-series analysis, we represent the same effect in a different format. Figure 3 shows the mean (black line) and ±σ (grey-shaded region) for p(x1) and p(x2). Figure 3a depicts our initial state of ignorance and figure 3b after we observe x1. Note how the observation changes the location and uncertainty of the distribution over x2. Why stop at only two variables? We can extend this example to arbitrarily large numbers of variables, the relationships between which are defined by an ever larger covariance. Figure 4 shows the posterior distribution for a 10 day example in which observations are made at locations 2, 6 and 8. Figure 4a shows the posterior mean and ±σ as in our previous examples. Figure 4b extends the posterior distribution evaluation densely in the same interval (here, we evaluate the distribution over several hundred points). We note that the ‘discrete’ distribution is now rather continuous. In principle, we can extend this procedure to the limit in which the locations of the xi are infinitely dense (here, on the real line) and so the infinite joint distribution over them all is equivalent to a distribution over a function space. In practice, we will not need to work with such infinite spaces, it is sufficient that we can choose to evaluate the probability distribution over the function at any location on the real line and that we incorporate any observations we may have at any other points. We note, crucially, that the locations of observations and points we wish to investigate the function are not constrained to lie on any predefined sample points; hence, we are working in continuous time with a GP.
The change in distributions on x1 and x2 is here presented in a form more familiar to time-series analysis. (a) The initial, vague, distributions (the black line showing the mean and the grey shading ±σ) and (b) subsequent to observing x1. The distribution over x2 has become less uncertain and the most-likely ‘forecast’ of x2 has also shifted. (a) The posterior distribution (the black line showing the mean and the grey shading ±σ) for a 10 day example, with observations made at locations 2, 6 and 8. (b) Evaluates the posterior densely in the interval [1,10] showing how arbitrarily dense evaluation gives rise to a ‘continuous’ posterior distribution with time. As we have seen, the covariance forms the beating heart of GP inference. How do we formulate a covariance over arbitrarily large sets? The answer lies in defining a covariance kernel function, k(xi,xj), which provides the covariance element between any two (arbitrary) sample locations, xi and xj say. For a set of locations x={x1,x2,…,xn}, we hence may define the covariance matrix as
3.1
This means that the entire function evaluation, associated with the points in x, is a draw from a multi-variate Gaussian distribution,
3.2
where y={y1,y2,…,yn} are the dependent function values, evaluated at locations x1,…,xn, and μ is a mean function, again evaluated at the locations of the x variables (which we will briefly revisit later). If we believe that there is noise associated with the observed function values, yi, then we may fold this noise term into the covariance. As we expect noise to be uncorrelated from sample to sample in our data, so the noise term adds only to the diagonal of K, giving a modified covariance for noisy observations of the form
3.3
where I is the identity matrix and σ2 is a hyperparameter representing the noise variance. How do we evaluate the GP posterior distribution at some test datum, x* say? We start with considering the joint distribution of the observed data
(consisting of x and associated values y) augmented by x* and y*,
3.4
where K(x,x*) is the column vector formed from k(x1,x*),…,k(xn,x*) and K(x*,x) is its transpose. We find, after some manipulation, that the posterior distribution over y* is Gaussian with mean and variance given by
3.5
and
3.6
We may readily extend this to infer the GP at a set of locations outside our observations, at x* say, to evaluate the posterior distribution of y(x*). The latter is readily obtained once more by extending the above equations and using standard results for multi-variate Gaussians. We obtain a posterior mean and variance given by
3.7
where
3.8
and
3.9
in which we use the shorthand notation for the covariance, K(a,b), defined as
3.10
If we believe (and in most situations we do) that the observed data are corrupted by a noise process, we would replace the K(x,x) term above with, for example, V(x,x) from equation (3.3) above. What should the functional form of the kernel function k(xi,xj) be? To answer this, we will start by considering what the covariance elements indicate. In our simple two-dimensional example, the off-diagonal elements define the correlation between the two variables. By considering time series in which we believe the informativeness of past observations, in explaining current data, is a function of how long ago we observed them, we then obtain stationary covariance functions that are dependent on |xi−xj|. Such covariance functions can be represented as the Fourier transform of a normalized probability density function (via Bochner’s theorem [1]); this density can be interpreted as the spectral density of the process. The most widely used covariance function of this class is arguably the squared exponential (SE) function, given by
3.11
In equation (3.11), we see two more hyperparameters, namely h,λ, which respectively govern the output scale of our function and the input, or time, scale. The role of inference in GP models is to refine vague distributions over many, very different curves, to more precise distributions that are focused on curves that explain our observed data. As the form of these curves is uniquely controlled by the hyperparameters, so, in practice, inference proceeds by refining distributions over them. As h controls the gain, or magnitude, of the curves, we set this to h=1 to generate figure 5, which shows curves drawn from a GP (with an SE covariance function) with varying λ=0.1,1,10 (figure 5a–c). The important question of how we infer the hyperparameters is left until later in this paper, in §4. We note that to be a valid covariance function, k(), implies only that the resultant covariance matrix, generated using the function, is guaranteed to be positive (semi-)definite. As a simple example, figure 6a shows a small sample of six observed data points, shown as dots, along with error bars associated with each. The seventh datum, with ‘?’ beneath it, is unobserved. We fit a GP with the SE covariance kernel (equation (3.11)). Figure 6b shows the GP posterior mean (black curve) along with ±2σ (the posterior standard deviation). Although only a few samples are observed, corresponding to the set of x,y of equations (3.8) and (3.9), we here evaluate the function on a fine set of points, evaluating the corresponding y* posterior mean and variance using these equations and hence providing interpolation between the noisy observations (this explains the past) and extrapolation for x*>0 that predicts the future. In this simple example, we have used a ‘simple’ covariance function. As the sum of valid covariance functions is itself a valid covariance function (more on this in §3c(i) later) so we may entertain more complex covariance structures, corresponding to our prior belief regarding the data. Figure 7 shows GP modelling of observed (noisy) data for which we use slightly more complex covariances. Figure 7a shows data modelled using a sum of SE covariances, one with a bias towards shorter characteristic time scales than the other. We see how this combination elegantly allows us to model a system with both long- and short-term dynamics. Figure 7b uses an SE kernel, with bias towards longer time-scale dynamics, along with a periodic component kernel (which we will discuss in more detail in §3c(i)). Note here how extrapolation outside the data indicates a strong posterior belief regarding the continuance of periodicity.
(a–c) Functions drawn from a GP with a squared exponential covariance function with output scale h=1 and length scales λ=0.1 (a), 1 (b), 10 (c). (Online version in colour.) (a) Given six noisy data points (error bars are indicated with vertical lines), we are interested in estimating a seventh at x*=0.2. (b) The solid line indicates an estimation of y* for x* across the range of the plot. Along with the posterior mean, the posterior uncertainty, ±2σ, is shaded. (Online version in colour.) (a) Estimation of y* (solid line) and ±2σ posterior deviance for a function with short-term and long-term dynamics, and (b) long-term dynamics and a periodic component. Observations are shown as pluses. As in the previous example, we evaluate the posterior GP over an extended range to show both interpolation and extrapolation. (Online version in colour.) We start by considering a simple example, shown in figure 8. Figure 8a shows a set of data points and the GP posterior distribution excluding observation of the right-most datum (darker shaded point). Figure 8b depicts the same inference including this last datum. We see how the posterior variance shrinks as we make the observation. The previous example showed how making an observation, even of a noisy time series, shrinks our uncertainty associated with beliefs about the function local to the observation. We can see this even more clearly if we successively extrapolate until we see another datum, as shown in figure 9. Rather than observations coming on a fixed time-interval grid, we can imagine a scenario in which observations are costly to acquire, and we wish to find a natural balance between sampling and reducing uncertainty in the functions of interest. This concept leads us naturally in two directions. Firstly, for the active requesting of observations when our uncertainty has grown beyond acceptable limits (of course these limits are related to the cost of sampling and observation and the manner in which uncertainty in the time series can be balanced against this cost) and secondly to dropping previously observed samples from our model. The computational cost of GPs is dominated by the inversion of a covariance matrix (as in equation (3.9)) and hence scales with the cube of the number of retained samples. This leads to an adaptive sample retention. Once more, the balance is problem specific, in that it relies on the trade-off between computational speed and (for example) forecasting uncertainty. The interested reader is pointed to Osborne et al. [2] for more detailed discussions. We provide some examples of active data selection in operation in real problem domains later in this study.
A simple example of a GP applied sequentially. (a) The posterior mean and ±2σ prior to observing the right-most datum (darker shaded) and (b) after observation. (Online version in colour.) The GP is run sequentially making forecasts until a new datum is observed. Once we make an observation, the posterior uncertainty drops to zero (assuming noiseless observations). (Online version in colour.) The prior mean of a GP represents whatever we expect for our function before seeing any data. The covariance function of a GP specifies the correlation between any pair of outputs. This can then be used to generate a covariance matrix over our set of observations and predictants. Fortunately, there exist a wide variety of functions that can serve in this purpose [3,4], which can then be combined and modified in a further multitude of ways. This gives us a great deal of flexibility in our modelling of functions, with covariance functions available to model periodicity, delay, noise and long-term drifts and other phenomena. In the following section, we briefly describe commonly used kernels. We start with simple white noise, and then consider common stationary covariances, both uni- and multi-dimensional. We finish this section with periodic and quasi-periodic kernel functions. The interested reader is referred to Rasmussen & Williams [1] for more details. Although the following list is not exclusive by any means, it provides details for most of the covariance functions suitable for analysis of time series. We note once more that sums (and products) of valid covariance kernels give valid covariance functions (i.e. the resultant covariance matrices are positive (semi-)definite) and so we may entertain with ease multiple explanatory hypotheses. The price we pay lies in the extra complexity of handling the increased number of hyperparameters.  White noise with variance σ2 is represented by
3.12
This kernel allows us to entertain uncertainty in our observed data and is so typically found added to other kernels (as we saw in equation (3.3)).  The SE kernel is given by
3.13
where h is an output-scale amplitude and λ is an input (length, or time) scale. This gives rather smooth variations with a typical time scale of λ and admits functions drawn from the GP that are infinitely differentiable.  The rational quadratic (RQ) kernel is given by
3.14
where α is known as the index. Rasmussen & Williams [1] show that this is equivalent to a scale mixture of SE kernels with different length scales, the latter distributed according to a Beta distribution with parameters α and λ−2. This gives variations with a range of time scales, the distribution peaking around λ but extending to significantly longer period (but remaining rather smooth). When , the RQ kernel reduces to the SE kernel with length scale λ.  The Matérn class of covariance functions is defined by
3.15
where h is the output scale, λ is the input scale, Γ() is the standard Gamma function and
is the modified Bessel function of second order. The additional hyperparameter ν controls the degree of differentiability of the resultant functions modelled by a GP with a Matérn covariance function, such that they are only
times differentiable. As
so the functions become infinitely differentiable and the Matérn kernel becomes the SE one. Taking
gives the exponential kernel
3.16
which results in functions that are only once differentiable, and correspond to the Ornstein–Ulenbeck process, the continuous time equivalent of a first-order autoregressive model, AR(1). Indeed, as discussed in Rasmussen & Williams [1], time-series models corresponding to AR(p) processes are discrete time equivalents of GP models with Matérn covariance functions with .  Multiple inputs and outputs. The simple distance metric, |x1−x2|, used thus far clearly allows only for the simplest case of a one-dimensional input x, which we have hitherto tacitly assumed to represent a time measure. In general, however, we assume our input space has finite dimension and write x(e) for the value of the eth element in x and denote
as the value of the eth element at the ith index point. In such scenarios, we entertain multiple exogenous variables. Fortunately, it is not difficult to extend covariance functions to allow for these multiple input dimensions. Perhaps the simplest approach is to take a covariance function that is the product of one-dimensional covariances over each input (the product correlation rule [5]),
3.17
where k(e) is a valid covariance function over the eth input. As the product of covariances is a covariance, so equation (3.17) defines a valid covariance over the multi-dimensional input space. We can also introduce distance functions appropriate for multiple inputs, such as the Mahalanobis distance,
3.18
where Σ is a covariance matrix over the input variable vector x. Note that this is a matrix which represents hyperparameters of the model, and should not be confused with covariances formed from covariance functions (which are always denoted by K in this study). If Σ is a diagonal matrix, its role in equation (3.18) is simply to provide an individual input scale
for the eth dimension. However, by introducing off-diagonal elements, we can allow for correlations among the input dimensions. To form the multi-dimensional kernel, we simply replace the scaled distance measure |xi−xj|/λ of, e.g. equation (3.13) with d(M)(x1,x2) from equation (3.18) above. For multi-dimensional outputs, we consider a multi-dimensional space consisting of a set of time series along with a label l, which indexes the time series, and x denoting time. Together, these thence form the two-dimensional set of [l,x]. We will then exploit the fact that a product of covariance functions is a covariance function in its own right, and write
taking covariance function terms over both time and time-series label. If the number of time series is not too large, we can arbitrarily represent the covariance matrix over the labels, using the spherical decomposition [6]. This allows us to represent any covariance structure over the labels. More details of this approach, which enables the dependencies between time series to be modelled, are found in Roberts et al. [7], and we use this as the focus of one of our examples in §5.  Periodic and quasi-periodic kernels. Note that a valid covariance function under any arbitrary (smooth) map remains a valid covariance function [8,1]. For any function , a covariance function k() defined over the range of x gives rise to a valid covariance k′() over the domain of u. Hence, we can use simple, stationary covariances in order to construct more complex (possibly non-stationary) covariances. A particularly relevant example of this,
3.19
allows us to modify our simple covariance functions above to model periodic functions. We can now take this covariance over u as a valid covariance over x. As a result, we have the covariance function, for the example of the SE (3.13),
3.20
In this case, the output scale h serves as the amplitude and T is the period. The hyperparameter w is a ‘roughness’ parameter that serves a role similar to the input scale λ in stationary covariances. With this formulation, we can perform inference about functions of arbitrary roughness and with arbitrary period. Indeed a periodic covariance function can be constructed from any kernel involving the squared distance (xi−xj)2 by replacing the latter with , where T is the period. The length scale w is now relative to the period, and letting
gives sinusoidal variations, while increasingly small values of w give periodic variations with increasingly complex harmonic content. Similar periodic functions could be constructed from any kernel. Other periodic functions could also be used, so long as they give rise to a symmetric, positive definite covariance matrix –
is merely the simplest. As described in Rasmussen & Williams [1], valid covariance functions can be constructed by adding or multiplying simpler covariance functions. Thus, we can obtain a quasi-periodic kernel simply by multiplying a periodic kernel with one of the basic stationary kernels described earlier. The latter then specifies the rate of evolution of the periodic signal. For example, we can multiply equation (3.20) with an SE kernel,
3.21
to model a quasi-periodic signal with a single evolutionary time scale λ. Examples of functions drawn from these kernels are shown in figure 10. There are many more types of covariance functions in use, including some (such as the Matérn family above) that are better suited to model rougher, less smooth variations. However, the SE and RQ kernels already offer a great degree of freedom with relatively few hyperparameters, and covariance functions based on these are widely used to model time-series data.
Random draws from GPs with different kernels. (a) Shows the SE kernel (equation (3.13), with h=1, λ=1), (b) the RQ (equation (3.14), with h=1, λ=1 and α=0.5) and (c) a periodic kernel based on the SE (equation (3.20), with h=1, T=2 and w=0.5). (d) Shows a quasi-periodic kernel constructed by multiplying the periodic kernel of equation (3.13) (with h=1, T=2, w=1) with the RQ kernel of equation (3.14) (with λ=4 and α=0.5). (e,f) Show noisy versions of this kernel obtained by adding, respectively, a white noise term (equation (3.13), with σ=0.2) and an SE term (equation (3.13), with h=0.1, λ=0.1). Each line consists of equally spaced samples over the interval [−5,5], and is offset from the previous one by 3 for clarity. The random number generated was initiated with the same seed before generating the samples shown in each panel. (Online version in colour.)  Changepoints. We now describe how to construct appropriate covariance functions for functions that experience sudden changes in their characteristics. This section is meant to be expository; the covariance functions we describe are intended as examples rather than an exhaustive list of possibilities. To ease exposition, we assume the (single) input variable of interest, x, represents time. If additional features are available, they may be readily incorporated into the derived covariances [1]. A drastic change in covariance: we start by considering a function of interest as well behaved, except for a drastic change at the point xc, which separates the function into two regions with associated covariance functions k1(⋅,⋅;θ1) before xc and k2(⋅,⋅;θ2) after, where θ1 and θ2 represent the values of any hyperparameters associated with k1 and k2, respectively. The change is so drastic that the observations before xc are completely uninformative about the observations after the changepoint. The full set of hyperparameters for this covariance function are hence the hyperparameters of the two covariance functions as well as the location of the changepoint, xc. This covariance function is easily seen to be semi-positive definite and hence admissible [9,10]. The covariance function, and an example draw from the GP associated with it, are presented in the left-most plots of figure 11.
Example covariance functions (a) for the modelling of data with changepoints and associated draws (b) from the resultant GPs, indicating what kind of data that they might be appropriate for. Each changepoint covariance function is drawn as a bold line, with the standard SE kernel shown as kSE for comparison (thin line). For ease of comparison, we fix the location of the changepoint hyperparameter to xc=500 and plot the functions over the interval from 460≤x≤560. (Online version in colour.) A drastic change in covariance with constraints: suppose a continuous function of interest is best modelled by different covariance functions, before and after a changepoint xc. The function values after the changepoint are conditionally independent of the function values before, given the value at the changepoint itself. This represents an extension to the drastic covariance described earlier; our two regions can be drastically different, but we can still enforce continuity and smoothness constraints across the boundary between them. We call this covariance function the continuous conditionally independent covariance function. This covariance function can be extended to multiple changepoints, boundaries in multi-dimensional spaces, and also to cases where function derivatives are continuous at the changepoint. For proofs and details of this covariance function, the reader is invited to see Osborne et al. [10] and Reece et al. [11]. A sudden change in input scale: suppose a function of interest is well behaved, except for a drastic change in the input scale λ at time xc, which separates the function into two regions with different degrees of long-term dependence. We let λ1 and λ2 represent the input scale of the function before and after the changepoint at xc, respectively. The hyperparameters of this covariance consist of the two input scales, λ1,λ2 along with a common output scale, h, and the changepoint location, xc. The second panel in figure 11 shows an example covariance function of this form (figure 11a) and an example function (figure 11b). A sudden change in output scale: we now consider a function of interest as well behaved, except for a drastic change in the output scale h at time xc, which separates the function into two regions. As before, we let h1 and h2 represent the output scales before and after the changepoint at xc. The full set of hyperparameters of this model consists of the two output scales, h1,h2, a common input scale, λ, and the location of the changepoint, xc. The third panel of figure 11 shows an example covariance and associated example function. We note that we may readily combine changes in input and output scale into a single changepoint covariance (an example of which is shown in the right-most plots of figure 11). A change in observation likelihood: hitherto, we have taken the observation likelihood as being defined by a single GP. We now consider other possible observation models, motivated by fault detection and removal [11,12]. For example, a sensor fault implies that the relationship between the underlying process model and the observed values is temporarily corrupted. In situations where a model of the fault is known, the faulty observations need not be discarded; they may still contain valuable information about the plant process. The interested reader is directed to Reece et al. [11,12], in which covariances for biased readings, stuck sensors and sensor drifts are discussed. As the mean function will dominate our forecasts in regions far from the data, the choice of the prior mean function can have a profound impact on our predictions and must be chosen with this in mind. In the majority of cases in the literature, we find vague (i.e. high uncertainty) flat mean functions used. This choice is reinforced by considering the prior mean function as the expectation function, prior to any observed data, of our domain beliefs. In the vast majority of situations, the symmetry of our ignorance (i.e. we are equally unsure that a trend is up or down) leads to flat, often zero-offset, mean functions. As a simple example, we may have domain knowledge that our functions have a linear drift term, but we do not know the magnitude or direction. Whatever prior we place over the gradient of the drift will be necessarily symmetric and leads to a zero mean with variance defined by the vagueness of our priors. If we do have such domain knowledge, then we are free to incorporate this into our GP models. For example, consider the case in which we know that the observed time series consists of a deterministic component and an unknown additive component. Draws from our GP are hence
3.22
in which the mean function, m, has hyperparameters θm that encode domain knowledge regarding the deterministic component and the covariance matrix K has hyperparameters θC. For example, we may know that our observations are obtained from an underlying exponential decay with an unknown additive function along with coloured noise. Our mean function will hence be of the form
where A,a are unknown hyperparameters. Figure 12a shows a standard SE covariance GP used to model a small set of noisy data samples (dots) drawn from a function with an underlying exponential decay. The GP models the observed data well, but long-term predictions are naturally dominated by a flat prior mean function. In figure 12b, a GP with identical covariance is used, but the mean function is that of an exponential decay with unknown hyperparameters. Even a few data points are sufficient for the exponential function hyperparameters to be inferred, leading to long-term forecasts that are dominated by a (albeit uncertain) decay function.
The effect of including a simple mean function. (a) A GP model with a flat prior mean and SE covariance function. The noisy observations are indicated by dots. The posterior from the GP is shown, along with ±2σ. (b) The same covariance function is used, but now the mean function has extra hyperparameters corresponding to an exponential decay with unknown time constant and scale. We see that the long-term forecasts in this example encode our prior belief in the decay function. (Online version in colour.) GP models have a number of hyperparameters (owing to both the covariance and mean functions) that we must marginalize2 in order to perform inference. That is, we must first assign a prior p(θ) to these hyperparameters, informed by our domain knowledge. For example, in assigning a prior to the period of a tidal signal (as in §5a), we would use a prior that expressed that the most important period was of the order of days, rather than nanoseconds or millenia. In the absence of hard domain knowledge, these priors are chosen to be diffuse: for example, a Gaussian with high variance. Then, the quantity we are interested in is
4.1
which requires two integrals to be evaluated. These are both typically non-analytic, owing to the complex form of the likelihood
when considered as a function of hyperparameters θ. As such, we are forced to resort to approximate techniques. Approximating an integral requires two problems to be solved. First, we need to make observations of the integrand, to explore it, and then those observations need to be used to construct an estimate for the integral. There are a number of approaches to both problems. Optimizing an integrand (figure 13) is one fairly effective means of exploring it: we will take samples around the maxima of the integrand, which are likely to describe the majority of the mass comprising the integral. A local optimizer, such as a gradient ascent algorithm, will sample the integrand around the peak local to the start point, giving us information pertinent to at least that part of the integrand. If we use a global optimizer, our attempts to find the global extremum will ultimately result in all the integrand being explored, as desired.
Samples (dots) obtained by optimizing the log-likelihood (grey curve) using a global optimizer, and the maximum-likelihood approximation (vertical line) of the likelihood surface. (Online version in colour.) Maximizing an integrand is most common when performing maximum likelihood. The integrands in (4.1) are proportional to the likelihood : if the prior p(θ) is relatively flat, the likelihood will explain most of the variation of the integrands as a function of θ. Maximizing the likelihood hence gives a reasonable means of integrand exploration, as above. Maximum likelihood, however, specifies a generally unreasonable means of integral estimation: the likelihood is approximated as a Dirac delta function located at the θ that maximized the likelihood. As per figure 13, this completely ignores the width of the integrands, leading to potentially problematic features [13]. This approximation finds use when the likelihood is very peaked, as is the case when we have a great deal of data. A slightly more sophisticated approach to integral estimation is to take a Laplace approximation, which fits a Gaussian around the maximum-likelihood peak. This gives at least some representation of the width of the integrands. Yet further sophistication is displayed by the methods of variational Bayes [14], which treat the fitting of probability distributions to the problematic terms in our integrands as an optimization problem. Monte Carlo techniques represent a very popular means of exploring an integrand. Simple Monte Carlo techniques draw random samples from the prior p(ϕ), to which our integrands are proportional. Note that (4.1) can be rewritten as
4.2
More sophisticated Markov chain Monte Carlo techniques [15] attempt to generate samples from the hyperparameter posterior
4.3
to which (4.2) is proportional (figure 14 illustrates samples drawn using such a method). Sampling in this way ensures that we have many samples where the prior/posterior is large, and hence, where our integrands are likely to be large. This is a particular concern for multi-dimensional integrals, where the problem is complicated by the ‘curse of dimensionality’ [16]. Essentially, the volume of space that could potentially be explored is exponential in its dimension. However, a probability distribution, which must always have a total probability mass of one, will be highly concentrated in this space, ensuring our samples are likewise concentrated is a great boon. Moreover, Monte Carlo sampling ensures a non-zero probability of obtaining samples from any region where the prior is non-zero. This means that we can achieve some measure of broader exploration of our integrands.
Samples obtained by taking draws from the posterior using a Markov chain Monte Carlo method. Monte Carlo, does not, however, provide a very satisfactory means of integral estimation: it simply approximates the integral as the average over the obtained samples. As discussed by O’Hagan [17], this ignores the information content contained in the locations of the samples, leading to unsatisfactory behaviour. For example, imagine that we had three samples, two of which were identical: θ1=θ2. In this case, the identical value will receive two-thirds of the weight, whereas the equally useful other value will receive only one-third. This is illustrated in figure 15.
A set of samples that would lead to unsatisfactory behaviour from simple Monte Carlo. In an attempt to address these issues, Bayesian quadrature [18,19] provides a model-based means of integral estimation. This approach assumes GPs over the integrands, using the obtained samples to determine a distribution for the integrals (figure 16). This probabilistic approach means that we can use the obtained variance in the integral as a measure of our confidence in its estimate. Of course, we still need to determine the hyperparameters for the GPs over the integrands. This problem is solved by adopting simple covariance functions for these GPs and using maximum likelihood to fit their hyperparameters (the maximum-likelihood output scale even has a closed-form solution). This renders the approach computationally tractable, to complement its superior accuracy.
Bayesian quadrature fits a GP to the integrand, and thereby performs inference about the integral. (Online version in colour.) In the examples to follow, we will exclusively use Bayesian quadrature to marginalize the hyperparameters of our GP models. Where desired, similar techniques are also used to calculate the posteriors for such hyperparameters
4.4
Where the posterior for a hyperparameter is highly concentrated around a particular value, we will informally describe the hyperparameter as having been learned as having that value. In the following examples, we briefly illustrate the GP approach to practical time-series analysis, highlighting the use of a variety of covariance and mean functions. The first example we provide is based on real-time data that are collected by a set of weather, sea state and environment sensors on the south coast of the UK (see Roberts et al. [7] for more details). The network (Bramblemet) consists of four sensors (named Bramblemet, Sotonmet, Cambermet and Chimet), each of which measures a range of environmental variables (including wind speed and direction, air temperature, sea temperature and tide height) and makes up-to-date sensor measurements. We have two data streams for each variable at our disposal. The first is the real-time, but sporadic, measurements of the environmental variables; it is these that are presented as a multi-dimensional time series to the GP. Second we have access, retrospectively, to finer-grained data. We use this latter dataset for assessment only. Figure 17 illustrates the efficacy our GP prediction for a tide height dataset. In order to manage the four outputs of our tide function (one for each sensor), we rewrite so that we have a single output and inputs t, time, and l, a sensor label, as discussed in §3a and in §3c.
Prediction and regression of tide height data for (a) independent and (b) multi-output GPs. (Online version in colour.) Note that our covariance over time is the sum of a periodic term and a disturbance term. Both are of the Matérn form with . This form is a consequence of our expectation that the tides would be well modelled by the superposition of a simple periodic signal and an occasional disturbance signal due to exceptional conditions. Of course, for a better fit over the course of, say, a year, it would be possible to additionally incorporate longer term drifts and periods. The period T of the periodic covariance term was unsurprisingly learnt as being about half a day, whereas for the disturbance term, the time scale w was found to be about two and a half hours. Note that this latter result is concordant with our expectations for the time scales of the weather events we intend our disturbance term to model. Our algorithm learned that all four sensors were very strongly correlated, with spherical decomposition of the inferred correlation elements all very close to one. The hyperparameter matrix Σ of equation (3.18) (which defines relationships between variables) additionally gives an appropriate length scale for each sensor. From this inference, we determined weather events to have induced changes in tide height of the order of 20 per cent. We also make allowances for the prospect of relative latency among the sensors by incorporating delay variables, introduced by a vector of delays in time observations [7]. We found that the tide signals at the Cambermet and Chimet stations were delayed by about 10 minutes relative to the other two. This makes physical sense—the Bramblemet and Sotonmet stations are located to the west of the Cambermet and Chimet stations, and the timing of high tide increases from west to east within the English channel. Note the performance of our multi-output GP formalism when the Bramblemet sensor drops out at t=1.45 days. In this case, the independent GP quite reasonably predicts that the tide will repeat the same periodic signal it has observed in the past. However, the GP can achieve better results if it is allowed to benefit from the knowledge of the other sensors’ readings during this interval of missing data. Thus, in the case of the multi-output GP, by t=1.45 days, the GP has successfully determined that the sensors are all very strongly correlated. Hence, when it sees an unexpected low tide in the Chimet sensor data (caused in this case by the strong northerly wind), these correlations lead it to infer a similarly low tide in the Bramblemet reading. Hence, the multi-output GP produces significantly more accurate predictions during the missing data interval, with associated smaller error bars. Exactly the same effect is seen in the later predictions of the Chimet tide height, where the multi-output GP predictions use observations from the other sensors to better predict the high tide height at t=2.45 days. Note also that there are two brief intervals of missing data for all sensors just after both of the first two peak tides. During the second interval, the GP’s predictions for the tide are notably better than for the first—the greater quantity of data it has observed allows it to produce more accurate predictions. With time, the GP is able to build successively better models for the series. The predictive performances for our various algorithms over this dataset can be found in table 1. For the Kalman filter comparison, a history length of 16 observations was used to generate each prediction because this gave rise to the best predictive ability for the Kalman model on out-of-sample data. However, note that our multi-output GP, which exploits correlations between the sensors, and the periodicity in each individual sensors’ measurements, significantly outperforms both the Kalman filter and the independent GP [7]. The naive result is obtained by repeating the last observed sensor value as a forecast and is included as a baseline only.
Predictive performances for 5 day Bramblemet tide height dataset. We note the superior performance of the GP compared with a more standard Kalman filter model. Error metrics shown are root mean square error (r.m.s.e) and normalized mean square error (n.m.s.e.), which is presented on a logarithmic, decibel scale. We now demonstrate our active data selection algorithm. Using the fine-grained data (downloaded directly from the Bramblemet weather sensors), we can simulate how our GP would have chosen its observations had it been in control. Results from the active selection of observations from all four tide sensors are displayed in figure 18. Again, these plots depict dynamic choices; at time t, the GP must decide when next to observe, and from which sensor, given knowledge only of the observations recorded prior to t, in an attempt to maintain the uncertainty in tide height below 10 cm. The covariance function used was that described in the previous example, namely a sum of two
Matérn covariance functions, one stationary and the other of periodic form. Consider first the case shown in figure 18a, in which separate independent GPs are used to represent each sensor. Note that a large number of observations are taken initially as the dynamics of the sensor readings are learnt, followed by a low but constant rate of observation. By contrast, for the multi-output case shown in figure 18b, the GP is allowed to explicitly represent correlations and delays between the sensors. As mentioned earlier, this dataset is notable for the slight delay of the tide heights at the Chimet and Cambermet sensors relative to the Sotonmet and Bramblemet sensors, due to the nature of tidal flows in the area. Note that after an initial learning phase as the dynamics, correlations and delays are inferred, the GP chooses to sample predominantly from the undelayed Sotonmet and Bramblemet sensors. The dynamics of the tide height at the Sotonmet sensor are more complex than the other sensors owing to the existence of a ‘young flood stand’ and a ‘double high tide’ in Southampton. For this reason, the GP selects Sotonmet as the most informative sensor and samples it most often. Despite no observations of the Chimet sensor being made within the time span plotted, the resulting predictions remain remarkably accurate. Consequently, only 119 observations are required to keep the uncertainty below the specified tolerance, whereas 358 observations were required in the independent case. This represents another clear demonstration of how our prediction is able to benefit from the readings of multiple sensors.
(a) Comparison of active sampling of tide data using independent and (b) multi-output GPs. Note that, in the case of multi-output GPs, one sensor reading (Sotonmet) slightly leads the other readings and is hence sampled much more frequently. In some cases, such as the Cambermet readings, only occasional samples are taken, yet the GP forecasts are excellent. (Online version in colour.) In Garnett et al. [9] and Osborne et al. [10], a fully Bayesian framework was introduced for performing sequential time-series prediction in the presence of changepoints. The position of a particular changepoint becomes a hyperparameter of the model that is marginalized using Bayesian quadrature. If the locations of changepoints in the data are of interest, the full posterior distribution of these hyperparameters can be obtained given the data. The result is a robust time-series prediction algorithm that makes well-informed predictions, even in the presence of sudden changes in the data. If desired, the algorithm additionally performs changepoint and fault detection as a natural by-product of the prediction process. In this section, we briefly present some exemplar datasets and the associated changepoint inference. We first consider a canonical changepoint dataset, the minimum water levels of the Nile river during the period AD 622–1284 [20]. Several authors have found evidence supporting a change in input scale for this data around the year AD 722 [21]. The conjectured reason for this changepoint is the construction in AD 715 of a new device (a ‘nilometer’) on the island of Roda, which affected the nature and accuracy of the measurements. We performed one-step (next datum) lookahead prediction on this dataset using the input-scale changepoint covariance discussed earlier. The results can be seen in figure 19. Figure 19a shows our one-step predictions on the dataset, including the mean and ±σ error bars. Figure 19b shows the posterior distribution of the number of years since the last changepoint. A changepoint around AD 720–722 is clearly visible and agrees with previous results [21].
Prediction for the Nile dataset using input-scale changepoint covariance (a) and the corresponding posterior distribution for time since the changepoint (b). (Online version in colour.) As a second canonical changepoint dataset, we present the series of daily returns of the Dow–Jones industrial average between 3 July 1972 and 30 June 1975 [22]. This period included a number of newsworthy events that had significant macroeconomic influences, as reflected in the Dow–Jones returns. We performed sequential one-step (next datum) prediction on this data using a GP with a diagonal covariance that assumed all measurements were independent and identically distributed (as under the efficient market hypothesis, returns should be uncorrelated). However, the variance of these observations was assumed to undergo changes, and as such, we used a covariance that incorporated such changes in output scale. We had three hyperparameters to marginalize: the variance before the changepoint, the variance after the changepoint and, finally, the location of that changepoint. Our results are plotted in figure 20. Our model clearly identifies the important changepoints that likely correspond to the commencement of the Organization of the Petroleum Exporting Countries embargo on 19 October 1973, and the resignation of Richard Nixon as President of the USA on 9 August 1974. A weaker changepoint is identified early in 1973, which Adams & MacKay [22] speculate is due to the beginning of the Watergate scandal.
Online (sequential) one-step predictions (a) and posterior for the location of changepoint for the Dow–Jones industrial average data using an output-scale changepoint covariance (b). (Online version in colour.) Many Sun-like stars display quasi-periodic brightness variations on time scales of days to weeks, with amplitudes ranging from a few parts per million to a few per cent. These variations are caused by the evolution and rotational modulation of magnetically active regions, which are typically fainter than the surrounding photosphere. In this case, we may expect a range of both periodic covariance scales w and evolutionary time scales λ, corresponding to different active region sizes and lifetimes, respectively. This can be achieved by replacing one or both of the SE kernels in equation (3.13) by RQ kernels (equation (3.14)). Finally, we can also allow for short-term irregular variability or correlated observational noise by including a separate, additive SE or RQ kernel. For example, Pont et al. [23] used a GP with such quasi-periodic kernels to model the total irradiance variations of the Sun in order to predict its radial velocity variations. In figure 21, we show the results of a quasi-periodic GP regression to photometric observations of the well-known planet host star HD 189733, taken from Henry & Winn [24]. The kernel used consists of a periodic SE component (equation (3.21)) multiplied by an RQ term (equation (3.14)) to allow for a range of evolutionary time scales, plus an additive white noise term (equation (3.12)). Inference over the hyperparameters of interest yielded expected values of h=6.68 mmag, T=11.86 days, w=0.91, α=0.23, λ=17.81 days and σ=2.1 mmag, where σ is the amplitude of the white noise term. Our period is in excellent agreement with Henry & Winn [24]. The relatively long periodic length scale w indicates that the variations are dominated by a small number of fairly large active regions. The evolutionary term has a relatively short time scale, λ, but a shallow index, α, which is consistent with the notion that the active regions on this star evolve relatively fast and/or that, as in the Sun, active regions located at different latitudes have different rotation rates (known as differential rotation).
Predictive distribution for a quasi-periodic GP model using a mixed SE and RQ kernel, trained and conditioned on observations made with the 0.8 m Automated Patrol Telescope [24] using the Strömgren b and y filters. The dots represent the observations, the line is the mean of the predictive posterior distribution and the shaded region encompasses the ±σ interval. (Online version in colour.) One of the most successful ways of discovering and characterizing extra-solar planets (i.e. planets not in our solar system) is through observing transit light curves. A transit occurs when a planet periodically passes between its host star and the Earth, blocking a portion of the stellar light, and produces a characteristic dip in the light curve. From this transit, we can measure such physical parameters as the planet-to-star radius ratio and the inclination of the orbit. While transit light curves are readily described by a deterministic parametric function, real observations are corrupted by systematic noise in the detector, external state variables (such as the temperature of the detector, orbital phase, position of the host star on the charge-coupled device array, etc.), as well as the underlying flux variability of the host star. As it is not possible to produce a deterministic model to account for all these systematics, a GP may be used to place a distribution over possible artefact functions, modelling correlated noise as well as subtle changes in observed light curves due to external state variables. We hence encode the transit curve as the mean function of a GP. The covariance function has inputs given by time and external state variables (hence, this is a multi-input, single-output model). By integrating out our uncertainty (see §4) in the hyperparameters of the GP (which model all the systematic artefacts and noise processes), we can gain much more realistic inference of probability distribution of the transit function parameters (the hyperparameters of the mean function). For a detailed discussion of the application of GPs to transit light curves, see Gibson et al. [25], in which the instrumental systematics are represented by a GP with an SE kernel (equation (3.13)) and input parameters representing the external state variables. Robust inference of transit parameters is required to perform detailed studies of transiting systems, including the search for atomic and molecular signatures in the atmospheres of exoplanets. Figure 22 shows this GP model fitting to the time series of observations. More details are found in Gibson et al. [25].
As an example of a complex mean function, we here model data from an exoplanet transit light curve. The data are fitted with a GP with an exoplanet transit mean function and a squared exponential covariance kernel to model the correlated noise process and the effects of external state variables. The shaded regions are at ±1,2σ from the posterior mean. (Online version in colour.) In this paper, we have presented a brief outline of the conceptual and mathematical basis of GP modelling of time series. As ever, a practical implementation of the ideas concerned requires jumping algorithmic rather than theoretical hurdles, which we do not discuss here because of space constraints. Some introductory code may be found at ftp://ftp.robots.ox.ac.uk/pub/outgoing/mebden/misc/GPtut.zip and more general code can be downloaded from http://www.gaussianprocess.org/gpml. Space has not permitted discussion of exciting recent trends in GP modelling that allow for more explicit incorporation of differential equations governing the system dynamics (either observed or not), such as latent force models [26]. Further extensions, using GPs as building blocks in more complex probabilistic models, are of course possible, and recent research has also highlighted the use of GPs for numerical integration, global optimization, mixture-of-experts models, unsupervised learning models and much more. The authors thank Alex Rogers, Roman Garnett, Richard Mann, Tom Evans, Mark Smith and Chris Hart. Part of this work was funded by the UK research councils, whose support is gratefully acknowledged. One contribution of 17 to a Discussion Meeting Issue ‘Signal processing and inference for the physical sciences’. ↵1 This always feels rather disingenuous though, as these models do have hyperparameters, which we discuss later in this paper. These still need to be inferred! They are referred to as hyperparameters, as they govern such things as the scale of a distribution rather than acting explicitly on the functional form of the curves. ↵2 The process of marginalization refers to ‘integrating out’ uncertainty. For example, given p(y,θ)=p(y|θ)p(θ), we may obtain p(y) by marginalizing over the unknown parameter θ, such that . Thank you for your interest in spreading the word on Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. NOTE: We only request your email address so that the person you are recommending the page to knows that you wanted them to see it, and that it is not junk mail. We do not capture any email address. Please log in to add an alert for this article. Article reuse Celebrating 350 years of Philosophical Transactions Anniversary issue with free commentaries, archive material, videos and blogs. Copyright © 2018 The Royal Society"
43dc09a7b6,Introducing: Unity Machine Learning Agents – Unity Blog,"Language
Arthur Juliani, September 19, 2017 Our two previous blog entries implied that there is a role games can play in driving the development of Reinforcement Learning algorithms. As the world’s most popular creation engine, Unity is at the crossroads between machine learning and gaming. It is critical to our mission to enable machine learning researchers with the most powerful training scenarios, and for us to give back to the gaming community by enabling them to utilize the latest machine learning technologies. As the first step in this endeavor, we are excited to introduce Unity Machine Learning Agents. Machine Learning is changing the way we expect to get intelligent behavior out of autonomous agents. Whereas in the past the behavior was coded by hand, it is increasingly taught to the agent (either a robot or virtual avatar) through interaction in a training environment. This method is used to learn behavior for everything from industrial robots, drones, and autonomous vehicles, to game characters and opponents. The quality of this training environment is critical to the kinds of behaviors that can be learned, and there are often trade-offs of one kind or another that need to be made. The typical scenario for training agents in virtual environments is to have a single environment and agent which are tightly coupled. The actions of the agent change the state of the environment, and provide the agent with rewards.
The typical Reinforcement Learning training cycle. At Unity, we wanted to design a system that provide greater flexibility and ease-of-use to the growing groups interested in applying machine learning to developing intelligent agents. Moreover, we wanted to do this while taking advantage of the high quality physics and graphics, and simple yet powerful developer control provided by the Unity Engine and Editor. We think that this combination can benefit the following groups in ways that other solutions might not: We call our solution Unity Machine Learning Agents (ML-Agents for short), and are happy to be releasing an open beta version of our SDK today! The ML-Agents SDK allows researchers and developers to transform games and simulations created using the Unity Editor into environments where intelligent agents can be trained using Deep Reinforcement Learning, Evolutionary Strategies, or other machine learning methods through a simple to use Python API. We are releasing this beta version of Unity ML-Agents as open-source software, with a set of example projects and baseline algorithms to get you started. As this is an initial beta release, we are actively looking for feedback, and encourage anyone interested to contribute on our GitHub page. For more information on ML-Agents, continue reading below! For more detailed documentation, see our GitHub Wiki.
A visual depiction of how a Learning Environment might be configured within ML-Agents. The three main kinds of objects within any Learning Environment are: The states and observations of all agents with brains set to External are collected by the External Communicator, and communicated to our Python API for processing using your ML library of choice. By setting multiple agents to a single brain, actions can be decided in a batch fashion, opening the possibility of getting the advantages of parallel computation, when supported. For more information on how these objects work together within a scene, see our wiki page. With Unity ML-Agents, a variety of training scenarios are possible, depending on how agents, brains, and rewards are connected. We are excited to see what kinds of novel and fun environments the community creates. For those new to training intelligent agents, below are a few examples that can serve as inspiration. Each is a prototypical environment configurations with a description of how it can be created using the ML-Agents SDK. Beyond the flexible training scenarios made possible by the Academy/Brain/Agent system, ML-Agents also includes other features which improve the flexibility and interpretability of the training process.
Above each agent is a value estimate, corresponding to how much future reward the agent expects. When the right agent misses the ball, the value estimate drops to zero, since it expects the episode to end soon, resulting in no additional reward.
Different possible configurations of the GridWorld environment with increasing complexity.
Two different camera views on the same environment. When both are provided to an agent, it can learn to utilize both first-person and map-like information about the task to defeat the opponent. As mentioned above, we are excited to be releasing this open beta version of Unity Machine Learning Agents today, which can be downloaded from our GitHub page. This release is only the beginning, and we plan to iterate quickly and provide additional features for both those of you who are interested in Unity as a platform for Machine Learning research, and those of you who are focused on the potential of Machine Learning in game development. While this beta release is more focused on the former group, we will be increasingly providing support for the latter use-case. As mentioned above, we are especially interested in hearing about use-cases and features you would like to see included in future releases of Unity ML-Agents, and we will be welcoming Pull Requests made to the GitHub Repository. Please feel free to reach out to us at ml-agents@unity3d.com to share feedback and thoughts. If the project sparks your interests, come join the Unity Machine Learning team! Happy training! Comments are closed. Ruben Aster
Damn that’s cool! :) So, what I’ve noticed is that agents have a list of states with a fixed size, which is ok when you have a constant environment like 1 ball and 1 platform that tries to keep the ball on it. But how about having enemies which spawn dynamically? Or when these enemies shoot bullets? We’d need a dynamic list of states for that. How would you implement this scenario? Kirby
Heads up, you have a dead link: >
For more information on how these objects work together within a scene, see our wiki page. “wiki page” currently points here: https://github.com/Unity-Technologies/python-rl-control/wiki That repo doesn’t appear to exist anymore. Paul
I’m happy because this looks cool. But I’m sad because I didn’t think of it first! :'( Krishnan
Awesome!
When do we have more algorithms than just PPO? Riya Bansal
Thanks for sharing such a wonderful article.
http://www.broachindia.com/ Shrey Pareek
Hello,
This seems like a great toolbox for integrating Unity with Python. A quick question though. Any clue when the imitation learning tool would be available?
Thank you Michael Knight
Thanks for writing this article. I’ve always been interested in A.I. I would love to apply some of this to our Knight O.S. project.
http://myknightrider2000.blogspot.ca Arthur Drikis
Hi,
It’s a great tool and I’ve been really enjoying working with it the last few days.
Though, while messing around with it, I’ve noticed that the training process itself uses only about 12% of GPU power and around 50% of my cpu.
Am I missing some feature that would let me to use the GPU to its full potential? Arthur Juliani
Hi Arthur, You are correct in noticing that the reinforcement learning algorithm we use (PPO) isn’t as GPU efficient as it could be. The problem stems from the fact that the network is used in two ways: deciding actions (inference) and training (gradient descent). If the batch size is large enough, the training step can fully utilize the GPU. However, during experience collection, the network is only being used to pick actions, and this is much less computationally efficient. There are possible methods for better utilizing the GPU, such as having separate threads collect experience and update the network. These however add complexity to the system, and require tuning in and of themselves to ensure they are providing the level of benefit desired. It is something we are aware of, and will be keeping in mind as we develop future algorithms. Arthur Drikis
Thanks for the reply! I would like to ask another question, though.
So far I haven’t been able to successfully train a neural network in any environment other than 3DBall. In tennis I got to around 18M steps, and the cumulative reward seems to be stagnant with seemingly random spikes: http://prntscr.com/h4dhaq While training with GridWorld it looks like I’ve been getting worse and worse results. So far I’m at 4.5M steps, and the reward is just going down.
http://prntscr.com/h4diha I tried to mess around with buffer size, batch size and hidden unit amount, but it didn’t seem to make much of a difference. What am I doing wrong here?
Should I just wait for more steps? Also, it would be really cool if we could get our hands on the values that were used for training the pre-trained TF models for each of the examples. Data Science Training In Hyderabad
Hi,
Thanks for sharing such a wonderful article with us
We are expecting more articles from this blog Andrea Maria
Thank you for the information. Machine learning has its roots in statistics and mathematical optimization. Machine learning covers techniques in supervised and unsupervised learning for applications in prediction, analytics, and data mining. If you want machine learning services. Visit:https://www.usmsystems.com/machine-learning/ AnalyticsPath
Thanks For the Valuable information About Machine Learning and Other Professional Courses Trend Setting Today.
If Any Doubts regarding Machine Learning Please Visit this Website http://www.analyticspath.com/machine-learning-training-in-hyderabad duyth
sorry, i failed to understand but after running the 3DBall training, what’s the outcome from the training / I mean, is there a result file that we can get & reuse ? Arthur Juliani
Yep! You get a model file that you can add back into the project itself. See the Getting Started Guide for more info: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md#embedding-trained-brain-into-unity-environment-experimental Zara
Just curious. Can I use this on Android, IOS platform?
Even if it did, I guess with Unity as middle interpreter of python code, it will be overkill for normal phone CPU. Arthur Juliani
Hi Zara, by using our “Internal” brain, and the TensorFlowSharp plugin, you can put trained brains into projects for Android and iOS. You just won’t be able to do the actual training on those devices. For more information, see here: https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Using-TensorFlow-Sharp-in-Unity-(Experimental).md Jeffrey
I really love this.
I’ve been playing with it for the last three days. What I really need now, as a ML newb, is a step-by-step guide that answers a few questions.
I understand that this isn’t a good place for answering questions.
I don’t want answers here.
Just hoping that in time these questions will be answered in the documentation section of ml-agents on github. Thanks! What is the workflow?
When I run the training, do I “load saved model”?
Should I only be running a training once?
If I run it multiple times does it keep learning?
It seems my agent gets WORSE at tasks, not better even though I feel like I’m rewarding correctly.
There is clearly some learning going on but if I leave it going overnight (6-8 hours) there doesn’t seem to be any improvement. Do I run a training many times in a row by finishing one, then immediately rerunning the PPO script?
Or do I need to set everything up.
Set the steps to a crazy number.
Run once. Export model?
How do I set up the cameras?
Is there more than just adding them to the agent?
Do I need to set up the “resolution” stuff in the brain?
Along with a tuning guide. Steps. Learning rate.
What to change if things don’t seem to go right.
And how long for simple tasks. Arthur Juliani
Hi Jeffrey, Glad to hear that you are enjoying playing around with it!
When you run the PPO notebook (or ppo.py) the neural network model will be trained for the number of steps set in max_steps. If you interrupt training, you can set load_model to true, and then continue training. Once you reach the max_steps, if your model isn’t sufficiently trained you can continue training by increasing max_steps. I would recommend looking at the TensorBoard logs though to track performance. If your reward isn’t gradually increasing over time, there may be issues with the reward structure of the environment, or the agent may not be getting the information it needs to solve the task. We have a preliminary “Best Practices” document for creating environments https://github.com/Unity-Technologies/ml-agents/blob/master/docs/best-practices.md , but I agree that it would be a good idea to add a similar page for the training process itself, and that is something we will work on putting together. Chris
This is a great topic to dive into! swordmaster swordmaster
Great ！I developed an asset about machine learning for unity as well :
https://www.assetstore.unity3d.com/en/#!/content/93236
This artificial neural networks chose the back propagation algorithm to learn something,and the demo is
about a car learning how to get out of the maze. Sergei
Will there be future training with the teacher? Learning for the known action for the state list. Data, for example, is collected in Player mode. Data type state list -> action list Arthur Juliani
Hi Sergei, This feature is what we are calling “Imitation Learning.” It will be coming in the next release, which we hope to have out in the next few weeks! Michael
I’m trying to fallow the Getting Started with the Balance Ball Example tutorial and I managed to get through the tutorial on how to set up Python/TensorFlow (http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html) via the tutorial that was linked in the balance ball example tutorial. But when I try to run the jupyter PPO table I get this error :
ModuleNotFoundError
Traceback (most recent call last)
in ()
1 import numpy as np
2 import os
—-> 3 import tensorflow as tf
4
5 from ppo.history import * ModuleNotFoundError: No module named ‘tensorflow’ I’m not sure what I did wrong or what to try to do to fix this :( Any ideas at all would be helpful. Michael
Ok, so I was actually able to get past that last error above. I just did not realize I needed to install tensor into the python folder, my bad. But I do have a another question. I am having trouble observing the training process. I open anaconda and put in the tensorboard –logdir=’summaries line and it runs but I’m not able to do anything else without stopping it and I don’t see anything that shows me how the training is going. I let the training go on for a few minutes and then continued to the last cell and then continue past that and then stop running the table by pressing the interrupt kernel button. I look in the models folder but I don’t see an exported model. I do see model-50000.cptk.index and some other files but not a 3DBall.bytes so I am confused on how to properly end running the cells or if I did something else wrong. Any help would be welcome. Arthur Juliani
Hi Michael, you need to launch a web browser and navigate to localhost:6006 to view the training information once launching Tensorboard. I am unsure why you aren’t seeing the .bytes file. Do you get a message letting you know it was created when you run that cell? Andre Infante
Is there a timeline for supervised learning support? I have an application that would be much easier to train with supervision, and am not sure if I should wait or try to get it to train with the existing RL support. Arthur Juliani
Hi Andre, We are currently actively developing the imitation learning support, and hope to have it out within a few weeks. If you are adventurous, you can check out the dev-broadcast branch of the repository, where we are developing it. The feature allows brain types besides external to “broadcast” their states/actions to the python api for use in supervised learning. Of course, it is still in development so there will likely be bugs. If you happen to find any, please let us know! Andre Infante
I will check it out, thank you! Max
Arthur, Could you help me and stitch together your implementation of
A3C
algo into this?
I am looking at it but porting model seems to be above my skillset.
If not, I would gladly accept a short how-to of how shall I do it. Thanks! Arthur Juliani
Hi Max, We include an implementation of PPO with ML-Agents. PPO is a more reliable and efficient algorithm than A3C, so it is included instead. One difference is that the included PPO isn’t asynchronous, but it could be made to be though some adjustments. https://github.com/Unity-Technologies/ml-agents/tree/master/python Max
Thanks for pointing me in the right direction!
I reevaluated my approach and found a way to do my thing with PPO, where I will (maybe, will see how it will work without it) apply GA for evolutionary reasons.
I have another question tho, when I set 2 brains in academy,
I change every “brain_name” to “name” inside “for name in env.brain_names:”, but get this error: ” You have 2 brains, you need to feed a dictionary of brain names a keys, and actions as values”, at line “new_info = trainer.take_action(info, env, name)”. Can you show me how to properly feed it? Thanks! Bart Burkhardt
I got the 3DBall
project to work using the jupyter notebook.
But it looks like unity is running at 1 fps. Also it starts in a tiny window, Is it supposed to be so slow when learning? I tried on a Nvidia 1060 and 1070 Arthur Juliani
Hi Bart. When training, we speed up the engine to 100x, which causes a drop in frame-rate. Although it looks slow, the engine is actually processing thousands of steps of simulation, and is running correctly. Ryan Potter
Oh, …. you just made my day.
I was attempting to use Unity a few months ago as the environment in my AI research, but was struggling with implementing the ML algorithms I needed (CNNs, ANNs, etc) with C#.
I put it on ice and switched to a home-brew 2D environment in Python on Linux so I could make progress on the AI, keeping basic Unity-like structure so I could switch back at some point easily.
Looks like I can switch back now :) This is so perfect.
Thank You!! lee
Where is sample project files of these games ML used in this blog thread? Arthur Juliani
Hi Lee, You can find the example projects in the repository here: https://github.com/Unity-Technologies/ml-agents . mikejmc
Exciting direction… Any plans to work with Apple’s ML framework and Swift? Tejas Ramanuj
How to get Started with unity Machine Learning Xype
Oh and well I could at least try to help you out if you insist on continuing down this space when you could be spending the money completing features you have been putting off for a couple of years now which are more important….. You should talk to some of the colleges that focus on behavioural analysis and study. http://paco.psy.gla.ac.uk/index.php?option=com_jdownloads&view=viewcategories&Itemid=62 is an example there are 2 or 3 focused on this niche of research, that particular one uses mocap systems, taking volunteers to do natural motion, then studying and programming computers to recognize gender, attitude, and emotion from body motion. Another focuses on interaction between humans, and even more on general physics and flow of movement. A last one I try to avoid because it is funded by DARPA and that just spooks me. It is the recognition from a distance project. Ties right into this, and those schools would llikely be happy to provide you with their research, publications, and findings if
you in turn expand their knowledgebase and cases by publishing
your own findings based on the research guidelines defined in the project licenses to use the mocap databases. AI learning should definately be learning cases of how to react based on subtle actions of the thing it is interfacing against. Xype
Eh this is really interesting stuff, however as a content creation/game development/high end rendering platform which is awesome but has a lot of bugs and a undeserved bad reputation, I really do not thing you need to be here. Besides aren’t you guys afraid of AI…maybe it is time for hollywood to remake the old HAL “Would you like to play a game” …. Funny I saw an asset the other day that started with HAL, I don’t even know what it was, I saw that much and changed the page. Michael Bechauf
Really cool demo! I was able to run the 3DBall code and it worked nicely. A few questions though … How do you run the Tennis application? I did not find any instructions, so I assumed it would also require the PPO notebook, but training took a very long time. When I stopped training after 1000 iterations, and tried to persist the binary model, I got the error AttributeError: ‘NoneType’ object has no attribute ‘model_checkpoint_path’. Maybe I forgot something? Second, why do you call the internal Brain model experimental? Is that simply because you load the Tensorflow library into the Unity engine which may cause instabilities?
And finally, what are you plans regarding Unity libraries? Do you intend to develop your own ML models, or is the job of ML-agent essentially to provide convenient bindings to existing ML frameworks? Arthur Juliani
Hi Michael, You are right that training Tennis takes longer – at least 1 million steps. It is often the case that most complex Reinforcement Learning problems take in the millions of steps. For example, many ATARI games take roughly 200 million steps of training to achieve super-human performance.
In order to create the binary file you need to have at least one model checkpoint saved. To make those saves happen more frequently, you can adjust save_freq in the hyperparameters. We refer to internal as “experimental” because it hasn’t been thoroughly tested enough for us to recommend to game developers as a method for actually controlling game-ai in released games. In the coming months we hope to provide a version we feel strongly enough about take the “experimental” tag off. With ML-Agents we are currently supporting TensorFlow integration, but if other solutions present themselves in the future we may explore them as well. Renato Vargas
I’m currently using V-Rep for my research with RL. The project is game-related, but it’s also related to robotics, which it’s what prevents me from trying Unity for this specific use case. For example, having a NAO robot fully working ready for importing was really important.
I know it’s not directly related to ML, but do you guys have any plans to expand more towards the field of robotics / having robot models available to researches?
Having said that, I’ll definitely try it on different projects! Jordy Henry
Thats amazing, i have one question, Can I pass a previous made dataset to the brain, lets say for example, I have one script that save all the inputs of my players, and I have access to it, and I want to turn all this input into a dataset, to start to train a brain based on it.
It is possible ? Arthur Juliani
Hi Jordy, This feature is coming soon. You can read a little more about it above, under “Imitation Learning.” Ramy Dergham
What about the performance on mobile phones with games that contains huge number of states? A game like Poker for example have a huge amount of states and it has a partial observable environment were the agent can’t see the opponent’s cards. Is it possible to make use your ML for a game like Poker on mobile platforms? Arthur Juliani
Hi Ramy, This is a good question! The state-size of the problem doesn’t necessarily increase the complexity of the model. For example, learning from an 8-bit 128x128x3 pixel image contains a huge state-space 256^49152, yet we can use convolutional networks with a few layers to learn to generalize between them. A network like that can actually run relatively easily on a phone.
Of course, on the other side of that is something like AlphaGo, which ran on a supercomputer… I think for many games though it will be possible to distill the important information into a relatively small network which can generalize within the domain. Especially as smartphones begin to integrate more powerful ML-specific hardware, like Apple is doing with iPhone X. Jonney Shih
Hi, when machine learning is run? in Editor or Runtime or both?
Also can it be done offline? sanghwa
Can I use PyTorch instead of TensorFlow? Arthur Juliani
Hi, You can definitely use PyTorch for training! The only thing that won’t be possible is to embed the trained PyTorch model back into the Unity game/simulation itself. Liven
do we need python language knowledge for using ml-agent ? Arthur Juliani
Good question. Right now we are including a pre-made reinforcement learning algorithm called PPO with ML-Agents.
You should be able to use it to train simple agents without needing to understand how to modify it yourself.
We understand though that many Unity game developers mainly have expertise in C#, so we are exploring ways to enable developers to train agents without the need to manually interact with python. Liven
Thank you for the answer.
Really hope you’ll adapt the feature in C#. Right now, I am stuck at the “Installing Dependencies” step and sent an email for details. Andre
Until now I had only seen the Udacity self driving car demo that also worked with a socket connection. I could only get it to run on my old MacBook tough. Also I did the training on a cloud instance https://medium.com/towards-data-science/introduction-to-udacity-self-driving-car-simulator-4d78198d301d Really looking forward to running the projects form this blogpost as well. Berte Adam
Im having an issue with the installation of tensorflow, I’m running Windows 8.1 64bit Python 3.6.1 64bit anaconda 3
https://drive.google.com/open?id=0B6Px6xu8RYExa2M2cG5BeFBSOE1iTkNRcDRpY1lqaVNkMEtR Seb
Great !
How are trained brains saved ? Arthur Juliani
If you use TensorFlow to create the neural network, the saved Brain is saved as a TensorFlow model, which can be converted to a .bytes file,
and embedded into the Unity project directly. For more information, check out this walkthrough: https://github.com/Unity-Technologies/ml-agents/wiki/Getting-Started-with-Balance-Ball. Bond
I must yes, it’s Good But the Question here is Why is that Unity is Copying OpenAI strategy in Unity as is, AI learning from players is a Maths project from Dota2 But there are flaws with the system learning as with the kind of Pitch one has made Romano
Does this work on Windows too or Linux only? (Sorry if asking stupid question :D:D) Arthur Juliani
We are targeting support for Windows, Mac, and Linux (Plus eventually mobile and console). Currently Mac and Linux are the more heavily tested environments. If you encounter an issue on Windows, please let us know here: https://github.com/Unity-Technologies/ml-agents/issues. Renato Vargas
It seems like a good timing for a official release of the Linux editor :) yosider
Very cool project!!
I’m looking foward to supports for (spoken) language learning. Arthur Juliani
Hi! Unity actually already has a solution for speech recognition in games: https://labs.unity.com/article/speech-recognition-and-vr . I hope that is what you are looking for! Faizan Khan
Perfect! I have done my Machine Learning Subject Project in Game Machine Learning… I didn’t quite get anything sophisticated… I wish Unity Machine Learning would have introduced 5 months ago.🤔😥😍 Faizan Khan
Perfect! I have done my Machine Learning Subject Project in Game Machine Learning… I didn’t quite get anything sophisticated… I wish Unity Machine Learning would have introduced 5 months ago.🤔😥 Samuel Otero
Very cool, I was already wanting to do a game that implemented reinforcement learning in Unity and this will go a long way towards that. In my case the agent would start off with a trained behavior with the player being able to modify the behavior as part of the gameplay. apcrol
Great stuff but I wish to see more examples of applications for this agents. Arthur Juliani
More demo projects and videos are definitely on the way! We are also interested in sharing example projects that others might make. Alan Mattano
Ai and an external database can be good for automatically adjust the starting rendering settings looking to the player hardware when the game starts. kamran bigdely shamloo
This is immensely useful for game development and AI researchers. Unity is going the right direction. Laura
Cannot wait to see the Ecosystem demos! All Asset Store Community Events Machine Learning Made With Unity Rants & Raves Services Technology Unity Ads Unity Analytics Purchase Education Download Labs Resources About Unity Get Unity news To start receiving news from Unity Technologies, click on the link we've sent to your e-mail account. Something went wrong, please try again. Language Partners"
3e91f7e2a1,Which Bugs Will Hackers Exploit First? Machine Learning Promises a Better Guess - Defense One,"November 16, 2017
Topics
AP/
Efrem Lukatsky
A screen of an idle virus affected cash machine in a state-run OshchadBank says ""Sorry for inconvenience/Under repair"" in Kiev, Ukraine, Wednesday, June 28, 2017.
November 16, 2017
Most vulnerabilities are known; defenders need a better way to know which ones pose an imminent threat.
The vast majority of the bugs that hackers exploit aren’t fancy zero-days that no one has ever seen or reported. Most are vulnerabilities that have gotten out into the wild and spread via chat rooms and hacker forums on the dark web. Guessing which bugs will cause the most damage — useful in knowing which ones to patch first — is still mostly a guess. But researchers from Arizona State University have developed a machine-learning model to predict which vulnerabilities are the most likely to cause the next headline-grabbing incident. Today’s most common methods for anticipating the likelihood that a previously disclosed software vulnerability will cause major damage are imperfect at best. Take two bugs: one exploited by the WannaCry ransomware, which shut down hospitals and other institutions across the United States and Europe; and Heartbleed, a bug believed to have been discovered and exploited by the NSA. The latter was judged by the National Vulnerabilities Database’s common vulnerability scoring system to have a severity score 5-in-10, mix of likelihood of exploit and potential damage done; the former, 8.1 chances in 10. But other viruses that scored even higher had far less impact.   In a new paper, researchers at Arizona State University say their method is 266 percent better than the CVSS methodology at predicting whether a bug will be exploited. It uses web-crawling algorithms and random forest machine learning to search for discussions of new exploits on the dark web, a portion of the Internet that is only accessible via a special browser like Tor, in order to protect the anonymity of users. It extracts clues from how the hackers in the forum are discussing the bug, turning text into numerical values and storing them in a database. Once a bug hits the darker corners of the Internet, what determines the likelihood of a hacker exploiting it? The researchers found that only 2.4 percent of the bugs in the NVD are ever exploited. Among those that are, a few features stick out. If the bug had a proof-of-concept attached to it — meaning that someone actually tried it and it worked — the likelihood rose by 9 percent. If it was discussed on the dark web, the likelihood rose by 14 percent.
Subscribe
Receive daily email updates: Subscribe to the Defense One daily. Be the first to receive updates. The biggest jump factor was language. If the hackers discussed the bug in Chinese, the likelihood of a hacker using it rose to 9 percent. If it was English, the likelihood rose to 13 percent. If it was Russian, 40 percent. The method is also faster than current CVSS reporting. “In over 97% of the cases, our model makes correct predictions the day a vulnerability is disclosed. This is, on average, 13 days before any exploit is detected.” Mohammed Almukaynizi, one of the paper’s authors, told Defense One in an email.
Subscribe
Receive daily email updates: Subscribe to the Defense One daily. Be the first to receive updates. Show Comments Loading... Hide Comments
By using this service you agree not to post material that is obscene, harassing, defamatory, or
otherwise objectionable. Although Defenseone.com does not monitor comments posted to this site (and
has no obligation to), it reserves the right to delete, edit, or move any material that it deems
to be in violation of this rule.
« Previous Next » « Previous Next » © 2018 by Government Media Executive Group LLC. All rights reserved."
9a177186b0,How AI And Machine Learning Are Used To Transform The Insurance Industry,"Data has always been at the heart of the insurance industry. What has changed in our current reality to create massive disruption is the amount of data generated daily and the speed at which machines can process the info and uncover insights. We can no longer characterize the insurance industry as a sloth when it comes to innovation and technology. Artificial intelligence (AI) and machine learning are transforming the insurance industry in a number of ways. Shutterstock Artificial Intelligence and Machine Learning Transforming the Insurance Industry Although the concept and definition of artificial intelligence is still morphing as the technology matures, generally it is the idea of building machines that can think like humans. The term machine learning is used to describe the idea of teaching computers to learn in the same way humans do. It represents the leading edge of AI. Since insurance has always been data heavy, it is perfectly poised to be significantly impacted by AI. Here are just a few ways the insurance industry is being transformed. Insurance advice and customer service From the first interaction when determining what coverage is best to ongoing customer service, machines will continue to play an increasing role in customer service in the insurance industry. According to one survey, most customers don’t have an issue with interacting with a bot; 74% of consumers would be happy to get computer-generated insurance advice.
Consumers have come to expect personalized solutions, and AI makes that possible by reviewing a customer profile and providing recommendations for only insurance products that are relevant for that customer and that would be the best for them based on set criteria. Chatbots that work with messaging apps are started to be used in the industry to resolve claims and answer simple questions. Transaction and claims processing As a highly regulated industry, the insurance industry processes thousands of claims and responds to thousands of customer queries. AI is being used to improve this process and move claims through the system from initial report to communicating with the customer. In some cases, these claims do not require any human interaction at all. Those companies that have already begun to automate portions of their claims process are realizing the time savings and increased quality of service."
c0ba86cd84,Artificially intelligent 'judge' could help deliver verdicts in human rights trials - Irish Mirror Online,"A new AI system could help judges and lawyers identify cases of human rights violations before they reach court An artificial intelligence system has been developed that can correctly predict the outcome of human rights cases.
Researchers fed the system information on 584 cases relating to Articles 3, 6 and 8 of the European Convention on Human Rights , and asked it to label each case either as a ""violation"" or a ""non-violation"".
By automatically analysing the case text using a machine learning algorithm, the AI ""judge"" was able to predict the verdict of cases with 79% accuracy. It is thought that the system, developed by researchers at University College London, the University of Sheffield and the University of Pennsylvania, could one day help law firms choose which human rights cases to pursue. ""We don't see AI replacing judges or lawyers, but we think they'd find it useful for rapidly identifying patterns in cases that lead to certain outcomes,"" said Dr Nikolaos Aletras, who led the study at UCL Computer Science. ""It could also be a valuable tool for highlighting which cases are most likely to be violations of the European Convention on Human Rights."" The researchers explained that judgements by the European Court of Human Rights are often based on non-legal facts rather than just legal arguments. The most reliable factors for predicting the court's decision are often in the language used as well as the topics and circumstances mentioned in the case text. The AI system therefore looks for patterns in the text, and combines this with the factual information included in the case documents to determine the outcome. ""Previous studies have predicted outcomes based on the nature of the crime, or the policy position of each judge, so this is the first time judgements have been predicted using analysis of text prepared by the court,"" said co-author Dr Vasileios Lampos, UCL Computer Science.
Robot lawyer that overturned 160,000 parking tickets is now helping the homeless
Lampos said that, ideally, the next step would be for the researchers to test and refine their AI algorithm using applications made to the court, rather than the published judgements. ""We expect this sort of tool would improve efficiencies of high level, in demand courts, but to become a reality, we need to test it against more articles and the case data submitted to the court,"" he said.
The research paper was published today in the journal PeerJ Computer Science ."
27b21905eb,LexSet Is About To Revolutionize Interior Design With NextGen AI,"LexSight matches furnishings with what you already own Meet the Jarvis of home design, soon to be launched in 2018. The company is LexSet and it aims to revolutionize the way people shop, assemble and design for their homes. Everyone knows that decorating a new home can be challenging—so much so that marriages have broken up during the process. What makes home decorating so hard? Several factors: maybe you don’t have a design background and feel unsure what to pick; perhaps there’s not enough money to pay a designer, or the time to figure out how to assemble those shelves from IKEA. You’ve stayed up late and undergone sleep deprivation to search Houzz and Pinterest for the perfect chairs to go with your dining table, leaving you cranky and ill-tempered towards your partner in the morning, and you’re still no closer to finding the perfect chairs because the question remains: will they look good in YOUR home? If only you could actually see them in your living room! You’ve spent days dragging your partner to store after store in search of the perfect tile that expresses exactly who you are, only to discover that it doesn’t exist. Get ready for things to change with LexSet. The B2B API software is aimed to launch in Q2 of 2018 and its features will enable companies like Houzz, IKEA and Kohler to make designing, shopping, and assembly of home decor items exciting and easy. LexSet’s patented AI technology will soon provide a platform where people can quickly find and select items they want and see how they look in their actual home, and also receive guidance on how to create and put together their own designs using materials around them. LexAssemble shows you how to build and put things together HOW IT WORKS LexSight will enable spatial searches using images you take with your mobile camera that will provide material and product suggestions based on items you have in your home. Let’s say you want to find the perfect sofa  to go with your new rug: the program will use AR to show you how it looks in your actual space. You can also take a picture of your living room before you go to the store and use LexSight to see how different sofas will look there. Unlike the algorithms used by music and TV apps that simply use consumer choices to sort of “guess” what you might want, LexSet also uses algorithms that are based on principles of good design (like the Fibonacci sequence, for example) in collaboration with your personal design input to become a true design assistant.
LexGuide takes LexSet’s search feature one step further by enabling browsing of multiple retail items at once through “sets.” Now, instead of just saying “show me rugs that will go with my sofa,” you can say things like “show me a vintage kilim rug that will go with my cream-colored Scandinavian sofa and my Barcelona coffee table.”   LexTile is a dream come true for designers who are always searching for that perfect tile that only they can see in their mind’s eye. Using AR design algorithms, customers will be able to visit their favorite tile and bath store, input parameters and photos of their space, and design their own custom tile solutions.
LexSemble. LexSemble takes the guessing and frustration out of putting together furniture with an AR assistant program that guides you during assembly. It tracks your progress through your mobile device camera. To get help, all you have to do is ask it things like “I’m assembling a dresser, what’s the next step?” If half of what keeps you out of IKEA is the knowledge that when you  get home you’ll have to spend hours poring through instructions and blistering your fingers putting in wooden pegs to assemble, you can soon put those concerns in the past. LexTile lets you create your own tiles for kitchen and bath LexSet was started by by CTO Francis Bitonti together with his partner CEO Leslie Oliver Karpas. Both are veterans of design. Les is a serial entrepreneur in 3D technology who invented manufacturing processes for Turner Award-winning artist Anish Kapoor, as well as methods for 3D printing silicone for the medical devices company Metamason. Francis is the founder of Studio Bitonti and Bitonti Technology, a design and software studio focused on design and engineering solutions for additive manufacturing and generative design. LexSet makes his third company launch in the past decade. They came together with Azam Khan, Director of New Ventures at Intellectual Ventures under their new ISF (Invention Science Fund) incubator program to develop and prepare LexSet’s commercial release. Founded and led by Nathan Myrhvold, former Chief Technology Officer for Microsoft, Intellectual Ventures is a “global invention and investment business that creates, incubates, and commercializes impactful inventions.” The company works with inventors to create and launch cutting edge technology in their 87,000 square foot laboratory in Bellevue, Washington, just outside of Seattle. IV has already created 15 new businesses since its inception in 2000 which have produced over 400 jobs and raised more than $700 million in venture funding. Take Terra Power, which has redesigned nuclear power technology to utilize and manufacture power from uranium waste, normally the hazardous byproduct of nuclear reactions, in ways that are safer, cheaper and more sustainable, or Kymeta, which has developed metamaterial portable satellite antennae that allow for internet on the go all around the world, or Sunlight Payments, whose payment solutions provide NGOs and other relief organizations secure ways to deliver aid to low-income countries. They’ve also worked with inventors like Lowell Wood, “who has more patents to his name than Thomas Edison” (Edison had 2,3332, so that’s saying something). Now with their ISF Incubator program launched this year, they can further support inventors and bring new inventions to market event faster. The ISF Incubator program works in two ways: a long-term (1+ year) program where IV employees develop, foster and bring inventions to market; and their Accelerated Spin Out program which works with outside entrepreneurs to do the same during a much shorter (generally 6 months) program. LexSet is one of the first ventures to come out of the accelerated program, and as Azam Khan states, “Francis and Les are a great example of the kind of founding team we’re looking to work with, and we have the tools and resources to help them build a successful business around this technology.” Nothing is more personal than our homes and LexSet aims to take all of the hard work of measuring and guessing out of the process leaving you free to enjoy the fun, discovery and design. Look for it in 2018. The LexSet program at work"
f87a924a47,This AI Learns Your Fashion Sense and Invents Your Next Outfit - MIT Technology Review,"Hello, We noticed you're browsing in private or incognito mode. To continue reading this article, please exit incognito mode
or log in. Not an Insider? Subscribe now for unlimited access to online articles. Visitors are allowed 3 free articles per month (without a subscription), and private browsing prevents us from counting how many stories you've read. We hope you understand, and consider subscribing for unlimited online access. Click search or press enter Artificial intelligence might just spawn a whole new style trend: call it “predictive fashion.” In a paper published on the ArXiv, researchers from the University of California, San Diego, and Adobe have outlined a way for AI to not only learn a person’s style but create computer-generated images of items that match that style. The system could let retailers create personalized pieces of clothing, or could even be used to help predict broader fashion trends. The paper details two different algorithms. First, the researchers trained a convolutional neural network (CNN) to learn and classify a user’s preferences for certain items, using purchase data scraped from Amazon in six categories: shoes, tops, and pants for both women and men. This type of recommender model is common in the online retail world, usually showing up in an “Other items you might like” area at the bottom of a page. The team then used that information to train a generative adversarial network (GAN), a type of artificial intelligence that is especially proficient when it comes to generating realistic images. A GAN works by having two networks train on the same data. One of the networks generates fake images based on that data set, while the other network uses the same data to determine whether an image is real. This method lets the network improve its results. For this research, the GAN created multiple images of items for each user. GANs, which were created by Ian Goodfellow, one of MIT Technology Review’s 35 Innovators Under 35 for 2017, have been in the news recently: after a different research team trained them on real images of Hollywood stars, the networks were able to create eerily believable fake celebrity faces. The faces weren’t all perfect, though—some had blurred areas or were missing features like eyebrows. There were fewer such problems with the fashion project, largely because the images used to train the networks were all shot from the same angle on white backgrounds, which makes generating convincing images much easier—something that would be essential if they were ever to be used to sell clothing. Adding GANs to recommender systems could help online retailers figure out what customers want beyond the items that already exist. Still, researchers would need to figure out quite a few things before that could happen, including how to turn the two-dimensional computer-generated images into 3-D renderings that could be used to produce a piece of clothing. “It’s not like we are generating a sewing pattern,” says Julian McAuley, a computer scientist at UC San Diego and one of the paper’s authors. The team’s GAN also has a way to go before it can replace a stylist or even suggest a new outfit. At the moment, for a shopper who liked blue shirts, the GAN creates more blue shirts—hardly a revelation. Preference for black pants did feed into the GAN to create khakis, but the system can’t create a set of shoes that would go well with a certain pair of pants yet. “You’d have to read the tea leaves a little bit if you want to call that style or not,” McAuley says. Despite the current limitations, fashion seems ripe for an AI invasion; it’s an arena that has great data sets on customers’ interests, and there is a lot of money at stake. Amazon, for one, is already working on AI systems to provide a leg up in spotting fashion trends, and it has also done some work with GANs (see “Amazon Has Developed an AI Fashion Designer”). Alibaba, meanwhile, just debuted FashionAI, a technology that can recommend items to shoppers on the basis of what they brought into the dressing room. Costa Colbert, the chief scientist at Vue.ai, a fashion AI startup that recently revealed a method for creating fake fashion models using GANs, says that as promising as the UCSD and Adobe research appears to be, it requires so much data that it might be helpful only for the biggest names in online retail. “If all the person does is come in and click one thing, you aren’t going to be able to do much,” Colbert says. GANs will continue to make waves in online fashion, however. Colbert points out that some companies already let shoppers send in their personal measurements to get customized pieces. GANs could be a cheap, fast way to show users all the different options available—and, of course, sell more items. Time is running out to register for EmTech Digital. You don’t want to miss expert discussions on artificial intelligence.
Amazon,
artificial intelligence,
AI,
generative adversarial network,
GAN,
Ian Goodfellow,
fashion,
clothing,
predictive algorithms,
predictive fashion
Jackie Snow Associate Editor I am MIT Technology Review’s associate editor for artificial intelligence. I cover stories about where AI is currently, where it’s headed, and what’s wrong with the hype around the technology. My stories don’t have the word “Terminator”… More anywhere near them. Previously I worked for Fast Company and have been published by the New York Times, National Geographic, Wall Street Journal, and others.
Please read our commenting guidelines.
More videos
Intelligent Machines
Intelligent Machines
Intelligent Machines
Intelligent Machines
Artificial intelligence and robots are transforming how we work and live.
I rode in a bunch of autonomous cars so you don’t have to. by
Rachel Metz
A startup thinks autonomous cars will need remote humans as backup drivers. For now, it’s kind of nerve-racking. by
Rachel Metz
A grocery store in the U.K. has developed a robot to assist its maintenance workers. by
Will Knight
More from Intelligent Machines
In partnership with
Couchbase
In partnership with
Pure Storage
Sponsored by
VMware
Presented in partnership with
VMware
{! insider.display.menuOptionsLabel !}
Everything included in Insider Basic, plus the digital magazine, extensive archive, ad-free web experience, and discounts to partner offerings and MIT Technology Review events.
What's Included Unlimited 24/7 access to MIT Technology Review’s website The Download: our daily newsletter of what's important in technology and innovation Bimonthly print magazine (6 issues per year) Bimonthly digital/PDF edition Access to the magazine PDF archive—thousands of articles going back to 1899 at your fingertips Special interest publications Discount to MIT Technology Review events Special discounts to select partner offerings Ad-free web experience
*
{! insider.display.footerLabel !}
See international prices
See U.S. prices
Revert to MIT Enterprise Forum pricing
Revert to standard pricing
Follow us   The mission of MIT Technology Review is to equip its audiences with the intelligence to understand a world shaped by technology. BrowseInternationalEditions
MIT Technology Review © 2018
v.|eiπ|"
98d3e80ac8,Faster big-data analysis | MIT News,"Login
or
Subscribe Newsletter
A new MIT computer system speeds computations involving “sparse tensors,” multidimensional data arrays that consist mostly of zeroes.
Image: Christine Daniloff, MIT System for performing “tensor algebra” offers 100-fold speedups over previous software packages.
Larry Hardesty | MIT News Office
October 30, 2017
Abby AbazoriusEmail: abbya@mit.eduPhone: 617-253-2709MIT News Office 1 images for download
Access Media
Media can only be downloaded from the desktop version of this website.
We live in the age of big data, but most of that data is “sparse.” Imagine, for instance, a massive table that mapped all of Amazon’s customers against all of its products, with a “1” for each product a given customer bought and a “0” otherwise. The table would be mostly zeroes. With sparse data, analytic algorithms end up doing a lot of addition and multiplication by zero, which is wasted computation. Programmers get around this by writing custom code to avoid zero entries, but that code is complex, and it generally applies only to a narrow range of problems. At the Association for Computing Machinery’s Conference on Systems, Programming, Languages and Applications: Software for Humanity (SPLASH), researchers from MIT, the French Alternative Energies and Atomic Energy Commission, and Adobe Research recently presented a new system that automatically produces code optimized for sparse data. That code offers a 100-fold speedup over existing, non-optimized software packages. And its performance is comparable to that of meticulously hand-optimized code for specific sparse-data operations, while requiring far less work on the programmer’s part. The system is called Taco, for tensor algebra compiler. In computer-science parlance, a data structure like the Amazon table is called a “matrix,” and a tensor is just a higher-dimensional analogue of a matrix. If that Amazon table also mapped customers and products against the customers’ product ratings on the Amazon site and the words used in their product reviews, the result would be a four-dimensional tensor. “Sparse representations have been there for more than 60 years,” says Saman Amarasinghe, an MIT professor of electrical engineering and computer science (EECS) and senior author on the new paper. “But nobody knew how to generate code for them automatically. People figured out a few very specific operations — sparse matrix-vector multiply, sparse matrix-vector multiply plus a vector, sparse matrix-matrix multiply, sparse matrix-matrix-matrix multiply. The biggest contribution we make is the ability to generate code for any tensor-algebra expression when the matrices are sparse.” Joining Amarasinghe on the paper are first author Fredrik Kjolstad, an MIT graduate student in EECS; Stephen Chou, also a graduate student in EECS; David Lugato of the French Alternative Energies and Atomic Energy Commission; and Shoaib Kamil of Adobe Research. Custom kernels In recent years, the mathematical manipulation of tensors — tensor algebra — has become crucial to not only big-data analysis but machine learning, too. And it’s been a staple of scientific research since Einstein’s time. Traditionally, to handle tensor algebra, mathematics software has decomposed tensor operations into their constituent parts. So, for instance, if a computation required two tensors to be multiplied and then added to a third, the software would run its standard tensor multiplication routine on the first two tensors, store the result, and then run its standard tensor addition routine. In the age of big data, however, this approach is too time-consuming. For efficient operation on massive data sets, Kjolstad explains, every sequence of tensor operations requires its own “kernel,” or computational template. “If you do it in one kernel, you can do it all at once, and you can make it go faster, instead of having to put the output in memory and then read it back in so that you can add it to something else,” Kjolstad says. “You can just do it in the same loop.” Computer science researchers have developed kernels for some of the tensor operations most common in machine learning and big-data analytics, such as those enumerated by Amarasinghe. But the number of possible kernels is infinite: The kernel for adding together three tensors, for instance, is different from the kernel for adding together four, and the kernel for adding three three-dimensional tensors is different from the kernel for adding three four-dimensional tensors. Many tensor operations involve multiplying an entry from one tensor with one from another. If either entry is zero, so is their product, and programs for manipulating large, sparse matrices can waste a huge amount of time adding and multiplying zeroes. Hand-optimized code for sparse tensors identifies zero entries and streamlines operations involving them — either carrying forward the nonzero entries in additions or omitting multiplications entirely. This makes tensor manipulations much faster, but it requires the programmer to do a lot more work. The code for multiplying two matrices — a simple type of tensor, with only two dimensions, like a table — might, for instance, take 12 lines if the matrix is full (meaning that none of the entries can be omitted). But if the matrix is sparse, the same operation can require 100 lines of code or more, to track omissions and elisions. Enter Taco Taco adds all that extra code automatically. The programmer simply specifies the size of a tensor, whether it’s full or sparse, and the location of the file from which it should import its values. For any given operation on two tensors, Taco builds a hierarchical map that indicates, first, which paired entries from both tensors are nonzero and, then, which entries from each tensor are paired with zeroes. All pairs of zeroes it simply discards. Taco also uses an efficient indexing scheme to store only the nonzero values of sparse tensors. With zero entries included, a publicly released tensor from Amazon, which maps customer ID numbers against purchases and descriptive terms culled from reviews, takes up 107 exabytes of data, or roughly 10 times the estimated storage capacity of all of Google’s servers. But using the Taco compression scheme, it takes up only 13 gigabytes — small enough to fit on a smartphone. “Many research groups over the last two decades have attempted to solve the compiler-optimization and code-generation problem for sparse-matrix computations but made little progress,” says Saday Sadayappan, a professor of computer science and engineering at Ohio State University, who was not involved in the research. “The recent developments from Fred and Saman represent a fundamental breakthrough on this long-standing open problem.” “Their compiler now enables application developers to specify very complex sparse matrix or tensor computations in a very easy and convenient high-level notation, from which the compiler automatically generates very efficient code,” he continues. “For several sparse computations, the generated code from the compiler has been shown to be comparable or better than painstakingly developed manual implementations. This has the potential to be a real game-changer. It is one of the most exciting advances in recent times in the area of compiler optimization.” Topics: Research, School of
Engineering, Artificial intelligence, Data, Computer modeling, Computer Science and Artificial Intelligence Laboratory (CSAIL), Computer science and technology, Electrical Engineering & Computer Science (eecs), Machine learning, Software This Website is maintained by the MIT News Office, part of the Office of Communications.
MIT News Office
•
Building 11-400
Massachusetts Institute of Technology
•
Cambridge, MA 02139-4307"
e3965b8e97,A Behavioral Biometrics User Authentication Study Using Motion Data from Android Smartphones - IEEE Conference Publication,"Skip to Main Content
A not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.© Copyright 2018 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions."
37b1b2151a,Counterfeiters are using AI and machine learning to make better fakes,"It's terrifyingly easy to just make stuff up online these days, such is life in the post-truth era. But recent advancements in machine learning (ML) and artificial intelligence (AI) have compounded the issue exponentially. It's not just the news that's fake anymore but all sorts of media and consumer goods can now be knocked off thanks to AI. From audio tracks and video clips to financial transactions and counterfeit products -- even your own handwriting can be mimicked with startling levels of accuracy. But what if we could leverage the same computer systems that created these fakes to reveal them just as easily? People have been falling for trickery and hoaxes since forever. Human history is filled with false prophets, demagogues, snake-oil peddlers, grifters and con men. The problem is that these days, any two-bit huckster with a conspiracy theory and a supplement brand can hop on YouTube and instantly reach a global audience. And while the definition of ""facts"" now depends on who you're talking to, one thing that most people agreed to prior to January 20th this year is the veracity of hard evidence. Video and audio recordings have long been considered reliable sources of evidence but that's changing thanks to recent advances in AI. In July 2016, researchers at the University of Washington developed a machine learning system that not only accurately synthesizes a person's voice and vocal mannerisms but lip syncs their words onto a video. Essentially, you can fake anybody's voice and create a video of them saying whatever you want. Take the team's demo video, for example. They trained the ML system using footage of President Obama's weekly address. The recurrent neural network learned to associate various audio features with their respective mouth shapes. From there, the team generated CGI mouth movements, and with the help of 3D pose matching, ported the animated lips onto a separate video of the president. Basically, they're able to generate a photorealistic video using only its associated audio track. While the team took an outsized amount of blowback over the potential misuses of such technology, they had far more mundane uses for it in mind. ""The ability to generate high-quality video from audio could signicantly reduce the amount of bandwidth needed in video coding/transmission (which makes up a large percentage of current internet bandwidth),"" they suggested in their study, Synthesizing Obama: Learning Lip Sync from Audio. ""For hearing-impaired people, video synthesis could enable lip-reading from over-the-phone audio. And digital humans are central to entertainment applications like film special effects and games."" UW isn't the only facility looking into this sort of technology. Last year, a team from Stanford debuted the Face2Face system. Unlike UW's technology, which generates video from audio, Face2Face generates video from other video. It uses a regular webcam to capture the user's facial expressions and mouth shapes, then uses that information to deform the target YouTube video to best match the user's expressions and speech -- all in real time. AI-based audio-video transcription is a two-way street. Just as UW's system managed to generate video from an audio feed, a team from MIT's CSAIL figured out how to create audio from a silent video reel. And do it well enough to fool human audiences. ""When you run your finger across a wine glass, the sound it makes reflects how much liquid is in it,"" Andrew Owens, the paper's lead author told MIT News. ""An algorithm that simulates such sounds can reveal key information about objects' shapes and material types, as well as the force and motion of their interactions with the world."" The MIT's deep learning system was trained over the course of a few months using 1,000 videos containing some 46,000 sounds resulting from different objects being poked, struck or scraped with a drumstick. Like the UW algorithm, MIT's learned to associate different audio properties with specific onscreen actions and synthesize those sounds as the video played. When tested online against a video with authentic sound, people actually chose the fake audio over the real twice as often as the baseline algorithm. The MIT team figures that they can leverage this technology to help give robots better situational awareness. ""A robot could look at a sidewalk and instinctively know that the cement is hard and the grass is soft, and therefore know what would happen if they stepped on either of them,"" Owens said. ""Being able to predict sound is an important first step toward being able to predict the consequences of physical interactions with the world."" Research into audio synthesization isn't limited to universities; a number of major corporations are investigating the technology as well. Google, for example, has developed Wavenet, a ""deep generative model of raw audio waveforms."" Among the first iterations of computer-generated text-to-speech (TTS) systems is ""concatenative"" TTS. That's where a single person records a variety speech fragments, those are fed into a database and then reconstructed by a computer to form words and sentences. The problem is that the output sounds more like the MovieFone guy (ask your parents) than a real person. Waveform, on the other hand, is trained on waveforms of people speaking. The system samples those recordings for data points up to 16,000 times per second. To output sound, Waveform uses a model to predict what the next sound will be based on the sounds that came before it. The process is computationally expensive but does produce superior audio quality compared to the conventional TTS methods. In the future, robots could potentially forge your signature on official documents, if this AI-based handwriting mimic developed at the University College London is ever misused. Dubbed the ""My Text in Your Handwriting"" program, this system can accurately recreate a subject's handwriting with as little as a paragraph's input. The program is based on ""glyphs,"" essentially the unique traits of each person's handwriting. By measuring various aspects like horizontal and vertical spacing, connectors between letters and writing texture, the program can readily copy the style. ""Our software has lots of valuable applications. Stroke victims, for example, may be able to formulate letters without the concern of illegibility, or someone sending flowers as a gift could include a handwritten note without even going into the florist,"" Dr. Tom Haines, UCL Computer Science and lead author of the study, told UCL News. ""It could also be used in comic books where a piece of handwritten text can be translated into different languages without losing the author's original style."" And while this technology could be used to create forgeries, it can just as easily be leveraged to spot them as well. ""Forgery and forensic handwriting analysis are still almost entirely manual processes,"" Dr. Gabriel Brostow, of the UCL computer science department, said. ""But by taking the novel approach of viewing handwriting as texture-synthesis we can use our software to characterise handwriting to quantify the odds that something was forged."" Forgeries and faked products don't stop at the the bounds of the internet. Recent estimates by the Organisation for Economic Co-operation and Development put the global market for counterfeit goods at around $460 billion annually. And that's where the Entrupy authentication system comes in. ""In an ideal world, we shouldn't exist,"" Entrupy CEO Vidyuth Srinivasan lamented. ""The more we can instill trustworthiness in the market, the better it will be for commerce in general."" The company first imaged a wide variety of luxury goods and uses that database to help its customers -- generally those in secondary retail markets like vintage clothing stores or eBay sellers -- authenticate products with around 98.5 percent accuracy. Customers receive a handheld microscope and take various images of the product in question, such as the exterior, logo or interior lining. These photos are then fed into a mobile app and transmitted to the company's servers where a classification algorithm goes to work, differentiating between legitimate goods and counterfeits. If the product is real, the Entrupy will provide a certificate of authenticity. Although the company's product database is varied, there are limits to the system's current capabilities. Because it's optical, reflective or transparent items are no good, nor is anything without surface texture. Some things that it does not work on include porcelain, diamonds and glass, pure plastic and bare metal. Unlike the other AI-based systems discussed here, there's little chance of the Entrupy system being corrupted or gamed. ""We have had [counterfeiters] pose as real customers and legitimate businesses to try and buy [the system] and we're fine with it,"" Srinivasan explained. That's because the system doesn't actually tell the user which of the images they're taking are actually being used to verify the product's authenticity. ""We ask our customers to take images of different parts of the item because it's not just pure material [being used for verification]...,"" he continued. ""It's a holistic view of the different aspects of the item -- from the workmanship to the material used to the wear"" as well as a number of other contextual bits of metadata. What's more, the system is continually updated with new data, not just from the company's internal efforts of posing as secret buyers to acquire counterfeit goods, but also from the users themselves. Images taken during the authentication process -- whether the item turns out to be real or not -- are incorporated into the company's database, further improving the system's accuracy. ""In the near to medium future, I think that AI and ML will, as a counterfeiting solution, will definitely raise the bar,"" he concluded. ""It's a spy versus spy game, cat versus mouse."" Increasing our ability to spot fakes will force counterfeiters to up their game and start using better quality materials and better workmanship. That, however, will increase the production cost of these products, hopefully to a price that is no longer economically viable. ""The MO of any counterfeiter is to make something that they can sell a lot of, that can be easily produced and that does not cost a lot to produce a fake of,"" Srinivasan sid. ""Otherwise there's no profitability."" Similar measures have been adopted by Paypal, one of the the internet's top financial service providers, for cases of account fraud. ""Say my account was accessed today from San Francisco, tomorrow from NYC, and some other IP the day after,"" Hui Wang, Paypal's senior director of global risk sciences, told Engadget. This sort of activity is indicative of some kind of account takeover. ""In order to detect these kinds of fraud,"" she explained, ""we track the IP we track the machine and we track the network."" The company created an algorithm that looks at both the IP and the geolocation of that IP, then compares them to your account history to see if this matches up with previous actions. Paypal developed a proprietary technology that compares this IP location patten with other users, to see if there is a larger effect at work or there's a reasonable explanation for the movement -- i.e., perhaps you're flying through New York on business and buy a souvenir at the airport gift shop before continuing on the trip. The company's AI system also attempts to identify each previous IP, whether it's a hotel's secured ethernet connection or the public WiFi at the airport. ""[The algorithm] is retrieving tons of data from your account history and going beyond your account to look at the traffic on your network, like the other people using the same IP,"" Wang said. From this raw information, the algorithm selects specific data points and uses those to estimate whether the transaction is legitimate. Most of these actions and their subsequent decisions -- such as verifying or denying a payment -- are performed autonomously. However, if the algorithm's confidence value is too low, human investigators from the operations center will investigate the transaction manually. ""We are also in the process of ensuring that human intelligence can be fed back into the automated system,"" she continued, so that the ML system continually learns, improves and increases its accuracy. These sorts of systems, both those designed to generate fakes and those trained to uncover them, are still in their infancy. But in the coming decades, artificial intelligence and machine learning techniques will continue to improve, often in ways that we have yet to envision. There is a very real danger in technologies that can create uncannily convincing lies, hoaxes and fakes -- in front of our very eyes, no less. But, like movable type, radio and internet that came before it, AI systems like these, ones capable of generating photorealistic content, will only be as dangerous as the intentions of the people using it. And that's a terrifying thought. Andrew has lived in San Francisco since 1982 and has been writing clever things about technology since 2011. When not arguing the finer points of portable vaporizers and military defense systems with strangers on the internet, he enjoys tooling around his garden, knitting and binge watching anime. Featuring: the Best of CES 2018. The company called 'Meltdown' and 'Spectre' the most complex flaws in the past decade.
It ordered Samsung to stop selling any products that use the technology. The stream of its first matches peaked at over 360,000 viewers. A series of scandals have led many to call for YouTube to vet its videos."
a272fc39a4,How Does Quora Use Machine Learning In 2017?,"(Image: Creative Commons) How does Quora use machine learning in 2017? originally appeared on Quora: the place to gain and share knowledge, empowering people to learn from others and better understand the world. Answer by Nikhil Dandekar, Engineering Manager at Quora: Back in 2015, Xavier Amatriain, our VP of Engineering, wrote a great answer on how we use Machine Learning at Quora: How does Quora use machine learning in 2015? Since then, the usage of Machine Learning at Quora has grown a lot. We have not only gone deeper with bigger and better models for existing Machine Learning applications, but we also have expanded the set of areas where we use Machine Learning. In this answer, I'll try and paint a picture from the ground-up on how Quora is using Machine Learning in 2017.
Machine Learning use cases I'll walk through different parts of the product and talk about how we use Machine Learning in all of these parts. 1. Seeking information The main format of knowledge sharing on Quora is questions and answers. This starts with a user having a question, or an “information need”, that they want to satisfy. After a user asks a new question on Quora, we have a set of Machine Learning systems doing question understanding, i.e. extracting information from the question in a way that helps make the rest of flow easier for us. I'll describe a few of these question understanding systems. We care a lot about the quality of content, and it all starts with question quality. We have an ML system that takes a question, does question quality classification and helps us distinguish between high-quality and low-quality questions. Along with question quality, we also determine a few different question types, that help us determine how we should treat the question later on in the flow. Finally, we also do question-topic labeling, where we determine what topics the question is about. While most topic modeling applications deal with a large document text and a smallish topic ontology, we work with a short question text and more than a million potential topics to tag on the question, which makes it a much more challenging problem to solve. Topic labeling on Quora In all the question understanding models, we use features derived from the question and its context, e.g. the user who asked the question, the locale where the question was asked, etc. Another way to satisfy the user's information need is by letting them search for existing questions that answer what they are looking for. We have two main search systems: Ask Bar search, which powers the top-of-the-page search bar on the Quora homepage, and Full-text search, which is a deeper search that you can access by clicking on the “Search” option in the Ask Bar results. These search systems use different ranking algorithms that differ in terms of search speed, relevance, and the breadth and depth of the results they return. 2. Getting answers to questions The output of the question understanding systems forms an important input to the next step in the lifecycle of a question: getting answers from experts. Here too, we have Machine Learning systems that help us solve this problem better. Ask To Answer (A2A) is a feature of Quora that allows users to send requests to other users asking them to write an answer to a particular question. We frame A2A as a Machine Learning problem. We covered the details about the A2A system in this blog post: Ask To Answer as a Machine Learning Problem. Outside of A2A, the main way we match unanswered questions to expert answer-writers is via the Quora homepage feed. Ranking questions on the feed is a very important problem for us. We take into account the question properties as described above, user properties (more on that below), and a whole set of other raw and derived features as inputs to the ranking model, to generate a feed that is topical, relevant and personalized for you. E.g. here is a screenshot of my feed from a few days ago."
